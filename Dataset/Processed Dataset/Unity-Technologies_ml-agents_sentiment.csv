id,original_comment,processed_comment,source,sentiment_VADER,sentiment_textblob,sentiment_pattern,sentiment_bert,sentiment_spacy,max_voted_sentiment
2036993145,This issue was closed because it has been inactive for 14 days since being marked as stale. Please open a new issue for related bugs.,issue closed inactive day since marked stale please open new issue related,issue,negative,negative,neutral,neutral,negative,negative
2035832891,This issue is stale because it has been open for 30 days with no activity.,issue stale open day activity,issue,negative,negative,negative,negative,negative,negative
2032122406,"> @alex-mccarthy-unity I'm facing the same issue with v8.0.2
> 
> did you find or know of any workaround for this? thanks!

Unfortunately I don't know of any solutions :( I've found that retrying failed tasks has a good chance of the retry succeeding, but I know that that's an unsatisfying workaround.",facing issue find know thanks unfortunately know found good chance retry succeeding know unsatisfying,issue,positive,positive,positive,positive,positive,positive
2029009030,"[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/Unity-Technologies/ml-agents?pullRequest=4670) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/Unity-Technologies/ml-agents?pullRequest=4670) before we can accept your contribution.<br/><hr/>**Ervin Teng** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/Unity-Technologies/ml-agents?pullRequest=4670) it.</sub>",assistant check thank submission really appreciate like many open source ask sign contributor license agreement accept teng user need account able sign already account please add address used commit account already status still pending let u recheck,issue,positive,positive,positive,positive,positive,positive
2028692901,"[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/Unity-Technologies/ml-agents?pullRequest=6094) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/Unity-Technologies/ml-agents?pullRequest=6094) before we can accept your contribution.<br/><hr/>**左云龙** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/Unity-Technologies/ml-agents?pullRequest=6094) it.</sub>",assistant check thank submission really appreciate like many open source ask sign contributor license agreement accept user need account able sign already account please add address used commit account already status still pending let u recheck,issue,positive,positive,positive,positive,positive,positive
2027661631,Dear! This is the same error I am getting on my side. I am also trying to setup mlagents in unity. If you find some remedy to it. Please share with me as well.,dear error getting side also trying setup unity find remedy please share well,issue,positive,neutral,neutral,neutral,neutral,neutral
2021304567,"@alex-mccarthy-unity I'm facing the same issue with v8.0.2

did you find or know of any workaround for this? thanks!",facing issue find know thanks,issue,negative,positive,positive,positive,positive,positive
2016746599,"I am having this issue, any one got it fixed?",issue one got fixed,issue,negative,positive,neutral,neutral,positive,positive
2015564113,"I was able to solve it. The problem lies on the python version. When creating an environment say with conda, ensure your python version is compatible with pytorch. You can watch this video that helped and ensure to download thesame python version as he did when creating his conda environment [Youtube Video on ML agents Unity setup](https://www.youtube.com/watch?v=Dj-BsYtANE0&t=1s ).",able solve problem python version environment say ensure python version compatible watch video ensure python version environment video unity setup,issue,positive,positive,positive,positive,positive,positive
2014266732,"I figured it out. 

The error message is slightly unclear, I kept thinking that it is pointing me towards the python side of things or with my .yaml file. However, this is not the case. 

This error occurs when you have not set the same ""Behaviour Name"" value in your Unity Agent's Behavior Parameters component as the one in your yaml file. Make sure to check those two vales are the same.",figured error message slightly unclear kept thinking pointing towards python side file however case error set behaviour name value unity agent behavior component one file make sure check two,issue,negative,positive,positive,positive,positive,positive
1997317960,"Unfortunately, we still see the same failure after this change :( https://github.com/Unity-Technologies/ml-agents/actions/runs/8278579680/job/22651228262",unfortunately still see failure change,issue,negative,negative,negative,negative,negative,negative
1996675822,"Closing: I think https://github.com/Unity-Technologies/ml-agents/pull/5486 makes this obsolete, as it moved dodgeball code into a separate repo ( https://github.com/Unity-Technologies/ml-agents-dodgeball-env ) .",think obsolete code separate,issue,negative,neutral,neutral,neutral,neutral,neutral
1996661368,"Closing: the deprecation fix was merged in as part of https://github.com/Unity-Technologies/ml-agents/commit/f9b7399338ec164ac5447f634b9a9af04538467e .

Thanks for your contributions!",deprecation fix part thanks,issue,negative,positive,positive,positive,positive,positive
1996654944,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`.

If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",wo notify release get touch new version available rather skip next major minor version let know ignore major version ignore minor version change mind resolve,issue,negative,positive,neutral,neutral,positive,positive
1995926578,"> > @alex-mccarthy-unity Please rebase w/main (do not merge main in) and then we should be good.
> 
> Rebased onto `develop` and pushed.
> 
> But GitHub is still claiming there are conflicts to resolve, while showing ""Recent push is still being processed, and will show up in a bit"", which I've never seen before: <img alt=""Screenshot 2024-03-13 at 22 07 42"" width=""619"" src=""https://private-user-images.githubusercontent.com/122620276/312623014-d535bb4f-ecd9-4ed9-b47f-775f282a9f8f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTAzNjcwMTgsIm5iZiI6MTcxMDM2NjcxOCwicGF0aCI6Ii8xMjI2MjAyNzYvMzEyNjIzMDE0LWQ1MzViYjRmLWVjZDktNGVkOS1iNDdmLTc3NWYyODJhOWY4Zi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMzEzJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDMxM1QyMTUxNThaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1mNzYwYzNiZDE1NDc3NzY0MjE3NjY0YjI5Y2E4YjIwZGIyOGYyMDk4ODdmOWQ0MGMxMWE3MDE3YzJjNGNmZjVmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.cF5GUcmHsurH-gwlD-9rmI_P-rlAu8sCg8R8nqE3B6Y"">
> 
> I hope this won't last for many hours, like the report in https://github.com/orgs/community/discussions/78775 . In the mean time, I'll likely check in on this tomorrow.

I still see ""processing updates"" an hour later, so I re-sent this as https://github.com/Unity-Technologies/ml-agents/pull/6082",please rebase merge main good onto develop still resolve showing recent push still show bit never seen hope wo last many like report mean time likely check tomorrow still see hour later,issue,positive,positive,positive,positive,positive,positive
1995823828,"> @alex-mccarthy-unity Please rebase w/main (do not merge main in) and then we should be good.

Rebased onto `develop` and pushed.

But GitHub is still claiming there are conflicts to resolve, while showing ""Recent push is still being processed, and will show up in a bit"", which I've never seen before:
<img width=""619"" alt=""Screenshot 2024-03-13 at 22 07 42"" src=""https://github.com/Unity-Technologies/ml-agents/assets/122620276/d535bb4f-ecd9-4ed9-b47f-775f282a9f8f"">

I hope this won't last for many hours, like the report in https://github.com/orgs/community/discussions/78775 . In the mean time, I'll likely check in on this tomorrow.",please rebase merge main good onto develop still resolve showing recent push still show bit never seen hope wo last many like report mean time likely check tomorrow,issue,positive,positive,positive,positive,positive,positive
1995079643,@alex-mccarthy-unity Please rebase w/main (do not merge main in) and then we should be good.,please rebase merge main good,issue,positive,positive,positive,positive,positive,positive
1993089269,"I change the version of ML-Agents package,the it works. Firstly, the ml-agents-develop and ml-agents -release_21 both don't work. The ml-agents-release_15 works!",change version package work firstly work work,issue,negative,positive,positive,positive,positive,positive
1988994040,"Installing mlagents from pip seems to be broken because of the numpy dependancy issues. I had faced the same problem earlier. Once you have updated the setup.py files, instead of installing it using pip, you have to install it from the cloned mlagents repo.  Please read the section ""Install the mlagents Python package"" from the documentation [https://unity-technologies.github.io/ml-agents/Installation/](Installation).  

If its still not clear, check the pdf. @ICOnce has documented it well step wise. [https://github.com/Unity-Technologies/ml-agents/issues/6047#issuecomment-1943131006](url)



> Hi Guys, having an issue with this as well. Been trying to install it for class. We're required to use Python 3.10.11, and I changed the setup.py to use numpy 1.23.1 (version teacher said we gotta use), however, no matter what I do, when I run python -m pip install mlagents==1.0.0 in the console, it always installs numpy 1.21.2, I'll post the setup files and command below:
> 
> If you guys have any tips for fixing this, please tell me, been at it for 3 weeks, but I can't get my head around what's happening, I've reinstalled countless times, and deleted PIP Cache to prevent this (since before it used the cached numpy 1.21 as well)
> 
> Thank you in advance for your help

",pip broken faced problem instead pip install please read section install python package documentation still clear check well step wise hi issue well trying install class use python use version teacher said got ta use however matter run python pip install console always post setup command fixing please tell ca get head around happening countless time pip cache prevent since used well thank advance help,issue,positive,positive,neutral,neutral,positive,positive
1988409538,"Hi Guys, having an issue with this as well. Been trying to install it for class. We're required to use Python 3.10.11, and I changed the setup.py to use numpy 1.23.1 (version teacher said we gotta use), however, no matter what I do, when I run python -m pip install mlagents==1.0.0 in the console, it always installs numpy 1.21.2, I'll post the setup files and command below:

### Console:
(mlagents) D:\UnityHiren\MLAgents\ml-agents>python -m pip install mlagents==1.0.0
Collecting mlagents==1.0.0
  Downloading mlagents-1.0.0-py3-none-any.whl (171 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 171.3/171.3 kB 3.4 MB/s eta 0:00:00
Requirement already satisfied: tensorboard>=2.14 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents==1.0.0) (2.16.2)
Requirement already satisfied: onnx==1.12.0 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents==1.0.0) (1.12.0)
Requirement already satisfied: numpy<2.0,>=1.13.3 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents==1.0.0) (1.23.1)
Requirement already satisfied: attrs>=19.3.0 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents==1.0.0) (23.2.0)
Requirement already satisfied: protobuf<3.20,>=3.6 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents==1.0.0) (3.19.6)
Requirement already satisfied: torch>=1.13.1 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents==1.0.0) (2.2.1)
Requirement already satisfied: pyyaml>=3.1.0 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents==1.0.0) (6.0.1)
Requirement already satisfied: Pillow>=4.2.1 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents==1.0.0) (10.2.0)
Requirement already satisfied: six>=1.16 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents==1.0.0) (1.16.0)
Requirement already satisfied: grpcio<=1.48.2,>=1.11.0 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents==1.0.0) (1.48.2)
Collecting mlagents-envs==1.0.0
  Downloading mlagents_envs-1.0.0-py3-none-any.whl (89 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.5/89.5 kB ? eta 0:00:00
Requirement already satisfied: huggingface-hub>=0.14 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents==1.0.0) (0.21.4)
Requirement already satisfied: h5py>=2.9.0 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents==1.0.0) (3.10.0)
Requirement already satisfied: cattrs<1.7,>=1.1.0 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents==1.0.0) (1.5.0)
Requirement already satisfied: pypiwin32==223 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents==1.0.0) (223)
Requirement already satisfied: gym>=0.21.0 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents-envs==1.0.0->mlagents==1.0.0) (0.26.2)
Collecting numpy<2.0,>=1.13.3
  Downloading numpy-1.21.2.zip (10.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.3/10.3 MB 8.6 MB/s eta 0:00:00
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Requirement already satisfied: filelock>=3.4.0 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents-envs==1.0.0->mlagents==1.0.0) (3.13.1)
Requirement already satisfied: pettingzoo==1.15.0 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents-envs==1.0.0->mlagents==1.0.0) (1.15.0)
Requirement already satisfied: cloudpickle in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from mlagents-envs==1.0.0->mlagents==1.0.0) (3.0.0)
Requirement already satisfied: typing-extensions>=3.6.2.1 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from onnx==1.12.0->mlagents==1.0.0) (4.10.0)
Requirement already satisfied: pywin32>=223 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from pypiwin32==223->mlagents==1.0.0) (306)
Requirement already satisfied: packaging>=20.9 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from huggingface-hub>=0.14->mlagents==1.0.0) (24.0)
Requirement already satisfied: tqdm>=4.42.1 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from huggingface-hub>=0.14->mlagents==1.0.0) (4.66.2)
Requirement already satisfied: requests in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from huggingface-hub>=0.14->mlagents==1.0.0) (2.31.0)
Requirement already satisfied: fsspec>=2023.5.0 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from huggingface-hub>=0.14->mlagents==1.0.0) (2024.2.0)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from tensorboard>=2.14->mlagents==1.0.0) (0.7.2)
Requirement already satisfied: absl-py>=0.4 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from tensorboard>=2.14->mlagents==1.0.0) (2.1.0)
Requirement already satisfied: markdown>=2.6.8 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from tensorboard>=2.14->mlagents==1.0.0) (3.5.2)
Requirement already satisfied: setuptools>=41.0.0 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from tensorboard>=2.14->mlagents==1.0.0) (65.5.0)
Requirement already satisfied: werkzeug>=1.0.1 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from tensorboard>=2.14->mlagents==1.0.0) (3.0.1)
Requirement already satisfied: sympy in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from torch>=1.13.1->mlagents==1.0.0) (1.12)
Requirement already satisfied: networkx in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from torch>=1.13.1->mlagents==1.0.0) (3.2.1)
Requirement already satisfied: jinja2 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from torch>=1.13.1->mlagents==1.0.0) (3.1.3)
Requirement already satisfied: gym-notices>=0.0.4 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from gym>=0.21.0->mlagents-envs==1.0.0->mlagents==1.0.0) (0.0.8)
Requirement already satisfied: colorama in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.14->mlagents==1.0.0) (0.4.6)
Requirement already satisfied: MarkupSafe>=2.1.1 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from werkzeug>=1.0.1->tensorboard>=2.14->mlagents==1.0.0) (2.1.5)
Requirement already satisfied: certifi>=2017.4.17 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from requests->huggingface-hub>=0.14->mlagents==1.0.0) (2024.2.2)
Requirement already satisfied: idna<4,>=2.5 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from requests->huggingface-hub>=0.14->mlagents==1.0.0) (3.6)
Requirement already satisfied: charset-normalizer<4,>=2 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from requests->huggingface-hub>=0.14->mlagents==1.0.0) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from requests->huggingface-hub>=0.14->mlagents==1.0.0) (2.2.1)
Requirement already satisfied: mpmath>=0.19 in d:\unityhiren\mlagents\ml-agents\mlagents\lib\site-packages (from sympy->torch>=1.13.1->mlagents==1.0.0) (1.3.0)
Building wheels for collected packages: numpy
  Building wheel for numpy (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Building wheel for numpy (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [271 lines of output]
      setup.py:63: RuntimeWarning: NumPy 1.21.2 may not yet support Python 3.10.
        warnings.warn(
      Running from numpy source directory.
      Processing numpy/random\_bounded_integers.pxd.in
      Processing numpy/random\bit_generator.pyx
      Processing numpy/random\mtrand.pyx
      Processing numpy/random\_bounded_integers.pyx.in
      Processing numpy/random\_common.pyx
      Processing numpy/random\_generator.pyx
      Processing numpy/random\_mt19937.pyx
      Processing numpy/random\_pcg64.pyx
      Processing numpy/random\_philox.pyx
      Processing numpy/random\_sfc64.pyx
      Cythonizing sources
      blas_opt_info:
      blas_mkl_info:
      No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils
      customize MSVCCompiler
        libraries mkl_rt not found in ['D:\\UnityHiren\\MLAgents\\ml-agents\\mlagents\\lib', 'C:\\']
        NOT AVAILABLE

      blis_info:
        libraries blis not found in ['D:\\UnityHiren\\MLAgents\\ml-agents\\mlagents\\lib', 'C:\\']
        NOT AVAILABLE

      openblas_info:
        libraries openblas not found in ['D:\\UnityHiren\\MLAgents\\ml-agents\\mlagents\\lib', 'C:\\']
      get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'
      customize GnuFCompiler
      Could not locate executable g77
      Could not locate executable f77
      customize IntelVisualFCompiler
      Could not locate executable ifort
      Could not locate executable ifl
      customize AbsoftFCompiler
      Could not locate executable f90
      customize CompaqVisualFCompiler
      Could not locate executable DF
      customize IntelItaniumVisualFCompiler
      Could not locate executable efl
      customize Gnu95FCompiler
      Could not locate executable gfortran
      Could not locate executable f95
      customize G95FCompiler
      Could not locate executable g95
      customize IntelEM64VisualFCompiler
      customize IntelEM64TFCompiler
      Could not locate executable efort
      Could not locate executable efc
      customize PGroupFlangCompiler
      Could not locate executable flang
      don't know how to compile Fortran code on platform 'nt'
        NOT AVAILABLE

      accelerate_info:
        NOT AVAILABLE

      atlas_3_10_blas_threads_info:
      Setting PTATLAS=ATLAS
        libraries tatlas not found in ['D:\\UnityHiren\\MLAgents\\ml-agents\\mlagents\\lib', 'C:\\']
        NOT AVAILABLE

      atlas_3_10_blas_info:
        libraries satlas not found in ['D:\\UnityHiren\\MLAgents\\ml-agents\\mlagents\\lib', 'C:\\']
        NOT AVAILABLE

      atlas_blas_threads_info:
      Setting PTATLAS=ATLAS
        libraries ptf77blas,ptcblas,atlas not found in ['D:\\UnityHiren\\MLAgents\\ml-agents\\mlagents\\lib', 'C:\\']
        NOT AVAILABLE

      atlas_blas_info:
        libraries f77blas,cblas,atlas not found in ['D:\\UnityHiren\\MLAgents\\ml-agents\\mlagents\\lib', 'C:\\']
        NOT AVAILABLE

      D:\Temp\TEMP\Temp\pip-install-sm1w3kd6\numpy_b431fd2b9bcc42cd9217589f1c5d56db\numpy\distutils\system_info.py:2026: UserWarning:
          Optimized (vendor) Blas libraries are not found.
          Falls back to netlib Blas library which has worse performance.
          A better performance should be easily gained by switching
          Blas library.
        if self._calc_info(blas):
      blas_info:
        libraries blas not found in ['D:\\UnityHiren\\MLAgents\\ml-agents\\mlagents\\lib', 'C:\\']
        NOT AVAILABLE

      D:\Temp\TEMP\Temp\pip-install-sm1w3kd6\numpy_b431fd2b9bcc42cd9217589f1c5d56db\numpy\distutils\system_info.py:2026: UserWarning:
          Blas (http://www.netlib.org/blas/) libraries not found.
          Directories to search for the libraries can be specified in the
          numpy/distutils/site.cfg file (section [blas]) or by setting
          the BLAS environment variable.
        if self._calc_info(blas):
      blas_src_info:
        NOT AVAILABLE

      D:\Temp\TEMP\Temp\pip-install-sm1w3kd6\numpy_b431fd2b9bcc42cd9217589f1c5d56db\numpy\distutils\system_info.py:2026: UserWarning:
          Blas (http://www.netlib.org/blas/) sources not found.
          Directories to search for the sources can be specified in the
          numpy/distutils/site.cfg file (section [blas_src]) or by setting
          the BLAS_SRC environment variable.
        if self._calc_info(blas):
        NOT AVAILABLE

      non-existing path in 'numpy\\distutils': 'site.cfg'
      lapack_opt_info:
      lapack_mkl_info:
        libraries mkl_rt not found in ['D:\\UnityHiren\\MLAgents\\ml-agents\\mlagents\\lib', 'C:\\']
        NOT AVAILABLE

      openblas_lapack_info:
        libraries openblas not found in ['D:\\UnityHiren\\MLAgents\\ml-agents\\mlagents\\lib', 'C:\\']
        NOT AVAILABLE

      openblas_clapack_info:
        libraries openblas,lapack not found in ['D:\\UnityHiren\\MLAgents\\ml-agents\\mlagents\\lib', 'C:\\']
        NOT AVAILABLE

      flame_info:
        libraries flame not found in ['D:\\UnityHiren\\MLAgents\\ml-agents\\mlagents\\lib', 'C:\\']
        NOT AVAILABLE

      atlas_3_10_threads_info:
      Setting PTATLAS=ATLAS
        libraries lapack_atlas not found in D:\UnityHiren\MLAgents\ml-agents\mlagents\lib
        libraries tatlas,tatlas not found in D:\UnityHiren\MLAgents\ml-agents\mlagents\lib
        libraries lapack_atlas not found in C:\
        libraries tatlas,tatlas not found in C:\
      <class 'numpy.distutils.system_info.atlas_3_10_threads_info'>
        NOT AVAILABLE

      atlas_3_10_info:
        libraries lapack_atlas not found in D:\UnityHiren\MLAgents\ml-agents\mlagents\lib
        libraries satlas,satlas not found in D:\UnityHiren\MLAgents\ml-agents\mlagents\lib
        libraries lapack_atlas not found in C:\
        libraries satlas,satlas not found in C:\
      <class 'numpy.distutils.system_info.atlas_3_10_info'>
        NOT AVAILABLE

      atlas_threads_info:
      Setting PTATLAS=ATLAS
        libraries lapack_atlas not found in D:\UnityHiren\MLAgents\ml-agents\mlagents\lib
        libraries ptf77blas,ptcblas,atlas not found in D:\UnityHiren\MLAgents\ml-agents\mlagents\lib
        libraries lapack_atlas not found in C:\
        libraries ptf77blas,ptcblas,atlas not found in C:\
      <class 'numpy.distutils.system_info.atlas_threads_info'>
        NOT AVAILABLE

      atlas_info:
        libraries lapack_atlas not found in D:\UnityHiren\MLAgents\ml-agents\mlagents\lib
        libraries f77blas,cblas,atlas not found in D:\UnityHiren\MLAgents\ml-agents\mlagents\lib
        libraries lapack_atlas not found in C:\
        libraries f77blas,cblas,atlas not found in C:\
      <class 'numpy.distutils.system_info.atlas_info'>
        NOT AVAILABLE

      lapack_info:
        libraries lapack not found in ['D:\\UnityHiren\\MLAgents\\ml-agents\\mlagents\\lib', 'C:\\']
        NOT AVAILABLE

      D:\Temp\TEMP\Temp\pip-install-sm1w3kd6\numpy_b431fd2b9bcc42cd9217589f1c5d56db\numpy\distutils\system_info.py:1858: UserWarning:
          Lapack (http://www.netlib.org/lapack/) libraries not found.
          Directories to search for the libraries can be specified in the
          numpy/distutils/site.cfg file (section [lapack]) or by setting
          the LAPACK environment variable.
        return getattr(self, '_calc_info_{}'.format(name))()
      lapack_src_info:
        NOT AVAILABLE

      D:\Temp\TEMP\Temp\pip-install-sm1w3kd6\numpy_b431fd2b9bcc42cd9217589f1c5d56db\numpy\distutils\system_info.py:1858: UserWarning:
          Lapack (http://www.netlib.org/lapack/) sources not found.
          Directories to search for the sources can be specified in the
          numpy/distutils/site.cfg file (section [lapack_src]) or by setting
          the LAPACK_SRC environment variable.
        return getattr(self, '_calc_info_{}'.format(name))()
        NOT AVAILABLE

      numpy_linalg_lapack_lite:
        FOUND:
          language = c
          define_macros = [('HAVE_BLAS_ILP64', None), ('BLAS_SYMBOL_SUFFIX', '64_')]

      Warning: attempted relative import with no known parent package
      D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\_distutils\dist.py:275: UserWarning: Unknown distribution option: 'define_macros'
        warnings.warn(msg)
      running bdist_wheel
      running build
      running config_cc
      unifing config_cc, config, build_clib, build_ext, build commands --compiler options
      running config_fc
      unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options
      running build_src
      build_src
      building py_modules sources
      creating build
      creating build\src.win-amd64-3.10
      creating build\src.win-amd64-3.10\numpy
      creating build\src.win-amd64-3.10\numpy\distutils
      building library ""npymath"" sources
      Traceback (most recent call last):
        File ""D:\UnityHiren\MLAgents\ml-agents\mlagents\lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 353, in <module>
          main()
        File ""D:\UnityHiren\MLAgents\ml-agents\mlagents\lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 335, in main
          json_out['return_val'] = hook(**hook_input['kwargs'])
        File ""D:\UnityHiren\MLAgents\ml-agents\mlagents\lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 251, in build_wheel
          return _build_backend().build_wheel(wheel_directory, config_settings,
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\build_meta.py"", line 211, in build_wheel
          return self._build_with_temp_dir(['bdist_wheel'], '.whl',
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\build_meta.py"", line 197, in _build_with_temp_dir
          self.run_setup()
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\build_meta.py"", line 248, in run_setup
          super(_BuildMetaLegacyBackend,
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\build_meta.py"", line 142, in run_setup
          exec(compile(code, __file__, 'exec'), locals())
        File ""setup.py"", line 448, in <module>
          setup_package()
        File ""setup.py"", line 440, in setup_package
          setup(**metadata)
        File ""D:\Temp\TEMP\Temp\pip-install-sm1w3kd6\numpy_b431fd2b9bcc42cd9217589f1c5d56db\numpy\distutils\core.py"", line 169, in setup
          return old_setup(**new_attr)
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\__init__.py"", line 165, in setup
          return distutils.core.setup(**attrs)
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\_distutils\core.py"", line 148, in setup
          dist.run_commands()
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\_distutils\dist.py"", line 967, in run_commands
          self.run_command(cmd)
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\_distutils\dist.py"", line 986, in run_command
          cmd_obj.run()
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\wheel\bdist_wheel.py"", line 299, in run
          self.run_command('build')
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\_distutils\cmd.py"", line 313, in run_command
          self.distribution.run_command(command)
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\_distutils\dist.py"", line 986, in run_command
          cmd_obj.run()
        File ""D:\Temp\TEMP\Temp\pip-install-sm1w3kd6\numpy_b431fd2b9bcc42cd9217589f1c5d56db\numpy\distutils\command\build.py"", line 61, in run
          old_build.run(self)
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\_distutils\command\build.py"", line 135, in run
          self.run_command(cmd_name)
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\_distutils\cmd.py"", line 313, in run_command
          self.distribution.run_command(command)
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\_distutils\dist.py"", line 986, in run_command
          cmd_obj.run()
        File ""D:\Temp\TEMP\Temp\pip-install-sm1w3kd6\numpy_b431fd2b9bcc42cd9217589f1c5d56db\numpy\distutils\command\build_src.py"", line 144, in run
          self.build_sources()
        File ""D:\Temp\TEMP\Temp\pip-install-sm1w3kd6\numpy_b431fd2b9bcc42cd9217589f1c5d56db\numpy\distutils\command\build_src.py"", line 155, in build_sources
          self.build_library_sources(*libname_info)
        File ""D:\Temp\TEMP\Temp\pip-install-sm1w3kd6\numpy_b431fd2b9bcc42cd9217589f1c5d56db\numpy\distutils\command\build_src.py"", line 288, in build_library_sources
          sources = self.generate_sources(sources, (lib_name, build_info))
        File ""D:\Temp\TEMP\Temp\pip-install-sm1w3kd6\numpy_b431fd2b9bcc42cd9217589f1c5d56db\numpy\distutils\command\build_src.py"", line 378, in generate_sources
          source = func(extension, build_dir)
        File ""numpy\core\setup.py"", line 661, in get_mathlib_info
          st = config_cmd.try_link('int main(void) { return 0;}')
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\_distutils\command\config.py"", line 243, in try_link
          self._link(body, headers, include_dirs,
        File ""D:\Temp\TEMP\Temp\pip-install-sm1w3kd6\numpy_b431fd2b9bcc42cd9217589f1c5d56db\numpy\distutils\command\config.py"", line 163, in _link
          return self._wrap_method(old_config._link, lang,
        File ""D:\Temp\TEMP\Temp\pip-install-sm1w3kd6\numpy_b431fd2b9bcc42cd9217589f1c5d56db\numpy\distutils\command\config.py"", line 98, in _wrap_method
          ret = mth(*((self,)+args))
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\_distutils\command\config.py"", line 137, in _link
          (src, obj) = self._compile(body, headers, include_dirs, lang)
        File ""D:\Temp\TEMP\Temp\pip-install-sm1w3kd6\numpy_b431fd2b9bcc42cd9217589f1c5d56db\numpy\distutils\command\config.py"", line 106, in _compile
          src, obj = self._wrap_method(old_config._compile, lang,
        File ""D:\Temp\TEMP\Temp\pip-install-sm1w3kd6\numpy_b431fd2b9bcc42cd9217589f1c5d56db\numpy\distutils\command\config.py"", line 98, in _wrap_method
          ret = mth(*((self,)+args))
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\_distutils\command\config.py"", line 132, in _compile
          self.compiler.compile([src], include_dirs=include_dirs)
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\_distutils\_msvccompiler.py"", line 401, in compile
          self.spawn(args)
        File ""D:\Temp\TEMP\Temp\pip-build-env-j0x5tnf_\overlay\Lib\site-packages\setuptools\_distutils\_msvccompiler.py"", line 505, in spawn
          return super().spawn(cmd, env=env)
        File ""D:\Temp\TEMP\Temp\pip-install-sm1w3kd6\numpy_b431fd2b9bcc42cd9217589f1c5d56db\numpy\distutils\ccompiler.py"", line 88, in <lambda>
          m = lambda self, *args, **kw: func(self, *args, **kw)
      TypeError: CCompiler_spawn() got an unexpected keyword argument 'env'
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for numpy
Failed to build numpy
ERROR: Could not build wheels for numpy, which is required to install pyproject.toml-based projects

### ml-agents setup.py:

import os
import sys

from setuptools import setup, find_packages
from setuptools.command.install import install
from mlagents.plugins import ML_AGENTS_STATS_WRITER, ML_AGENTS_TRAINER_TYPE
import mlagents.trainers

VERSION = mlagents.trainers.__version__
EXPECTED_TAG = mlagents.trainers.__release_tag__

here = os.path.abspath(os.path.dirname(__file__))


class VerifyVersionCommand(install):
    """"""
    Custom command to verify that the git tag is the expected one for the release.
    Originally based on https://circleci.com/blog/continuously-deploying-python-packages-to-pypi-with-circleci/
    This differs slightly because our tags and versions are different.
    """"""

    description = ""verify that the git tag matches our version""

    def run(self):
        tag = os.getenv(""GITHUB_REF"", ""NO GITHUB TAG!"").replace(""refs/tags/"", """")

        if tag != EXPECTED_TAG:
            info = ""Git tag: {} does not match the expected tag of this app: {}"".format(
                tag, EXPECTED_TAG
            )
            sys.exit(info)


# Get the long description from the README file
with open(os.path.join(here, ""README.md""), encoding=""utf-8"") as f:
    long_description = f.read()

setup(
    name=""mlagents"",
    version=VERSION,
    description=""Unity Machine Learning Agents"",
    long_description=long_description,
    long_description_content_type=""text/markdown"",
    url=""https://github.com/Unity-Technologies/ml-agents"",
    author=""Unity Technologies"",
    author_email=""ML-Agents@unity3d.com"",
    classifiers=[
        ""Intended Audience :: Developers"",
        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",
        ""License :: OSI Approved :: Apache Software License"",
        ""Programming Language :: Python :: 3.10"",
    ],
    # find_namespace_packages will recurse through the directories and find all the packages
    packages=find_packages(exclude=[""*.tests"", ""*.tests.*"", ""tests.*"", ""tests""]),
    zip_safe=False,
    install_requires=[
        # Test-only dependencies should go in test_requirements.txt, not here.
        ""grpcio>=1.11.0,<=1.48.2"",
        ""h5py>=2.9.0"",
        ""mlagents_envs=={VERSION}"",
        ""numpy==1.23.1"",
        ""Pillow>=4.2.1"",
        ""protobuf>=3.6,<3.20"",
        ""pyyaml>=3.1.0"",
        ""torch>=2.1.1"",
        ""tensorboard>=2.14"",
        # adding six explicit dependency since tensorboard needs it but doesn't declare it as a dep
        ""six>=1.16"",
        # cattrs 1.1.0 dropped support for python 3.6, but 1.0.0 doesn't work for python 3.9
        # Since there's no version that supports both, we have to draw the line somewhere.
        ""cattrs>=1.1.0,<1.7; python_version>='3.8'"",
        ""attrs>=19.3.0"",
        ""huggingface_hub>=0.14"",
        'pypiwin32==223;platform_system==""Windows""',
        ""onnx==1.12.0"",
    ],
    python_requires="">=3.10.1,<=3.10.12"",
    entry_points={
        ""console_scripts"": [
            ""mlagents-learn=mlagents.trainers.learn:main"",
            ""mlagents-run-experiment=mlagents.trainers.run_experiment:main"",
            ""mlagents-push-to-hf=mlagents.utils.push_to_hf:main"",
            ""mlagents-load-from-hf=mlagents.utils.load_from_hf:main"",
        ],
        # Plugins - each plugin type should have an entry here for the default behavior
        ML_AGENTS_STATS_WRITER: [
            ""default=mlagents.plugins.stats_writer:get_default_stats_writers""
        ],
        ML_AGENTS_TRAINER_TYPE: [
            ""default=mlagents.plugins.trainer_type:get_default_trainer_types""
        ],
    },
    # TODO: Remove this once mypy stops having spurious setuptools issues.
    cmdclass={""verify"": VerifyVersionCommand},  # type: ignore
)

### ml-agents-envs setup.py:
import os
import sys
from setuptools import setup, find_packages
from setuptools.command.install import install
import mlagents_envs

VERSION = mlagents_envs.__version__
EXPECTED_TAG = mlagents_envs.__release_tag__

here = os.path.abspath(os.path.dirname(__file__))


class VerifyVersionCommand(install):
    """"""
    Custom command to verify that the git tag is the expected one for the release.
    Originally based on https://circleci.com/blog/continuously-deploying-python-packages-to-pypi-with-circleci/
    This differs slightly because our tags and versions are different.
    """"""

    description = ""verify that the git tag matches our version""

    def run(self):
        tag = os.getenv(""GITHUB_REF"", ""NO GITHUB TAG!"").replace(""refs/tags/"", """")

        if tag != EXPECTED_TAG:
            info = ""Git tag: {} does not match the expected tag of this app: {}"".format(
                tag, EXPECTED_TAG
            )
            sys.exit(info)


# Get the long description from the README file
with open(os.path.join(here, ""README.md""), encoding=""utf-8"") as f:
    long_description = f.read()

setup(
    name=""mlagents_envs"",
    version=VERSION,
    description=""Unity Machine Learning Agents Interface"",
    long_description=long_description,
    long_description_content_type=""text/markdown"",
    url=""https://github.com/Unity-Technologies/ml-agents"",
    author=""Unity Technologies"",
    author_email=""ML-Agents@unity3d.com"",
    classifiers=[
        ""Intended Audience :: Developers"",
        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",
        ""License :: OSI Approved :: Apache Software License"",
        ""Programming Language :: Python :: 3.10"",
    ],
    packages=find_packages(
        exclude=[""*.tests"", ""*.tests.*"", ""tests.*"", ""tests"", ""colabs"", ""*.ipynb""]
    ),
    zip_safe=False,
    install_requires=[
        ""cloudpickle"",
        ""grpcio>=1.11.0,<=1.48.2"",
        ""Pillow>=4.2.1"",
        ""protobuf>=3.6,<3.20"",
        ""pyyaml>=3.1.0"",
        ""gym>=0.21.0"",
        ""pettingzoo==1.15.0"",
        ""numpy==1.23.1"",
        ""filelock>=3.4.0"",
    ],
    python_requires="">=3.10.1,<=3.10.12"",
    # TODO: Remove this once mypy stops having spurious setuptools issues.
    cmdclass={""verify"": VerifyVersionCommand},  # type: ignore
)

If you guys have any tips for fixing this, please tell me, been at it for 3 weeks, but I can't get my head around what's happening, I've reinstalled countless times, and deleted PIP Cache to prevent this (since before it used the cached numpy 1.21 as well)

Thank you in advance for your help",hi issue well trying install class use python use version teacher said got ta use however matter run python pip install console always post setup command console python pip install eta requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied torch requirement already satisfied requirement already satisfied pillow requirement already satisfied six requirement already satisfied eta requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied gym zip eta build done getting build wheel done done requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied markdown requirement already satisfied requirement already satisfied requirement already satisfied torch requirement already satisfied torch requirement already satisfied jinja torch requirement already satisfied gym requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied torch building collected building wheel error error building wheel run successfully exit code output may yet support python running source directory module trying found available found available found matching could locate executable could locate executable could locate executable could locate executable could locate executable could locate executable could locate executable could locate executable could locate executable could locate executable could locate executable could locate executable could locate executable know compile code platform available available setting found available found available setting atlas found available atlas found available vendor blas found back blas library worse performance better performance easily switching blas library blas blas found available blas found search file section blas setting blas environment variable blas available blas found search file section setting environment variable blas available path found available found available found available flame found available setting found found found found class available found found found found class available setting found atlas found found atlas found class available found atlas found found atlas found class available found available found search file section setting environment variable return self name available found search file section setting environment variable return self name available found language none warning relative import known parent package unknown distribution option running running build running build compiler running build running building build building library recent call last file line module main file line main hook file line return file line return file line file line super file line compile code file line module file line setup file line setup return file line setup return file line setup file line file line file line run file line command file line file line run self file line run file line command file line file line run file line file line file line source extension file line st main void return file line body file line return file line ret self file line body file line file line ret self file line file line compile file line spawn return super file line lambda lambda self self got unexpected argument end output note error likely problem pip error building wheel build error could build install import o import import setup import install import import version class install custom command verify git tag one release originally based slightly description verify git tag version run self tag tag tag git tag match tag tag get long description file open setup unity machine learning unity intended audience topic artificial intelligence license apache license language python recurse find go version pillow torch six explicit dependency since need declare six support python work python since version draw line somewhere main main main main type entry default behavior remove spurious verify type ignore import o import import setup import install import version class install custom command verify git tag one release originally based slightly description verify git tag version run self tag tag tag git tag match tag tag get long description file open setup unity machine learning interface unity intended audience topic artificial intelligence license apache license language python pillow gym remove spurious verify type ignore fixing please tell ca get head around happening countless time pip cache prevent since used well thank advance help,issue,positive,positive,positive,positive,positive,positive
1988064773,"Thanks for this improvement!

This PR fell under the radar, but since then the same fixes have been committed in https://github.com/Unity-Technologies/ml-agents/pull/6069/files . I'll close this PR out.

Thanks again for looking for ways to contribute!",thanks improvement fell radar since close thanks looking way contribute,issue,positive,positive,positive,positive,positive,positive
1986233112,"
anaconda navigator
env/ create / name is env3103 / slected python 3.10.13
open terminal
goto downloads
cd .\ml-agents\
git clone --branch release_21 https://github.com/Unity-Technologies/ml-agents.git
conda install pytorch=1.13.1 -c pytorch
pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117

mlagents env setup.py-
    install_requires=[
        ""cloudpickle"",
        ""grpcio>=1.11.0,<=1.48.2"",
        ""Pillow>=4.2.1"",
        ""protobuf>=3.6,<3.20"",
        ""pyyaml>=3.1.0"",
        ""gym>=0.21.0"",
        ""pettingzoo==1.15.0"",
        ""numpy==1.21.3"",
        ""filelock>=3.4.0"",
    ],
    python_requires="">=3.10.1,<=3.10.13"",



mlagents setup.py-
   install_requires=[
        # Test-only dependencies should go in test_requirements.txt, not here.
        ""grpcio>=1.11.0,<=1.48.2"",
        ""h5py>=2.9.0"",
        f""mlagents_envs=={VERSION}"",
        ""numpy>=1.23.3"",
        ""Pillow>=4.2.1"",
        ""protobuf>=3.6,<3.20"",
        ""pyyaml>=3.1.0"",
        ""torch>=1.13.1"",
        ""tensorboard>=2.14"",
        # adding six explicit dependency since tensorboard needs it but doesn't declare it as a dep
        ""six>=1.16"",
        # cattrs 1.1.0 dropped support for python 3.6, but 1.0.0 doesn't work for python 3.9
        # Since there's no version that supports both, we have to draw the line somewhere.
        ""cattrs>=1.1.0,<1.7; python_version>='3.8'"",
        ""attrs>=19.3.0"",
        ""huggingface_hub>=0.14"",
        'pypiwin32==223;platform_system==""Windows""',
        ""onnx==1.12.0"",
    ],
    python_requires="">=3.10.1,<=3.10.13"",

python -m pip install ./ml-agents-envs
<GET ERROR>



ERROR: Cannot install mlagents, mlagents-envs==1.0.0 and mlagents==1.0.0 because these package versions have conflicting dependencies.

The conflict is caused by:
    mlagents 1.0.0 depends on protobuf<3.21 and >=3.6
    mlagents-envs 1.0.0 depends on protobuf<3.20 and >=3.6
    onnx 1.15.0 depends on protobuf>=3.20.2

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict

ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts",anaconda navigator create name python open terminal git clone branch install pip install pillow gym go version pillow torch six explicit dependency since need declare six support python work python since version draw line somewhere python pip install get error error install package conflicting conflict fix could try loosen range package remove package allow pip attempt solve dependency conflict error help visit,issue,negative,neutral,neutral,neutral,neutral,neutral
1982735188,"Very thanks for reply! But when I change the setup.py both in ml-agents and ml-agents-envs, I will face another bugERROR: Cannot install mlagents and mlagents==1.0.0 because these package versions have conflicting dependencies.


The conflict is caused by:
&nbsp; &nbsp; mlagents 1.0.0 depends on numpy&gt;=1.23.3
&nbsp; &nbsp; mlagents-envs 1.0.0 depends on numpy==1.21.2


To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict


ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts





------------------&nbsp;原始邮件&nbsp;------------------
发件人: ***@***.***&gt;; 
发送时间: 2024年3月7日(星期四) 下午3:22
收件人: ***@***.***&gt;; 
抄送: ***@***.***&gt;; ***@***.***&gt;; 
主题: Re: [Unity-Technologies/ml-agents] release-21-branch numpy can&#39;t install successful. (Issue #6047)





  
I do the same but face this problem
 
TypeError: CCompiler_spawn() got an unexpected keyword argument 'env' [end of output]
 
note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for numpy Failed to build numpy ERROR: Could not build wheels for numpy, which is required to install pyproject.toml-based projects
  
You might need to also change the Python version in the other setup file
 
—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you commented.Message ID: ***@***.***&gt;",thanks reply change face another install package conflicting conflict fix could try loosen range package remove package allow pip attempt solve dependency conflict error help visit release branch install successful issue face problem got unexpected argument end output note error likely problem pip error building wheel build error could build install might need also change python version setup file reply directly view id,issue,negative,positive,positive,positive,positive,positive
1982719626,"> I do the same but face this problem
> 
> TypeError: CCompiler_spawn() got an unexpected keyword argument 'env' [end of output]
> 
> note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for numpy Failed to build numpy ERROR: Could not build wheels for numpy, which is required to install pyproject.toml-based projects

You might need to also change the Python version in the other setup file",face problem got unexpected argument end output note error likely problem pip error building wheel build error could build install might need also change python version setup file,issue,negative,positive,neutral,neutral,positive,positive
1982563998,"I do the same but face this problem


TypeError: CCompiler_spawn() got an unexpected keyword argument 'env'
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for numpy
Failed to build numpy
ERROR: Could not build wheels for numpy, which is required to install pyproject.toml-based projects",face problem got unexpected argument end output note error likely problem pip error building wheel build error could build install,issue,negative,positive,neutral,neutral,positive,positive
1982212568,"> numpy==1.22.4

Can please share which torch version you have installed instead of what is instructed?",please share torch version instead instructed,issue,positive,neutral,neutral,neutral,neutral,neutral
1981257005,"@alex-mccarthy-unity Thank you Alex for the help

Earlier i was using python 3.10.11 and then i used 3.12.2, the problem is there is no installer available for Python - 3.10.12. Which is why i am not able to run it.",thank help python used problem installer available python able run,issue,negative,positive,positive,positive,positive,positive
1980596341,"Ah, I see that you're using python 3.12. I think that's why pettingzoo 1.15.0 can't be found below: it's discarded due to `1.15.0 Requires-Python >=3.7, <3.11` , which is causing the installation of `mlagents-envs` to fail.

```sh
ERROR: Ignored the following versions that require a different python version: 1.10.0 Requires-Python >=3.6, <3.10; 1.11.0 Requires-Python >=3.6, <3.10; 1.11.1 Requires-Python >=3.6, <3.10; 1.11.2 Requires-Python >=3.6, <3.10; 1.12.0 Requires-Python >=3.6, <3.10; 1.13.0 Requires-Python >=3.7, <3.10; 1.13.1 Requires-Python >=3.7, <3.10; 1.14.0 Requires-Python >=3.7, <3.11; 1.15.0 Requires-Python >=3.7, <3.11; 1.16.0 Requires-Python >=3.7, <3.11; 1.17.0 Requires-Python >=3.7, <3.11; 1.18.1 Requires-Python >=3.7, <3.11; 1.19.0 Requires-Python >=3.7, <3.11; 1.20.1 Requires-Python >=3.7, <3.11; 1.21.0 Requires-Python >=3.7, <3.11; 1.22.0 Requires-Python >=3.7, <3.11; 1.22.1 Requires-Python >=3.7, <3.11; 1.22.2 Requires-Python >=3.7, <3.11; 1.22.3 Requires-Python >=3.7, <3.12; 1.4.0 Requires-Python >=3.6, <3.9; 1.4.1 Requires-Python >=3.6, <3.9; 1.4.2 Requires-Python >=3.6, <3.9; 1.5.0 Requires-Python >=3.6, <3.10; 1.5.1 Requires-Python >=3.6, <3.10; 1.5.2 Requires-Python >=3.6, <3.10; 1.6.0 Requires-Python >=3.6, <3.10; 1.6.1 Requires-Python >=3.6, <3.10; 1.7.0 Requires-Python >=3.6, <3.10; 1.8.0 Requires-Python >=3.6, <3.10; 1.8.1 Requires-Python >=3.6, <3.10; 1.8.2 Requires-Python >=3.6, <3.10; 1.9.0 Requires-Python >=3.6, <3.10
ERROR: Could not find a version that satisfies the requirement pettingzoo==1.15.0 (from mlagents-envs) (from versions: 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.1.10, 0.1.11, 0.1.12, 0.1.13, 0.1.14, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5, 1.23.0, 1.23.1, 1.24.0, 1.24.1, 1.24.2, 1.24.3)
ERROR: No matching distribution found for pettingzoo==1.15.0
```

ML Agents currently requires Python 3.10. I'd recommend doing a clean install by running through the whole install guide at https://unity-technologies.github.io/ml-agents/Installation/#install-python-31012 (probably using a python environment like anaconda/miniconda)",ah see python think ca found due causing installation fail sh error following require different python version error could find version requirement error matching distribution found currently python recommend clean install running whole install guide probably python environment like,issue,negative,negative,neutral,neutral,negative,negative
1979257642,"Still same error
```
(vir) amankankriya@Amans-MacBook-Air ml-agents % python3.12 -m pip uninstall mlagents-envs 
WARNING: Skipping mlagents-envs as it is not installed.
(vir) amankankriya@Amans-MacBook-Air ml-agents % python3.12 -m pip uninstall mlagents     
WARNING: Skipping mlagents as it is not installed.
(vir) amankankriya@Amans-MacBook-Air ml-agents % python3.12 -m pip install ./ml-agents-envs
Processing ./ml-agents-envs
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... done
Collecting cloudpickle (from mlagents_envs==1.1.0.dev0)
  Using cached cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)
Collecting grpcio<=1.48.2,>=1.11.0 (from mlagents_envs==1.1.0.dev0)
  Using cached grpcio-1.48.2.tar.gz (22.0 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... done
Collecting Pillow>=4.2.1 (from mlagents_envs==1.1.0.dev0)
  Using cached pillow-10.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.7 kB)
Collecting protobuf<3.20,>=3.6 (from mlagents_envs==1.1.0.dev0)
  Using cached protobuf-3.19.6-py2.py3-none-any.whl.metadata (828 bytes)
Collecting pyyaml>=3.1.0 (from mlagents_envs==1.1.0.dev0)
  Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)
Collecting gym>=0.21.0 (from mlagents_envs==1.1.0.dev0)
  Using cached gym-0.26.2.tar.gz (721 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... done
INFO: pip is looking at multiple versions of mlagents-envs to determine which version is compatible with other requirements. This could take a while.
ERROR: Ignored the following yanked versions: 1.22.4
ERROR: Ignored the following versions that require a different python version: 1.10.0 Requires-Python >=3.6, <3.10; 1.11.0 Requires-Python >=3.6, <3.10; 1.11.1 Requires-Python >=3.6, <3.10; 1.11.2 Requires-Python >=3.6, <3.10; 1.12.0 Requires-Python >=3.6, <3.10; 1.13.0 Requires-Python >=3.7, <3.10; 1.13.1 Requires-Python >=3.7, <3.10; 1.14.0 Requires-Python >=3.7, <3.11; 1.15.0 Requires-Python >=3.7, <3.11; 1.16.0 Requires-Python >=3.7, <3.11; 1.17.0 Requires-Python >=3.7, <3.11; 1.18.1 Requires-Python >=3.7, <3.11; 1.19.0 Requires-Python >=3.7, <3.11; 1.20.1 Requires-Python >=3.7, <3.11; 1.21.0 Requires-Python >=3.7, <3.11; 1.22.0 Requires-Python >=3.7, <3.11; 1.22.1 Requires-Python >=3.7, <3.11; 1.22.2 Requires-Python >=3.7, <3.11; 1.22.3 Requires-Python >=3.7, <3.12; 1.4.0 Requires-Python >=3.6, <3.9; 1.4.1 Requires-Python >=3.6, <3.9; 1.4.2 Requires-Python >=3.6, <3.9; 1.5.0 Requires-Python >=3.6, <3.10; 1.5.1 Requires-Python >=3.6, <3.10; 1.5.2 Requires-Python >=3.6, <3.10; 1.6.0 Requires-Python >=3.6, <3.10; 1.6.1 Requires-Python >=3.6, <3.10; 1.7.0 Requires-Python >=3.6, <3.10; 1.8.0 Requires-Python >=3.6, <3.10; 1.8.1 Requires-Python >=3.6, <3.10; 1.8.2 Requires-Python >=3.6, <3.10; 1.9.0 Requires-Python >=3.6, <3.10
ERROR: Could not find a version that satisfies the requirement pettingzoo==1.15.0 (from mlagents-envs) (from versions: 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.1.10, 0.1.11, 0.1.12, 0.1.13, 0.1.14, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5, 1.23.0, 1.23.1, 1.24.0, 1.24.1, 1.24.2, 1.24.3)
ERROR: No matching distribution found for pettingzoo==1.15.0
(vir) amankankriya@Amans-MacBook-Air ml-agents % python3.12 -m pip install ./ml-agents     
Processing ./ml-agents
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... done
Collecting grpcio<=1.48.2,>=1.11.0 (from mlagents==1.1.0.dev0)
  Using cached grpcio-1.48.2.tar.gz (22.0 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... done
Collecting h5py>=2.9.0 (from mlagents==1.1.0.dev0)
  Using cached h5py-3.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.5 kB)
INFO: pip is looking at multiple versions of mlagents to determine which version is compatible with other requirements. This could take a while.
ERROR: Ignored the following versions that require a different python version: 0.10.0.dev0 Requires-Python >=3.5,<3.8; 0.29.0 Requires-Python >=3.7.2,<3.10.0; 0.30.0 Requires-Python >=3.8.13,<=3.10.8; 0.6.0 Requires-Python >=3.5,<=3.7; 0.6.1 Requires-Python >=3.5,<=3.7; 0.6.2 Requires-Python >=3.5,<=3.7; 0.8.0 Requires-Python >=3.5,<3.8; 0.8.1 Requires-Python >=3.5,<3.8; 0.8.2 Requires-Python >=3.5,<3.8; 0.9.0 Requires-Python >=3.5,<3.8; 0.9.1 Requires-Python >=3.5,<3.8; 0.9.2 Requires-Python >=3.5,<3.8; 0.9.3 Requires-Python >=3.5,<3.8; 1.0.0 Requires-Python >=3.10.1,<=3.10.12
ERROR: Could not find a version that satisfies the requirement mlagents-envs==1.1.0.dev0 (from mlagents) (from versions: 0.10.0.dev1, 0.10.0, 0.10.1, 0.11.0.dev0, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.18.0, 0.18.1, 0.19.0, 0.20.0, 0.21.0, 0.21.1, 0.22.0, 0.23.0, 0.24.0, 0.24.1, 0.25.0, 0.25.1, 0.26.0, 0.27.0, 0.28.0)
ERROR: No matching distribution found for mlagents-envs==1.1.0.dev0
```",still error python pip warning skipping python pip warning skipping python pip install build done getting build wheel done done done dev dev build done getting build wheel done done done pillow dev dev dev gym dev build done getting build wheel done done done pip looking multiple determine version compatible could take error following error following require different python version error could find version requirement error matching distribution found python pip install build done getting build wheel done done done dev build done getting build wheel done done done dev pip looking multiple determine version compatible could take error following require different python version dev error could find version requirement dev dev dev error matching distribution found dev,issue,negative,neutral,neutral,neutral,neutral,neutral
1979242940,">Could not find a version that satisfies the requirement pettingzoo==1.15.0 (from mlagents-envs)

It sounds like you might still have mlagents-envs installed from before, and its leftovers might be conflicting with your new installation.

Just to confirm, can you try uninstalling both the `mlagents-envs` and `mlagents` packages (the ones installed from pip) before re-installing from the latest `develop` version?

```sh
python -m pip uninstall mlagents-envs
python -m pip uninstall mlagents
python -m pip install ./ml-agents-envs
python -m pip install ./ml-agents
```",could find version requirement like might still might conflicting new installation confirm try pip latest develop version sh python pip python pip python pip install python pip install,issue,negative,positive,positive,positive,positive,positive
1979221750,"if i run this after installing ml-agents
```
cd ml-agents
python -m pip install ./ml-agents-envs
python -m pip install ./ml-agents
```
This is the error code
```
Processing ./ml-agents-envs
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... done
Collecting cloudpickle (from mlagents_envs==1.1.0.dev0)
  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)
Collecting grpcio<=1.48.2,>=1.11.0 (from mlagents_envs==1.1.0.dev0)
  Using cached grpcio-1.48.2.tar.gz (22.0 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... done
Collecting Pillow>=4.2.1 (from mlagents_envs==1.1.0.dev0)
  Downloading pillow-10.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.7 kB)
Collecting protobuf<3.20,>=3.6 (from mlagents_envs==1.1.0.dev0)
  Downloading protobuf-3.19.6-py2.py3-none-any.whl.metadata (828 bytes)
Collecting pyyaml>=3.1.0 (from mlagents_envs==1.1.0.dev0)
  Downloading PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)
Collecting gym>=0.21.0 (from mlagents_envs==1.1.0.dev0)
  Downloading gym-0.26.2.tar.gz (721 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 721.7/721.7 kB 746.5 kB/s eta 0:00:00
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... done
INFO: pip is looking at multiple versions of mlagents-envs to determine which version is compatible with other requirements. This could take a while.
ERROR: Ignored the following yanked versions: 1.22.4
ERROR: Ignored the following versions that require a different python version: 1.10.0 Requires-Python >=3.6, <3.10; 1.11.0 Requires-Python >=3.6, <3.10; 1.11.1 Requires-Python >=3.6, <3.10; 1.11.2 Requires-Python >=3.6, <3.10; 1.12.0 Requires-Python >=3.6, <3.10; 1.13.0 Requires-Python >=3.7, <3.10; 1.13.1 Requires-Python >=3.7, <3.10; 1.14.0 Requires-Python >=3.7, <3.11; 1.15.0 Requires-Python >=3.7, <3.11; 1.16.0 Requires-Python >=3.7, <3.11; 1.17.0 Requires-Python >=3.7, <3.11; 1.18.1 Requires-Python >=3.7, <3.11; 1.19.0 Requires-Python >=3.7, <3.11; 1.20.1 Requires-Python >=3.7, <3.11; 1.21.0 Requires-Python >=3.7, <3.11; 1.22.0 Requires-Python >=3.7, <3.11; 1.22.1 Requires-Python >=3.7, <3.11; 1.22.2 Requires-Python >=3.7, <3.11; 1.22.3 Requires-Python >=3.7, <3.12; 1.4.0 Requires-Python >=3.6, <3.9; 1.4.1 Requires-Python >=3.6, <3.9; 1.4.2 Requires-Python >=3.6, <3.9; 1.5.0 Requires-Python >=3.6, <3.10; 1.5.1 Requires-Python >=3.6, <3.10; 1.5.2 Requires-Python >=3.6, <3.10; 1.6.0 Requires-Python >=3.6, <3.10; 1.6.1 Requires-Python >=3.6, <3.10; 1.7.0 Requires-Python >=3.6, <3.10; 1.8.0 Requires-Python >=3.6, <3.10; 1.8.1 Requires-Python >=3.6, <3.10; 1.8.2 Requires-Python >=3.6, <3.10; 1.9.0 Requires-Python >=3.6, <3.10
ERROR: Could not find a version that satisfies the requirement pettingzoo==1.15.0 (from mlagents-envs) (from versions: 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.1.10, 0.1.11, 0.1.12, 0.1.13, 0.1.14, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5, 1.23.0, 1.23.1, 1.24.0, 1.24.1, 1.24.2, 1.24.3)
ERROR: No matching distribution found for pettingzoo==1.15.0
(vir) amankankriya@Amans-MacBook-Air ml-agents % python3.12 -m pip install ./ml-agents     
Processing ./ml-agents
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... done
Collecting grpcio<=1.48.2,>=1.11.0 (from mlagents==1.1.0.dev0)
  Using cached grpcio-1.48.2.tar.gz (22.0 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... done
Collecting h5py>=2.9.0 (from mlagents==1.1.0.dev0)
  Downloading h5py-3.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.5 kB)
INFO: pip is looking at multiple versions of mlagents to determine which version is compatible with other requirements. This could take a while.
ERROR: Ignored the following versions that require a different python version: 0.10.0.dev0 Requires-Python >=3.5,<3.8; 0.29.0 Requires-Python >=3.7.2,<3.10.0; 0.30.0 Requires-Python >=3.8.13,<=3.10.8; 0.6.0 Requires-Python >=3.5,<=3.7; 0.6.1 Requires-Python >=3.5,<=3.7; 0.6.2 Requires-Python >=3.5,<=3.7; 0.8.0 Requires-Python >=3.5,<3.8; 0.8.1 Requires-Python >=3.5,<3.8; 0.8.2 Requires-Python >=3.5,<3.8; 0.9.0 Requires-Python >=3.5,<3.8; 0.9.1 Requires-Python >=3.5,<3.8; 0.9.2 Requires-Python >=3.5,<3.8; 0.9.3 Requires-Python >=3.5,<3.8; 1.0.0 Requires-Python >=3.10.1,<=3.10.12
ERROR: Could not find a version that satisfies the requirement mlagents-envs==1.1.0.dev0 (from mlagents) (from versions: 0.10.0.dev1, 0.10.0, 0.10.1, 0.11.0.dev0, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.18.0, 0.18.1, 0.19.0, 0.20.0, 0.21.0, 0.21.1, 0.22.0, 0.23.0, 0.24.0, 0.24.1, 0.25.0, 0.25.1, 0.26.0, 0.27.0, 0.28.0)
ERROR: No matching distribution found for mlagents-envs==1.1.0.dev0
```

```
ERROR: Could not find a version that satisfies the requirement pettingzoo==1.15.0
ERROR: No matching distribution found for mlagents-envs==1.1.0.dev0
```

There is no problem when i run same code on google colab. But due to limited gpu time, i wanted to train it on my local system and upload to hf.",run python pip install python pip install error code build done getting build wheel done done done dev dev build done getting build wheel done done done pillow dev dev dev gym dev eta build done getting build wheel done done done pip looking multiple determine version compatible could take error following error following require different python version error could find version requirement error matching distribution found python pip install build done getting build wheel done done done dev build done getting build wheel done done done dev pip looking multiple determine version compatible could take error following require different python version dev error could find version requirement dev dev dev error matching distribution found dev error could find version requirement error matching distribution found dev problem run code due limited time train local system,issue,negative,negative,neutral,neutral,negative,negative
1978975296,"I'll admit that I don't know the exact status of this project's releases, but I'd expect those versions to be newer based on https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Installation.md#installing-mlagents .

If you're up for it, one option is to try to build/install from the `develop` branch on this repo, rather than installing from pip. Instructions: https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Installation.md",admit know exact status project expect based one option try develop branch rather pip,issue,negative,positive,positive,positive,positive,positive
1978839419,"```pip3 install mlagents ``` installed succesfully.
Here is the snapshot from pip list. 
```
mlagents                0.30.0
mlagents-envs           0.30.0
```

> output of `which mlagents-learn` and `which mlagents-push-to-hf`
```
(bi) amankankriya@Amans-MacBook-Air DeepRL % which mlagents-learn
/Users/amankankriya/Documents/DeepRL/bi/bin/mlagents-learn
(bi) amankankriya@Amans-MacBook-Air DeepRL % which mlagents-push-to-hf
mlagents-push-to-hf not found
```
",pip install snapshot pip list output found,issue,negative,neutral,neutral,neutral,neutral,neutral
1978667573,"> @alex-mccarthy-unity I have `cmake` installed on my local system. My problem: `mlagents-learn` is working fine. But when i run `mlagents-push-to-hf` is shows `zsh:1: command not found: mlagents-push-to-hf`

Interesting. Can you include the output of `which mlagents-learn` and `which mlagents-push-to-hf` ? Here's mine:

```sh
(mlagents-develop) alex.mccarthy@alex ~  % which mlagents-learn           
/Users/alex.mccarthy/miniconda3/envs/mlagents-develop/bin/mlagents-learn
(mlagents-develop) alex.mccarthy@alex ~ % which mlagents-push-to-hf
/Users/alex.mccarthy/miniconda3/envs/mlagents-develop/bin/mlagents-push-to-hf
```

And to confirm: did you get `pip3 install mlagents` working correctly? The log in the first comment shows an installation error: I'd expect that step to install both `mlagents-learn` and `mlagents-push-to-hf` if it runs successfully (since both commands are configured [here](https://github.com/Unity-Technologies/ml-agents/blob/86a3a6e5f75ffb86ad2993ed2a0fb3169779d17e/ml-agents/setup.py#L80-L82))",local system problem working fine run command found interesting include output mine sh confirm get pip install working correctly log first comment installation error expect step install successfully since,issue,negative,positive,positive,positive,positive,positive
1978593028,"@alex-mccarthy-unity I have ```cmake``` installed on my local system. 
My problem: ```mlagents-learn``` is working fine. 
But when i run ```mlagents-push-to-hf``` is shows ```zsh:1: command not found: mlagents-push-to-hf```",local system problem working fine run command found,issue,negative,positive,positive,positive,positive,positive
1976868672,"This is the key part of your error:
```
FileNotFoundError: [Errno 2] No such file or directory: '/Users/amankankriya/Documents/Deep RL/myenv/lib/python3.10/site-packages/cmake/data/CMake.app/Contents/bin/cmake'
```

You need to install `cmake`. It looks like you're on OS X: one way to install `cmake` is via homebrew: https://formulae.brew.sh/formula/cmake",key part error file directory need install like o one way install via,issue,negative,neutral,neutral,neutral,neutral,neutral
1975480154,"I didn’t fix it, I just gave up. You could try changing the MLAgents version, if you keep having trouble you can remake the issue. Good Luck!",fix gave could try version keep trouble remake issue good luck,issue,negative,positive,positive,positive,positive,positive
1960107023,"For reference this is my process to install ML-Agents:

1. clone the ML-Agents repository --branch release_21
2. Go to Unity project, Package Manager -> Unity Registry -> Search for ""ML Agents""
3. The only search result I get is 2.0.1 version which seems pretty outdated? So I don't install it. 
4. Then I click the ""+"" sign and add from disk, then I select the package.json in ""com.unity.ml-agents"". 
5. I immediately get the above stated errors - even before explicitly importing the 3DBall scene. 

C:\ml-agents\com.unity.ml-agents\Runtime\Policies\BehaviorParameters.cs(1,13): error CS0234: The type or namespace name 'Barracuda' does not exist in the namespace 'Unity' (are you missing an assembly reference?)

C:\ml-agents\com.unity.ml-agents\Runtime\Policies\BehaviorParameters.cs(105,16): error CS0246: The type or namespace name 'NNModel' could not be found (are you missing a using directive or an assembly reference?)

C:\ml-agents\com.unity.ml-agents\Runtime\Policies\BehaviorParameters.cs(98,9): error CS0246: The type or namespace name 'NNModel' could not be found (are you missing a using directive or an assembly reference?)


",reference process install clone repository branch go unity project package manager unity registry search search result get version pretty outdated install click sign add disk select immediately get stated even explicitly scene error type name exist missing assembly reference error type name could found missing directive assembly reference error type name could found missing directive assembly reference,issue,negative,negative,negative,negative,negative,negative
1960036035,"I am still facing this issue. Upon further checking, it seems like it is only fixed for the develop branch but not yet pushed to the latest release branch. ""release-21"" still suffers with this issue and imports Barracuda in ""com.unity.ml-agents\Runtime\Policies\BehaviorParameters.cs"" in the first line of the code. ",still facing issue upon like fixed develop branch yet latest release branch still issue barracuda first line code,issue,negative,positive,positive,positive,positive,positive
1958548172,bless this thread it actually worked,bless thread actually worked,issue,negative,neutral,neutral,neutral,neutral,neutral
1953945862,"I would like to express my support for updating the interface.
The change seems to be relatively straightforward, as intended by the maintainers of Gymnasium.",would like express support interface change relatively straightforward intended gymnasium,issue,positive,positive,positive,positive,positive,positive
1953553051,"> > I do, I found some solutions but still not working, and the same error appears... is that a version issue from python/torch?
> 
> I saw your behavior type is Heuristic. Change your behavior type to default if you want it controlled by python 

I see, then I will re-do the project and update it to default... because I've made a lot of changes looking up in some tutorials and how-to-solve guides with similar issues to prevent any config conflict with this",found still working error version issue saw behavior type heuristic change behavior type default want python see project update default made lot looking similar prevent conflict,issue,negative,neutral,neutral,neutral,neutral,neutral
1951648677,"> I do, I found some solutions but still not working, and the same error appears... is that a version issue from python/torch?

I saw your behavior type is Heuristic. Change your behavior type to default if you want it controlled by python ",found still working error version issue saw behavior type heuristic change behavior type default want python,issue,negative,neutral,neutral,neutral,neutral,neutral
1951645893,Your behavior type should be default if you want train it.,behavior type default want train,issue,negative,neutral,neutral,neutral,neutral,neutral
1951594649,"I do, I found some solutions but still not working, and the same error appears... is that a version issue from python/torch?",found still working error version issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1951582937,"You can try go to Window ->Package Manager -> search InputSystem -> install.
See if error fixed.",try go window package manager search install see error fixed,issue,negative,positive,neutral,neutral,positive,positive
1947820913,"> > I ended up fixing all the issues, here is a document about how I fixed everything [MLAgents setup guide.pdf](https://github.com/Unity-Technologies/ml-agents/files/14275768/MLAgents.setup.guide.pdf)
> 
> Very detailed teaching, thank you!!

NP!",ended fixing document fixed everything setup detailed teaching thank,issue,negative,positive,positive,positive,positive,positive
1943131006,"I ended up fixing all the issues, here is a document about how I fixed everything
[MLAgents setup guide.pdf](https://github.com/Unity-Technologies/ml-agents/files/14275768/MLAgents.setup.guide.pdf)
",ended fixing document fixed everything setup,issue,negative,positive,neutral,neutral,positive,positive
1943091032,"I had this issue as well and fixed it by following @ashwinsnambiar recommendation of following @onurkurum https://github.com/Unity-Technologies/ml-agents/issues/6008#issuecomment-1815607844

""Workaround:
Changed
""numpy==1.21.2"",
to
""numpy==1.23.3"",

in the file path-to-ml-agents\ml-agents-envs\setup.py""",issue well fixed following recommendation following file,issue,negative,positive,neutral,neutral,positive,positive
1941947421,"I had faced a similar issue. It seemed to be caused by an incorrect numpy version. I removed the existing numpy and mlagents packages from the virtual env and reinstalled everything by using the setup.py from a branch release. 
Please check the following link for the same.
https://github.com/Unity-Technologies/ml-agents/issues/6008#issuecomment-1814741015",faced similar issue incorrect version removed virtual everything branch release please check following link,issue,negative,neutral,neutral,neutral,neutral,neutral
1895613823,"> I am having a problem that causes this error when running `mlagents-learn --help` after having changed the numpy version and successfully gotten it to install ![image](https://private-user-images.githubusercontent.com/145397921/296998620-ba7f06eb-55ee-4fdc-853e-a11c74817a46.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDU0OTA5ODIsIm5iZiI6MTcwNTQ5MDY4MiwicGF0aCI6Ii8xNDUzOTc5MjEvMjk2OTk4NjIwLWJhN2YwNmViLTU1ZWUtNGZkYy04NTNlLWExMWM3NDgxN2E0Ni5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMTE3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDExN1QxMTI0NDJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0xZTIwNmMwNjExMjk2MTJiNGNmZDQ4MGE5NjVmZGQ3ZGI3ZGRjNTk5MGM5NzU1NDQzYzhiNzU5ZWUxYzA2YTIyJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.nFJ9HrPUO2NjquxDm87Zg4l45v-r9I1jnuRPCjvS3ZM)

I seem to have solved, *that* issue, but now it gives a numpy error upon typing mlagents-learn
",problem error running help version successfully gotten install image seem issue error upon,issue,negative,positive,positive,positive,positive,positive
1893374437,"I am having a problem that causes this error when running `mlagents-learn --help` after having changed the numpy version and successfully gotten it to install
![image](https://github.com/Unity-Technologies/ml-agents/assets/145397921/ba7f06eb-55ee-4fdc-853e-a11c74817a46)
",problem error running help version successfully gotten install image,issue,negative,positive,positive,positive,positive,positive
1890855288,"Dear developer

Yet not changed in GitHub.
Today, three hours have melted away. ",dear developer yet today three melted away,issue,negative,neutral,neutral,neutral,neutral,neutral
1888697365,@miguelalonsojr this is still actual. I looked up the docs just today. Can we reopen?,still actual today reopen,issue,negative,neutral,neutral,neutral,neutral,neutral
1881421400,"thanks ,change numpy version to 1.23.1  in setup.py slove this problem!",thanks change version problem,issue,negative,positive,positive,positive,positive,positive
1880810902,"Sorry maybe this is not serious enough to be called a bug, but I don't know how to change the label.",sorry maybe serious enough bug know change label,issue,negative,negative,negative,negative,negative,negative
1880059802,Could you please explain what the bug was here? I still get this issue even after installing ml-agents from the develop branch and using the package.json from this,could please explain bug still get issue even develop branch,issue,negative,neutral,neutral,neutral,neutral,neutral
1875579512,"I followed those steps but met with many issues with the numpy version. I ended up finding the right solution for me, and it was to change the numpy verison in the `setup.py` file in the `ml-agents-envs` folder to `""numpy>=1.21.2,<1.24.0""` rather than the other suggested ways. It's shown in this screenshot:
![image](https://github.com/Unity-Technologies/ml-agents/assets/43866181/c3f0d73a-81c8-4cfa-8f6e-bb340e2e6027)
which is taken from this branch of the ml-agents repo: https://github.com/Unity-Technologies/ml-agents/blob/develop/ml-agents-envs/setup.py.

Thanks for the help, though!",met many version ended finding right solution change file folder rather way shown image taken branch thanks help though,issue,positive,positive,positive,positive,positive,positive
1875272313,"> Hey, I've been having the same issue as you so I tried changing the numpy version in the file like you showed and it allowed me to install the packages but now I get another error and I was wondering if you were getting it too. I get an error that says: `Warning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (function operator ())`. Have you gotten this before and do you know how to fix this?

![image](https://github.com/Unity-Technologies/ml-agents/assets/142788404/5277bfd7-3c53-4c39-b3ee-41940c330154)

I change numpy version to 1.22.4 because it support python3.10.12
",hey issue tried version file like install get another error wondering getting get error warning initialize module version version function operator gotten know fix image change version support python,issue,negative,neutral,neutral,neutral,neutral,neutral
1875264975,"> Hey, I've been having the same issue as you so I tried changing the numpy version in the file like you showed and it allowed me to install the packages but now I get another error and I was wondering if you were getting it too. I get an error that says: `Warning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (function operator ())`. Have you gotten this before and do you know how to fix this?

General flow:

- Install Microsoft visual C++
- Install Visual Studio with Unity and C++
- clone mlagents
- Reboot
- Creating a virtual environment python==3.10.12 (anaconda)
- installation pytorch (according to mlagents document version) : pip3 install torch ~ = 1.13.1 - f https://download.pytorch.org/whl/torch_stable.html (follow mlagent document)
- Change setup.py in /ml-agents-envs (it may not support numpy)
- I changed it to ""numpy==1.22.4"", (setup.py content) can't use numpy Most likely mlagents default numpy version is no longer supported by python3.10.12
- After that it's probably unity to install the mlagent plugin
- You can type 'mlagens-learn' in python with mlagent enabled in CMD and if it pops up, it's successful",hey issue tried version file like install get another error wondering getting get error warning initialize module version version function operator gotten know fix general flow install visual install visual studio unity clone virtual environment anaconda installation according document version pip install torch follow document change may support content ca use likely default version longer python probably unity install type python successful,issue,negative,positive,positive,positive,positive,positive
1875142674,Maybe you come cross this problem and try:https://stackoverflow.com/questions/71986643/userwarning-failed-to-initialize-numpy-module-compiled-against-api-version-0xf,maybe come cross problem try,issue,negative,neutral,neutral,neutral,neutral,neutral
1874878184,"Hey, I've been having the same issue as you so I tried changing the numpy version in the file like you showed and it allowed me to install the packages but now I get another error and I was wondering if you were getting it too. I get an error that says:
`Warning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (function operator ())`.
Have you gotten this before and do you know how to fix this?",hey issue tried version file like install get another error wondering getting get error warning initialize module version version function operator gotten know fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1874644159,@josmuniz The three changes to **ml-agents-envs/setup.py** that I put in the original post in this thread should solve that issue.,three put original post thread solve issue,issue,negative,positive,positive,positive,positive,positive
1873063617,"I think that I have the same issues but Mac 14.2. I try to install it directly : 
(mlagents) josemuniz@MacBook-Pro-2 ml-agents % pip install ""onnx==1.13.0""
pip install ""mlagents-envs==1.0.0""
pip install ""protobuf==3.20.2""

but there are incompatibility between recommended versions.

Note: 
Collecting onnx==1.13.0
  Using cached onnx-1.13.0-cp310-cp310-macosx_10_12_universal2.whl (14.0 MB)
Requirement already satisfied: numpy>=1.16.6 in /Users/josemuniz/miniforge3/envs/mlagents/lib/python3.10/site-packages (from onnx==1.13.0) (1.21.2)
Requirement already satisfied: protobuf<4,>=3.20.2 in /Users/josemuniz/miniforge3/envs/mlagents/lib/python3.10/site-packages (from onnx==1.13.0) (3.20.2)
Requirement already satisfied: typing-extensions>=3.6.2.1 in /Users/josemuniz/miniforge3/envs/mlagents/lib/python3.10/site-packages (from onnx==1.13.0) (4.9.0)
Installing collected packages: onnx
  Attempting uninstall: onnx
    Found existing installation: onnx 1.15.0
    Uninstalling onnx-1.15.0:
      Successfully uninstalled onnx-1.15.0
Successfully installed onnx-1.13.0
Requirement already satisfied: mlagents-envs==1.0.0 in ./ml-agents-envs (1.0.0)
Requirement already satisfied: cloudpickle in /Users/josemuniz/miniforge3/envs/mlagents/lib/python3.10/site-packages (from mlagents-envs==1.0.0) (3.0.0)
Requirement already satisfied: grpcio<=1.48.2,>=1.11.0 in /Users/josemuniz/miniforge3/envs/mlagents/lib/python3.10/site-packages (from mlagents-envs==1.0.0) (1.48.2)
Requirement already satisfied: Pillow>=4.2.1 in /Users/josemuniz/miniforge3/envs/mlagents/lib/python3.10/site-packages (from mlagents-envs==1.0.0) (10.1.0)
Collecting protobuf<3.20,>=3.6 (from mlagents-envs==1.0.0)
  Using cached protobuf-3.19.6-py2.py3-none-any.whl (162 kB)
Requirement already satisfied: pyyaml>=3.1.0 in /Users/josemuniz/miniforge3/envs/mlagents/lib/python3.10/site-packages (from mlagents-envs==1.0.0) (6.0.1)
Requirement already satisfied: gym>=0.21.0 in /Users/josemuniz/miniforge3/envs/mlagents/lib/python3.10/site-packages (from mlagents-envs==1.0.0) (0.26.2)
Requirement already satisfied: pettingzoo==1.15.0 in /Users/josemuniz/miniforge3/envs/mlagents/lib/python3.10/site-packages (from mlagents-envs==1.0.0) (1.15.0)
Requirement already satisfied: numpy==1.21.2 in /Users/josemuniz/miniforge3/envs/mlagents/lib/python3.10/site-packages (from mlagents-envs==1.0.0) (1.21.2)
Requirement already satisfied: filelock>=3.4.0 in /Users/josemuniz/miniforge3/envs/mlagents/lib/python3.10/site-packages (from mlagents-envs==1.0.0) (3.13.1)
Requirement already satisfied: six>=1.5.2 in /Users/josemuniz/miniforge3/envs/mlagents/lib/python3.10/site-packages (from grpcio<=1.48.2,>=1.11.0->mlagents-envs==1.0.0) (1.16.0)
Requirement already satisfied: gym-notices>=0.0.4 in /Users/josemuniz/miniforge3/envs/mlagents/lib/python3.10/site-packages (from gym>=0.21.0->mlagents-envs==1.0.0) (0.0.8)
Installing collected packages: protobuf
  Attempting uninstall: protobuf
    Found existing installation: protobuf 3.20.2
    Uninstalling protobuf-3.20.2:
      Successfully uninstalled protobuf-3.20.2
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
onnx 1.13.0 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.
Successfully installed protobuf-3.19.6
Collecting protobuf==3.20.2
  Using cached protobuf-3.20.2-py2.py3-none-any.whl (162 kB)
Installing collected packages: protobuf
  Attempting uninstall: protobuf
    Found existing installation: protobuf 3.19.6
    Uninstalling protobuf-3.19.6:
      Successfully uninstalled protobuf-3.19.6
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
mlagents-envs 1.0.0 requires protobuf<3.20,>=3.6, but you have protobuf 3.20.2 which is incompatible.
Successfully installed protobuf-3.20.2",think mac try install directly pip install pip install pip install incompatibility note requirement already satisfied requirement already satisfied requirement already satisfied collected found installation successfully uninstalled successfully requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied pillow requirement already satisfied requirement already satisfied gym requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied six requirement already satisfied gym collected found installation successfully uninstalled error pip dependency resolver currently take account behaviour source following dependency incompatible successfully collected found installation successfully uninstalled error pip dependency resolver currently take account behaviour source following dependency incompatible successfully,issue,positive,positive,positive,positive,positive,positive
1871975476,"Same with : ""Reward signal Rnd is not supported with the POCA trainer; results may be unexpected.""",reward signal trainer may unexpected,issue,positive,positive,neutral,neutral,positive,positive
1868858467,"That's a terrible solution @miguelalonsojr 
I cloned release_21 because that is what docs say to do.
Then if I try to fetch and merge or pull branch `develop` I get git error messages about merge conflicts.
So please edit the documentation to say pull the `develop` branch rather than release_21.  ",terrible solution say try fetch merge pull branch develop get git error merge please edit documentation say pull develop branch rather,issue,negative,negative,negative,negative,negative,negative
1854601261,"I re-installed everything and changed the numpy==1.23.0 line 63 in the setup.py file and then downloaded your updated sample and everything works until I get to the training phase of the example. It starts working fine, but then it looks like Unity locks up and the command line gives errors saying its waiting on Unity. The output is:
(mlagents) D:\Users\lokia\Documents\Unity\ML Agents 02 Prototype\ml-agents>mlagents-learn config/ppo/3DBall.yaml --run-id=first3DBallRun --force
D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\torch\__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)

            ┐  ╖
        ╓╖╬│╡  ││╬╖╖
    ╓╖╬│││││┘  ╬│││││╬╖
 ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗
 ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣
 ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣
 ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣
 ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜
 ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣
 ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣
   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣
      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜
          ╙╬╬╬╣╣╣╜
             ╙

 Version information:
  ml-agents: 1.0.0,
  ml-agents-envs: 1.0.0,
  Communicator API: 1.5.0,
  PyTorch: 2.1.1+cpu
D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\torch\__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
[INFO] Connected to Unity environment with package version 3.0.0-exp.1 and communication version 1.5.0
[INFO] Connected new brain: 3DBall?team=0
[WARNING] Deleting TensorBoard data events.out.tfevents.1702495905.Lokiare.42372.0 that was left over from a previous run.
[INFO] Hyperparameters for behavior name 3DBall:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   64
          buffer_size:  12000
          learning_rate:        0.0003
          beta: 0.001
          epsilon:      0.2
          lambd:        0.99
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       linear
          beta_schedule:        linear
          epsilon_schedule:     linear
        checkpoint_interval:    500000
        network_settings:
          normalize:    True
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       5
        even_checkpoints:       False
        max_steps:      500000
        time_horizon:   1000
        summary_freq:   12000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
[INFO] 3DBall. Step: 12000. Time Elapsed: 16.267 s. Mean Reward: 1.127. Std of Reward: 0.751. Training.
[WARNING] Restarting worker[0] after 'The Unity environment took too long to respond. Make sure that :
         The environment does not need user interaction to launch
         The Agents' Behavior Parameters > Behavior Type is set to ""Default""
         The environment and the Python interface have compatible versions.
         If you're running on a headless server without graphics support, turn off display by either passing --no-graphics option or build your Unity executable as server build.'
D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\torch\__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\torch\csrc\tensor\python_tensor.cpp:453.)
  _C._set_default_tensor_type(t)
[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
[ERROR] UnityEnvironment worker 0: environment raised an unexpected exception.
Traceback (most recent call last):
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\mlagents\trainers\subprocess_env_manager.py"", line 163, in worker
    env = env_factory(worker_id, side_channels)
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\mlagents\trainers\learn.py"", line 189, in create_unity_environment
    return UnityEnvironment(
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\mlagents_envs\environment.py"", line 242, in __init__
    if not UnityEnvironment._check_communication_compatibility(
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\mlagents_envs\environment.py"", line 96, in _check_communication_compatibility
    if unity_communicator_version.version[0] == 0:
AttributeError: 'StrictVersion' object has no attribute 'version'
[INFO] Exported results\first3DBallRun\3DBall\3DBall-12102.onnx
[INFO] Copied results\first3DBallRun\3DBall\3DBall-12102.onnx to results\first3DBallRun\3DBall.onnx.
Traceback (most recent call last):
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""D:\Users\lokia\anaconda3\envs\mlagents\Scripts\mlagents-learn.exe\__main__.py"", line 7, in <module>
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\mlagents\trainers\learn.py"", line 267, in main
    run_cli(parse_command_line())
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\mlagents\trainers\learn.py"", line 263, in run_cli
    run_training(run_seed, options, num_areas)
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\mlagents\trainers\learn.py"", line 137, in run_training
    tc.start_learning(env_manager)
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\mlagents_envs\timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\mlagents\trainers\trainer_controller.py"", line 175, in start_learning
    n_steps = self.advance(env_manager)
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\mlagents_envs\timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\mlagents\trainers\trainer_controller.py"", line 233, in advance
    new_step_infos = env_manager.get_steps()
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\mlagents\trainers\env_manager.py"", line 124, in get_steps
    new_step_infos = self._step()
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\mlagents\trainers\subprocess_env_manager.py"", line 420, in _step
    self._restart_failed_workers(step)
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\mlagents\trainers\subprocess_env_manager.py"", line 328, in _restart_failed_workers
    self.reset(self.env_parameters)
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\mlagents\trainers\env_manager.py"", line 68, in reset
    self.first_step_infos = self._reset_env(config)
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\mlagents\trainers\subprocess_env_manager.py"", line 446, in _reset_env
    ew.previous_step = EnvironmentStep(ew.recv().payload, ew.worker_id, {}, {})
  File ""D:\Users\lokia\anaconda3\envs\mlagents\lib\site-packages\mlagents\trainers\subprocess_env_manager.py"", line 101, in recv
    raise env_exception
AttributeError: 'StrictVersion' object has no attribute 'version'",everything line file sample everything work get training phase example working fine like unity command line saying waiting unity output force please use triggered internally version information communicator please use triggered internally listening port start training pressing play button unity editor connected unity environment package version communication version connected new brain warning data left previous run behavior name beta epsilon false linear linear linear normalize true simple memory none hyper deterministic false extrinsic gamma strength normalize false simple memory none hyper deterministic false none false threaded false none none step time mean reward reward training warning worker unity environment took long respond make sure environment need user interaction launch behavior behavior type set default environment python interface compatible running headless server without graphic support turn display either passing option build unity executable server build please use triggered internally listening port start training pressing play button unity editor error worker environment raised unexpected exception recent call last file line worker file line return file line file line object attribute copied recent call last file line return code none file line code file line module file line main file line file line file line wrapped return file line file line wrapped return file line advance file line file line step file line file line reset file line file line raise object attribute,issue,positive,negative,neutral,neutral,negative,negative
1849176623,Updated my comment to say numpy 1.23.1 worked best.  Thanks! ,comment say worked best thanks,issue,positive,positive,positive,positive,positive,positive
1848435881,"Faced similar issue, also had onnx related issues when trying to stop training and save (ctrl-c). What work for me was: 
`pip install onnx==1.13`. 
Saw errors : ""ERROR: pip's dependency resolver does not currently..."" but onnx successfully installed and now the models save (onnx files get created).",faced similar issue also related trying stop training save work pip install saw error pip dependency resolver currently successfully save get,issue,positive,positive,positive,positive,positive,positive
1847960232,"I'm getting the same issue and I've followed the instructions to install everything successfully. The errors don't show up until you install the ball sample, but they are in a different directory (see first image of issues thread).
I'm on:
Unity 2022.3.6f1
Windows 11
",getting issue install everything successfully show install ball sample different directory see first image thread unity,issue,negative,positive,positive,positive,positive,positive
1834353914,"I would suggest creating a separate training environment that bypasses much of the manual configuration, e.g. menu selection, etc. The procedural map gen and env setup should happen before the academy script starts to step. You can change the execution order of any script in the project settings: https://docs.unity3d.com/Manual/class-MonoManager.html  The env setup scripts should come first, and then the MLA scripts. You can also extend the timeout of the mlagents-learn command in the training config file under env_settings: https://unity-technologies.github.io/ml-agents/Training-ML-Agents/#environment-settings. You can even spawn agents, you just have to make sure that everything is done before the Agent script is executed for the first time. You can even try to mess with the academy, by adding a script before the MLA scripts that pauses the Academy stepping by setting Academy.AutomaticSteppingEnabled = false. Once things are ready, you can then have another script to enable the academy. You should check out the Unity event lifecycle docs, they may also be helpful in understanding the even execution loop. https://docs.unity3d.com/Manual/ExecutionOrder.html",would suggest separate training environment much manual configuration menu selection procedural map gen setup happen academy script step change execution order script project setup come first also extend command training file even spawn make sure everything done agent script executed first time even try mess academy script academy stepping setting false ready another script enable academy check unity event may also helpful understanding even execution loop,issue,positive,positive,positive,positive,positive,positive
1834297646,"Hi @miguelalonsojr ! Thank you very much for your response. I'm building on top of a large Unity project (originally a game for Steam) where there is a menu from which you start the game. When that option is selected, there is a process of generating the procedural map, which is quite big, the navmesh, spawning the player, and connecting with the servers... I have added MLA on top to spawn an AI player and I am trying to train it in the game environment. However, the player spawning process happens after maybe 2 seconds, and it's where I have programmatically added the BehaviourParameters and DecisionRequester components, but the mlagents-learn pipeline has already defined an empty model at that point. I understand that MLA is not meant to be modified at runtime, and I am now attempting this trick of having a GameObject deactivated in the scene and activating it later. However, it is challenging since many components for the GameObject need to be added after the world is generated...",hi thank much response building top large unity project originally game steam menu start game option selected process generating procedural map quite big spawning player added top spawn ai player trying train game environment however player spawning process maybe programmatically added pipeline already defined empty model point understand meant trick scene later however since many need added world,issue,negative,positive,neutral,neutral,positive,positive
1834203622,"How long after the game begins does the agent spawn? Typically, you can train the agent in a training environment to build a policy and then use that policy in the actual game environment. The way MLA is architected is such  that for training, the assumption is that all agents are present in the game. There are workarounds such as stepping the academy manually or having the agent present in the environment but disabled (you can then instead of spawning a new object, just enable it), etc etc. You don't necessarily have to train in the same environment as the final game, as long as the agent is configured the same (i.e. same obs and action space) and the environment is more or less the same in terms of tasks/reward, then you can train the agent in one scene (env) and use it in another, where you can control things like spawning etc. You still might have the situation where you will need to pause the academy from stepping or stepping it manually. This isn't a bug. I'm curious, what's your environment or use case?",long game agent spawn typically train agent training environment build policy use policy actual game environment way training assumption present game stepping academy manually agent present environment disabled instead spawning new object enable necessarily train environment final game long agent action space environment le train agent one scene use another control like spawning still might situation need pause academy stepping stepping manually bug curious environment use case,issue,positive,negative,negative,negative,negative,negative
1827063452,"> Oh, I see what the issue is. The 3DBall example package is stale and needs to be updated. I can take care of that. Thanks for submitting the issue.

Thanks for your hard work ^_^",oh see issue example package stale need take care thanks issue thanks hard work,issue,positive,negative,neutral,neutral,negative,negative
1827062676,"> I had the same issue. But @benzhangdragonplus, did uncheck Barracuda package resolve the issue?
> 
> For me I think what I did was to install the Barracuda 3.0.0 from package manager and remove the ml-agents.extensions package. Not sure which of those actually helped, but it got me moving forward.

**replay：**
Unchecking doesn't work either. You can use the release 20 version directly, and there is currently no problem. Secondly, the reason for the error is that the 3DBall example has not been updated, while other examples such as match-3 have no issues
",issue uncheck barracuda package resolve issue think install barracuda package manager remove package sure actually got moving forward work either use release version directly currently problem secondly reason error example,issue,negative,positive,positive,positive,positive,positive
1826885403,"I have to do more tests, but it seems not distinguish between ""Detectable tags"" or doesn't take them in account at all. I will report back, when I have a proper min. repr. example. Be patient with me. ",distinguish detectable take account report back proper min example patient,issue,negative,neutral,neutral,neutral,neutral,neutral
1826874849,"What you can do is use a buffer sensor, which allows you to add variable length observations and use an attention block to process the input. This is configured automatically if you just add a buffer sensor to the agent. It does have a maximum size. https://unity-technologies.github.io/ml-agents/Learning-Environment-Design-Agents/#variable-length-observations. And yes, rewards a scalar float, which can be negative. Closing as the buffer sensor is the solution for variable length observations.",use buffer sensor add variable length use attention block process input automatically add buffer sensor agent maximum size yes scalar float negative buffer sensor solution variable length,issue,negative,negative,negative,negative,negative,negative
1826872441,"On second thought, I'll keep this open. I might noodle around some of these over the holidays.",second thought keep open might noodle around,issue,negative,neutral,neutral,neutral,neutral,neutral
1826870957,"I had the same issue. But @benzhangdragonplus, did uncheck Barracuda package resolve the issue?

For me I think what I did was to install the Barracuda 3.0.0 from package manager and remove the ml-agents.extensions package. Not sure which of those actually helped, but it got me moving forward.",issue uncheck barracuda package resolve issue think install barracuda package manager remove package sure actually got moving forward,issue,positive,positive,positive,positive,positive,positive
1826863409,"> I've met the same problem, and confused the whole day (cry~).
> 
> Hope I've seen this issue early. Anyway, looking for your update~~

Thanks for your understanding. I'm the only maintainer of ML-Agents currently, so things are bound to slip through the cracks. I should have this update done sometime this week. Thanks for reaching out.",met problem confused whole day hope seen issue early anyway looking thanks understanding maintainer currently bound slip update done sometime week thanks reaching,issue,positive,positive,neutral,neutral,positive,positive
1826863047,"I've met the same problem, and confused the whole day (cry~).

Hope I've seen this issue early.  Anyway, looking for your update~~",met problem confused whole day hope seen issue early anyway looking,issue,negative,negative,neutral,neutral,negative,negative
1826791493,"Oh, I see what the issue is. The 3DBall example package is stale and needs to be updated. I can take care of that. Thanks for submitting the issue.",oh see issue example package stale need take care thanks issue,issue,positive,negative,negative,negative,negative,negative
1826772087,"> > Did you upgrade from a previous version of MLA? If so, It may be that you need to delete your package cache as it might be causing conflicts. Barracuda was deprecated with the last release. Seems like you have an old version of MLA that is causing conflicts. You can see here that there's no mention of Barracuda in the BehaviorParameters script: https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Runtime/Policies/BehaviorParameters.cs
> 
> Thank you for your reply！！！ Yes, the source code does not have Barracuda, but when importing the 3D Balance Ball example, the package has the Barracuda option. Do I need to uncheck it? like this： ![1701001741997](https://private-user-images.githubusercontent.com/112932125/285650660-27d53a46-1b68-44f7-a1e8-d98201f03eaf.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDEwMDIwNzAsIm5iZiI6MTcwMTAwMTc3MCwicGF0aCI6Ii8xMTI5MzIxMjUvMjg1NjUwNjYwLTI3ZDUzYTQ2LTFiNjgtNDRmNy1hMWU4LWQ5ODIwMWYwM2VhZi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBSVdOSllBWDRDU1ZFSDUzQSUyRjIwMjMxMTI2JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIzMTEyNlQxMjI5MzBaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1kNTU5OTQwNDdiNGUyYzczNzE2MzU2YmYwMmQ2Yjk5MmFhMjFiZjkxN2E5MzQ4YTE3OTc3NjNmMDY0MDlkNTFlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.iFXmS5y4CSURyMNd0SU7aiQn7exAhVDqJoMmSU37ZFY)

By the way, when I import the examples, the source code content will also change and Barracuda will be automatically added
![1701001854951](https://github.com/Unity-Technologies/ml-agents/assets/112932125/6525ce91-a8ba-482b-b0c3-efd43ad339de)
",upgrade previous version may need delete package cache might causing barracuda last release like old version causing see mention barracuda script thank yes source code barracuda balance ball example package barracuda option need uncheck like way import source code content also change barracuda automatically added,issue,positive,negative,neutral,neutral,negative,negative
1826771227,"> Did you upgrade from a previous version of MLA? If so, It may be that you need to delete your package cache as it might be causing conflicts. Barracuda was deprecated with the last release. Seems like you have an old version of MLA that is causing conflicts. You can see here that there's no mention of Barracuda in the BehaviorParameters script: https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Runtime/Policies/BehaviorParameters.cs

Thank you for your reply！！！
Yes, the source code does not have Barracuda, but when importing the 3D Balance Ball example, the package has the Barracuda option. Do I need to uncheck it?
like this：
![1701001741997](https://github.com/Unity-Technologies/ml-agents/assets/112932125/27d53a46-1b68-44f7-a1e8-d98201f03eaf)
",upgrade previous version may need delete package cache might causing barracuda last release like old version causing see mention barracuda script thank yes source code barracuda balance ball example package barracuda option need uncheck like,issue,positive,negative,neutral,neutral,negative,negative
1826759738,Ah. I see it now. Thanks for pointing this out. Usually broken image link bugs are from folks using the docs directly on GH. Will reopen and fix.,ah see thanks pointing usually broken image link directly reopen fix,issue,negative,negative,neutral,neutral,negative,negative
1826759140,"Nothing planned for now. MLA is in maintenance mode for now. Minor bug fixes, dependency updates, minor features. We are, however, accepting PRs, so if anyone from the community would like to make this upgrade and submit a PR, that would be amazing.",nothing maintenance mode minor bug dependency minor however anyone community would like make upgrade submit would amazing,issue,positive,positive,positive,positive,positive,positive
1826758599,"Thanks for the detailed analysis. Can you submit a PR to add this note in the docs when using ""threaded""?",thanks detailed analysis submit add note threaded,issue,negative,positive,positive,positive,positive,positive
1826757924,Please try the MLA setup with the develop branch of the repo instead of the release 21 branch. It's been fixed on develop. Please let me know if it worked out.,please try setup develop branch instead release branch fixed develop please let know worked,issue,positive,positive,neutral,neutral,positive,positive
1826757565,Try to the installation again with the develop branch instead of the release_21 branch. This issue was fixed on develop.,try installation develop branch instead branch issue fixed develop,issue,negative,positive,neutral,neutral,positive,positive
1826757276,"Did you upgrade from a previous version of MLA? If so, It may be that you need to delete your package cache as it might be causing conflicts. Barracuda was deprecated with the last release. Seems like you have an old version of MLA that is causing conflicts. You can see here that there's no mention of Barracuda in the BehaviorParameters script: https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/Runtime/Policies/BehaviorParameters.cs",upgrade previous version may need delete package cache might causing barracuda last release like old version causing see mention barracuda script,issue,negative,negative,neutral,neutral,negative,negative
1826748338,"After changing setup.py to:
1.24.3 or 1.22.4 (I have numpy 1.24.3), I got this:
```
ERROR: Ignored the following versions that require a different python version: 1.10.0 Requires-Python >=3.6, <3.10; 1.11.0 Requires-Python >=3.6, <3.10; 1.11.1 Requires-Python >=3.6, <3.10; 1.11.2 Requires-Python >=3.6, <3.10; 1.12.0 Requires-Python >=3.6, <3.10; 1.13.0 Requires-Python >=3.7, <3.10; 1.13.1 Requires-Python >=3.7, <3.10; 1.14.0 Requires-Python >=3.7, <3.11; 1.15.0 Requires-Python >=3.7, <3.11; 1.16.0 Requires-Python >=3.7, <3.11; 1.17.0 Requires-Python >=3.7, <3.11; 1.18.1 Requires-Python >=3.7, <3.11; 1.19.0 Requires-Python >=3.7, <3.11; 1.20.1 Requires-Python >=3.7, <3.11; 1.21.0 Requires-Python >=3.7, <3.11; 1.22.0 Requires-Python >=3.7, <3.11; 1.22.1 Requires-Python >=3.7, <3.11; 1.22.2 Requires-Python >=3.7, <3.11; 1.4.0 Requires-Python >=3.6, <3.9; 1.4.1 Requires-Python >=3.6, <3.9; 1.4.2 Requires-Python >=3.6, <3.9; 1.5.0 Requires-Python >=3.6, <3.10; 1.5.1 Requires-Python >=3.6, <3.10; 1.5.2 Requires-Python >=3.6, <3.10; 1.6.0 Requires-Python >=3.6, <3.10; 1.6.1 Requires-Python >=3.6, <3.10; 1.7.0 Requires-Python >=3.6, <3.10; 1.8.0 Requires-Python >=3.6, <3.10; 1.8.1 Requires-Python >=3.6, <3.10; 1.8.2 Requires-Python >=3.6, <3.10; 1.9.0 Requires-Python >=3.6, <3.10
ERROR: Could not find a version that satisfies the requirement pettingzoo==1.15.0 (from mlagents-envs) (from versions: 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.1.10, 0.1.11, 0.1.12, 0.1.13, 0.1.14, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.24.0, 1.24.1, 1.24.2)
ERROR: No matching distribution found for pettingzoo==1.15.0
```",got error following require different python version error could find version requirement error matching distribution found,issue,negative,neutral,neutral,neutral,neutral,neutral
1826742785,I am still interest in this being implemented. Please reopen.,still interest please reopen,issue,positive,neutral,neutral,neutral,neutral,neutral
1826581384,"If you installed grpcio to get protoc in order to compile ONNX, I don't think it's necessary anymore with the above changes to setup.py as seems like pip then uses a pre-compiled ONNX. I'm not using conda though, just pip in a pyenv-virtualenv, so maybe your setup is different.",get order compile think necessary like pip though pip maybe setup different,issue,negative,neutral,neutral,neutral,neutral,neutral
1826484133,"> Mac OS 13.5 user here. Had same issue with ONNX yesterday and your suggested fixes worked for me.
> 
> P.S - I also had uninstall the grpcio installed through pip since it brings an incomptible version with M2 macs, and instead install a different variant with anaconda `conda install grpcio` (with channel set to conda-forge) instead. But that seems to be a separate issue.

`conda install grpcio` helped!",mac o user issue yesterday worked also pip since version instead install different variant anaconda install channel set instead separate issue install,issue,negative,neutral,neutral,neutral,neutral,neutral
1826470659,"On the page you just linked @miguelalonsojr, if you click on the ""Toolkit Documentation"" on the left and then ""Making a New Learning Environment"" in the center, you reach the page I linked to with broken images.",page linked click documentation left making new learning environment center reach page linked broken,issue,negative,negative,neutral,neutral,negative,negative
1826470515,"Got it. I misunderstood and thought you were replicating crawler from scratch. This is a known bug with the Unity editor inspector component. If you close the scene and reopen it, the hierarchy should appear in the sensor.",got misunderstood thought crawler scratch known bug unity editor inspector component close scene reopen hierarchy appear sensor,issue,negative,neutral,neutral,neutral,neutral,neutral
1826469756,The doc markdown files are made for our webdocs. Image links are coded for mkdocs. Have a look at the docs here: https://unity-technologies.github.io/ml-agents/,doc markdown made image link look,issue,negative,neutral,neutral,neutral,neutral,neutral
1825614603,"Mac OS 13.5 user here.  Had same issue with ONNX yesterday and your suggested fixes worked for me. 

P.S - I also had uninstall the grpcio installed through pip since it brings an incomptible version with M2 macs, and  instead install a different variant with anaconda  `conda install grpcio` (with channel set to conda-forge) instead.  But that seems to be a separate issue.",mac o user issue yesterday worked also pip since version instead install different variant anaconda install channel set instead separate issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1825220893,"Im not sure if my understanding of example environment is wrong, but isn't the crawler one of the example environment? The bug is simply the rigid body sensor component not assigning the root body properly to the hierarchy.

So basically:
1. Just open up the crawler environment
2. Open up the CrawlerBase prefab
3. Remove the Rigid Body Sensor Component from the inspector
4.  Put a new rigid Body Sensor Component
5. Assign the root body with the body of the crawler
6. The hierarchy on the component won't assign the other connected joints (which is my current problem)

There's no other thing that I added to replicate this bug, maybe other than the fact that ML Agent Extension needed to be installed to the package manager also.

I hope maybe an explanation could be given on to why its behaving like that, or maybe just that my way is not the correct way to assign that component like that.",sure understanding example environment wrong crawler one example environment bug simply rigid body sensor component root body properly hierarchy basically open crawler environment open prefab remove rigid body sensor component inspector put new rigid body sensor component assign root body body crawler hierarchy component wo assign connected current problem thing added replicate bug maybe fact agent extension package manager also hope maybe explanation could given like maybe way correct way assign component like,issue,negative,positive,neutral,neutral,positive,positive
1824265437,I got this problem today too. Thank you ! I think it is a mistake in the setup.py.,got problem today thank think mistake,issue,negative,neutral,neutral,neutral,neutral,neutral
1823535887,@miguelalonsojr please update the setup.py in ml-agents/ml-agent-envs/ folder - thank you!,please update folder thank,issue,positive,neutral,neutral,neutral,neutral,neutral
1823521941,"After running >mlagents-learn --help
I get this error (NumPy doesn't initialize - do you have the same issue?):

[W ..\torch\csrc\utils\tensor_numpy.cpp:77] Warning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (function operator ())",running help get error initialize issue warning initialize module version version function operator,issue,negative,neutral,neutral,neutral,neutral,neutral
1823333042,"Bumping the required version of numpy in the ml-agents\ml-agents-envs\setup file to 1.23.1 seems to have worked:

Before:
    install_requires=[
        ""cloudpickle"",
        ""grpcio>=1.11.0,<=1.48.2"",
        ""Pillow>=4.2.1"",
        ""protobuf>=3.6,<3.20"",
        ""pyyaml>=3.1.0"",
        ""gym>=0.21.0"",
        ""pettingzoo==1.15.0"",
        ""numpy==1.21.2"",
        ""filelock>=3.4.0"",
    ],

After:
    install_requires=[
        ""cloudpickle"",
        ""grpcio>=1.11.0,<=1.48.2"",
        ""Pillow>=4.2.1"",
        ""protobuf>=3.6,<3.20"",
        ""pyyaml>=3.1.0"",
        ""gym>=0.21.0"",
        ""pettingzoo==1.15.0"",
        ""numpy==1.23.1"",
        ""filelock>=3.4.0"",
    ],",bumping version file worked pillow gym pillow gym,issue,negative,neutral,neutral,neutral,neutral,neutral
1823156424,"Okay i will try it, thanks. If it will not work I will just wait for a more ""official"" upgrade, thanks for help.",try thanks work wait official upgrade thanks help,issue,positive,positive,positive,positive,positive,positive
1822783490,"We are unable to help reproduce bugs with custom environments.  Please attempt to reproduce your issue with one of the example environments, or provide a minimal patch to one of the environments needed to reproduce the issue.",unable help reproduce custom please attempt reproduce issue one example provide minimal patch one reproduce issue,issue,positive,negative,negative,negative,negative,negative
1822777982,"There are no plans to add any of these algorithms, but thank you for the suggestion. Feel free to submit a PR that adds these, as we are accepting community PRs.",add thank suggestion feel free submit community,issue,positive,positive,positive,positive,positive,positive
1822775089,I'm working on an upgrade to pytorch 2.1.1 that should land soon on develop. It should fix this issue. Here's the upgrade branch: https://github.com/Unity-Technologies/ml-agents/tree/develop-pytorch-2,working upgrade land soon develop fix issue upgrade branch,issue,negative,neutral,neutral,neutral,neutral,neutral
1822772657,"Please see the note: We are unable to help reproduce bugs with custom environments. Please attempt to reproduce your issue with one of the example environments, or provide a minimal patch to one of the environments needed to reproduce the issue.

Please try to reproduce with one of the example environments.",please see note unable help reproduce custom please attempt reproduce issue one example provide minimal patch one reproduce issue please try reproduce one example,issue,positive,negative,negative,negative,negative,negative
1822144459,using conda and python 3.10.12 I managed to make it work with some minor modifications. Refer to: https://github.com/Unity-Technologies/ml-agents/issues/6016,python make work minor refer,issue,negative,negative,neutral,neutral,negative,negative
1821805709,"> What python version are you running?

3.11 because  in docs is ""Install Python 3.10.12 or Higher"" and 3.10.12 is not accesable (3.10.13 too): 

""No installers

According to the release calendar specified in [PEP 619](https://www.python.org/dev/peps/pep-0619/), Python 3.10 is now in the ""security fixes only"" stage of its life cycle: 3.10 branch only accepts security fixes and releases of those are made irregularly in source-only form until October 2026. Python 3.10 isn't receiving regular bug fixes anymore, and binary installers are no longer provided for it. Python 3.10.11 was the last full bugfix release of Python 3.10 with binary installers.""

and I want to be near it to not cause any problems or sth",python version running install python higher according release calendar pep python security stage life cycle branch security made irregularly form python regular bug binary longer provided python last full release python binary want near cause,issue,positive,positive,positive,positive,positive,positive
1815626946,"Im also having the same issue, however changing the numpy version in the setup.py file to numpy = 1.21.6 worked to alleviate the issue but runs into a warning that it fails to initialise.",also issue however version file worked alleviate issue warning,issue,negative,neutral,neutral,neutral,neutral,neutral
1815607844,"Workaround:
Changed 
`""numpy==1.21.2"",`
to
`""numpy==1.23.3"",`

in the file path-to-ml-agents\ml-agents-envs\setup.py

numpy 1.21.2 is not compatible with python 3.10.12 (I dont see a cp310 for win64 bit: https://pypi.org/project/numpy/1.21.2/#files)

numpy 1.23.3 works fine with python 3.10.12

note: with numpy 1.24, I got a float error with numpy which led to this:

> NumPy 1.20 (release notes) deprecated numpy.float, numpy.int, and similar aliases, causing them to issue a deprecation warning
> NumPy 1.24 (release notes) removed these aliases altogether, causing an error when they are used.
> 

final pip list

```
Package                 Version
----------------------- ------------
absl-py                 2.0.0
attrs                   23.1.0
cachetools              5.3.2
cattrs                  1.5.0
certifi                 2023.7.22
charset-normalizer      3.3.2
cloudpickle             3.0.0
colorama                0.4.6
filelock                3.13.1
fsspec                  2023.10.0
google-auth             2.23.4
google-auth-oauthlib    1.1.0
grpcio                  1.48.2
gym                     0.26.2
gym-notices             0.0.8
h5py                    3.10.0
huggingface-hub         0.19.4
idna                    3.4
Markdown                3.5.1
MarkupSafe              2.1.3
mlagents                1.0.0
mlagents-envs           1.0.0
numpy                   1.23.3
oauthlib                3.2.2
onnx                    1.12.0
packaging               23.2
PettingZoo              1.15.0
Pillow                  10.1.0
pip                     23.3.1
protobuf                3.19.6
pyasn1                  0.5.0
pyasn1-modules          0.3.0
pypiwin32               223
pywin32                 306
PyYAML                  6.0.1
requests                2.31.0
requests-oauthlib       1.3.1
rsa                     4.9
setuptools              65.5.0
six                     1.16.0
tensorboard             2.15.1
tensorboard-data-server 0.7.2
torch                   1.13.1+cu117
tqdm                    4.66.1
typing_extensions       4.8.0
urllib3                 2.1.0
Werkzeug                3.0.1
```
```

confirmation:

>mlagents-learn --help
usage: mlagents-learn.exe [-h] [--env ENV_PATH] [--resume] [--deterministic] [--force] [--run-id RUN_ID] [--initialize-from RUN_ID] [--seed SEED] [--inference] [--base-port BASE_PORT] [--num-envs NUM_ENVS]
                          [--num-areas NUM_AREAS] [--debug] [--env-args ...] [--max-lifetime-restarts MAX_LIFETIME_RESTARTS] [--restarts-rate-limit-n RESTARTS_RATE_LIMIT_N]
                          [--restarts-rate-limit-period-s RESTARTS_RATE_LIMIT_PERIOD_S] [--torch] [--tensorflow] [--results-dir RESULTS_DIR] [--timeout-wait TIMEOUT_WAIT] [--width WIDTH] [--height HEIGHT]
                          [--quality-level QUALITY_LEVEL] [--time-scale TIME_SCALE] [--target-frame-rate TARGET_FRAME_RATE] [--capture-frame-rate CAPTURE_FRAME_RATE] [--no-graphics] [--torch-device DEVICE]
                          [trainer_config_path]

positional arguments:
  trainer_config_path

options:
  -h, --help            show this help message and exit
  --env ENV_PATH        Path to the Unity executable to train (default: None)
  --resume              Whether to resume training from a checkpoint. Specify a --run-id to use this option. If set, the training code loads an already trained model to initialize the neural network before
                        resuming training. This option is only valid when the models exist, and have the same behavior names as the current agents in your scene. (default: False)
  --deterministic       Whether to select actions deterministically in policy. `dist.mean` for continuous action space, and `dist.argmax` for deterministic action space (default: False)
  --force               Whether to force-overwrite this run-id's existing summary and model data. (Without this flag, attempting to train a model with a run-id that has been used before will throw an
                        error. (default: False)
  --run-id RUN_ID       The identifier for the training run. This identifier is used to name the subdirectories in which the trained model and summary statistics are saved as well as the saved model
                        itself. If you use TensorBoard to view the training statistics, always set a unique run-id for each training run. (The statistics for all runs with the same id are combined as if
                        they were produced by a the same session.) (default: ppo)
  --initialize-from RUN_ID
                        Specify a previously saved run ID from which to initialize the model from. This can be used, for instance, to fine-tune an existing model on a new environment. Note that the
                        previously saved models must have the same behavior parameters as your current environment. (default: None)
  --seed SEED           A number to use as a seed for the random number generator used by the training code (default: -1)
  --inference           Whether to run in Python inference mode (i.e. no training). Use with --resume to load a model trained with an existing run ID. (default: False)
  --base-port BASE_PORT
                        The starting port for environment communication. Each concurrent Unity environment instance will get assigned a port sequentially, starting from the base-port. Each instance will
                        use the port (base_port + worker_id), where the worker_id is sequential IDs given to each instance from 0 to (num_envs - 1). Note that when training using the Editor rather than an
                        executable, the base port will be ignored. (default: 5005)
  --num-envs NUM_ENVS   The number of concurrent Unity environment instances to collect experiences from when training (default: 1)
  --num-areas NUM_AREAS
                        The number of parallel training areas in each Unity environment instance. (default: 1)
  --debug               Whether to enable debug-level logging for some parts of the code (default: False)
  --env-args ...        Arguments passed to the Unity executable. Be aware that the standalone build will also process these as Unity Command Line Arguments. You should choose different argument names if
                        you want to create environment-specific arguments. All arguments after this flag will be passed to the executable. (default: None)
  --max-lifetime-restarts MAX_LIFETIME_RESTARTS
                        The max number of times a single Unity executable can crash over its lifetime before ml-agents exits. Can be set to -1 if no limit is desired. (default: 10)
  --restarts-rate-limit-n RESTARTS_RATE_LIMIT_N
                        The maximum number of times a single Unity executable can crash over a period of time (period set in restarts-rate-limit-period-s). Can be set to -1 to not use rate limiting with
                        restarts. (default: 1)
  --restarts-rate-limit-period-s RESTARTS_RATE_LIMIT_PERIOD_S
                        The period of time --restarts-rate-limit-n applies to. (default: 60)
  --torch               (Removed) Use the PyTorch framework. (default: False)
  --tensorflow          (Removed) Use the TensorFlow framework. (default: False)
  --results-dir RESULTS_DIR
                        Results base directory (default: results)
  --timeout-wait TIMEOUT_WAIT
                        The period of time to wait on a Unity environment to startup for training. (default: 60)

Engine Configuration:
  --width WIDTH         The width of the executable window of the environment(s) in pixels (ignored for editor training). (default: 84)
  --height HEIGHT       The height of the executable window of the environment(s) in pixels (ignored for editor training) (default: 84)
  --quality-level QUALITY_LEVEL
                        The quality level of the environment(s). Equivalent to calling QualitySettings.SetQualityLevel in Unity. (default: 5)
  --time-scale TIME_SCALE
                        The time scale of the Unity environment(s). Equivalent to setting Time.timeScale in Unity. (default: 20)
  --target-frame-rate TARGET_FRAME_RATE
                        The target frame rate of the Unity environment(s). Equivalent to setting Application.targetFrameRate in Unity. (default: -1)
  --capture-frame-rate CAPTURE_FRAME_RATE
                        The capture frame rate of the Unity environment(s). Equivalent to setting Time.captureFramerate in Unity. (default: 60)
  --no-graphics         Whether to run the Unity executable in no-graphics mode (i.e. without initializing the graphics driver. Use this only if your agents don't use visual observations. (default: False)

Torch Configuration:
  --torch-device DEVICE
                        Settings for the default torch.device used in training, for example, ""cpu"", ""cuda"", or ""cuda:0"" (default: None)

```",file compatible python dont see win bit work fine python note got float error led release similar causing issue deprecation warning release removed altogether causing error used final pip list package version gym markdown pillow pip six torch confirmation help usage resume deterministic force seed seed inference torch width width height height device positional help show help message exit path unity executable train default none resume whether resume training specify use option set training code already trained model initialize neural network training option valid exist behavior current scene default false deterministic whether select policy continuous action space deterministic action space default false force whether summary model data without flag train model used throw error default false identifier training run identifier used name trained model summary statistic saved well saved model use view training statistic always set unique training run statistic id combined produced session default specify previously saved run id initialize model used instance model new environment note previously saved must behavior current environment default none seed seed number use seed random number generator used training code default inference whether run python inference mode training use resume load model trained run id default false starting port environment communication concurrent unity environment instance get assigned port sequentially starting instance use port sequential given instance note training editor rather executable base port default number concurrent unity environment collect training default number parallel training unity environment instance default whether enable logging code default false unity executable aware build also process unity command line choose different argument want create flag executable default none number time single unity executable crash lifetime set limit desired default maximum number time single unity executable crash period time period set set use rate limiting default period time default torch removed use framework default false removed use framework default false base directory default period time wait unity environment training default engine configuration width width width executable window environment editor training default height height height executable window environment editor training default quality level environment equivalent calling unity default time scale unity environment equivalent setting unity default target frame rate unity environment equivalent setting unity default capture frame rate unity environment equivalent setting unity default whether run unity executable mode without graphic driver use use visual default false torch configuration device default used training example default none,issue,negative,negative,negative,negative,negative,negative
1814778420,"thank you very much and i will try it soon



---Original---
From: ***@***.***&gt;
Date: Thu, Nov 16, 2023 23:55 PM
To: ***@***.***&gt;;
Cc: ***@***.******@***.***&gt;;
Subject: Re: [Unity-Technologies/ml-agents] ERROR: Failed building wheel fornumpy when installing ml-agents==1.0.0 (Issue #6008)




 
Just a heads up for anyone that's still stuck on this. The numpy version was incorrect and has been updated on the develop branch - I was told by a Unity employee and confirmed with this link on the ml-agents commit history.
 
f3dc8f6
 
—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you commented.Message ID: ***@***.***&gt;",thank much try soon date subject error building wheel issue anyone still stuck version incorrect develop branch told unity employee confirmed link commit history reply directly view id,issue,negative,positive,positive,positive,positive,positive
1814741015,"Just a heads up for anyone that's still stuck on this. The numpy version was incorrect and has been updated on the develop branch - I was told by a Unity employee and confirmed with this link on the ml-agents commit history.

https://github.com/Unity-Technologies/ml-agents/commit/f3dc8f615044c9226c7e7ed308e0aadc1def3b4d",anyone still stuck version incorrect develop branch told unity employee confirmed link commit history,issue,negative,positive,positive,positive,positive,positive
1812837882,"Or can you share me a YouTube video that tells how to download and setup mlagent, pytorch etc... but the newest version (realease) without this ""pytorch issue"" (that have a solution for this)",share video setup version without issue solution,issue,negative,neutral,neutral,neutral,neutral,neutral
1809695952,"Thank you very much! I have 4 fighter agents, but as soon as one fighter is destroyed, unity just freezes.  And the program don't throw any errors, it just stand still.  However, I solve the problem by referring to #228 . I'm just wondering why the agent script is set directly on the corresponding gameobject, and once an agent is destroyed (please note that there are other living agents in the environment), the program freezes.
",thank much fighter soon one fighter unity program throw stand still however solve problem wondering agent script set directly corresponding agent please note living environment program,issue,positive,positive,positive,positive,positive,positive
1808141447,"I fixed it! Just don't execute follow code:
![图片](https://github.com/Unity-Technologies/ml-agents/assets/48547245/e5818432-93a0-420c-b1f8-11d992c3cd53)
Because this pytorch version is incompile with numpy==1.22.4, which is the mlagents-env's request.",fixed execute follow code version request,issue,negative,positive,neutral,neutral,positive,positive
1808004357,"Hey, I'm not an expert on MLAgents so this might be wrong, but it could be because of this reason : https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Design-Agents.md (scroll down to destroying agents).

You need at least on agent at all times, perhaps that's the issue? Otherwise, send the error messages you were getting before and I can try to help.",hey expert might wrong could reason scroll need least agent time perhaps issue otherwise send error getting try help,issue,negative,negative,negative,negative,negative,negative
1807823802,"Thank you for your reply, and I agree with you. I once set the Agent script directly on the fighter gameobject, which caused the program to freeze once the fighter gameobject was destroyed. Now that I set the fighter gameobject as a child of the empty object and set the Agent script on the empty object, then the problem is solved. But why would dynamically destroying and generating agents directly cause the program to crash? #228 ",thank reply agree set agent script directly fighter program freeze fighter set fighter child empty object set agent script empty object problem would dynamically generating directly cause program crash,issue,negative,neutral,neutral,neutral,neutral,neutral
1807155242,"You have already found a solution, I did it. it may not be perfect and there may be some inconsistencies, but I got it. Maybe in the future it won't work, but for now it does.
I used:

1. Copilot 
2. Installation documentation https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Installation.md
3. Video of a youtuber https://www.youtube.com/watch?v=RANRz9oyzko
4. This chat.

(read all the way to the end and then try it)

Anyway, I did all this on python 3.9.13, this method works in both anaconda and venv (it's about the same, but I said anyway to make people more comfortable). After creating the environment you can upgrade pip (currently it works with or without upgrading) but I've always upgraded it so here's the pip upgrade code (if you need it):
>`python -m pip install --upgrade pip`

And now you can start installing PyTorch with this command (this command is taken from the documentation):
>`pip3 install torch~=1.13.1 -f https://download.pytorch.org/whl/torch_stable.html`

It doesn't work the usual way, so it's better to use this one.
Then we need to install Protobuf version 3.20.3 (I've seen it in the video, I haven't tried other versions, but it works with this version):
>`pip install protobuf==3.20.3`

And now you can install ML Agents itself, this is done with the command:
>`pip install mlagents`

Yes, I just installed mlagents from Pypi, because mlagents from the repository doesn't allow installation if python version is lower than python 3.10, and it doesn't install on python 3.10. The commands that install PyTorch, ProtoBuf and ML Agents can be installed scattered and will still work (if you install everything). And tadam, now in theory it should work, you can check it by running the command:
>`mlagents-learn -h`

If you see the list of commands, you have done everything correctly and everything works.
I tried several variants, several download combinations and came to this.
I hope I have helped someone with this.

Translated with www.DeepL.com/Translator (free version) 😀
(Sorry if you see any spelling or logical errors.)",already found solution may perfect may got maybe future wo work used copilot installation documentation video chat read way end try anyway python method work anaconda said anyway make people comfortable environment upgrade pip currently work without always pip upgrade code need python pip install upgrade pip start command command taken documentation pip install work usual way better use one need install version seen video tried work version pip install install done command pip install yes repository allow installation python version lower python install python install scattered still work install everything theory work check running command see list done everything correctly everything work tried several several came hope someone free version sorry see spelling logical,issue,positive,positive,positive,positive,positive,positive
1806847322,"it's hard to say when you haven't sent any code? this is obviously a wild shot but I'd guess that you have a reference to one of the fighters in your code somewhere and you're trying to call a method / access a property from it but since the fighter has been destroyed now, it produces an exception (because you're trying to access a game object that hasn't been destroyed). But that's just a wild guess, send your exceptions / snippets of your code here. ",hard say sent code obviously wild shot guess reference one code somewhere trying call method access property since fighter exception trying access game object wild guess send code,issue,negative,negative,negative,negative,negative,negative
1806846980,"This doesn't related to MLAgents but you can find some tips here: https://stackoverflow.com/questions/72340379/vs-code-doesnt-recognise-using-unityengine-as-a-valid-namespace-what-can-i-d

```
Delete all .csproj files on the project's root.

Then edit->preferences->external tools-> Regenerate project files.

On VSCode F1->Reload Window
```

If that doesn't work, try recreating another project and moving the files or perhaps you didn't install something in Unity properly.",related find delete project root external regenerate project reload window work try another project moving perhaps install something unity properly,issue,negative,neutral,neutral,neutral,neutral,neutral
1806718898,I faced this question too. i tried py3.11 but not success yet,faced question tried success yet,issue,positive,positive,positive,positive,positive,positive
1806412383,oh yeah. i cloned release_20 instead. as far as i know thats how you install that early of a numpy version though. weird that 21 requires a different py version,oh yeah instead far know thats install early version though weird different version,issue,negative,negative,neutral,neutral,negative,negative
1806389352,"> fixed it by downgrading to python 3.9.13

Are you trying to install Release 21? Aka ml-agents==1.0.0? 

Because I attempted on python 3.9.13 but am still getting this error.

```
(mlagents_python_3_9_13) D:\MProjects\AIsland\ml-agents-master>pip install ./ml-agents
Processing d:\mprojects\aisland\ml-agents-master\ml-agents
  Preparing metadata (setup.py) ... done
Collecting grpcio<=1.48.2,>=1.11.0 (from mlagents==1.0.0)
  Downloading grpcio-1.48.2-cp39-cp39-win_amd64.whl (3.6 MB)
     ---------------------------------------- 3.6/3.6 MB 15.3 MB/s eta 0:00:00
Collecting h5py>=2.9.0 (from mlagents==1.0.0)
  Downloading h5py-3.10.0-cp39-cp39-win_amd64.whl.metadata (2.5 kB)
INFO: pip is looking at multiple versions of mlagents to determine which version is compatible with other requirements. This could take a while.
ERROR: Ignored the following versions that require a different python version: 0.10.0.dev0 Requires-Python >=3.5,<3.8; 0.6.0 Requires-Python >=3.5,<=3.7; 0.6.1 Requires-Python >=3.5,<=3.7; 0.6.2 Requires-Python >=3.5,<=3.7; 0.8.0 Requires-Python >=3.5,<3.8; 0.8.1 Requires-Python >=3.5,<3.8; 0.8.2 Requires-Python >=3.5,<3.8; 0.9.0 Requires-Python >=3.5,<3.8; 0.9.1 Requires-Python >=3.5,<3.8; 0.9.2 Requires-Python >=3.5,<3.8; 0.9.3 Requires-Python >=3.5,<3.8; 1.0.0 Requires-Python >=3.10.1,<=3.10.12
ERROR: Could not find a version that satisfies the requirement mlagents_envs==1.0.0 (from mlagents) (from versions: 0.10.0.dev1, 0.10.0, 0.10.1, 0.11.0.dev0, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.18.0, 0.18.1, 0.19.0, 0.20.0, 0.21.0, 0.21.1, 0.22.0, 0.23.0, 0.24.0, 0.24.1, 0.25.0, 0.25.1, 0.26.0, 0.27.0, 0.28.0, 0.29.0, 0.30.0)
ERROR: No matching distribution found for mlagents_envs==1.0.0
```

```
1.0.0 Requires-Python >=3.10.1,<=3.10.12
```",fixed python trying install release aka python still getting error pip install done eta pip looking multiple determine version compatible could take error following require different python version dev error could find version requirement dev dev error matching distribution found,issue,negative,positive,neutral,neutral,positive,positive
1805891001,"getting this aswell. im using
python -m pip install ./ml-agents-envs
python -m pip install ./ml-agents
though.",getting aswell python pip install python pip install though,issue,negative,neutral,neutral,neutral,neutral,neutral
1793750644,"Hi @bikcrum , it looks like the modification doesn't work anymore with the latest release.  Could you provide an updated one(to  use M1 GPU)? Thanks.
Here is the version information:

 Version information:
  ml-agents: 1.1.0.dev0,
  ml-agents-envs: 1.1.0.dev0,
  Communicator API: 1.5.0,
  PyTorch: 2.1.0


",hi like modification work latest release could provide one use thanks version information version information dev dev communicator,issue,positive,positive,positive,positive,positive,positive
1784118211,"Hi, I need this to implement difficulty scaling on an agent and would like to manually modify how the action gets sampled from the output of the neural network. I could not find how to access the action probabilities directly. 

As the issue is closed: Has this been implemented in the recent version of ML-Agents?",hi need implement difficulty scaling agent would like manually modify action output neural network could find access action directly issue closed recent version,issue,negative,positive,neutral,neutral,positive,positive
1784072398,"The images links are for generating mkdocs, not for viewing on GH. Please use our webdocs: https://unity-technologies.github.io/ml-agents/Background-Machine-Learning/",link generating please use,issue,negative,neutral,neutral,neutral,neutral,neutral
1784070815,Installing directly from the develop branch of the repo should resolve this issue and was taken care of with this PR: https://github.com/Unity-Technologies/ml-agents/pull/5997,directly develop branch resolve issue taken care,issue,positive,positive,neutral,neutral,positive,positive
1784070022,This has already been patched on develop and will be available next release. https://github.com/Unity-Technologies/ml-agents/pull/5997,already develop available next release,issue,negative,positive,positive,positive,positive,positive
1783805185,"Additional note:
After posting this issue, I've seen in commit notes and PRs that python version *must be* 3.10.12 
The doc's however, say ""python 3.10.12 _or higher_"".
Finally, the doc's say: run `conda create -n mlagents python=3.10 && conda activate mlagents`, this results in python 3.10.13 being installed. Which is higher than 3.10.12.
To be honest I think the maintainer should simply update the numpy version in setup.py to avoid all the headaches caused by the the doc's and numpy errors.
Can confirm: Windows + conda + python 3.10.13 + numpy 1.23.1 _is a working combination_.
",additional note posting issue seen commit python version must doc however say python finally doc say run create activate python higher honest think maintainer simply update version avoid doc confirm python working,issue,positive,positive,positive,positive,positive,positive
1783711585,"I see you changed https://github.com/Unity-Technologies/ml-agents/tree/develop/ml-agents-envs/setup.py

but also I think the change is needed in https://github.com/Unity-Technologies/ml-agents/tree/develop/ml-agents/setup.py
",see also think change,issue,negative,neutral,neutral,neutral,neutral,neutral
1783707090,"Solution here: https://github.com/Unity-Technologies/ml-agents/issues/6002
TLDR:
numpy 1.21.x and 1.22.x aren't compatible with the install guide you followed.
numpy 1.23.1 works fine. You need to edit ./ml-agents-dev/setup.py and ./ml-agents/setup.py 

OP, python 3.10.0 I didn't test, but I anticipate it may be a problem. In addition to updating numpy, you may need to update python version. My test included python 3.10.13 (from conda)
The final binary version of python 3.10 is https://www.python.org/downloads/release/python-31011/
however it also might be a problem (the guide says version 3.10.12+ is required).",solution compatible install guide work fine need edit python test anticipate may problem addition may need update python version test included python final binary version python however also might problem guide version,issue,negative,positive,positive,positive,positive,positive
1783704062,"Environment info:
` (mlagents)` PS C:\ML2\ml-agents> python --version
Python 3.10.13
(mlagents) PS C:\ML2\ml-agents> pip list
Package                 Version
----------------------- ------------
absl-py                 2.0.0
attrs                   23.1.0
cachetools              5.3.2
cattrs                  1.5.0
certifi                 2023.7.22
charset-normalizer      3.3.1
cloudpickle             3.0.0
colorama                0.4.6
filelock                3.13.0
fsspec                  2023.10.0
google-auth             2.23.3
google-auth-oauthlib    1.1.0
grpcio                  1.48.2
gym                     0.26.2
gym-notices             0.0.8
h5py                    3.10.0
huggingface-hub         0.18.0
idna                    3.4
Markdown                3.5
MarkupSafe              2.1.3
mlagents                1.0.0
mlagents-envs           1.0.0
numpy                   1.23.1
oauthlib                3.2.2
onnx                    1.12.0
packaging               23.2
PettingZoo              1.15.0
Pillow                  10.1.0
pip                     23.3
protobuf                3.19.6
pyasn1                  0.5.0
pyasn1-modules          0.3.0
pypiwin32               223
pywin32                 306
PyYAML                  6.0.1
requests                2.31.0
requests-oauthlib       1.3.1
rsa                     4.9
setuptools              68.0.0
six                     1.16.0
tensorboard             2.15.0
tensorboard-data-server 0.7.2
torch                   1.13.1+cu117
tqdm                    4.66.1
typing_extensions       4.8.0
urllib3                 2.0.7
Werkzeug                3.0.1
wheel                   0.41.2 '",environment python version python pip list package version gym markdown pillow pip six torch wheel,issue,negative,neutral,neutral,neutral,neutral,neutral
1778481332,"I'm getting this same issue after following the install instructions [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Installation.md)

I just started a new venv and ran only these commands:
* pip3 install torch~=1.13.1 -f https://download.pytorch.org/whl/torch_stable.html
* pip install ./ml-agents-envs
* pip install ./ml-agents
* mlagents-learn config/ppo/3DBall.yaml --run-id=first3DBallRun

Ran the 3DBall Test from the [into docs](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Getting-Started.md#training-a-new-model-with-reinforcement-learning) and it errors out after I hit play in unity with ""[W ..\torch\csrc\utils\tensor_numpy.cpp:77] Warning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xe (function operator ())""

torch: 1.13.1+cu117
numpy: 1.21.2
mlagents: 1.0.0 (pulled from release21)
mlagents-envs: 1.0.0 (pulled from release21)
Unity 2022.3.4f1",getting issue following install new ran pip install pip install pip install ran test hit play unity warning initialize module version version function operator torch release release unity,issue,negative,positive,neutral,neutral,positive,positive
1777368805,"Had this issue as well, release 21 didn't work for me at all (Windows) for the reasons listed here, however after struggling with different python and numpy versions I came across this page! Just moving to the develop branch to include the bumped numpy version worked for me. Should there be a mention of this somewhere in [the documentation](https://unity-technologies.github.io/ml-agents/Installation/)? I don't know if others are also having problems but it would've saved me a lot of time if this was in the release or I'd known about it before",issue well release work listed however struggling different python came across page moving develop branch include version worked mention somewhere documentation know also would saved lot time release known,issue,negative,neutral,neutral,neutral,neutral,neutral
1775408086,Solved it with a bit of trial and error. Installing CMAKE via Homebrew did the job (I guess)...,bit trial error via job guess,issue,negative,neutral,neutral,neutral,neutral,neutral
1771747449,"~this is also technically separate, but had to `pip install protobuf==3.20.1` as well to remove some of the issues mentioned [here](https://github.com/protocolbuffers/protobuf/issues/10051)), im not sure if this actually stopped it from running but any `mlagents-learn` command would've thrown errors otherwise~ this was ok

The packages installed werent from the repo when i tested originally, installing the two packages in the correct order instead also results the same",also technically separate pip install well remove sure actually stopped running command would thrown werent tested originally two correct order instead also,issue,negative,positive,positive,positive,positive,positive
1770310114,"Hey, 

@miguelalonsojr solved this problem in a previous PR, but I don't know if this fix is included in a release yet. 

If you still get this problem, you can check your setup.py file. The numpy version in that file should be something like this:
""numpy>=1.14.1,<1.24""

And it installed numpy 1.23.5 for me using these settings.

Check this issue for more info: [Issue 5826](https://github.com/Unity-Technologies/ml-agents/issues/5826)",hey problem previous know fix included release yet still get problem check file version file something like check issue issue,issue,negative,negative,negative,negative,negative,negative
1770184606,"Also, I got this error
Python==3.10.11
Environment: cmd
Unity Version: Unity 2022.1.24
",also got error environment unity version unity,issue,negative,neutral,neutral,neutral,neutral,neutral
1765981646,Good! The same question gym -> gymnasium.,good question gym gymnasium,issue,negative,positive,positive,positive,positive,positive
1764505090,"https://github.com/Unity-Technologies/ml-agents/tree/develop/Project/Assets/ML-Agents/Examples/3DBall/Scenes

All of our example environments are in one Unity project. It's the scene called Visual3DBall.",example one unity project scene,issue,negative,neutral,neutral,neutral,neutral,neutral
1763771241,"> @OmarVector So, I've made the changes to support running MLA with Sentis on 2021.3. It's on this branch: https://github.com/Unity-Technologies/ml-agents/tree/fix-remove-batched-raycasts-2021 Unfortunately, I will not be able to merge this into develop, as we do not want to explicitly add support for Unity versions older than 2022.3. Plus, these changes remove any errors with the ML-Agents package, but some of the example scenes do not work on 2021.3 since they've been updated to 2022.3. However, we'll keep this branch around for those that want to use MLA + Sentis in 2021.3.
> 
> You can always install ML-Agents from the github repo directly by using the following git url:
> 
> `git+https://github.com/Unity-Technologies/ml-agents.git?path=com.unity.ml-agents#fix-remove-batched-raycasts-2021`
> 
> and installing the package through the package manager git URL installation process. Give it a shot and let me know how it goes.
> 
> Thanks again for supporting ML-Agents! I'm closing this issue.

It went well, the performance is much higher than the version of 2022 but I believe its due to URP version as well

for the performance compared to our 2021 version, is almost the same, I may consider slightly improvement ... its difficult to observe the difference compared to 2022 due to frame spikes on webgl due to GC... its always in range 30-50 FPS 

Thank you for providing this custom version of sentis: ))",made support running branch unfortunately able merge develop want explicitly add support unity older plus remove package example work since however keep branch around want use always install directly following git package package manager git installation process give shot let know go thanks supporting issue went well performance much higher version believe due version well performance version almost may consider slightly improvement difficult observe difference due frame due always range thank providing custom version,issue,positive,positive,neutral,neutral,positive,positive
1763507641,"I'm getting the same error message as @Kontra21. Installing protobuf didn't help for me either.

```
Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""C:\Users\ebrei\OneDrive\Documents\Unity\ML Platformer Testing\venv\Scripts\mlagents-learn.exe\__main__.py"", line 4, in <module>
  File ""C:\Users\ebrei\OneDrive\Documents\Unity\ML Platformer Testing\venv\Lib\site-packages\mlagents\trainers\learn.py"", line 2, in <module>
    from mlagents import torch_utils
  File ""C:\Users\ebrei\OneDrive\Documents\Unity\ML Platformer Testing\venv\Lib\site-packages\mlagents\torch_utils\__init__.py"", line 1, in <module>
    from mlagents.torch_utils.torch import torch as torch  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\ebrei\OneDrive\Documents\Unity\ML Platformer Testing\venv\Lib\site-packages\mlagents\torch_utils\torch.py"", line 6, in <module>
    from mlagents.trainers.settings import TorchSettings
  File ""C:\Users\ebrei\OneDrive\Documents\Unity\ML Platformer Testing\venv\Lib\site-packages\mlagents\trainers\settings.py"", line 644, in <module>
    class TrainerSettings(ExportableSettings):
  File ""C:\Users\ebrei\OneDrive\Documents\Unity\ML Platformer Testing\venv\Lib\site-packages\mlagents\trainers\settings.py"", line 667, in TrainerSettings
    cattr.register_structure_hook(
  File ""C:\Users\ebrei\OneDrive\Documents\Unity\ML Platformer Testing\venv\Lib\site-packages\cattr\converters.py"", line 207, in register_structure_hook
    self._structure_func.register_cls_list([(cl, func)])
  File ""C:\Users\ebrei\OneDrive\Documents\Unity\ML Platformer Testing\venv\Lib\site-packages\cattr\dispatch.py"", line 55, in register_cls_list
    self._single_dispatch.register(cls, handler)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\Lib\functools.py"", line 864, in register
    raise TypeError(
TypeError: Invalid first argument to `register()`. typing.Dict[mlagents.trainers.settings.RewardSignalType, mlagents.trainers.settings.RewardSignalSettings] is not a class or union type.
```",getting error message help either recent call last file frozen line file frozen line file platformer line module file platformer line module import file platformer line module import torch torch file platformer line module import file platformer line module class file platformer line file platformer line file platformer line handler file line register raise invalid first argument register class union type,issue,negative,positive,neutral,neutral,positive,positive
1763428814,"@OmarVector So, I've made the changes to support running MLA with Sentis on 2021.3. It's on this branch: https://github.com/Unity-Technologies/ml-agents/tree/fix-remove-batched-raycasts-2021 Unfortunately, I will not be able to merge this into develop, as we do not want to explicitly add support for Unity versions older than 2022.3. Plus, these changes remove any errors with the ML-Agents package, but some of the example scenes do not work on 2021.3 since they've been updated to 2022.3. However, we'll keep this branch around for those that want to use MLA + Sentis in 2021.3.

You can always install ML-Agents from the github repo directly by using the following git url: 

`git+https://github.com/Unity-Technologies/ml-agents.git?path=com.unity.ml-agents#fix-remove-batched-raycasts-2021`

and installing the package through the package manager git URL installation process. Give it a shot and let me know how it goes.

Thanks again for supporting ML-Agents! I'm closing this issue.",made support running branch unfortunately able merge develop want explicitly add support unity older plus remove package example work since however keep branch around want use always install directly following git package package manager git installation process give shot let know go thanks supporting issue,issue,positive,positive,positive,positive,positive,positive
1762610675,"> What version of 2021 are you using? I can patch ML-Agents to exclude features related to 2022.3 and have it available on the develop branch. I also replied to your thread on the Sentis discussion board. https://discussions.unity.com/t/ml-agent-and-sentis-pipeline/282043/15 Reopenning.

We are using 2021.3.8f1

Really appreciated, lets continue the discussion here for easier collaboration  :))",version patch exclude related available develop branch also thread discussion board really continue discussion easier collaboration,issue,negative,positive,positive,positive,positive,positive
1761479525,What version of 2021 are you using? I can patch ML-Agents to exclude features related to 2022.3 and have it available on the develop branch. I also replied to your thread on the Sentis discussion board. https://discussions.unity.com/t/ml-agent-and-sentis-pipeline/282043/15 Reopenning.,version patch exclude related available develop branch also thread discussion board,issue,negative,positive,positive,positive,positive,positive
1761337495,"> There are some new features in the latest release of MLA the depend on 2022, unrelated to Sentis. As Unity evolves, so does MLA.

Thanks for explanation, would be possible for those features to be optional?

In my imagination, ML agents hasnt updated for very long time, and most of the users who use it are mostly using old version of unity which difficult for sudden update... so we may expect at least we can still use the package without those exclusive feature for 2022 and up.

However, its just suggestion hopefully put in to account

Thank you:)",new latest release depend unrelated unity thanks explanation would possible optional imagination long time use mostly old version unity difficult sudden update may expect least still use package without exclusive feature however suggestion hopefully put account thank,issue,positive,positive,neutral,neutral,positive,positive
1761332868,"There are some new features in the latest release of MLA the depend on 2022, unrelated to Sentis. As Unity evolves, so does MLA.",new latest release depend unrelated unity,issue,negative,positive,positive,positive,positive,positive
1761290946,"> You can use any version that you want, if you install the last registry release from the package manager, which was 2.0.1. Keep in mind, that you'll have to also install the correct python versions as well. You can find this info on the releases page. https://github.com/Unity-Technologies/ml-agents/releases
> 
> For installing older versions from GitHub, you'll have to switch to the correct release branch, e.g. release 20, which was released for 2021.3. https://github.com/Unity-Technologies/ml-agents/releases/tag/release_20 If you want to use the bleeding edge available on the develop branch, then you'll have to upgrade to the editor version listed in package.json https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/package.json.


May I ask what is the science behind forcing upgrade to 2022 to use the latest package version, despite sentis already working on 2021 as far as I'm concern

Our project heavly rely on webgl support, and upgrading to 2022 effect our performance a lot due to having to upgrade URP to newer version and many render features we have no longer working .",use version want install last registry release package manager keep mind also install correct python well find page older switch correct release branch release want use bleeding edge available develop branch upgrade editor version listed may ask science behind forcing upgrade use latest package version despite already working far concern project rely support effect performance lot due upgrade version many render longer working,issue,positive,positive,positive,positive,positive,positive
1756099992,"This might be my limited view on things, but most Unity devs I know (90%) work on Windows, so taking mac & linux first approach might be not a wise idea and always come back with issues. ",might limited view unity know work taking mac first approach might wise idea always come back,issue,negative,positive,positive,positive,positive,positive
1756076217,I cannot reproduce this error on mac or linux. Are trying this on Windows?,reproduce error mac trying,issue,negative,neutral,neutral,neutral,neutral,neutral
1756060138,"Yes, my conda env is set to use 3.10.12. However the numpy version in [setup.py](https://github.com/Unity-Technologies/ml-agents/blob/97386af74e0153ee045fb2240516f5734692680c/ml-agents-envs/setup.py#L63C9-L63C24) is ""numpy==1.21.2"".


And python 3.10.12 and numpy 1.21.2 is not compatible. That was the error I got. Then I changed numpy version in setup.py to ""numpy>=1.14.1,<1.24"" to solve this error. And it installed numpy 1.23.5.



",yes set use however version python compatible error got version solve error,issue,negative,neutral,neutral,neutral,neutral,neutral
1756047860,I have no issues installing mlagents from pypi and do not get the numpy issue described here. ML-Agents release 21 is pinned to python version 3.10.12. Please make sure that your conda env is set to use this version of python. Otherwise you'll likely run into problems. ,get issue release pinned python version please make sure set use version python otherwise likely run,issue,positive,positive,positive,positive,positive,positive
1756019033,"This issue still there for release branch 21. I used [TheRisenPhoenix](https://github.com/TheRisenPhoenix)'s method solve, but I think it should be fixed before closing this ticket.",issue still release branch used method solve think fixed ticket,issue,negative,positive,neutral,neutral,positive,positive
1755993728,"This is a problem related with my env. I had to change Python version from 3.10.13 to 3.10.12, to pass version check. I changed it after installing packages, so some packages were having trouble. Closing the issue, since its a user problem",problem related change python version pas version check trouble issue since user problem,issue,negative,negative,neutral,neutral,negative,negative
1753269486,"Yeah. If that's the case, then it's likely that your agent's observation space is too big for your hardware. Can you try to reproduce this with one of our example environments that use visual observations like Visual3D ball or GridWorld? We can't use custom environments for fixing bugs unless we can rero the bug with one of our example environments. If you succeed in reproducing the bug, then please open a new ticket with the details.",yeah case likely agent observation space big hardware try reproduce one example use visual like ball ca use custom fixing unless bug one example succeed bug please open new ticket,issue,positive,positive,neutral,neutral,positive,positive
1753260448,"> > @miguelalonsojr this also still relevant.
> 
> This seems like a resource issue and not a bug. As @Wanfanel mentioned, depending on your environment settings and training configuration settings, you may run into memory issues. My suggestion is to profile the Unity environment separate from training to make sure that there are no memory leaks in Unity. As far as the python trainers go, all of the observations from the agent are converted to tensors and buffered. If you have torch compatible GPU installed in your system, the ML-Agents training pipeline will automatically use that GPU for training.

I already tested that. Memory is building up on python side. And yes I use my rtx 3060 with torch.",also still relevant like resource issue bug depending environment training configuration may run memory suggestion profile unity environment separate training make sure memory unity far python go agent converted torch compatible system training pipeline automatically use training already tested memory building python side yes use torch,issue,positive,positive,positive,positive,positive,positive
1753251096,"> @miguelalonsojr this also still relevant.

This seems like a resource issue and not a bug. As @Wanfanel mentioned, depending on your environment settings and training configuration settings, you may run into memory issues. My suggestion is to profile the Unity environment separate from training to make sure that there are no memory leaks in Unity. As far as the python trainers go, all of the observations from the agent are converted to tensors and buffered. If you have torch compatible GPU installed in your system, the ML-Agents training pipeline will automatically use that GPU for training.",also still relevant like resource issue bug depending environment training configuration may run memory suggestion profile unity environment separate training make sure memory unity far python go agent converted torch compatible system training pipeline automatically use training,issue,positive,positive,positive,positive,positive,positive
1753211818,Thanks for reaching out. This was closed en masse to cleanup old issues and requests for determining which issues are still relevant for the community. Re-opening. ,thanks reaching closed en masse cleanup old still relevant community,issue,positive,positive,positive,positive,positive,positive
1752854683,No. Thanks for reaching out. This was closed en masse to clean up old issues and requests. Re-opening as this should be an easy addition. No ETA on implementation though. But will keep open.,thanks reaching closed en masse clean old easy addition eta implementation though keep open,issue,positive,positive,positive,positive,positive,positive
1752851879,"We are upgrading to Sentis and will have an official release soon. If the problem persists after the next release, please open a new issue.",official release soon problem next release please open new issue,issue,negative,positive,neutral,neutral,positive,positive
1752395821,"I see it has been now ""closed as complete"".
Does it mean that request MLA-2401 has been fully implemented and it's available in the upcoming release of ML-Agents?",see closed complete mean request fully available upcoming release,issue,negative,positive,neutral,neutral,positive,positive
1752360161,"@miguelalonsojr May I ask since its closed and marked as complete, does it  really resolved and improved the webgl inference speed? ... like now we can use GPU on webgl ?",may ask since closed marked complete really resolved inference speed like use,issue,positive,positive,neutral,neutral,positive,positive
1752182511,"the link is still broken, not fixed.",link still broken fixed,issue,negative,negative,negative,negative,negative,negative
1752182203,"this isnt completed. the value is still set to 200f, see https://github.com/Unity-Technologies/ml-agents/blob/6203c7e2db605b231bec6c8edd56c272400211fb/Project/Assets/ML-Agents/Examples/DungeonEscape/Scripts/PushAgentEscape.cs#L66",value still set see,issue,negative,neutral,neutral,neutral,neutral,neutral
1752173318,Please update to the latest version of ML-Agents available on the develop branch of this repo.,please update latest version available develop branch,issue,negative,positive,positive,positive,positive,positive
1752173128,"The DQN trainer is an example trainer for those that want to implement their own custom trainer. Unfortunately, there are no plans to add ONNX export to the DQN trainer example. Closing as this is not a bug.",trainer example trainer want implement custom trainer unfortunately add export trainer example bug,issue,negative,negative,negative,negative,negative,negative
1752172529,"Gym unity has been deprecated and gym functionality has been moved into the ml-agents-envs package. Also, please update to the latest version of ML-Agents available on the develop branch of this repo. Also, please update to Unity version 2022.3 LTS as well.",gym unity gym functionality package also please update latest version available develop branch also please update unity version well,issue,positive,positive,positive,positive,positive,positive
1752172328,"Please update to the latest version of ML-Agents available on the develop branch of this repo. Also, please update to Unity version 2022.3 LTS as well.",please update latest version available develop branch also please update unity version well,issue,positive,positive,positive,positive,positive,positive
1752171183,"If you train a policy outside of the ML-Agents training pipeline, you'll have to export it properly. At the end of this colab, https://github.com/Unity-Technologies/ml-agents/blob/develop/colab/Colab_UnityEnvironment_2_Train.ipynb, you can find details of how to export a custom model for use in Unity with ML-Agents. If you need further assistance, please post in the forums. https://forum.unity.com/forums/ml-agents.453/ Closing as this is not a bug. ",train policy outside training pipeline export properly end find export custom model use unity need assistance please post bug,issue,negative,neutral,neutral,neutral,neutral,neutral
1752169086,Please upgrade to the latest version of ML-Agents available on the develop branch of this repo and try again.,please upgrade latest version available develop branch try,issue,negative,positive,positive,positive,positive,positive
1752167772,"Please upgrade to the latest version of ML-Agents available on the develop branch of this repo and try again. Also, as @JvanderKaaij noted, runtime instantiation of ML-Agents components is tricky if not done properly and can lead to instability. It's not recommended.",please upgrade latest version available develop branch try also noted tricky done properly lead instability,issue,negative,positive,positive,positive,positive,positive
1752167281,Please update to the latest version of ML-Agents available on the develop branch of this repo and try again.,please update latest version available develop branch try,issue,negative,positive,positive,positive,positive,positive
1752166833,Please update to the latest version of ML-Agents on the develop branch of the repo and test to see if the issue persists.,please update latest version develop branch test see issue,issue,negative,positive,positive,positive,positive,positive
1752166555,"This is likely not due to ML-Agents, but to running Unity with other workloads: https://forum.unity.com/threads/constant-crashes-failed-to-present-d3d11-swapchain-due-to-device-removed.987834/",likely due running unity,issue,negative,negative,neutral,neutral,negative,negative
1752114132,This is a development branch for an unreleased feature. Closing.,development branch unreleased feature,issue,negative,neutral,neutral,neutral,neutral,neutral
1752113787,"Unfortunately, we can't support headless server builds as this time. However, if you're running on a server, similar to running this on google colab, you'll have to setup XVFB, since any ML-Agents environment that uses camera sensors for visual observations, as GridWorld does, needs a framebuffer to render to. Closing as this is not a bug in our current feature set. If you need further assistance on setting this up, please refer to our forums: https://forum.unity.com/forums/ml-agents.453. Closing as this is not a bug.",unfortunately ca support headless server time however running server similar running setup since environment camera visual need render bug current feature set need assistance setting please refer bug,issue,negative,negative,negative,negative,negative,negative
1752112722,"If you want more examples, you'll have to clone this repo. We don't want more than the one example in the package, as it bloats the package size unnecessarily. Closing as this is not a bug.",want clone want one example package package size unnecessarily bug,issue,negative,negative,negative,negative,negative,negative
1752112496,"I have confirmed the Camera sensor component is available in 2022.3 and can be added through the editor add component feature. It may be that your library cache needs to be rebuilt. Try re-importing all assets or close unity, delete the Library folder in your project and re-open it in Unity. Closing as this is not a bug.",confirmed camera sensor component available added editor add component feature may library cache need rebuilt try asset close unity delete library folder project unity bug,issue,negative,positive,positive,positive,positive,positive
1752111311,No. The HF integration is already on develop. It will be available in our official next release.,integration already develop available official next release,issue,negative,positive,positive,positive,positive,positive
1752111056,You're looking at old documentation. Please use our current webdocs: https://unity-technologies.github.io/ml-agents/.,looking old documentation please use current,issue,negative,positive,neutral,neutral,positive,positive
1752093919,"Unfortunately, multi-GPU training is not currently supported (adding support for distributed training is TBD). However, the main bottleneck currently is not actually the policy updates in PyTorch. It's the environment rollout/experience buffering from Unity to python, due to gRPC serialization/deserialization. This is a known limitation and will likely be the case until supported for things like shared memory or direct tensor creation in GPU mem by Unity. Both of which are not currently planned.",unfortunately training currently support distributed training however main bottleneck currently actually policy environment unity python due known limitation likely case like memory direct tensor creation mem unity currently,issue,negative,negative,neutral,neutral,negative,negative
1752093030,"I've updated the installation instructions. Give it a shot: https://unity-technologies.github.io/ml-agents/Installation/

TL; DR: install from the repo.",installation give shot install,issue,negative,neutral,neutral,neutral,neutral,neutral
1751791348,"When you look at the 3DBall: 3D Balance Ball example, There are twelve parallel threads running (12 agents): https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/images/balance.png
If you increase the number of agents (e.g. 48),  it will increase the GPU usage. However, 2 x 4090 GPUs are overkill for this example.
Another option is to change the parameters in the YAML file. This depends on your project's complexity.

Let me know GPU usage (%) and training time when you increase the number of agents by a factor of 10 (120 agents).",look balance ball example twelve parallel running increase number increase usage however example another option change file project complexity let know usage training time increase number factor,issue,positive,neutral,neutral,neutral,neutral,neutral
1751590666,This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.,thread automatically locked since recent activity closed please open new issue related,issue,negative,positive,neutral,neutral,positive,positive
1750817139,"I'm a rookie and I was just running the demo of ml-agents(platforms to keep balls) ,so don't understand question1
q2 about 20to30percent i guess since I haven't been to lab for a whole week, can't remember well XD
q3 not too long just a few minutes
q4 ML-Agents Release 19 with pytorch==1.12.1",rookie running keep understand question guess since lab whole week ca remember well long release,issue,negative,positive,neutral,neutral,positive,positive
1750809856,"Thanks a lot, i'll check this link first. See if it works.
",thanks lot check link first see work,issue,negative,positive,positive,positive,positive,positive
1744836717,"[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/Unity-Technologies/ml-agents?pullRequest=5980) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/Unity-Technologies/ml-agents?pullRequest=5980) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/Unity-Technologies/ml-agents?pullRequest=5980) it.</sub>",assistant check thank submission really appreciate like many open source ask sign contributor license agreement accept sub already status still pending let u recheck,issue,positive,positive,positive,positive,positive,positive
1737927129,"Hi,
I am curious to know:

1. How many parallel threads are you running?
2. What is the GPU usage (%)?
3. How long does it take to complete the training?
4. What version of Pytorch are you using?

Did you check https://stackoverflow.com/questions/73267607/how-to-train-model-with-multiple-gpus-in-pytorch ?
",hi curious know many parallel running usage long take complete training version check,issue,negative,positive,neutral,neutral,positive,positive
1732908529,"By the way, I also tried restarting the program and restarting the training, but neither solved the problem :(",way also tried program training neither problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1727134631,"I also noticed another thing, which I believe is probably happening in the original example above as well.

After resuming, the coefficient sometimes stops being logged.
If we look at the coefficient in the screenshot, we see, judging by the gridlines, that it goes up until about 2.4 million or so.

The training log shows that that training was resumed at around 2.5 million. Means that here the coefficient is not actually shown properly in the screenshot. In some cases it does get logged properly, thats how I even got the idea for this in the first place, but its possibly a separate bug that made this problem even less obvious.

However, we do see the curiosity and entropy (not coefficient) shoot way up, which by my estimation explains the deterioration in this example quite well. Those in turn would be caused by the coefficient which is not logged correctly.",also another thing believe probably happening original example well coefficient sometimes logged look coefficient see go million training log training around million coefficient actually shown properly get logged properly thats even got idea first place possibly separate bug made problem even le obvious however see curiosity entropy coefficient shoot way estimation deterioration example quite well turn would coefficient logged correctly,issue,negative,positive,neutral,neutral,positive,positive
1727101963,"I have been able to reproduce this and also found a workaround.

When a SAC training run is resumed, the initial entropy coefficient (init_entcoef) is set to whatever the value is in the config file. If it is not set in the config file, it is set to the default value (1.0). The initial entropy is then used to reset the current policy entropy coefficient in the actual training run after resuming.

Since entropy is a measure of randomness/exploration this usually causes the policy to deteriorate after resuming.

By setting the init_entcoef to whatever was logged last logged in tensorboard under ""Policy
/Continuous Entropy Coeff"", I was able to resume without any issues.

My suggestion is, that if the config file defines the same initial entropy coefficent as the currently stored (previous) config file or does not define it at all, the resume should probably use whatever entropy coefficent was last seen before saving. 
",able reproduce also found sac training run initial entropy coefficient set whatever value file set file set default value initial entropy used reset current policy entropy coefficient actual training run since entropy measure usually policy deteriorate setting whatever logged last logged policy entropy able resume without suggestion file initial entropy currently previous file define resume probably use whatever entropy last seen saving,issue,positive,positive,neutral,neutral,positive,positive
1726960403,"In addition, I found that there might be an another bug in `ml-agents/ml-agents-envs/mlagents_envs/envs/unity_pettingzoo_base_env.py` (line 168-171):

```C#
if action.continuous is not None:
    self._current_action[current_behavior].continuous[
        current_index
    ] = action.continuous[0]
....
```

This line sets the **whole** action of the agent to the **first** element of the given action. 
Assumes that the number of continuous actions is 3, and the given action is `[0.3, 0.9, 0.1]`, then the final action for current agent will become **`[0.3, 0.3, 0.3]`**, which is not correct.

I'm not sure if it is just a typo and I could delete the `[0]` directly, or there might be a missing dimension of the given action? (e.g. **`[[0.3, 0.9, 0.1]]`** instead of `[0.3, 0.9, 0.1]`)

In my case, I simply delete the `[0]` and it works like a charm.",addition found might another bug line none line whole action agent first element given action number continuous given action final action current agent become correct sure typo could delete directly might missing dimension given action instead case simply delete work like charm,issue,positive,positive,positive,positive,positive,positive
1710007625,"Been a while since I set it up, but if I remember correctly I edited the requirements in the setup.py files. There are acceptable version ranges specified in there which you can change (risking a dysfunctional application afterwards, of course).",since set remember correctly acceptable version change application afterwards course,issue,negative,neutral,neutral,neutral,neutral,neutral
1709347642,"I found a fix to my problem. I had to install a version of PyTorch 1.11 that is compatible with CUDA 11.3 (torch 1.11+cu113), hope this helps anyone stuck on the same issue.",found fix problem install version compatible torch hope anyone stuck issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1709290740,"> Very much looking forward to this. I'm using a 3960 Threadripper and a RTX 4090 and getting horrible performance. GPU sits at around 3-5% and most of the CPU idles with a single thread (out of 48!) at 100%. So my training is bottlenecked by the single-core-performance of my CPU, as a result I'm currently forced to do training on my MacBook.
> 
> So - anything on the horizon?

Hey! Can you tell me how you set up ml-agents to work on a 4090? Currently my PyTorch versions are complaining that 1.11 is incompatible with a 4090 while 2.0 is too high for ml-agents. I'm unsure what the workaround to my problem is and want to understand how you made it to work.

Thanks!",much looking forward getting horrible performance around single thread training result currently forced training anything horizon hey tell set work currently incompatible high unsure problem want understand made work thanks,issue,negative,negative,negative,negative,negative,negative
1699025312,"I had the same error and that's how I ended up here. For me, it was caused because I'm instantiating the BehaviourParameter and DecisionRequester components in runtime. Somehow that caused this error. I fixed it by adding the components to an GameObject that's Inactive at the time and then activating it after with SetActive(true). Not sure if that might fix it for you, but it might give a clue of what goes wrong on your side.",error ended somehow error fixed inactive time true sure might fix might give clue go wrong side,issue,negative,positive,positive,positive,positive,positive
1681566685,"And I can indeed see the Camera Sensor Component script in packages, but it is not present in the add component of the game object",indeed see camera sensor component script present add component game object,issue,negative,negative,negative,negative,negative,negative
1681564974,"thanks，my version is 2.0.1, but when I switched to the same version as yours, there was still no camera sensor component and there were warnings. And the editor recommends me to use 2.0.1",version switched version still camera sensor component editor use,issue,negative,neutral,neutral,neutral,neutral,neutral
1681165121,"Are there any other ML-Agents components you can find in Unity? I just checked release-20's default Project and I am able to search for and add the camera sensor to an agent in `SoccerTwos`.

Are you using the release version directly (which has its own install of ml-agents included, or installing from the Package Manager?

I'm using `Unity Editor 2022.3.7f1`
I'm using `ML Agents 2.3.0-exp.3` (installed from `../ml-agents/ml-agents-release_20/com.unity.ml-agents`)

The `CameraSensor.cs` and `CameraSensorComponent.cs` exists in `ml-agents-release_20\com.unity.ml-agents\Runtime\Sensors`

Does this issue occur if you open the project named `Project` with Unity Hub and search for the camera sensor component?",find unity checked default project able search add camera sensor agent release version directly install included package manager unity editor issue occur open project project unity hub search camera sensor component,issue,negative,positive,positive,positive,positive,positive
1681046260,"I would avoid setting the `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python` for the sake of speed.

Try installing Protobuf 3.20.3
`pip3 install protobuf==3.20.3`

Release-20 installs the wrong version of protobuf.",would avoid setting sake speed try pip install wrong version,issue,negative,negative,negative,negative,negative,negative
1670682063,"Same overall issue, different error
```Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""C:\Users\xcc\AppData\Local\Programs\Python\Python311\Scripts\mlagents-learn.exe\__main__.py"", line 4, in <module>
  File ""C:\Users\xcc\AppData\Local\Programs\Python\Python311\Lib\site-packages\mlagents\trainers\learn.py"", line 2, in <module>
    from mlagents import torch_utils
  File ""C:\Users\xcc\AppData\Local\Programs\Python\Python311\Lib\site-packages\mlagents\torch_utils\__init__.py"", line 1, in <module>
    from mlagents.torch_utils.torch import torch as torch  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\xcc\AppData\Local\Programs\Python\Python311\Lib\site-packages\mlagents\torch_utils\torch.py"", line 6, in <module>
    from mlagents.trainers.settings import TorchSettings
  File ""C:\Users\xcc\AppData\Local\Programs\Python\Python311\Lib\site-packages\mlagents\trainers\settings.py"", line 644, in <module>
    class TrainerSettings(ExportableSettings):
  File ""C:\Users\xcc\AppData\Local\Programs\Python\Python311\Lib\site-packages\mlagents\trainers\settings.py"", line 667, in TrainerSettings
    cattr.register_structure_hook(
  File ""C:\Users\xcc\AppData\Local\Programs\Python\Python311\Lib\site-packages\cattr\converters.py"", line 207, in register_structure_hook
    self._structure_func.register_cls_list([(cl, func)])
  File ""C:\Users\xcc\AppData\Local\Programs\Python\Python311\Lib\site-packages\cattr\dispatch.py"", line 55, in register_cls_list
    self._single_dispatch.register(cls, handler)
  File ""C:\Users\xcc\AppData\Local\Programs\Python\Python311\Lib\functools.py"", line 864, in register
    raise TypeError(
TypeError: Invalid first argument to `register()`. typing.Dict[mlagents.trainers.settings.RewardSignalType, mlagents.trainers.settings.RewardSignalSettings] is not a class or union type.",overall issue different error recent call last file frozen line file frozen line file line module file line module import file line module import torch torch file line module import file line module class file line file line file line handler file line register raise invalid first argument register class union type,issue,negative,positive,neutral,neutral,positive,positive
1665460435,Thank you so much for the clarification. It is now clear what I should do :),thank much clarification clear,issue,positive,positive,positive,positive,positive,positive
1665439534,"You can use any version that you want, if you install the last registry release from the package manager, which was 2.0.1. Keep in mind, that you'll have to also install the correct python versions as well. You can find this info on the releases page. https://github.com/Unity-Technologies/ml-agents/releases 

For installing older versions from GitHub, you'll have to switch to the correct release branch, e.g. release 20, which was released for 2021.3. https://github.com/Unity-Technologies/ml-agents/releases/tag/release_20 If you want to use the bleeding edge available on the develop branch, then you'll have to upgrade to the editor version listed in package.json https://github.com/Unity-Technologies/ml-agents/blob/develop/com.unity.ml-agents/package.json.",use version want install last registry release package manager keep mind also install correct python well find page older switch correct release branch release want use bleeding edge available develop branch upgrade editor version listed,issue,negative,positive,positive,positive,positive,positive
1665407135,Does that mean I cannot run ML-agents in a lower version of Unity?,mean run lower version unity,issue,negative,negative,negative,negative,negative,negative
1665404494,The package was updated to use Unity 2022.3. The API for batched ray casts has changed a bit in 2022.3. Upgrading Unity to 2022.3 should fix the issue.,package use unity ray bit unity fix issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1665385505,"I just downloaded this new version, and when running the 3DBall tutorial environment a set of errors appeared related to the RayPerceptionSensor. I created the issue (https://github.com/Unity-Technologies/ml-agents/issues/5958) about it. ",new version running tutorial environment set related issue,issue,negative,positive,neutral,neutral,positive,positive
1665382230,"This error could be related to the most recent commit on the main branch - ""Update to 2022.3 LTS and batched raycasts""",error could related recent commit main branch update,issue,negative,positive,neutral,neutral,positive,positive
1663253383,@erikfrey @automata @mvuksano anyone of them please check this PR ,automaton anyone please check,issue,negative,neutral,neutral,neutral,neutral,neutral
1663142177,"3.9.17 has been working for me so far. though i also had to change pytorch and protobuf versions. Of all the install instructions ive tested including the hummingbird, codmonkey and official guides : https://www.youtube.com/watch?v=RANRz9oyzko is the only one that was correct and working as of time of writing (3/8/23).",working far though also change install tested hummingbird official one correct working time writing,issue,negative,positive,neutral,neutral,positive,positive
1662877332,Try installing `importlib-metadata` package (4.4 or 4.8) using `python -m pip install importlib-metadata==4.4` ,try package python pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
1653710362,"I guess there's still no update on this, right?
Such a simple feature would be extremely useful, especially when starting a large batch of standalone builds",guess still update right simple feature would extremely useful especially starting large batch,issue,negative,positive,positive,positive,positive,positive
1650359346,This PR also updates the SoccerTwos env to use the new batched raycast functionality,also use new functionality,issue,negative,positive,positive,positive,positive,positive
1648053283,The paths are correct for mkdocs generation. Please see https://unity-technologies.github.io/ml-agents/,correct generation please see,issue,negative,neutral,neutral,neutral,neutral,neutral
1645671895,"@alexge233 Is there any update on this? Would be cool to have wandb support in unity ml agents
",update would cool support unity,issue,positive,positive,positive,positive,positive,positive
1643403183,"I can confirm that the version downloadable with Unity's package manager (release 16?) works on Python 3.8.10 and that going too much newer than that won't work (3.8.13 is ok I think, but 3.9 is not).

Or I should say it *mostly* works, just make sure to specify version 0.26.0 and not 0.30.0 and there's three or so deprecated Numpy things that will break, but the console will print out the relevant file and the thing that doesn't exist and it's easy enough to change `np.float/np.bool` to `float/bool` and drop an `Int64`. And I think this is just because the dependencies list didn't list a numpy version and so its pulling a newer one than was built against.",confirm version unity package manager release work python going much wo work think say mostly work make sure specify version three break console print relevant file thing exist easy enough change drop think list list version one built,issue,negative,positive,positive,positive,positive,positive
1642940303,"So, I figured out as I had the same problem. Even though it may make it slower, try running "" Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python "" in the Command prompt (if windows),

I do not know what to do for any OS's, but this solved it for me.",figured problem even though may make try running set command prompt know o,issue,negative,neutral,neutral,neutral,neutral,neutral
1635383388,"Downgrading to Python 3.8.10 was sufficient to get things working, but I did run into one further error that required manually fixing something (in this case one line of one py file).

```
  File ""C:\Users\draco\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\mlagents\trainers\buffer.py"", line 211, in AgentBufferField
    self, pad_value: np.float = 0, dtype: np.dtype = np.float32
  File ""C:\Users\draco\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\numpy\__init__.py"", line 305, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'float'.
`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
```

All I had to do was change line 211 in buffer.py to `float` instead of `np.float`

And that let it try and load a config file, which solved the directory issue.

```
> mlagents-learn config/pewpew_config.yaml --run-id=PewPew
> TrainerConfigError: Config file could not be found at C:\Users\draco\Documents\Unity Projects\pewpew\Assets\config\config\pewpew_config.yaml
```

Apparently the full path is interpreted from my current directory, so the ""run from anywhere"" isn't *really* true.",python sufficient get working run one error manually fixing something case one line one file file line self file line raise module attribute alias float avoid error code use float modify behavior safe specifically scalar type use originally guidance see original release note change line float instead let try load file directory issue file could found apparently full path current directory run anywhere really true,issue,negative,positive,positive,positive,positive,positive
1635357784,"> Are you installing from source or from pypi?

Just ran into the original issue myself using Python 3.9.13
I saw that one of the dependencies required Python 3.8 to 3.10 and so I went with 3.9 as the subversions weren't directly accessible from the windows store, so it was the only quick-easy version I could install that matched the known requirements.

As for how I'm installing it; pypi.

```
PS C:\> mlagents-learn config/pewpew_config.yaml --run-id=PewPew
Traceback (most recent call last):
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\Users\draco\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\Scripts\mlagents-learn.exe\__main__.py"", line 4, in <module>
  File ""C:\Users\draco\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\mlagents\trainers\learn.py"", line 2, in <module>
    from mlagents import torch_utils
  File ""C:\Users\draco\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\mlagents\torch_utils\__init__.py"", line 1, in <module>
    from mlagents.torch_utils.torch import torch as torch  # noqa
  File ""C:\Users\draco\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\mlagents\torch_utils\torch.py"", line 6, in <module>
    from mlagents.trainers.settings import TorchSettings
  File ""C:\Users\draco\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\mlagents\trainers\settings.py"", line 625, in <module>
    class TrainerSettings(ExportableSettings):
  File ""C:\Users\draco\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\mlagents\trainers\settings.py"", line 648, in TrainerSettings
    cattr.register_structure_hook(
  File ""C:\Users\draco\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\cattr\converters.py"", line 207, in register_structure_hook
    self._structure_func.register_cls_list([(cl, func)])
  File ""C:\Users\draco\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\cattr\dispatch.py"", line 55, in register_cls_list
    self._single_dispatch.register(cls, handler)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\lib\functools.py"", line 855, in register
    raise TypeError(
TypeError: Invalid first argument to `register()`. typing.Dict[mlagents.trainers.settings.RewardSignalType, mlagents.trainers.settings.RewardSignalSettings] is not a class.
PS C:\U> python --version
Python 3.9.13
PS C:\>
```

In addition, the docs don't indicate the correct way to point `mlagents-learn` at the config file; it just says you can ""run the command from anywhere"" and doesn't indicate exactly where the config file is supposed to go:
![image](https://github.com/Unity-Technologies/ml-agents/assets/1750680/edc3d837-0fc3-465c-a32f-4f7cb19a42b6)
![image](https://github.com/Unity-Technologies/ml-agents/assets/1750680/2fee839d-9046-46c7-a877-b948dfb4dab6)

Not that specifying a full path helps. I will be downgrading python next.",source ran original issue python saw one python went directly accessible store version could install known recent call last file line return code none file line code file line module file line module import file line module import torch torch file line module import file line module class file line file line file line handler file line register raise invalid first argument register class python version python addition indicate correct way point file run command anywhere indicate exactly file supposed go image image full path python next,issue,negative,positive,positive,positive,positive,positive
1624253721,"ERROR: Ignored the following versions that require a different python version: 0.10.0.dev0 Requires-Python >=3.6,<3.7;

I have Python 3.11.1 
So I Have to downgrade python too? ",error following require different python version dev python downgrade python,issue,negative,neutral,neutral,neutral,neutral,neutral
1623636967,"Hi, I have the same error ""The type 'IAsyncEnumerable<T>' exists in both 'System.Interactive.Async, Version=3.0.1000.0, Culture=neutral, PublicKeyToken=94bc3704cddfc263' and 'netstandard, Version=2.1.0.0""
I tried the reactive framework, adding alias for System.Interactive.Async, but Unity just resets them afterwards back to global, I tried to decompile the dll and add that constraint but I failed, I even went and copy pasted your System.Interactive.Async.dll file into my place but that didn't work either.
For context, I also have a ML Agents game in Unity version 2022.2.6f1, and now I tried adding a chat gpt NPC to start the game with, using OpenAI-API-dotnet from OkGoDoIt. Thanks so much for all the help!",hi error type tried reactive framework alias unity afterwards back global tried add constraint even went copy pasted file place work either context also game unity version tried chat start game thanks much help,issue,negative,negative,neutral,neutral,negative,negative
1605594777,I really like this feature and inclusion and think it would be beneficial,really like feature inclusion think would beneficial,issue,positive,positive,positive,positive,positive,positive
1601748454,"onehot_branches = [
            torch.nn.functional.one_hot(_act.T, action_size[i]).float()
            for i, _act in enumerate(discrete_actions.long().T)
        ]
return onehot_branches



I just followed the error message, and it seems to be working.",enumerate return error message working,issue,negative,neutral,neutral,neutral,neutral,neutral
1598010110,I have cloned it but I am pretty new to the command line so I have no idea how to open its directory in the command line.,pretty new command line idea open directory command line,issue,negative,positive,positive,positive,positive,positive
1596244404,"This issue has been automatically marked as stale because it has not had activity in the last 90 days. It will be closed in the next 30 days if no further activity occurs. Thank you for your contributions.
",issue automatically marked stale activity last day closed next day activity thank,issue,negative,negative,neutral,neutral,negative,negative
1595770157,"I got desperate and installed ml agents again using python 3.9 and numpy 1.21.1. I also created my project and venv separately, they used to be in the cloned ml-agents repository. Surprisingly, I managed to launch my project, but unfortunately I did not understand the root of this error and the only thing I can advise in such a situation is to reinstall ml-agents again, preferably with python version 3.8.13 to 3.10 and using the ""pip install mlagents"" command, since it also installs everything the necessary routines.
![w345y](https://github.com/Unity-Technologies/ml-agents/assets/130354848/59e105f9-a214-45de-86db-4b2c7246691c)
",got desperate python also project separately used repository surprisingly launch project unfortunately understand root error thing advise situation reinstall preferably python version pip install command since also everything necessary wy,issue,negative,negative,neutral,neutral,negative,negative
1595352196,"I'm interesting in implementing this myself, with medium guidance and help from community (mainly to get onboarded, and some pointers in the code where i can add this) since i'm new to Unity ML Agents (but not IL and RL itself).",interesting medium guidance help community mainly get code add since new unity,issue,positive,positive,positive,positive,positive,positive
1594517455,"Did you clone the repo? If so, go to the repo directory and then execute that command. This is an editable source based installation. If you're not in the ml-agents repo directory, it won't work. https://brandonrozek.com/blog/pipeditable/",clone go directory execute command source based installation directory wo work,issue,negative,neutral,neutral,neutral,neutral,neutral
1594258970,"Got a few errors trying that:

(base) BitReapers@Bit-Air ~ % pip install -e ./ml-agents-envs

ERROR: ./ml-agents-envs is not a valid editable requirement. It should either be a path to a local project or a VCS URL (beginning with bzr+http, bzr+https, bzr+ssh, bzr+sftp, bzr+ftp, bzr+lp, bzr+file, git+http, git+https, git+ssh, git+git, git+file, hg+file, hg+http, hg+https, hg+ssh, hg+static-http, svn+ssh, svn+http, svn+https, svn+svn, svn+file).
(base) BitReapers@Bit-Air ~ % pip install -e. ./ml-agents

ERROR: Directory './ml-agents' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.

(base) BitReapers@Bit-Air ~ % 
",got trying base pip install error valid requirement either path local project beginning base pip install error directory neither found base,issue,negative,negative,negative,negative,negative,negative
1591003700,Anybody find an answer to this? I am struggling with the same issue,anybody find answer struggling issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1590971583,"Hmmm... very strange. Let me look into this some more. I see that you're also on Windows, which can be troublesome at times. I'll follow up.",strange let look see also troublesome time follow,issue,negative,negative,neutral,neutral,negative,negative
1590962505,"It didn't help, but thanks a lot anyway.
![w5u](https://github.com/Unity-Technologies/ml-agents/assets/130354848/df177c93-e689-4851-8f27-ade202d5ce75)
![3456yt](https://github.com/Unity-Technologies/ml-agents/assets/130354848/43ac43d3-445f-4823-93e1-7450bd31577f)
",help thanks lot anyway,issue,positive,positive,positive,positive,positive,positive
1590013689,Please update to the latest version of ml-agents available on the develop branch of the repo. You're running on a very old version.,please update latest version available develop branch running old version,issue,negative,positive,positive,positive,positive,positive
1589987142,I noticed that you are using torch 2.0.1. We have not updated ML-Agents yet to use PyTorch 2.x. This error is likely due to incompatibility issues with PyTorch 2.x and other packages like numpy.,torch yet use error likely due incompatibility like,issue,negative,negative,neutral,neutral,negative,negative
1589984663,"I cannot reproduce this with ml-agents installed from source on the develop branch. Can you try to re-install ml-agents from the github repo?

`pip install ./ml-agents-envs`

then

`pip install  ./ml-agents`


The intended behavior is to stop training when the scene stops, but it does not stop the python trainer itself. This is not a bug. There is a limit on the number of disconnects as well, to make sure if there's a problem with an environment, it does not start and stop training for ever. The typical training use case is to train with a binary build of the environment, not training in the editor. In editor training is mostly recommended for debugging as you are building the environment. Once the environment is built, running training with an executable build is faster and the recommended approach. See: https://unity-technologies.github.io/ml-agents/Learning-Environment-Executable/",reproduce source develop branch try pip install pip install intended behavior stop training scene stop python trainer bug limit number well make sure problem environment start stop training ever typical training use case train binary build environment training editor editor training mostly building environment environment built running training executable build faster approach see,issue,negative,positive,positive,positive,positive,positive
1586292843,"Just to add. I have tried in many different ways, with different python versions (3.7.x, 3.8.x and 3.9.x). I also tried strictly with installation instructions in official github page. At the log's end there's a error related with ONNX, but this is not the problem. Despite that, I also installed ONNX separately, but the behavior is always the same.",add tried many different way different python also tried strictly installation official page log end error related problem despite also separately behavior always,issue,negative,positive,positive,positive,positive,positive
1585091995,"Version 1.23.0 doesn't work for me. However, thanks for the idea to double-check the guide, maybe I missed something.",version work however thanks idea guide maybe something,issue,negative,positive,positive,positive,positive,positive
1585022252,I had a similar issue today with the 3DBall example. I got it to work by upgrading numpy to version 1.23.0. I'm not sure what effect this may have on other parts of the framework though.,similar issue today example got work version sure effect may framework though,issue,negative,positive,positive,positive,positive,positive
1582150939,"@AbhijitBaruah
I have same issue in my custom environment, do you have any suggestion if you have experience to made it better?",issue custom environment suggestion experience made better,issue,negative,positive,positive,positive,positive,positive
1578498878,"This should be sent to the develop branch, not main. Can you resubmit the patch to develop?",sent develop branch main resubmit patch develop,issue,negative,positive,positive,positive,positive,positive
1578026999,"Hey there, I updated the doc: removed tennis illustration and added the link in ML-Agents-Overview.md#Training in Competitive Multi-Agent Environments with Self-Play",hey doc removed tennis illustration added link training competitive,issue,negative,neutral,neutral,neutral,neutral,neutral
1576943820,"I had similar issues that were resolved by using the following fixed versions:
```
""numpy==1.21.2"",  
'protobuf==3.20.2',
'onnx==1.14',
'gym==0.23.1',
```",similar resolved following fixed,issue,negative,positive,neutral,neutral,positive,positive
1576568499,"We'll be working on an update to pytorch 2.x separately. For now, it should remain pinned. Closing.",working update separately remain pinned,issue,negative,neutral,neutral,neutral,neutral,neutral
1576554053,"This is not a bug. If you need assistance, please post your request in the forums: https://forum.unity.com/forums/ml-agents.453/",bug need assistance please post request,issue,negative,neutral,neutral,neutral,neutral,neutral
1576551155,"We are unable to help reproduce bugs with custom environments. Please attempt to reproduce your issue with one of the example environments, or provide a minimal patch to one of the environments needed to reproduce the issue. Also, please fill out our bug report template: https://github.com/Unity-Technologies/ml-agents/blob/develop/.github/ISSUE_TEMPLATE/bug_report.md",unable help reproduce custom please attempt reproduce issue one example provide minimal patch one reproduce issue also please fill bug report template,issue,positive,negative,negative,negative,negative,negative
1576548486,"You should install protobuf v3.19.6. This version should be compatible with both ml-agents and tensorboard. This is from a fresh install of ml-agents in a new mambe env.

```⇒  mamba list
# packages in environment at /home/miguelalonsojr/mambaforge/envs/mlagents:
#
# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                 conda_forge    conda-forge
_openmp_mutex             4.5                       2_gnu    conda-forge
absl-py                   1.3.0                    pypi_0    pypi
attrs                     22.2.0                   pypi_0    pypi
bzip2                     1.0.8                h7f98852_4    conda-forge
ca-certificates           2022.12.7            ha878542_0    conda-forge
cachetools                5.2.0                    pypi_0    pypi
cattrs                    1.5.0                    pypi_0    pypi
certifi                   2022.12.7                pypi_0    pypi
charset-normalizer        2.1.1                    pypi_0    pypi
cloudpickle               2.2.0                    pypi_0    pypi
filelock                  3.9.0                    pypi_0    pypi
google-auth               2.15.0                   pypi_0    pypi
google-auth-oauthlib      0.4.6                    pypi_0    pypi
grpcio                    1.51.1                   pypi_0    pypi
gym                       0.26.2                   pypi_0    pypi
gym-notices               0.0.8                    pypi_0    pypi
h5py                      3.7.0                    pypi_0    pypi
idna                      3.4                      pypi_0    pypi
ld_impl_linux-64          2.39                 hcc3a1bd_1    conda-forge
libffi                    3.4.2                h7f98852_5    conda-forge
libgcc-ng                 12.2.0              h65d4601_19    conda-forge
libgomp                   12.2.0              h65d4601_19    conda-forge
libnsl                    2.0.0                h7f98852_0    conda-forge
libsqlite                 3.40.0               h753d276_0    conda-forge
libuuid                   2.32.1            h7f98852_1000    conda-forge
libzlib                   1.2.13               h166bdaf_4    conda-forge
markdown                  3.4.1                    pypi_0    pypi
markupsafe                2.1.1                    pypi_0    pypi
mlagents                  0.31.0.dev0               dev_0    <develop>
mlagents-envs             0.31.0.dev0               dev_0    <develop>
ncurses                   6.3                  h27087fc_1    conda-forge
numpy                     1.21.2                   pypi_0    pypi
oauthlib                  3.2.2                    pypi_0    pypi
openssl                   3.0.7                h0b41bf4_1    conda-forge
pettingzoo                1.15.0                   pypi_0    pypi
pillow                    9.4.0                    pypi_0    pypi
pip                       22.3.1             pyhd8ed1ab_0    conda-forge
protobuf                  3.19.6                   pypi_0    pypi
pyasn1                    0.4.8                    pypi_0    pypi
pyasn1-modules            0.2.8                    pypi_0    pypi
python                    3.10.8          h4a9ceb5_0_cpython    conda-forge
pyyaml                    6.0                      pypi_0    pypi
readline                  8.1.2                h0f457ee_0    conda-forge
requests                  2.28.1                   pypi_0    pypi
requests-oauthlib         1.3.1                    pypi_0    pypi
rsa                       4.9                      pypi_0    pypi
setuptools                65.6.3             pyhd8ed1ab_0    conda-forge
six                       1.16.0                   pypi_0    pypi
tensorboard               2.11.0                   pypi_0    pypi
tensorboard-data-server   0.6.1                    pypi_0    pypi
tensorboard-plugin-wit    1.8.1                    pypi_0    pypi
tk                        8.6.12               h27826a3_0    conda-forge
torch                     1.11.0                   pypi_0    pypi
typing-extensions         4.4.0                    pypi_0    pypi
tzdata                    2022g                h191b570_0    conda-forge
urllib3                   1.26.13                  pypi_0    pypi
werkzeug                  2.2.2                    pypi_0    pypi
wheel                     0.38.4             pyhd8ed1ab_0    conda-forge
xz                        5.2.6                h166bdaf_0    conda-forge
```",install version compatible fresh install new mamba list environment name version build channel gym markdown dev develop dev develop pillow pip python six torch wheel,issue,negative,positive,positive,positive,positive,positive
1576530236,"install ml-agents-envs first, then ml-agents, if you are install from source. Execute

`pip install -e ./ml-agents-envs`
`pip install -e. ./ml-agents`

from the source directory. Otherwise, when installing ml-agents from source, without having first installed ml-agents-envs, the pip installer will install ml-agents-envs from pypi, which may have conflicts.",install first install source execute pip install pip install source directory otherwise source without first pip installer install may,issue,negative,positive,positive,positive,positive,positive
1576526731,Please submit this question on the forum: https://forum.unity.com/forums/ml-agents.453/,please submit question forum,issue,negative,neutral,neutral,neutral,neutral,neutral
1576522981,"> I don't think I rebase with develop on the main repo. HF-integration was created from develop because develop is the default branch in ML-Agents. Do I need to change something on this part?

You should rebase with the develop branch from the main repo. It may have diverged from the time that you created your fork/feature. I'll rerun the workflows and see if there are conflicts.",think rebase develop main develop develop default branch need change something part rebase develop branch main may time rerun see,issue,negative,positive,positive,positive,positive,positive
1573454035,"I've just added the documentation and run black let me know if you want me to change something else.

I don't think I rebase with develop on the main repo. HF-integration was created from develop because develop is the default branch in ML-Agents. Do I need to change something on this part?
",added documentation run black let know want change something else think rebase develop main develop develop default branch need change something part,issue,negative,neutral,neutral,neutral,neutral,neutral
1570637060,"Hi, i get the same sort of error (numpy.float32 instead of int32) and upgrading to version 1.22.0 didnt work for me ",hi get sort error instead version didnt work,issue,negative,neutral,neutral,neutral,neutral,neutral
1568342295,I updated based on what you asked ✅. Sorry for the big delay I was cleaning the notifications today and I saw this one.,based sorry big delay cleaning today saw one,issue,negative,negative,negative,negative,negative,negative
1566822098,"Hi, I also had this same issue. The solution which worked for me was by upgrading the numpy version to 1.22.0
The upgradation will give you an incompatibility error saying ""mlagents-envs requires numpy==1.21.2"", which you can just ignore.",hi also issue solution worked version give incompatibility error saying ignore,issue,negative,neutral,neutral,neutral,neutral,neutral
1566163024,"I also filed a similar PR #5922, but it removes the second entry since that one pins numpy to an exact version which is problematic.",also similar second entry since one exact version problematic,issue,negative,positive,neutral,neutral,positive,positive
1559312608,(cc: @miguelalonsojr) I was hoping you could take a look; it’s a really small fix that’s pretty important.,could take look really small fix pretty important,issue,positive,positive,positive,positive,positive,positive
1555934490,"> Later I found the workaround described in this thread, and it solved the situation:
> 
> #5794 pip install importlib-metadata==4.4
> 
> Never-the-less, either the bug should be corrected, or the instructions updated. I spent hours trying to install mlagents and getting different errors due to version incompatibilities which are not documented...

actual god
half want to build a temple for you",later found thread situation pip install either bug corrected spent trying install getting different due version actual god half want build temple,issue,negative,negative,neutral,neutral,negative,negative
1553103334,"Hi @mra16r , I don't think that's a question you should ask here, since it's more about how Unity works instead of how ML-Agents works.
Either way, you could setup an array of checkpoints with a specific order, and the agent gets a positive reward if he goes through them in order, and a negative one if he skips one for instance.",hi think question ask since unity work instead work either way could setup array specific order agent positive reward go order negative one one instance,issue,positive,negative,neutral,neutral,negative,negative
1547966852,"~~Edit ✅ We changed the logger to MLAgents logger. However, the MLAgents logger does not display our info messages on collab. I'm wondering why 🤔 ?~~

Edit thanks to @osanseviero for the solution 🤗 

The integration is ready feel free to tell me if you want us to change something 🤗 ",logger logger however logger display wondering edit thanks solution integration ready feel free tell want u change something,issue,positive,positive,positive,positive,positive,positive
1539102320,"To setup the training configuration you need to create a .yaml file as explained in  [docs/Training-ML-Agents.md](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Training-ML-Agents.md#training-configurations)
Maybe this walkthrough can help (September 22, 2021) : https://www.gocoder.one/blog/training-agents-using-ppo-with-unity-ml-agents/
They create a .yaml file in config/ppo as ""a trainer configuration file"".

Otherwise, I found an old article (2020, with ml-agents release_1) that guides on how to apply PPO algorithms using just the mlagents-envs package. Maybe it can help you find a workaround instead of using /ml-agents/ml-agents/mlagents/trainers/settings.py in the meantime?
https://medium.com/analytics-vidhya/ppo-algorithm-with-custom-rl-environment-made-with-unity-engine-effed6d98b9d
",setup training configuration need create file maybe help create file trainer configuration file otherwise found old article apply package maybe help find instead,issue,positive,positive,neutral,neutral,positive,positive
1538890571,"Is ""trainer_config.yaml"" a provided file or a custom file?",provided file custom file,issue,negative,neutral,neutral,neutral,neutral,neutral
1538587803,"Updated ✅ what **do you prefer for logging**? Usually, I like to print messages so that people can see them on their console or notebook but logging is also a correct solution. 🤔 

",prefer logging usually like print people see console notebook logging also correct solution,issue,positive,negative,negative,negative,negative,negative
1537162131,"You should also change the timescale if you want training to take less time, otherwise its running at the same pace just not rendering

you should be able to handle a higher timescale without rendering anything ",also change want training take le time otherwise running pace rendering able handle higher without rendering anything,issue,negative,positive,positive,positive,positive,positive
1535847966,"Also waiting for native support for MPS device type. Hoping it will be implemented soon, subscribing to the thread for eventual updated",also waiting native support device type soon thread eventual,issue,negative,neutral,neutral,neutral,neutral,neutral
1532212450,"@KhalfounMehdi Solution : `pip install protobuf==3.20 ` worked for me. Thanks!. @ufodriverr  try this solution.

",solution pip install worked thanks try solution,issue,positive,positive,positive,positive,positive,positive
1529250684,"I fixed it by downgrading python, but another issue has risen:
E:\mlgame\ml-agents-release_20\Project\Assets>mlagents-learn config/trainer_config.yaml --run-id=test
Traceback (most recent call last):
  File ""c:\users\user\appdata\local\programs\python\python37\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\user\appdata\local\programs\python\python37\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37\Scripts\mlagents-learn.exe\__main__.py"", line 7, in <module>
  File ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\mlagents\trainers\learn.py"", line 260, in main
    run_cli(parse_command_line())
  File ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\mlagents\trainers\learn.py"", line 52, in parse_command_line
    return RunOptions.from_argparse(args)
  File ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\mlagents\trainers\settings.py"", line 931, in from_argparse
    key
mlagents.trainers.exception.TrainerConfigError: The option default was specified in your YAML file, but is invalid.",fixed python another issue risen recent call last file line file line code file line module file line main file line return file line key option default file invalid,issue,negative,positive,neutral,neutral,positive,positive
1528187436,"@miguelalonsojr 
![image](https://user-images.githubusercontent.com/7818629/235268902-52dd8edf-1fae-4011-a6e3-5649e286c899.png)
https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Getting-Started.md#installation
https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Installation.md
It still tells to install torch 1.7.1

Will try to install everything like this and see what will happen:
Python latest
Install torch
install ml-agents

So it will take all latest releases.
=== Updated
Nope. Same thing. I guess i give up until there will be some tutorial. Im fine with learning it afterwards but i need something to start with with out spending 999 hours on it XD
I remember it was working on release and i was able to train sample scene using Unitys tutorial, and then try it for my self but now i feel like Unity forgot about ML-Agents =(",image still install torch try install everything like see happen python latest install torch install take latest nope thing guess give tutorial fine learning afterwards need something start spending remember working release able train sample scene tutorial try self feel like unity forgot,issue,positive,positive,positive,positive,positive,positive
1525042175,"I did a bit more digging, have some more details if it helps, for me with self play it happens on restart and near the start of the session for team change.  but no more through that training session until next swap.  Workaround I did is always mask 0 if I get 0 do a EpisodeInterrupted() for all agents and restart the episode and made it so no training will resume until all the agents have reinitialised.

Running headless across 10 environments they same issue happens at exactly the same time across all environments.

In the logs below the first int is the academy step the float is the fixed update since start.

This is consistent for every time there is this issue, If you look at the end of the log, on the same fixed update for some reason the academy step resets to 0 after WriteDiscreteActionMask is called then calls BeginEpisode on both agents then jumps back to on action received but passes a 0

log : 15512 : 666.82 : playerO : ChangePlayer New Player
log : 15513 : 666.84 : playerO : RequestDecision : Start : PerformingMove
log : 15513 : 666.84 : playerO : RequestDecision : End : PerformingMove
log : 15514 : 666.86 : Agent Player : playerO : CollectObservations : PerformingMove
log : 15514 : 666.86 : Agent Player : playerO : CollectObservations : 1, 0, 0.5, 0, 0, 0, 0, 0, 0.5
log : 15514 : 666.86 : Agent Player : playerO : WriteDiscreteActionMask : PerformingMove
log : 15514 : 666.86 : Agent Player : playerO : WriteDiscreteActionMask : End
log : 15514 : 666.86 : Agent Player : playerO : OnActionReceived : PerformingMove
log : 15514 : 666.86 : Agent Player : playerO : OnActionReceived : Could Win : False
log : 15514 : 666.86 : Agent Player : playerO : OnActionReceived : Placed Piece : 2
log : 15514 : 666.86 : Agent Player : playerO : OnActionReceived : Could Block : True
log : 15514 : 666.86 : Agent Player : playerO : OnActionReceived : Could Block but did not : -1.5
log : 15514 : 666.86 : Agent Player : playerO : OnActionReceived : End
log : 15514 : 666.86 : playerO : RequestDecision : Start : ObservingMove
log : 15514 : 666.86 : playerO : RequestDecision : End : ObservingMove
log : 15515 : 666.88 : Agent Player : playerO : CollectObservations : ObservingMove
log : 15515 : 666.88 : Agent Player : playerO : CollectObservations : 1, 1, 0.5, 0, 0, 0, 0, 0, 0.5
log : 15515 : 666.88 : Agent Player : playerO : WriteDiscreteActionMask : ObservingMove
log : 15515 : 666.88 : Agent Player : playerO : WriteDiscreteActionMask : End
log : 15515 : 666.88 : Agent Player : playerO : OnActionReceived : ObservingMove
log : 15515 : 666.88 : Agent Player : playerO : OnActionReceived : Took No Action Observation Only swap to next player
log : 15515 : 666.88 : Agent Player : playerO : OnActionReceived : End
log : 15515 : 666.88 : playerO : ChangePlayer Current Player
log : 15515 : 666.88 : playerX : ChangePlayer New Player
log : 15516 : 666.9 : playerX : RequestDecision : Start : PerformingMove
log : 15516 : 666.9 : playerX : RequestDecision : End : PerformingMove
log : 15517 : 666.92 : Agent Player : playerX : CollectObservations : PerformingMove
log : 15517 : 666.92 : Agent Player : playerX : CollectObservations : 1, 1, 0.5, 0, 0, 0, 0, 0, 0.5
log : 15517 : 666.92 : Agent Player : playerX : WriteDiscreteActionMask : PerformingMove
log : 15517 : 666.92 : Agent Player : playerX : WriteDiscreteActionMask : End
log : 0 : 666.92 : Agent Player : playerX : OnEpisodeBegin : 
log : 0 : 666.92 : Agent Player : playerO : OnEpisodeBegin : 
log : 0 : 666.92 : Agent Player : playerX : OnActionReceived : PerformingMove
warn : 0 : 666.92 : Agent Player : playerX : OnActionReceived : Invalid action, received : 0

",bit digging self play restart near start session team change training session next swap always mask get restart episode made training resume running headless across issue exactly time across first academy step float fixed update since start consistent every time issue look end log fixed update reason academy step back action received log new player log start log end log agent player log agent player log agent player log agent player end log agent player log agent player could win false log agent player piece log agent player could block true log agent player could block log agent player end log start log end log agent player log agent player log agent player log agent player end log agent player log agent player took action observation swap next player log agent player end log current player log new player log start log end log agent player log agent player log agent player log agent player end log agent player log agent player log agent player warn agent player invalid action received,issue,negative,positive,positive,positive,positive,positive
1523787966,"Hey there 👋 here is the integration update:

From outside nothing changed like the 3000 models already in the Hugging Face Hub, we run `mlagents-push-to-hf --run-id --local-dir --repo-id --commit-message` to upload a model and `mlagents-load-from-hf --repo-id --local-dir` to load a model.

The models: https://huggingface.co/models?library=ml-agents

- We changed t**he upload system by using create_repo and upload_folder.**
- We **changed the model card** you can see an example of it here: 
https://huggingface.co/ThomasSimonini/testpyramidsrndNewintegration

If you want to try the implementation I wrote a notebook (start at START HERE 😄 ): https://colab.research.google.com/drive/1omWRS5ArBC6kkyOV_-oG54Iz3BORvAsO?usp=sharing

Some questions:
1. Do you want to use a logger or print for the info given to the user during the downloading and the uploading?
2. Omar for the model card do you have something you want me to change?
3. Omar for setup.py I use `""huggingface_hub~=0.8"",`.
4. I have a problem specific to Colab: ImportError: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.9/dist-packages/tensorboard/compat/__init__.py)
What solutions do we have for that? I suppose it's due to Colab that now uses Python 3.9

🤗 ",hey integration update outside nothing like already hugging face hub run model load model system model card see example want try implementation wrote notebook start start want use logger print given user model card something want change use problem specific import name suppose due python,issue,negative,negative,neutral,neutral,negative,negative
1522505267,I cloned the repository and extracted the ml-agents folder into my unity project. I used the setup.py to install the dependencies. It has at least started training for now. ,repository extracted folder unity project used install least training,issue,negative,negative,negative,negative,negative,negative
1522239295,"The yamato tests are still failing. Also, I see a bunch of new project settings files. Can you delete those?",still failing also see bunch new project delete,issue,negative,positive,positive,positive,positive,positive
1521720838,"> Your buffer is currently using 26GB of memory, leaving only 6GB for your editor and system. However, in reality, this amount may even be less. If your buffer and data are too large, you may run out of memory. To avoid this issue, you can try using a smaller buffer size such as 1024, which would only require 2.6GB of memory for the buffer.

Ouh. I see. You calculated the 26GB like in the example of your previous comment?",buffer currently memory leaving editor system however reality amount may even le buffer data large may run memory avoid issue try smaller buffer size would require memory buffer see calculated like example previous comment,issue,negative,positive,neutral,neutral,positive,positive
1521701547,"Your buffer is currently using 26GB of memory, leaving only 6GB for your editor and system. However, in reality, this amount may even be less. If your buffer and data are too large, you may run out of memory. To avoid this issue, you can try using a smaller buffer size such as 1024, which would only require 2.6GB of memory for the buffer.",buffer currently memory leaving editor system however reality amount may even le buffer data large may run memory avoid issue try smaller buffer size would require memory buffer,issue,negative,positive,neutral,neutral,positive,positive
1521601535,"> I don't know how to access your project. The buffer size of 10240 is not small. May I ask what resolution your vision sensor has?
> 
> 10240 x resolution (320 x 200) x 8 bytes (for greyscale 0-255, or 24 bytes for color) = 5,242,880,000 bytes, which is approximately 5GB for your buffer.

My project is shared right here in to 2 ways in ZIP (read post and first comment). You can just click and download. 

My resolution is big, 800x400 grayscale. I know its huge, but the project requires such. And in general, should the memory just keep growing because of this, until running out of 32 GB? Shouldn't the memory be released at some points? Maybe I don't understand yet something how the buffers/batches work?",know access project buffer size small may ask resolution vision sensor resolution color approximately buffer project right way zip read post first comment click resolution big know huge project general memory keep growing running memory maybe understand yet something work,issue,positive,positive,neutral,neutral,positive,positive
1521488084,@miguelalonsojr @KhalfounMehdi Will try those solutions later. Have no time for now =( But will try do them this weekend.,try later time try weekend,issue,negative,neutral,neutral,neutral,neutral,neutral
1521402367,"it's an error with protobuf ( protocol buffers ) library
just do
pip install protobuf==3.20 @miguelalonsojr  @ufodriverr ",error protocol library pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
1518683919,"I’m using yaml files for configurations of the training and objects to spawn in the environment. Please read my original issue again. There seems to be a connection when using yaml and unity build and when this connection is interrupted the build freezes/crashes
On 22 Apr 2023 at 16:09 +0100, Miguel Alonso Jr. ***@***.***>, wrote:
> Here are the details of how to use executables (builds) for training: https://unity-technologies.github.io/ml-agents/Learning-Environment-Executable/
> —
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you authored the thread.Message ID: ***@***.***>
",training spawn environment please read original issue connection unity build connection interrupted build wrote use training reply directly view id,issue,negative,positive,positive,positive,positive,positive
1518682215,"Thank you for confirming. Are you making a build of Unity and then pressing play in the editor and then closing the editor/build window? If so, that's not the way to use builds or the editor. Builds are stand alone executables and don't need the editor to be used for training. When you make a build, it can run outside the editor. You don't even need the editor running to use a build for training. If you are training in the editor, it runs within the editor, not in a build, and you cannot close the editor during training. These are two different methods of training.",thank confirming making build unity pressing play editor window way use editor stand alone need editor used training make build run outside editor even need editor running use build training training editor within editor build close editor training two different training,issue,negative,neutral,neutral,neutral,neutral,neutral
1518676885,"I just tried it and it’s the same error regardless of the environment.
On 22 Apr 2023 at 15:26 +0100, Miguel Alonso Jr. ***@***.***>, wrote:
> Our environments are part of this repository. If you open the Project folder in Unity, you'll see all of our examples.
> —
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you authored the thread.Message ID: ***@***.***>
",tried error regardless environment wrote part repository open project folder unity see reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1518671986,"Our environments are part of this repository. If you open the Project folder in Unity, you'll see all of our examples.",part repository open project folder unity see,issue,negative,neutral,neutral,neutral,neutral,neutral
1518670925,"> Thanks for the request. We're planning these updates for our next release.

Great to hear, thanks! We are planning a major pettingzoo release either this coming Tuesday or the following Tuesday. The only breaking change is removing the “return_info” argument for reset(), to match gymnasium and always return an info dict. We also fixed a number of bugs, and don’t intend to do any other major releases for a while, so it would be great if ml-agents could be compatible with it. 

I would be happy to help double check that things are fully compatible if you want help.",thanks request next release great hear thanks major release either coming following breaking change removing argument reset match gymnasium always return also fixed number intend major would great could compatible would happy help double check fully compatible want help,issue,positive,positive,positive,positive,positive,positive
1518626852,Thanks for the request. We're planning these updates for our next release.,thanks request next release,issue,negative,positive,neutral,neutral,positive,positive
1518626607,That tutorial may be outdated. Please follow the installation instructions here: https://unity-technologies.github.io/ml-agents/Installation/ and our getting started guide here: https://unity-technologies.github.io/ml-agents/Getting-Started/ ,tutorial may outdated please follow installation getting guide,issue,negative,negative,negative,negative,negative,negative
1518626002,"Hello! The platforms that Unity supports are listed here: https://support.unity.com/hc/en-us/articles/206336795-What-platforms-are-supported-by-Unity- and here https://docs.unity3d.com/ScriptReference/BuildTarget.html. We support desktop for training, and all Unity supported platforms for inference/deployment. Unfortunately, PowerPC is not among the list of supported architectures.",hello unity listed support training unity unfortunately among list,issue,negative,negative,negative,negative,negative,negative
1518624111,Can you do a clean install using the github repo from the release 20 tag? We deprecated torch 1.7. https://github.com/Unity-Technologies/ml-agents/releases/tag/release_20,clean install release tag torch,issue,negative,positive,positive,positive,positive,positive
1518623059,"Please resubmit your bug report using our template. This will allow us to try to reproduce the error. Also, We are unable to help reproduce bugs with custom environments. Please attempt to reproduce your issue with one of the example environments, or provide a minimal patch to one of the environments needed to reproduce the issue. If it turns our this is not a bug, I'd recommend posting to our forums and seek help from the community: https://forum.unity.com/forums/ml-agents.453/. ",please resubmit bug report template allow u try reproduce error also unable help reproduce custom please attempt reproduce issue one example provide minimal patch one reproduce issue turn bug recommend posting seek help community,issue,positive,negative,negative,negative,negative,negative
1518622630,I understand your frustration and would love to help. We'd be happy to accept a contribution that improves our documentation and makes it easier to follow. Our code of conduct for the community (https://github.com/Unity-Technologies/ml-agents/blob/develop/CODE_OF_CONDUCT.md) helps keep everyone engaged in a positive manner. I'd recommend posting questions about how to use ML-Agents to our forums: https://forum.unity.com/forums/ml-agents.453/ Our github issues are for bugs and feature requests.,understand frustration would love help happy accept contribution documentation easier follow code conduct community keep everyone engaged positive manner recommend posting use feature,issue,positive,positive,positive,positive,positive,positive
1518615722,"Please resubmit your bug report using our template. This will allow us to try to reproduce the error. Also, We are unable to help reproduce bugs with custom environments. Please attempt to reproduce your issue with one of the example environments, or provide a minimal patch to one of the environments needed to reproduce the issue.",please resubmit bug report template allow u try reproduce error also unable help reproduce custom please attempt reproduce issue one example provide minimal patch one reproduce issue,issue,positive,negative,negative,negative,negative,negative
1518615267,"If you look closely, all of your parameters are being used. When training begins, ml-agents prints ALL of the parameters, including the defaults. If you don't include a parameter in your training config file, ml-agents uses the default. It doesn't mean that yours are not being utilized.",look closely used training include parameter training file default mean,issue,negative,negative,negative,negative,negative,negative
1518614440,"You should be using ML-Agents from the repo, not from the package manager. Version 2.01. of the Unity package does not work with version 0.30.0 of the python package. You can either use the develop branch of the repo for bleeding edge, or our last release https://github.com/Unity-Technologies/ml-agents/releases/tag/release_20 . It will be a while before a new package is available through the package manager in the editor.",package manager version unity package work version python package either use develop branch bleeding edge last release new package available package manager editor,issue,negative,positive,positive,positive,positive,positive
1518614038,"I'm unable to reproduce this issue. If you are using the latest version of ml-agents from github on the develop branch, you should install ml-agents-envs and ml-agents from the repo, not from pypi. If you're using ml-agents from pypi, you should make sure that you're using the example environments on the correct release branch. The latest release that was published to pypi is release 20: https://github.com/Unity-Technologies/ml-agents/releases/tag/release_20 ",unable reproduce issue latest version develop branch install make sure example correct release branch latest release release,issue,negative,positive,positive,positive,positive,positive
1518611498,"I do not have access to your environments. If you search AnimalAI on GitHub you will see the environment
On 22 Apr 2023 at 12:24 +0100, Miguel Alonso Jr. ***@***.***>, wrote:
> It would be great if you can reproduce this with one of our environments and update the bug report accordingly. Thanks!
> —
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you authored the thread.Message ID: ***@***.***>
",access search see environment wrote would great reproduce one update bug report accordingly thanks reply directly view id,issue,positive,positive,positive,positive,positive,positive
1518611183,Please resubmit your bug report using our template https://github.com/Unity-Technologies/ml-agents/blob/develop/.github/ISSUE_TEMPLATE/bug_report.md. This will allow us to try to reproduce the error.,please resubmit bug report template allow u try reproduce error,issue,negative,neutral,neutral,neutral,neutral,neutral
1518610133,It would be great if you can reproduce this with one of our environments and update the bug report accordingly. Thanks!,would great reproduce one update bug report accordingly thanks,issue,positive,positive,positive,positive,positive,positive
1518609688,"It’s a custom environment but I am assuming the same thing would happen with your environments as the problem probably lies with the connection interruption between python and unity via yaml files.
On 22 Apr 2023 at 12:20 +0100, Miguel Alonso Jr. ***@***.***>, wrote:
> Is this with a custom Unity environment or one of our example environments?
> —
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you authored the thread.Message ID: ***@***.***>
",custom environment assuming thing would happen problem probably connection interruption python unity via wrote custom unity environment one example reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1518609397,"Hi! We are unable to help reproduce bugs with custom environments. Please attempt to reproduce your issue with one of the example environments, or provide a minimal patch to one of the environments needed to reproduce the issue.",hi unable help reproduce custom please attempt reproduce issue one example provide minimal patch one reproduce issue,issue,positive,negative,negative,negative,negative,negative
1518608829,Is this with a custom Unity environment or one of our example environments?,custom unity environment one example,issue,negative,neutral,neutral,neutral,neutral,neutral
1518606339,"@simoninithomas Is this ready to go? I can get it reviewed and merged to develop next week if it's ready, assuming CI passes.",ready go get develop next week ready assuming,issue,positive,positive,positive,positive,positive,positive
1518279283,"I've been fighting with this for a week as well, i think it's def a bug.  It's not just self play even if you have 2 agents learning off each other you get the same issue with discrete actions.  I did loads of testing I have 0 100% masked all the time and I still get 0 passed across as an action every now and then.  I'm training turn based so it's really obvious when it goes out of sync, I don't think it would notice for more general actions.  I worked around it killing any episodes where 0 got passed but even then it was unstable.   I'm doing it manually and it's rock solid just takes longer, so train one agent against random/sample set heuristics then train the other again back against the brain from that and repeat.  Had a poke about in the source code, found a few if actionmask is null return empty array of 0 but couldn't pin down exactly where it was happening. 

33 : 1 : playerO : CollectObservations : observationAndMoveInProgress : True
33 : 1 : playerO : CollectObservations : postMoveObservationInProgress : False
33 : 1 : playerO : CollectObservations : PlayerO Observations : 0, 0, 0, 0.5, 0, 0, 0, 0, 0
33 : 1 : playerO : WriteDiscreteActionMask : Start
33 : 1 : playerO : WriteDiscreteActionMask : postMoveObservationInProgress : False
33 : 1 : playerO : WriteDiscreteActionMask : End
33 : 1 : playerO : ProcessAction : Start
38 : 1 : playerO : OnActionReceived : observationAndMoveInProgress : True : playerO : OnActionReceived : 0
38 : 1 : playerO : OnActionReceived : postMoveObservationInProgress : False : playerO : OnActionReceived : 0
38 : 1 : playerO : OnActionReceived: got unexpected value : 0",fighting week well think bug self play even learning get issue discrete testing masked time still get across action every training turn based really obvious go sync think would notice general worked around killing got even unstable manually rock solid longer train one agent set train back brain repeat poke source code found null return empty array could pin exactly happening true false start false end start true false got unexpected value,issue,negative,negative,neutral,neutral,negative,negative
1515359065,I should also add that there is an established communication between unity and python/yaml config files and that communication needs to be deactivated or terminated and maybe the window will not freeze/crash when clicked on the X (close) button or pressed on Q button on keyboard which will terminate the connection first then close the build window. Any help would be appreciated.,also add established communication unity communication need maybe window close button button keyboard terminate connection first close build window help would,issue,negative,positive,positive,positive,positive,positive
1513869252,"I don't know how to access your project. The buffer size of 10240 is not small. May I ask what resolution your vision sensor has?

10240 x resolution (320 x 200) x 8 bytes (for greyscale 0-255, or 24 bytes for color) = 5,242,880,000 bytes, which is approximately 5GB for your buffer.
",know access project buffer size small may ask resolution vision sensor resolution color approximately buffer,issue,negative,negative,negative,negative,negative,negative
1506472947,"Some people reported issues opening the scene, so I am including the project as package export:

[AI_DIST_PERC_exp_package.zip](https://github.com/Unity-Technologies/ml-agents/files/11219296/AI_DIST_PERC_exp_package.zip)






",people opening scene project package export,issue,negative,neutral,neutral,neutral,neutral,neutral
1498512448,"Hey I apologize my tone but it's incredibly frustrating that I can't even follow your suggestion if I can't decipher what is there to at least get it working one time.  It's beyond the level of making contributions which is really sad since Unity has alot going for it.

Get Outlook for Android<https://aka.ms/AAb9ysg>
________________________________
From: Elliot Tower ***@***.***>
Sent: Thursday, April 6, 2023 2:05:02 PM
To: Unity-Technologies/ml-agents ***@***.***>
Cc: Kim, Nobutaka ***@***.***>; Author ***@***.***>
Subject: Re: [Unity-Technologies/ml-agents] Worst instructions ever (Issue #5899)


I’m not a maintainer of this repository but if you want help with something it’s probably not the best idea to be rude and make issues like this. If you have specific changes to documentation which you think would be helpful for other users you can spend the time you spent making this instead making a pull request.

—
Reply to this email directly, view it on GitHub<https://github.com/Unity-Technologies/ml-agents/issues/5899#issuecomment-1498496761>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AGAFZKJDTA5EDQENWZYUEKLW7ZFH5ANCNFSM6AAAAAAWRKB7JM>.
You are receiving this because you authored the thread.Message ID: ***@***.***>
",hey apologize tone incredibly ca even follow suggestion ca decipher least get working one time beyond level making really sad since unity going get outlook android tower sent kim author subject worst ever issue maintainer repository want help something probably best idea rude make like specific documentation think would helpful spend time spent making instead making pull request reply directly view id,issue,negative,negative,neutral,neutral,negative,negative
1498510186,"Can’t offer help but would be curious if you figure out the solution on your own, as I am interested in doing a similar thing with another board game. Maybe you could post your full code/repo once you have it working so other people can use it for reference. 

If you just want to train an agent to play a board game and don’t care as much about the rendering, you could try implementing your game using [PettingZoo](https://github.com/Farama-Foundation/PettingZoo) and trying various training methods there, to ensure that the issue is not due to unity. I can’t comment on how well the integration works with unity ml-agents and pettingzoo but there are a good amount of tutorials on here. Disclaimer that I’m a maintainer of PettingZoo so I’m obviously biased but we have a discord and active dev team which can help with any questions you encounter.

I’m not 100% sure but I believe you can use PettingZoo with the unity ml-agents low level API in order to control the environment through python. I am planning to test this out in the near future so I can let you know if I am able to get it working.",offer help would curious figure solution interested similar thing another board game maybe could post full working people use reference want train agent play board game care much rendering could try game trying various training ensure issue due unity comment well integration work unity good amount disclaimer maintainer obviously discord active dev team help encounter sure believe use unity low level order control environment python test near future let know able get working,issue,positive,positive,neutral,neutral,positive,positive
1498503810,"> I am still unable to import UnityAECEnv. Is it from mlagents_envs.envs import UnityAECEnv or am I missing something?
> I am trying to work with UnitToPettingZooWrapper and tried to import UnityAECEnv but getting the error
> ImportError: cannot import name 'UnityAECEnv' from 'mlagents_envs.environment'

I think the correct import statement would be `from mlagent_envs.envs.unity_aec_env import UnityAECEnv`. If you check the [envs directory](https://github.com/Unity-Technologies/ml-agents/tree/develop/ml-agents-envs/mlagents_envs/envs) you can see it’s within the unity_aec_env file, and then if you check [`__init__.py`](https://github.com/Unity-Technologies/ml-agents/blob/develop/ml-agents-envs/mlagents_envs/envs/__init__.py) you can see it doesn’t import the submodules automatically (i.e., you have to import explicitly from the folder which the class is located, rather than from envs import UnityAECEnv).

Not sure if this is intentional or not but a lot of other repositories will have their `__init__.py` files import the submodules so that users have an easier time doing what you tried to do. Adding the lines `from unity_aec_env imprt UnityAECEnv` and so forth to the init file will allow your command to work.

Also if you look at the documentation it shows that the class is from the unity_aec_env file, just for future reference if you want to make sure you are importing correctly and not have to look through the source code: https://unity-technologies.github.io/ml-agents/Python-PettingZoo-API-Documentation/#mlagents_envs.envs.unity_aec_env.UnityAECEnv",still unable import import missing something trying work tried import getting error import name think correct import statement would import check directory see within file check see import automatically import explicitly folder class rather import sure intentional lot import easier time tried forth file allow command work also look documentation class file future reference want make sure correctly look source code,issue,positive,positive,neutral,neutral,positive,positive
1498496761,I’m not a maintainer of this repository but if you want help with something it’s probably not the best idea to be rude and make issues like this. If you have specific changes to documentation which you think would be helpful for other users you can spend the time you spent making this instead making a pull request.,maintainer repository want help something probably best idea rude make like specific documentation think would helpful spend time spent making instead making pull request,issue,positive,positive,positive,positive,positive,positive
1498490929,"Sorry for the confusion, I didn’t mean to create two issues for this.",sorry confusion mean create two,issue,negative,negative,negative,negative,negative,negative
1498490071,"Hi, just bumping this as I would love to be able to use gymnasium/pettingzoo with unity ml-agents. I would be happy to contribute any help which is needed, just wanted to see if this is on the radar and how difficult it would be. 

We are cutting a new pettingzoo release shortly with some important fixes (no major breaking changes) and I was thinking after that is out it would be great to try and update this project to be compatible. Gymnasium had a new release V28 recently and I would be happy to contribute to make that compatible as well.",hi bumping would love able use unity would happy contribute help see radar difficult would cutting new release shortly important major breaking thinking would great try update project compatible gymnasium new release recently would happy contribute make compatible well,issue,positive,positive,positive,positive,positive,positive
1497283009,"So another document says use the full path but it's still not working: 
```
mlagents_envs.exception.UnityEnvironmentException: Couldn't launch the /mnt/c/nobut/Documents/gameAI/ml-agents-release_18_branch/ml-agents-release_18_branch/Project/MLAgentsBuilds/3DBall2/3DBall environment. Provided filename does not match any environments.
```
Why don't you say that everywhere?  Also give an example with pictures.  Or make a video.",another document use full path still working could launch environment provided match say everywhere also give example make video,issue,negative,positive,positive,positive,positive,positive
1497110061,"I renamed it to 3DBall in a new subfolder but still get
```
(mlagents) nyck33@NyckLenovoIdeapad3:/mnt/c/Users/nobut/Documents/gameAI/ml-agents-release_18_branch/ml-agents-release_18_branch/Project/MLAgentsBuilds$ mlagents-learn /mnt/c/Users/nobut/Documents/gameAI/ml-agents-release_18_branch/ml-agents-release_18_branch/config/ppo/3DBall.yaml --env=3DBall2 --run-id=3dball2


                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓


 Version information:
  ml-agents: 0.27.0,
  ml-agents-envs: 0.27.0,
  Communicator API: 1.5.0,
  PyTorch: 1.8.1+cu102
[INFO] Learning was interrupted. Please wait while the graph is generated.
Traceback (most recent call last):
  File ""/home/nyck33/miniconda3/envs/mlagents/bin/mlagents-learn"", line 8, in <module>
    sys.exit(main())
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/learn.py"", line 250, in main
    run_cli(parse_command_line())
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/learn.py"", line 246, in run_cli
    run_training(run_seed, options)
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/learn.py"", line 125, in run_training
    tc.start_learning(env_manager)
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py"", line 198, in start_learning
    raise ex
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py"", line 173, in start_learning
    self._reset_env(env_manager)
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py"", line 105, in _reset_env
    env_manager.reset(config=new_config)
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/env_manager.py"", line 68, in reset
    self.first_step_infos = self._reset_env(config)
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py"", line 333, in _reset_env
    ew.previous_step = EnvironmentStep(ew.recv().payload, ew.worker_id, {}, {})
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py"", line 98, in recv
    raise env_exception
mlagents_envs.exception.UnityEnvironmentException: Couldn't launch the 3DBall2 environment. Provided filename does not match any environments.
```",new still get version information communicator learning interrupted please wait graph recent call last file line module main file line main file line file line file line wrapped return file line raise ex file line file line wrapped return file line file line reset file line file line raise could launch environment provided match,issue,negative,positive,neutral,neutral,positive,positive
1497085353,"Then you contradict yourself

For example, if you are training with a 3DBall executable, and you saved it to the directory where you installed the ML-Agents Toolkit, run:

mlagents-learn config/ppo/3DBall.yaml --env=3DBall --run-id=firstRun

So I am using the folder name or the name of the executable????

As you can see, my executable is called UnityEnvironment it looks like, exactly the same for every ****ing build I do.  
",contradict example training executable saved directory run folder name name executable see executable like exactly every ing build,issue,negative,positive,positive,positive,positive,positive
1497057371,"I try to use the name of the subfolder to which I built as env_name and get
```
(mlagents) nyck33@NyckLenovoIdeapad3:/mnt/c/Users/nobut/Documents/gameAI/ml-agents-rereleasrrrelease_18_branch/ml-agents-release_18_branch/Project$ mlagents-learn /mnt/c/Users/nobut/Documents/game
AI/ml-agents-release_18_branch/ml-agents-release_18_branch/config/ppo/3DBall.yaml --env=3DBallBuild --run-id=firstRunExec --resume


                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓


 Version information:
  ml-agents: 0.27.0,
  ml-agents-envs: 0.27.0,
  Communicator API: 1.5.0,
  PyTorch: 1.8.1+cu102
[INFO] Learning was interrupted. Please wait while the graph is generated.
Traceback (most recent call last):
  File ""/home/nyck33/miniconda3/envs/mlagents/bin/mlagents-learn"", line 8, in <module>
    sys.exit(main())
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/learn.py"", line 250, in main
    run_cli(parse_command_line())
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/learn.py"", line 246, in run_cli
    run_training(run_seed, options)
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/learn.py"", line 125, in run_training
    tc.start_learning(env_manager)
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py"", line 198, in start_learning
    raise ex
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py"", line 173, in start_learning
    self._reset_env(env_manager)
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py"", line 105, in _reset_env
    env_manager.reset(config=new_config)
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/env_manager.py"", line 68, in reset
    self.first_step_infos = self._reset_env(config)
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py"", line 333, in _reset_env
    ew.previous_step = EnvironmentStep(ew.recv().payload, ew.worker_id, {}, {})
  File ""/home/nyck33/miniconda3/envs/mlagents/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py"", line 98, in recv
    raise env_exception
mlagents_envs.exception.UnityEnvironmentException: Couldn't launch the 3DBallBuild environment. Provided filename does not match any environments.
```",try use name built get resume version information communicator learning interrupted please wait graph recent call last file line module main file line main file line file line file line wrapped return file line raise ex file line file line wrapped return file line file line reset file line file line raise could launch environment provided match,issue,negative,positive,neutral,neutral,positive,positive
1497045203,"Click Build:
In the File dialog, navigate to your ML-Agents directory.
Assign a file name and click Save.
(For Windows）With Unity 2018.1, it will ask you to select a folder instead of a file name. Create a subfolder within the root directory and select that folder to build. In the following steps you will refer to this subfolder's name as env_name. You cannot create builds in the Assets folder


That never happens, I only get to select a folder after clicking Build and then there is no opportunity to assign a file name.  
I end up with this (at the same level as Assets folder)

![image](https://user-images.githubusercontent.com/25189545/230011500-8114caa7-4a87-4e2d-937e-dad8ef072724.png)
",click build file navigate directory assign file name click save unity ask select folder instead file name create within root directory select folder build following refer name create asset folder never get select folder build opportunity assign file name end level asset folder image,issue,positive,neutral,neutral,neutral,neutral,neutral
1496820977,"(For Windows）With Unity 2018.1, it will ask you to select a folder instead of a file name. Create a subfolder within the root directory and select that folder to build. In the following steps you will refer to this subfolder's name as env_name. You cannot create builds in the Assets folder


I can't create builds in the Assets folder if I""m on Windows and Unity 2018.1 or regardless of OS and Unity version?  You need to be clear.",unity ask select folder instead file name create within root directory select folder build following refer name create asset folder ca create asset folder unity regardless o unity version need clear,issue,positive,positive,neutral,neutral,positive,positive
1495798140,"Here' s an example of how bad ""Loading a Unity Environment

Python-side communication happens through UnityEnvironment which is located in [environment.py](https://unity-technologies.github.io/ml-agents/ml-agents-envs/mlagents_envs/environment.py). To load a Unity environment from a built binary file, put the file in the same directory as envs.""  What is envs????",example bad loading unity environment communication load unity environment built binary file put file directory,issue,negative,negative,negative,negative,negative,negative
1493038323,I have the same issue. I have 4 continuous actions and no discrete actions. I am using a camera sensor input. Latest MLAgents version.,issue continuous discrete camera sensor input latest version,issue,negative,positive,positive,positive,positive,positive
1491171635,"It seems like the only valid link used to load in the environment zip is the mac link in the registry_entries.py; no other link appears active on the storage.googleapis.com URL path
![image](https://user-images.githubusercontent.com/34527220/229001255-9b51cb8a-7f26-419c-abdb-9fe333cbccc3.png)
",like valid link used load environment zip mac link link active path image,issue,positive,negative,negative,negative,negative,negative
1489577394,"I also get this error or something related during my practice. But with 6 continuous actions using PPO

Error:
mlagents\trainers\torch\utils.py:314: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\TensorShape.cpp:3281.)

Versions
ml-agents: 0.26.0, ml-agents-envs: 0.26.0, Communicator API: 1.5.0, PyTorch: 1.13.1+cu117. ",also get error something related practice continuous error use dimension reverse shape throw error future release consider transpose matrix reverse tensor triggered internally communicator,issue,negative,neutral,neutral,neutral,neutral,neutral
1489554578,"Hello

I could make it work with this config with r20, mlagents 0.30.0 in w11

1. Create Conda env with Python 3.8.16 and install mlagents 
2. Install these
  % conda install numpy
  % conda install Pytorch=1.8.0
  % conda install h5py
  % conda install grpcio

3. If pytorch is not available from current channels try this
conda install -c conda-forge Pytorch=1.8.0
or try this
conda install -c pytorch pytorch=1.8.0

4. Then you may have problems with PIL error
Just:
pip uninstall pillow
pip install pillow

5. Same, you may have an error with DLL load from h5py
pip uninstall h5py
pip install h5py

If is still missing something (that I dont think so) ... pip install scipy

Test with mlagents-learn --help

Regards
",hello could make work create python install install install install install install available current try install try install may error pip pillow pip install pillow may error load pip pip install still missing something dont think pip install test help,issue,negative,positive,neutral,neutral,positive,positive
1484279251,"I am still unable to import UnityAECEnv. Is it from mlagents_envs.envs import UnityAECEnv or am I missing something?
I am trying to work with UnitToPettingZooWrapper and tried to import UnityAECEnv but getting the error 
ImportError: cannot import name 'UnityAECEnv' from 'mlagents_envs.environment'
",still unable import import missing something trying work tried import getting error import name,issue,negative,negative,negative,negative,negative,negative
1481971578,"Hi, I am working on a similar scenario at the moment. I think you have written a '=' too much. Try it with: 
`mlagents-learn soccer.yaml --run-id==test1 --env==AIsoccer_Linux.x86_64 --force --no-graphics`

Unfortunately not everything works for me either. When I run the command:
`mlagents-learn --env='/home/AIGameLinux.x86_64' --no-graphics --run-id=Attempt1 --force`
the following output is given:
`mlagents_envs.exception.UnityEnvironmentException: Environment shut down with return code 127.`",hi working similar scenario moment think written much try force unfortunately everything work either run command force following output given environment shut return code,issue,negative,negative,neutral,neutral,negative,negative
1481522870,"Very much looking forward to this. I'm using a 3960 Threadripper and a RTX 4090 and getting horrible performance. GPU sits at around 3-5% and most of the CPU idles with a single thread (out of 48!) at 100%. So my training is bottlenecked by the single-core-performance of my CPU, as a result I'm currently forced to do training on my MacBook. 

So - anything on the horizon?",much looking forward getting horrible performance around single thread training result currently forced training anything horizon,issue,negative,negative,negative,negative,negative,negative
1477506263,"Facing similar problem. Agent does not end episode even though EndEpisode() called from OnActionReceived() with if condition:
```
if ((this.transform.position.y > 12000) || this.transform.position.y < 1000)
            {
                this.AddReward(-3f);
                Debug.Log(""Boundary Reward added: "" + this.GetCumulativeReward());
                this.EndEpisode();
                Debug.Log(""Episode not ended"");
            }
```
This always prints ""Episode not ended"" even thought the script should have stopped executing after calling EndEpisode in the previous line. Please help.",facing similar problem agent end episode even though condition boundary reward added episode ended always episode ended even thought script stopped calling previous line please help,issue,positive,negative,neutral,neutral,negative,negative
1469236322,"> I want to wrap Unity to multi-agent gym environment, how can I do it? Where can I download gym_unity 0.30.0?

I think this is probably what you're looking for: https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Python-PettingZoo-API.md
Note that this uses an old version of pettingzoo and is not compatible with the current pettingzoo API (current version is 1.22.3, unity ml-agents codebase uses 1.15.0)",want wrap unity gym environment think probably looking note old version compatible current current version unity,issue,negative,positive,neutral,neutral,positive,positive
1467103417,"> I have tried what @trsh suggested on my work computer and completely uninstalled all python products
> 
> * Anaconda
> * Pycharm
> * Spyder
> * Python 3.10.x
> * Python launcher
> 
> Then only installed python 3.9.13, recreated the venv and followed the installation procedure outlined in the documentation. No alarms, no errors, only a warning due to use of deprecated setup.py which will be dropped in future release, because the way packages install will change or something. Then the other warning was pip version update available, which I opted to ignore as well.
> 
> So I can confirm clean environment, the setup procedure should work as described.
> 
> What I have not tested yet are the examples in the unity project file, to see if they get a connection to the ml-agent. That will have to be tested and verified next, but at least there are no more errors during the setup procedure.

How do I make sure I have gotten removed them all and how do I install 3.9.13 because the python website does not have an installer for that version",tried work computer completely uninstalled python anaconda python python launcher python installation procedure outlined documentation warning due use future release way install change something warning pip version update available ignore well confirm clean environment setup procedure work tested yet unity project file see get connection tested next least setup procedure make sure gotten removed install python installer version,issue,negative,positive,positive,positive,positive,positive
1465637789,"> i was using python version 3.9 and tried with 3.8 same error.

Your problem seems to bee different. But I would still try a CLEAN (read 1 above) install of python 3.9.x (latest of 3.9) and your ml-agents venv.

I.e. its not enough to reinstall Python and bang around same venv, that was created with higher version. At least that's what I observed.",python version tried error problem bee different would still try clean read install python latest enough reinstall python bang around higher version least,issue,negative,positive,positive,positive,positive,positive
1465356983,"I have tried what @trsh suggested on my work computer and completely uninstalled all python products
- Anaconda
- Pycharm
- Spyder
- Python 3.10.x
- Python launcher

Then only installed python 3.9.13, recreated the venv and followed the installation procedure outlined in the documentation. 
No alarms, no errors, only a warning due to use of deprecated setup.py which will be dropped in future release, because the way packages install will change or something. Then the other warning was pip version update available, which I opted to ignore as well.

So I can confirm clean environment, the setup procedure should work as described.

What I have not tested yet are the examples in the unity project file, to see if they get a connection to the ml-agent. 
That will have to be tested and verified next, but at least there are no more errors during the setup procedure.

",tried work computer completely uninstalled python anaconda python python launcher python installation procedure outlined documentation warning due use future release way install change something warning pip version update available ignore well confirm clean environment setup procedure work tested yet unity project file see get connection tested next least setup procedure,issue,negative,positive,neutral,neutral,positive,positive
1465335132,i was using python version 3.9 and tried with 3.8 same error.,python version tried error,issue,negative,neutral,neutral,neutral,neutral,neutral
1465269193,"I think this is also related https://github.com/Unity-Technologies/ml-agents/issues/5826#issuecomment-1403248637 , there is an alternative solution, but that seems even more complicated 🗡️ ",think also related alternative solution even complicated dagger,issue,negative,negative,negative,negative,negative,negative
1465268063,"> I have tried it with 3.9 as stated above. Repeated same steps and it still did fail. However I noticed it was using cache for numpy when executing: ""C:\Users\Admin\AppData\Local\Programs\Python\Python39\python.exe"" -m pip install mlagents_envs==0.30.0 I probably should clean the pip cache, but not sure how to do that. Later tonight I will completely nuke my python install and only install 3.9 as suggested and try again. However, seeing my entire Sunday go to waste I'm cautious to spend more time experimenting. Need firm solution.

I had same issue. The numpy version that is used in ml-agents just doesn't work with newer Python. So you need to nuke your higher python version and install 3.9.x, OR switch with pyenv (or similar), but make sure it worked by opening new cmd and writing `python --version`. Then delete your venv, you created for ml-agents previously and create new one (which will now use 3.9.x python and fixate it). Then activate and install all stuff as usual.",tried stated repeated still fail however cache pip install probably clean pip cache sure later tonight completely nuke python install install try however seeing entire go waste cautious spend time need firm solution issue version used work python need nuke higher python version install switch similar make sure worked opening new writing python version delete previously create new one use python fixate activate install stuff usual,issue,negative,positive,neutral,neutral,positive,positive
1465256162,"I have tried it with 3.9 as stated above. Repeated same steps and it still did fail. However I noticed it was using cache for numpy when executing: 
""C:\Users\Admin\AppData\Local\Programs\Python\Python39\python.exe"" -m pip install mlagents_envs==0.30.0
I probably should clean the pip cache, but not sure how to do that.
Later tonight I will completely nuke my python install and only install 3.9 as suggested and try again. However, seeing my entire Sunday go to waste I'm cautious to spend more time experimenting. Need firm solution.",tried stated repeated still fail however cache pip install probably clean pip cache sure later tonight completely nuke python install install try however seeing entire go waste cautious spend time need firm solution,issue,negative,positive,neutral,neutral,positive,positive
1465233418,"> > Switch to Python 3.9. And recreate the venv, etc. Above versions seem to fail in many scenarios.
> 
> I have the same problems as the two people above, could you explain what would be the command prompts to do that please?

Just uninstall newer Python and install 3.9. Repeat the ml-agents python part installation process as usual. If you want multiple python versions, you can try to use pyenv or similar.
 ",switch python recreate seem fail many two people could explain would command please python install repeat python part installation process usual want multiple python try use similar,issue,negative,negative,neutral,neutral,negative,negative
1465222788,"> Switch to Python 3.9. And recreate the venv, etc. Above versions seem to fail in many scenarios.

I have the same problems as the two people above, could you explain what would be the command prompts to do that please?",switch python recreate seem fail many two people could explain would command please,issue,negative,neutral,neutral,neutral,neutral,neutral
1465186060,"Switch to Python 3.9. And recreate the venv, etc. Above versions seem to fail in many scenarios.",switch python recreate seem fail many,issue,negative,neutral,neutral,neutral,neutral,neutral
1465110244,"I have the same issue. Attempting to follow the documentation setting up the ""release_20"" docs.
Initially it was complaining about Visual Studio C++ 14.0 and also about Building wheel for numpy (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Building wheel for numpy (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [279 lines of output]
      setup.py:63: RuntimeWarning: NumPy 1.21.2 may not yet support Python 3.10.

I solved that problem with Visual Studio C++ 14., but the other issue with NumPy persists.

Initially I have tried to use venv and have tried to set-up vens with different versions of python via
""<path_to_python(version)_executable>"" -m 
""C:\Users\Admin\AppData\Local\Programs\Python\Python310\python.exe"" -m pip install --no-cache-dir numpy-1.22.4
I have tried different versions of python
3.8.0 (found I need at least 3.8.13)
3.9.0 (same problem just with a lower version of numpy)
3.10.7 (where I started)
3.11.2 (figured why not try latest, but same)

Then after having python sorted I went through all of the tries below, for each version of python. So imagine iterating through below commands multiple times. Having slow as internet you can imagine what that was like.




then I attempted 
to install numpy via wheels, and tried different ones including mkl. Here one example
""C:\Users\Admin\AppData\Local\Programs\Python\Python310\python.exe"" -m pip install ""D:\downloads\unsorted downloads\numpy-1.22.4+mkl-cp310-cp310-win_amd64.whl""

Since the abvoe also failed, tried:
""C:\Users\Admin\AppData\Local\Programs\Python\Python310\python.exe"" -m pip install --upgrade pip
followed by
""C:\Users\Admin\AppData\Local\Programs\Python\Python310\python.exe"" -m pip install torch~=1.7.1 -f https://download.pytorch.org/whl/torch_stable.html
-> of course that failed,
ERROR: Could not find a version that satisfies the requirement torch~=1.7.1 (from versions: 1.11.0, 1.11.0+cpu, 1.11.0+cu113, 1.11.0+cu115, 1.12.0, 1.12.0+cpu, 1.12.0+cu113, 1.12.0+cu116, 1.12.1, 1.12.1+cpu, 1.12.1+cu113, 1.12.1+cu116, 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.1, 1.13.1+cpu, 1.13.1+cu116, 1.13.1+cu117)
ERROR: No matching distribution found for torch~=1.7.1
-> so I changed it to:
""C:\Users\Admin\AppData\Local\Programs\Python\Python310\python.exe"" -m pip install torch~=1.11.0 -f https://download.pytorch.org/whl/torch_stable.html

but then the next command still failed with numpy installation, as above
""C:\Users\Admin\AppData\Local\Programs\Python\Python310\python.exe"" -m pip install mlagents_envs==0.30.0

figured I give editing the setup.py file a try and modified the numpy dependencies as below:
install_requires=[
        ""cloudpickle"",
        ""grpcio>=1.11.0"",
        #""numpy>=1.14.1"",
        ""Pillow>=4.2.1"",
        ""protobuf>=3.6"",
        ""pyyaml>=3.1.0"",
        ""gym>=0.21.0"",
        ""pettingzoo==1.15.0"",
        ""numpy==1.22.4"",
        #""numpy==1.21.2"",
        ""filelock>=3.4.0"",
    ],

For some reason it looked really good in the beginning, as it pulled the right version of numpy, but then it reverted back to 1.14.1 and failed again, despite the line being invalidated.
Used this command to do it:
""C:\Users\Admin\AppData\Local\Programs\Python\Python310\python.exe"" -m pip install --no-cache-dir --force-reinstall mlagents_envs==0.30.0

I have wasted my entire Sunday afternoon trying to get this to work....


Not sure what else I could try....",issue follow documentation setting initially visual studio also building wheel error error building wheel run successfully exit code output may yet support python problem visual studio issue initially tried use tried different python via version pip install tried different python found need least problem lower version figured try latest python sorted went version python imagine multiple time slow imagine like install via tried different one example pip install since also tried pip install upgrade pip pip install course error could find version requirement error matching distribution found pip install next command still installation pip install figured give file try pillow gym reason really good beginning right version back despite line used command pip install wasted entire afternoon trying get work sure else could try,issue,negative,positive,positive,positive,positive,positive
1455119864,"I want to wrap Unity to multi-agent gym environment, how can I do it?
Where can I download gym_unity 0.30.0?",want wrap unity gym environment,issue,negative,neutral,neutral,neutral,neutral,neutral
1452436802,"Ok I see now that there are two different wrappers, One for the AEC Petting Zoo API and the other for the Parallel Petting Zoo API. UnityAECEnv and UnityParallelEnv. Your documentation should be updated to reflect that. 

The env wrapper works great though.",see two different one zoo parallel zoo documentation reflect wrapper work great though,issue,positive,positive,positive,positive,positive,positive
1442654271,"I've the same issue as you, I did update the to Burst 3.0 , I was able to run the build on Webgl using Burst as interference device.

Yet, the amount of GC that is produced by burrcuda is too much and its the main reason about FPS spikes.

From my side, I cut as much as anything that uses CPU, and was able to make it run 30-40 FPS on 1080P / intel i5 with UHD 630 GPU mini mac, for 4Vs4 in reach visual environment , but what is annoying is the spikes. otherwise, on higher device like macbook pro, was able to achieve 60FPS 4K on webgl, but still there is spikes that drop fps few frames from time to time.

My last profiling, I found that burrcuda uses 60% of main thread with tons of GC and render thread is waiting for main thread to finish GC which is reason why the spikes happens.

I hope they really consider resolving this, otherwise, on phone , run 60fps stable on mini iphone SE",issue update burst able run build burst interference device yet amount produced much main reason side cut much anything able make run mac reach visual environment annoying otherwise higher device like pro able achieve still drop time time last found main thread render thread waiting main thread finish reason hope really consider otherwise phone run stable se,issue,negative,positive,positive,positive,positive,positive
1426587611,Hi.  Does increasing the time scale affect the accuracy of the simulation if I used fixed update and physics ?,hi increasing time scale affect accuracy simulation used fixed update physic,issue,negative,positive,neutral,neutral,positive,positive
1423174214,"Thanks for the information provided. I managed to get it run but having below errors. not sure if able to find a way to move forward.  here is the log

``
Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 1.13.1
[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0
[INFO] Connected new brain: myTest?team=0
[INFO] Hyperparameters for behavior name myTest:
	trainer_type:	ppo
	hyperparameters:
	  batch_size:	2048
	  buffer_size:	16384
	  learning_rate:	0.0001
	  beta:	0.005
	  epsilon:	0.1
	  lambd:	0.95
	  num_epoch:	4
	  shared_critic:	True
	  learning_rate_schedule:	linear
	  beta_schedule:	linear
	  epsilon_schedule:	linear
	network_settings:
	  normalize:	False
	  hidden_units:	256
	  num_layers:	3
	  vis_encode_type:	resnet
	  memory:
	    sequence_length:	128
	    memory_size:	512
	  goal_conditioning_type:	hyper
	  deterministic:	False
	reward_signals:
	  extrinsic:
	    gamma:	0.8
	    strength:	1.0
	    network_settings:
	      normalize:	False
	      hidden_units:	128
	      num_layers:	2
	      vis_encode_type:	simple
	      memory:	None
	      goal_conditioning_type:	hyper
	      deterministic:	False
	init_path:	None
	keep_checkpoints:	5
	checkpoint_interval:	500000
	max_steps:	16000000
	time_horizon:	128
	summary_freq:	20000
	threaded:	False
	self_play:	None
	behavioral_cloning:	None
loc(""total derivative last state""(""(mpsFileLoc): /AppleInternal/Library/BuildRoots/9e200cfa-7d96-11ed-886f-a23c4f261b56/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm"":228:0)): error: input types 'tensor<1x128x256xf32>' and 'tensor<1x16x256xf32>' are not broadcast compatible
LLVM ERROR: Failed to infer result type(s).
zsh: abort      mlagents-learn Assets/config/laserAgent.yaml --run-id=myTest0208
(gpu)  % [ERROR] UnityEnvironment worker 0: environment raised an unexpected exception.
Traceback (most recent call last):
  File ""/opt/anaconda3/envs/gpu/lib/python3.10/site-packages/mlagents/trainers/subprocess_env_manager.py"", line 175, in worker
    req: EnvironmentRequest = parent_conn.recv()
  File ""/opt/anaconda3/envs/gpu/lib/python3.10/multiprocessing/connection.py"", line 255, in recv
    buf = self._recv_bytes()
  File ""/opt/anaconda3/envs/gpu/lib/python3.10/multiprocessing/connection.py"", line 419, in _recv_bytes
    buf = self._recv(4)
  File ""/opt/anaconda3/envs/gpu/lib/python3.10/multiprocessing/connection.py"", line 388, in _recv
    raise EOFError
EOFError
Process Process-1:
Traceback (most recent call last):
  File ""/opt/anaconda3/envs/gpu/lib/python3.10/site-packages/mlagents/trainers/subprocess_env_manager.py"", line 175, in worker
    req: EnvironmentRequest = parent_conn.recv()
  File ""/opt/anaconda3/envs/gpu/lib/python3.10/multiprocessing/connection.py"", line 255, in recv
    buf = self._recv_bytes()
  File ""/opt/anaconda3/envs/gpu/lib/python3.10/multiprocessing/connection.py"", line 419, in _recv_bytes
    buf = self._recv(4)
  File ""/opt/anaconda3/envs/gpu/lib/python3.10/multiprocessing/connection.py"", line 388, in _recv
    raise EOFError
EOFError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/envs/gpu/lib/python3.10/multiprocessing/process.py"", line 314, in _bootstrap
    self.run()
  File ""/opt/anaconda3/envs/gpu/lib/python3.10/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""/opt/anaconda3/envs/gpu/lib/python3.10/site-packages/mlagents/trainers/subprocess_env_manager.py"", line 235, in worker
    _send_response(EnvironmentCommand.ENV_EXITED, ex)
  File ""/opt/anaconda3/envs/gpu/lib/python3.10/site-packages/mlagents/trainers/subprocess_env_manager.py"", line 150, in _send_response
    parent_conn.send(EnvironmentResponse(cmd_name, worker_id, payload))
  File ""/opt/anaconda3/envs/gpu/lib/python3.10/multiprocessing/connection.py"", line 211, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File ""/opt/anaconda3/envs/gpu/lib/python3.10/multiprocessing/connection.py"", line 416, in _send_bytes
    self._send(header + buf)
  File ""/opt/anaconda3/envs/gpu/lib/python3.10/multiprocessing/connection.py"", line 373, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
/opt/anaconda3/envs/gpu/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 3 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '",thanks information provided get run sure able find way move forward log version information communicator listening port start training pressing play button unity editor connected unity environment package version communication version connected new brain behavior name beta epsilon true linear linear linear normalize false memory hyper deterministic false extrinsic gamma strength normalize false simple memory none hyper deterministic false none threaded false none none total derivative last state error input broadcast compatible error infer result type abort error worker environment raised unexpected exception recent call last file line worker file line file line file line raise process recent call last file line worker file line file line file line raise handling exception another exception recent call last file line file line run file line worker ex file line file line send file line header file line write broken pipe appear semaphore clean shutdown appear,issue,positive,negative,neutral,neutral,negative,negative
1413582371,"@amagwka thank you. I know the theory, but my point here is that ml-agents documentation lacks information about this. Especially the undocumented hyperparameter. ",thank know theory point documentation information especially undocumented,issue,negative,neutral,neutral,neutral,neutral,neutral
1404041365,"Just saw in the code that it is possible to use `gail->use_actions : false` to do this.
It's actually written here :
https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Training-Configuration-File.md#gail-intrinsic-reward
But I couldn't find this file by searching in google or github.",saw code possible use false actually written could find file searching,issue,negative,negative,negative,negative,negative,negative
1403792371,"anyway i solved installing Python 3.9.x

I hope the maintainers fix these limits ASAP, since it's quite an issue to manage so many different Python versions (especially in multi-platform mac+windows classes)",anyway python hope fix since quite issue manage many different python especially class,issue,negative,positive,positive,positive,positive,positive
1403248637,"Currently, you have to clone the repository, and **adjust** the following lines:
In `ml-agents/setup.py`: 
`python_requires="">=3.8.13,<3.11.0""` (line 82)

In `ml-agents-envs/setup.py`:
`""numpy>=1.14.1,<1.24""` (line 54)
and
`python_requires="">=3.8.13,<3.11.0""` (line 63)

and **delete** `""numpy==1.21.2""` (line 60)

Then, **first install torch** using your needed configuration (https://pytorch.org/get-started/locally/) and then install your locally changed python packages:
```
pip3 install -e ./ml-agents-envs
pip3 install -e ./ml-agents
```

I hope this helps!",currently clone repository adjust following line line line delete line first install torch configuration install locally python pip install pip install hope,issue,negative,positive,neutral,neutral,positive,positive
1402802518,"I am having the exact issue in windows 10 with python 3.10.8, torch 1.13.1, pip 22.3.1. What can I do?",exact issue python torch pip,issue,negative,positive,positive,positive,positive,positive
1399635371,"(or I suppose one could do `>=3.8.13,<3.11.0`, if one doesn't feel that python follows semver. But I think python do a reasonable job on semver?)",suppose one could one feel python think python reasonable job,issue,negative,positive,positive,positive,positive,positive
1399635149,"Good observation. Really that constraint should probably be `>=3.8.13,<4.0.0` I reckon?",good observation really constraint probably reckon,issue,negative,positive,positive,positive,positive,positive
1399634423,"ml-agents 0.30.0 has this consttraints: python_requires="">=3.8.13,<=3.10.8"",
latest Python is 3.10.9 ... we have ot to install Python 3.10.8 or modify /ml-agents/setup.py

",latest python install python modify,issue,negative,positive,positive,positive,positive,positive
1385520326,"Having this issue on Arch Linux
Python 3.10.9
Pip 22.3.1
Same error message as original post",issue arch python pip error message original post,issue,negative,positive,positive,positive,positive,positive
1374388711,"In reinforcement learning (RL), deterministic action selection refers to the process of selecting actions based on a deterministic policy. A deterministic policy is a function that maps states to actions, such that for a given state, the policy always returns the same action. It actually choose action with the highest probability from action probabilities.

However, deterministic action selection can also have some drawbacks. For example, it may be less effective in environments where there is significant uncertainty or stochasticity, as it may be difficult to determine a single optimal action in these cases.

Related discussions:
#2643
#2112",reinforcement learning deterministic action selection process based deterministic policy deterministic policy function given state policy always action actually choose action highest probability action however deterministic action selection also example may le effective significant uncertainty may difficult determine single optimal action related,issue,negative,positive,neutral,neutral,positive,positive
1370822223,"the link should be similar like the previous one.
https://github.com/Unity-Technologies/ml-agents/blob/release_19_docs/docs/Readme.md",link similar like previous one,issue,negative,negative,neutral,neutral,negative,negative
1367750440,"> Hi, I am curious to know how much gain you get when the GPU is used. What is your Apple silicon Spec? Thanks

I have M1 Max 24-core GPU. However, the performance was worse than the CPU. My task was fairly simple so it might be true that GPU didn't have an advantage and probably be useful for the more complex tasks (like CNN layers). I tested the same task on RTX 3050 and it took a lead over M1. I am waiting for the official changes in the Pytorch and ml-agents library that incorporates apple silicon GPU and hopefully will be better than what workaround we have now.",hi curious know much gain get used apple silicon spec thanks however performance worse task fairly simple might true advantage probably useful complex like tested task took lead waiting official library apple silicon hopefully better,issue,positive,positive,neutral,neutral,positive,positive
1367632487,"Hi,
I am curious to know how much gain you get when the GPU is used.
What is your Apple silicon Spec?
Thanks",hi curious know much gain get used apple silicon spec thanks,issue,positive,positive,positive,positive,positive,positive
1367189691,"Temporarily, I installed mlagent library with --no-deps that omits Pytorch installation and installed Pytorch 1.13 manually. After that, I monkey patched the [torch.py](https://github.com/Unity-Technologies/ml-agents/blob/28ec36a74384f37eea75753b50a01436931dcaca/ml-agents/mlagents/torch_utils/torch.py) code in this repository. 

Changes: https://github.com/pytorch/pytorch/issues/82296#issuecomment-1367188432",temporarily library installation manually monkey code repository,issue,negative,negative,neutral,neutral,negative,negative
1365850458,"(oh, mind you, this won't detect links that go to the wrong place, that actually exists...)",oh mind wo detect link go wrong place actually,issue,negative,negative,negative,negative,negative,negative
1365850090,"@ Unity: note that you can add a link checker to your CI server using e.g. markdown-link-check https://github.com/tcort/markdown-link-check e.g. see following example:

https://github.com/hughperkins/VeriGPU/blob/501caddd100e5245658008bb42f17ef8e97d4192/cicd/check_markdown.sh",unity note add link checker server see following example,issue,negative,neutral,neutral,neutral,neutral,neutral
1365838900,"Also, please be aware that numpy 1.24 removed the already deprecated float type and replaced it with the python equivalent. However, it is still used in some places of the mlagents lib, which breaks the code.
I therefore suggest to either migrate those code segments, or restrict numpy to <1.24.

@hughperkins Perhaps, I think it has something to do with building the wheels, and some build dependencies might be OS specific, do it's possible that this is a Windows-only issue.

",also please aware removed already float type python equivalent however still used code therefore suggest either migrate code restrict perhaps think something building build might o specific possible issue,issue,negative,positive,neutral,neutral,positive,positive
1364995579,"Same for `Our latest, stable release is Release 20. Click [here](https://github.com/Unity-Technologies/ml-agents/tree/release_20_docs/docs/Readme.md) to get started with the latest release of ML-Agents.`, it references the same readme again, but should reference `https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Getting-Started.md`
(it's correct on the develop branch, but not in the main branch, where you usually start off)",latest stable release release click get latest release reference correct develop branch main branch usually start,issue,negative,positive,positive,positive,positive,positive
1364729265,"> That last one seems to be more ""guesswork"" as in, tweak some values, see if it improves training time or efficiency, stick with what works unless the performance starts to deteriorate.

Yes and no. Yes, figuring out what works is typically an empirical question. But 'no', because there are things you can look at to decide what to tweak next. How do you know if your model is failing because it's under-capacity? under-regularized? not exploring enough? etc.  (For the last point, you can see for example my video about entropy regularization https://studio.youtube.com/video/1ppslywmIPs/edit ).

Happy to Zoom to look into your specific problems, might give me inspiration for new videos :)",last one guesswork tweak see training time efficiency stick work unless performance deteriorate yes yes work typically empirical question look decide tweak next know model failing exploring enough last point see example video entropy regularization happy zoom look specific might give inspiration new,issue,positive,positive,positive,positive,positive,positive
1364726743,"Not much has changed actually. I did not have more time to sink into the issue, and most of the answer seem to boil down to ""start to study more on how machine learning actually works""
Things like how dataset distribution works, how to train for edge cases, and how to balance out a neural network. That last one seems to be more ""guesswork"" as in, tweak some values, see if it improves training time or efficiency, stick with what works unless the performance starts to deteriorate.

It goes deep, and i don't have the time to do a legitimate deep dive on the subject sadly. Hope you find what you are looking for though, i am sure most people run into the same hurdles once they actually start to apply ML.",much actually time sink issue answer seem boil start study machine learning actually work like distribution work train edge balance neural network last one guesswork tweak see training time efficiency stick work unless performance deteriorate go deep time legitimate deep dive subject sadly hope find looking though sure people run actually start apply,issue,positive,negative,neutral,neutral,negative,negative
1362405790,"It's very tedious, but you could modify some of the methods in demo_loader.

I created some utilities for modifying the demo files in my environment.
You would need to make a few changes in order to get it to work for your environment though. (model name, etc.)
<details>
<summary>demo_util.py</summary>

```python
import random
from mlagents.trainers.demo_loader import get_demo_files, load_demonstration, write_demo
from mlagents_envs.communicator_objects.demonstration_meta_pb2 import DemonstrationMetaProto
from mlagents_envs.communicator_objects.brain_parameters_pb2 import BrainParametersProto
from mlagents_envs.communicator_objects.space_type_pb2 import SpaceTypeProto
from mlagents_envs.communicator_objects.brain_parameters_pb2 import ActionSpecProto
from mlagents_envs.communicator_objects.observation_pb2 import ObservationProto, CompressionTypeProto, ObservationTypeProto
from mlagents_envs.communicator_objects.agent_info_pb2 import AgentInfoProto
from mlagents_envs.communicator_objects.agent_action_pb2 import AgentActionProto
from mlagents_envs.communicator_objects.agent_info_action_pair_pb2 import AgentInfoActionPairProto
from google.protobuf.internal.decoder import _DecodeVarint32  # type: ignore
from google.protobuf.internal.encoder import _EncodeVarint  # type: ignore
from mlagents_envs.rpc_utils import behavior_spec_from_proto, steps_from_proto
INITIAL_POS = 33
SUPPORTED_DEMONSTRATION_VERSIONS = frozenset([0, 1])

def debug_read_demo(path: str):
    behavior_spec = None
    brain_param_proto = None
    info_action_pairs = []
    total_expected = 0

    with open(path, ""rb"") as fp:
        data = fp.read()
    next_pos, pos, obs_decoded = 0, 0, 0
    while pos < len(data):
        next_pos, pos = _DecodeVarint32(data, pos)
        if obs_decoded == 0:
            meta_data_proto = DemonstrationMetaProto()
            meta_data_proto.ParseFromString(data[pos : pos + next_pos])
            if (
                meta_data_proto.api_version
                not in SUPPORTED_DEMONSTRATION_VERSIONS
            ):
                raise RuntimeError(
                    f""Can't load Demonstration data from an unsupported version ({meta_data_proto.api_version})""
                )
            total_expected += meta_data_proto.number_steps
            pos = INITIAL_POS
        if obs_decoded == 1:
            brain_param_proto = BrainParametersProto()
            brain_param_proto.ParseFromString(data[pos : pos + next_pos])
            pos += next_pos
        if obs_decoded > 1:
            agent_info_action = AgentInfoActionPairProto()
            agent_info_action.ParseFromString(data[pos : pos + next_pos])
            if behavior_spec is None:
                behavior_spec = behavior_spec_from_proto(
                    brain_param_proto, agent_info_action.agent_info
                )
            info_action_pairs.append(agent_info_action)
            if len(info_action_pairs) == total_expected:
                break
            pos += next_pos
        obs_decoded += 1

    print(f""meta_data_proto: {meta_data_proto}"")
    print(f""brain_param_proto: {brain_param_proto}"")

    print('')
    print(brain_param_proto.action_spec.action_descriptions)

def get_demonstration_meta_proto(
    api_version: int=1,
    demonstration_name: str=""IKExcavator"",
    number_steps: int=99999,
    number_episodes: int=99,
    mean_reward: float=12.345
) -> DemonstrationMetaProto:
    demonstration_meta_proto = DemonstrationMetaProto()
    demonstration_meta_proto.api_version = api_version
    demonstration_meta_proto.demonstration_name = demonstration_name
    demonstration_meta_proto.number_steps = number_steps
    demonstration_meta_proto.number_episodes = number_episodes
    demonstration_meta_proto.mean_reward = mean_reward
    return demonstration_meta_proto

def get_brain_parameters_proto(
    agent_name: str=""IK_Excavator"", team_id: int=0,
    is_training: bool=False,
    num_continuous_actions: int=0,
    num_discrete_actions: int=4,
    discrete_branch_sizes: int=[3,3,3,3],
    action_descriptions: list[str]=[]
) -> BrainParametersProto:
    brain_parameters_proto = BrainParametersProto()
    brain_parameters_proto.vector_action_size_deprecated.MergeFrom(discrete_branch_sizes)
    brain_parameters_proto.vector_action_descriptions_deprecated.MergeFrom(action_descriptions)
    brain_parameters_proto.vector_action_space_type_deprecated = SpaceTypeProto.continuous \
        if num_continuous_actions > 0 else SpaceTypeProto.discrete
    brain_parameters_proto.brain_name = f""{agent_name}?team={team_id}""
    brain_parameters_proto.is_training = is_training # ?

    action_spec_proto = ActionSpecProto()
    action_spec_proto.num_continuous_actions = num_continuous_actions
    action_spec_proto.num_discrete_actions = num_discrete_actions
    action_spec_proto.discrete_branch_sizes.MergeFrom(discrete_branch_sizes)
    action_spec_proto.action_descriptions.MergeFrom(action_descriptions)
    
    brain_parameters_proto.action_spec.CopyFrom(action_spec_proto)
    return brain_parameters_proto

def get_frame_pair_proto(
    gs_obs: list[float]=[1, 0], other_obs: list[float]=None,
    reward: float=0.0, done: bool=False, max_step_reached: bool=False, id: int=0,
    discrete_actions: list[int]=[1,1,1,1]
) -> AgentInfoActionPairProto:
    def get_gs_obs_proto(val: list[float]=[1, 0]) -> ObservationProto:
        observation_proto = ObservationProto()
        obs_float_proto = ObservationProto.FloatData()
        obs_count = len(val)
        obs_float_proto.data.extend(val)
        observation_proto.shape.MergeFrom([obs_count])
        observation_proto.compression_type = CompressionTypeProto.NONE
        observation_proto.float_data.CopyFrom(obs_float_proto)
        observation_proto.compressed_channel_mapping.MergeFrom([])
        observation_proto.dimension_properties.MergeFrom([1]) # ?
        observation_proto.observation_type = ObservationTypeProto.GOAL_SIGNAL
        observation_proto.name = 'GoalScoopSensor'
        return observation_proto

    def get_other_obs_proto(val: list[float]=None) -> ObservationProto:
        observation_proto = ObservationProto()
        obs_float_proto = ObservationProto.FloatData()
        if val is None:
            val = []
            for j in range(4):
                for i in range(17):
                    val.append(i + j)
        obs_float_proto.data.MergeFrom(val)
        observation_proto.shape.MergeFrom([len(val)])
        observation_proto.compression_type = CompressionTypeProto.NONE
        observation_proto.float_data.CopyFrom(obs_float_proto)
        observation_proto.compressed_channel_mapping.MergeFrom([])
        observation_proto.dimension_properties.MergeFrom([1]) # ?
        observation_proto.observation_type = ObservationTypeProto.DEFAULT
        observation_proto.name = 'StackingSensor_size4_VectorSensor_size17'
        return observation_proto

    agent_info_proto = AgentInfoProto()
    agent_info_proto.reward = reward
    agent_info_proto.done = done
    agent_info_proto.max_step_reached = max_step_reached
    agent_info_proto.id = id
    agent_info_proto.action_mask.MergeFrom([])
    agent_info_proto.observations.append(get_gs_obs_proto(gs_obs))
    agent_info_proto.observations.append(get_other_obs_proto(other_obs))

    agent_info_proto.group_id = 0
    agent_info_proto.group_reward = 0

    agent_action_proto = AgentActionProto()
    agent_action_proto.vector_actions_deprecated.MergeFrom([])
    agent_action_proto.value = 0.0
    agent_action_proto.continuous_actions.MergeFrom([])
    agent_action_proto.discrete_actions.MergeFrom(discrete_actions)

    pair_proto = AgentInfoActionPairProto()
    pair_proto.agent_info.CopyFrom(agent_info_proto)
    pair_proto.action_info.CopyFrom(agent_action_proto)
    return pair_proto

from mlagents_envs.base_env import BehaviorSpec
from mlagents_envs.timers import hierarchical_timer, timed

@timed
def load_demo(
    file_path: str,
) -> tuple[DemonstrationMetaProto, BrainParametersProto, list[AgentInfoActionPairProto]]:
    """"""
    Loads and parses a demonstration file.
    :param file_path: Location of demonstration file (.demo).
    :return: BrainParameter and list of AgentInfoActionPairProto containing demonstration data.
    """"""

    # First 32 bytes of file dedicated to meta-data.
    file_paths = get_demo_files(file_path)
    behavior_spec = None
    brain_param_proto = None
    info_action_pairs = []
    total_expected = 0
    for _file_path in file_paths:
        with open(_file_path, ""rb"") as fp:
            with hierarchical_timer(""read_file""):
                data = fp.read()
            next_pos, pos, obs_decoded = 0, 0, 0
            while pos < len(data):
                next_pos, pos = _DecodeVarint32(data, pos)
                if obs_decoded == 0:
                    meta_data_proto = DemonstrationMetaProto()
                    meta_data_proto.ParseFromString(data[pos : pos + next_pos])
                    if (
                        meta_data_proto.api_version
                        not in SUPPORTED_DEMONSTRATION_VERSIONS
                    ):
                        raise RuntimeError(
                            f""Can't load Demonstration data from an unsupported version ({meta_data_proto.api_version})""
                        )
                    total_expected += meta_data_proto.number_steps
                    pos = INITIAL_POS
                if obs_decoded == 1:
                    brain_param_proto = BrainParametersProto()
                    brain_param_proto.ParseFromString(data[pos : pos + next_pos])
                    pos += next_pos
                if obs_decoded > 1:
                    agent_info_action = AgentInfoActionPairProto()
                    agent_info_action.ParseFromString(data[pos : pos + next_pos])
                    if behavior_spec is None:
                        behavior_spec = behavior_spec_from_proto(
                            brain_param_proto, agent_info_action.agent_info
                        )
                    info_action_pairs.append(agent_info_action)
                    if len(info_action_pairs) == total_expected:
                        break
                    pos += next_pos
                obs_decoded += 1
    if not behavior_spec:
        raise RuntimeError(
            f""No BrainParameters found in demonstration file at {file_path}.""
        )
    return meta_data_proto, brain_param_proto, info_action_pairs

def example_write_demo(path: str='test.demo'):
    brain_parameters_proto = get_brain_parameters_proto(
        agent_name='IK_Excavator',
        num_discrete_actions=4, discrete_branch_sizes=[3,3,3,3]
    )
    print(f""brain_parameters_proto\n{brain_parameters_proto}"")

    agent_info_protos = []
    for i in range(100):
        agent_info_protos.append(
            get_frame_pair_proto(
                gs_obs=[1,0], other_obs=[random.random() for j in range(17*4)],
                reward=random.random()*10, done=random.random() < 0.1,
                discrete_actions=[random.choice(list(range(size))) for size in brain_parameters_proto.action_spec.discrete_branch_sizes]
            )
        )

    demonstration_meta_proto = get_demonstration_meta_proto(
        demonstration_name='test',
        number_steps=len(agent_info_protos),
        number_episodes=list.count([agent.agent_info.done for agent in agent_info_protos], True),
        mean_reward=sum([agent.agent_info.reward for agent in agent_info_protos]) / len(agent_info_protos)
    )
    print(f""demonstration_meta_proto\n{demonstration_meta_proto}"")

    write_demo(
        demo_path=path,
        meta_data_proto=demonstration_meta_proto,
        brain_param_proto=brain_parameters_proto,
        agent_info_protos=agent_info_protos
    )
    print(load_demonstration(path))
```

</details>

<details>
<summary>remove_rewards_from_demo.py</summary>

```python
import os
from mlagents.trainers.demo_loader import write_demo, \
    get_demo_files
from demo_util import load_demo

src_folder = ""/home/clayton/Unity/Projects/excav_simul_3d/Demo/GSSReduced0""
dst_folder = ""/home/clayton/Unity/Projects/excav_simul_3d/Demo/GSSReduced0_NoRewardDemo""
src_paths = get_demo_files(src_folder)

os.makedirs(dst_folder, exist_ok=True)
for src_path in src_paths:
    src_filename = os.path.basename(src_path)
    dst_path = f""{dst_folder}/{src_filename}""
    meta_data_proto, brain_param_proto, info_action_pairs = load_demo(src_path)
    for i in range(len(info_action_pairs)):
        info_action_pairs[i].agent_info.reward = 0
    meta_data_proto.mean_reward = 0
    write_demo(
        demo_path=dst_path,
        meta_data_proto=meta_data_proto,
        brain_param_proto=brain_param_proto,
        agent_info_protos=info_action_pairs
    )
    print(f""{src_path}\n->{dst_path}"")
```

</details>

@miguelalonsojr 
I would also like mlagents to provide an easier way to modify the demo files after they are created.
For example, suppose I spend 2 hours recording a demo for my agent, train, and then realize that one of the observations is causing problems. I don't want to have to re-record the demos again. It would be easier to just loop through all of the observations in the demo file and remove the problematic observation. I think that ml-agents should provide utilities that would make this possible. ",tedious could modify environment would need make order get work environment though model name summary python import random import import import import import import import import import import type ignore import type ignore import path none none open path data data data data raise ca load demonstration data unsupported version data data none break print print print print return list else return list float list float reward done id list list float return list float none range range return reward done id return import import timed timed list demonstration file param location demonstration file return list demonstration first file none none open data data data data raise ca load demonstration data unsupported version data data none break raise found demonstration file return path print range range list range size size agent true agent print print path summary python import o import import range print would also like provide easier way modify example suppose spend recording agent train realize one causing want demo would easier loop file remove problematic observation think provide would make possible,issue,positive,negative,neutral,neutral,negative,negative
1356479505,"Using VS Code, I had to quit VSCode at least once, and quit Unity at least once, in order for VSCode to recognize mlagents things. This is after installing the MLAgents package into Unity. I installed `release_20`, from downloading the github repo. (I agree that VSCode != VS, but just in case useful).",code quit least quit unity least order recognize package unity agree case useful,issue,positive,negative,neutral,neutral,negative,negative
1356479273,"Interestingly I have python 3.10 and numpy 1.21.2 installed, and everyting works swimmingly.

```
(10-unityml) Hughs-MacBook-Air:python hugh$ pip freeze | grep numpy
numpy==1.21.2
(10-unityml) Hughs-MacBook-Air:python hugh$ python -V
Python 3.10.7
```

Maybe this problem is specific to windows though? (I'm on a Macbook M2)",interestingly python work swimmingly python pip freeze python python python maybe problem specific though,issue,negative,positive,positive,positive,positive,positive
1356478604,"I'm just a bystander, i.e. zero affiliation with Unity, but I am attempting to make youtube videos/tutorials, on reinforcement learning. I'm looking for potential ideas/inspiration. It seems like you created this ticket several months ago, and probably figured out the answers to all your questions by now? But if that's not the case, what are your thoughts on having a Zoom call sometime, so that I can find out more about the problems and challenges you are encountering?",bystander zero affiliation unity make reinforcement learning looking potential like ticket several ago probably figured case zoom call sometime find,issue,negative,neutral,neutral,neutral,neutral,neutral
1356477049,"I feel like this has been fixed in `release_20`?  https://github.com/Unity-Technologies/ml-agents/blob/5b6cb9878c611d5c50585f6400ebb8fe16ee8f1f/ml-agents/setup.py#L71

<img width=""732"" alt=""Screen Shot 2022-12-17 at 4 36 28 PM"" src=""https://user-images.githubusercontent.com/123560/208266846-4820ed85-2ee5-42fc-b54e-e40979f8406b.png"">
",feel like fixed screen shot,issue,negative,positive,neutral,neutral,positive,positive
1345096019,"Well I am sorry to have wasted your time, but I didn't see there was a Cuda option on the windows Task Manager Performances section...

![usageHere](https://user-images.githubusercontent.com/97137454/206828380-18f6d452-b346-4401-91b8-c140317403b1.PNG)

There is therefore no bug and everything works fine.
",well sorry wasted time see option task manager section therefore bug everything work fine,issue,negative,negative,neutral,neutral,negative,negative
1340617763,"I updated to release20 along with the new unity extensions and it did not change.
I also installed CUDA 11.6 from NVIDA, tried multiple versions of pytorch including the latest and nightly, nothing seems to be working.

edit: Also, I do not have any gym-unity package in this new installation, as I can't find the recommanded version anywhere. gym-unity==0.30. is required for mlagents release20 but it is nowhere to be found.",release along new unity change also tried multiple latest nightly nothing working edit also package new installation ca find version anywhere release nowhere found,issue,negative,positive,positive,positive,positive,positive
1338056922,We just cut release 20. Can you update and check to see if the problem persists?,cut release update check see problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1338047374,"We just cut release 20, can you update and give it a try?",cut release update give try,issue,negative,neutral,neutral,neutral,neutral,neutral
1336502377,"Did you install CUDA tool's from Nvidia?
If so, check your CUDA tool's version with the Torch version: torch 1.7.1+cu110.
I had an issue with the 'curiosity' parameter. I fixed it by installing the correct version of the CUDA tools: [https://github.com/Unity-Technologies/ml-agents/issues/5793](https://github.com/Unity-Technologies/ml-agents/issues/5793)",install tool check tool version torch version torch issue parameter fixed correct version,issue,negative,positive,neutral,neutral,positive,positive
1331385278,"Hi @trsh, 

Thanks for reporting this bug, it is fixed now (#5832). 
",hi thanks bug fixed,issue,negative,positive,positive,positive,positive,positive
1324058203,"It looks like Dependabot already created PR #5814 which would solve this issue.
I also noticed, that numpy is referenced twice in the setup file, which probably shouldn't be the case:
```
    install_requires=[
        ""cloudpickle"",
        ""grpcio>=1.11.0"",
        ""numpy>=1.14.1"",             <====
        ""Pillow>=4.2.1"",
        ""protobuf>=3.6"",
        ""pyyaml>=3.1.0"",
        ""gym>=0.21.0"",
        ""pettingzoo==1.15.0"",
        ""numpy==1.21.2"",             <====
        ""filelock>=3.4.0"",
    ],
```",like already would solve issue also twice setup file probably case pillow gym,issue,positive,neutral,neutral,neutral,neutral,neutral
1323190907,"Ok, thank you for your help, I'm not familiar with unity, So I just wanna a demo about wrap the mlagents-env. But in any cases, thank you for your answer!",thank help familiar unity wan na wrap thank answer,issue,positive,positive,neutral,neutral,positive,positive
1322774135,The first error in the Unity log looks like the one to focus on for the environment timeout.  Something is wrong with your gRPC library.  This [forum post](https://forum.unity.com/threads/mlagents-unexpected-exception-when-trying-to-initialize-communication.1112350/) describes a similar case.,first error unity log like one focus environment something wrong library forum post similar case,issue,negative,negative,neutral,neutral,negative,negative
1322393809,"If I'm understanding your question properly this might help:
https://docs.unity3d.com/Manual/PublishingBuilds.html",understanding question properly might help,issue,negative,neutral,neutral,neutral,neutral,neutral
1320537267,"[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/Unity-Technologies/ml-agents?pullRequest=5814) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/Unity-Technologies/ml-agents?pullRequest=5814) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: maryamhonari<br/>:x: dependabot[bot]<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/Unity-Technologies/ml-agents?pullRequest=5814) it.</sub>",assistant check thank submission really appreciate like many open source ask sign contributor license agreement accept bot sub already status still pending let u recheck,issue,positive,positive,positive,positive,positive,positive
1318300345,"> Later I found the workaround described in this thread, and it solved the situation:
> 
> #5794 pip install importlib-metadata==4.4
> 
> Never-the-less, either the bug should be corrected, or the instructions updated. I spent hours trying to install mlagents and getting different errors due to version incompatibilities which are not documented...

very thank you, god, i almost getting crazy, already read lots of blogs but find no solutions. For almost 3 days, god. Thank you for this important message.",later found thread situation pip install either bug corrected spent trying install getting different due version thank god almost getting crazy already read lot find almost day god thank important message,issue,positive,negative,neutral,neutral,negative,negative
1316137320,Looks interesting! Is the source code available? ,interesting source code available,issue,negative,positive,positive,positive,positive,positive
1312797681,"Thank you, @Nana1330! That solution worked!",thank nana solution worked,issue,positive,neutral,neutral,neutral,neutral,neutral
1312522866,"> I still have the same problem and I have tried your solution @LeezPayne. 
> ![image](https://user-images.githubusercontent.com/68777530/200382151-7d336a32-aa76-4797-b280-05a4e9ced723.png)
> 

Try changing the metadata version to 4.8",still problem tried solution image try version,issue,negative,neutral,neutral,neutral,neutral,neutral
1312522607,"> It seems that  `ml-agents` depends on the old `importlib-metadata` library. I solved this issue by ` pip install importlib-metadata==4.4`

This worked for me. I changed the metadata version to 4.8.0 and it worked.",old library issue pip install worked version worked,issue,negative,positive,neutral,neutral,positive,positive
1305986583,"I still have the same problem and I have tried your solution @LeezPayne. 
![image](https://user-images.githubusercontent.com/68777530/200382151-7d336a32-aa76-4797-b280-05a4e9ced723.png)
",still problem tried solution image,issue,negative,neutral,neutral,neutral,neutral,neutral
1299448609,"This issue has been automatically closed because it has not had activity in the last 120 days. If this issue is still valid, please ping a maintainer. Thank you for your contributions.
",issue automatically closed activity last day issue still valid please ping maintainer thank,issue,positive,negative,neutral,neutral,negative,negative
1298424435,"Later I found the workaround described in this thread, and it solved the situation:

https://github.com/Unity-Technologies/ml-agents/issues/5794
pip install importlib-metadata==4.4

Never-the-less, either the bug should be corrected, or the instructions updated. I spent hours trying to install mlagents and getting different errors due to version incompatibilities which are not documented...",later found thread situation pip install either bug corrected spent trying install getting different due version,issue,negative,negative,neutral,neutral,negative,negative
1298313868,"I'm also experiencing this issue. Please assist ASAP
",also issue please assist,issue,negative,neutral,neutral,neutral,neutral,neutral
1295989475,"Yes, the toolkit has been modified so the Agent can receive rewards for conflicting objectives. There is a weight passed in while training. This allows the agent to display different behaviours based on the different weight during inference. The three cubes have the same brain but different weights applied. Here is a video showing how I trained the multiple objectives. https://youtu.be/ELWM4Y-9Gwc ",yes agent receive conflicting weight training agent display different based different weight inference three brain different applied video showing trained multiple,issue,negative,neutral,neutral,neutral,neutral,neutral
1294003198,"You must add if condition in OnActionReceived function. For example:
`if(distanceFromTarget < distanceRequired {
AddReward(1.0f);
EndEpisode();
}`",must add condition function example,issue,negative,neutral,neutral,neutral,neutral,neutral
1292215945,"Problem solved. I figured out it was tensorflow causing the error. See [this](https://stackoverflow.com/questions/49084934/illegal-instruction-4-when-importing-tensorflow-in-python-3-6) and [this](https://github.com/tensorflow/tensorflow/issues/25822). The default installed tf is not compatibale on my side(not too sure if it's m1 chip related, but I didn't reproduce the error on another intel mac). I installed tf from conda-forge channel and the issue solved. ",problem figured causing error see default side sure chip related reproduce error another mac channel issue,issue,negative,positive,positive,positive,positive,positive
1288227768,"> 

Thank you!
The problem I was having was that my ml-agents version was set to an older version in the venv that I was running it.
Also sorry for not including my Environment info, I will do so next time.

My env:
Unity Version: Unity 2021.3.8f1
OS + version: Windows 10 pro
ML-Agents version: ML-agents v0.29.0 (I was using v0.16.0 by accident (dont do this))
Torch version: 1.12.1
Environment: Python 3.7.13

Also the links that you have included don't exist or don't work because I couldn't find them.",thank problem version set older version running also sorry environment next time unity version unity o version pro version accident dont torch version environment python also link included exist work could find,issue,negative,negative,negative,negative,negative,negative
1288172690,"There are three agents training at the beginning of your video. Is this what you state ""To implement multiple objectives and add weight to those objectives so an Agent could display multiple behaviours without the need to retrain.""?

Have your implementation different from the example '3DBall' [https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md](url)? Where 12 agents are running to train the same model.",three training beginning video state implement multiple add weight agent could display multiple without need implementation different example running train model,issue,negative,neutral,neutral,neutral,neutral,neutral
1288170599,I fixed it. It was the CUDA tool's compatibility issue.,fixed tool compatibility issue,issue,negative,positive,neutral,neutral,positive,positive
1288170218,"Hi,
I followed the: [https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Create-New.md](url)
I have attaced a version 'RollerBall' that works on my system. You should be able to import it and test it with the VRollerBall.yaml.

If it does not work on your system, then the problem is on the ml-agents installation. Did you test any of the example: [https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md](url)

Why haven’t you provided the Environment info as the template states?

My Environment

- Unity Version: Unity 2020.3.33f1
- OS + version: Windows 11
- ML-Agents version: ML-Agents v0.29.0
- Torch version: 1.9.1+cu111
- Environment: Python 3.8.10

**Note**: 'cu111' for Nvidia GPU

[VRollerBall.zip](https://github.com/Unity-Technologies/ml-agents/files/9847212/VRollerBall.zip)
",hi version work system able import test work system problem installation test example provided environment template environment unity version unity o version version torch version environment python note,issue,negative,positive,positive,positive,positive,positive
1287561745,"> Increase the 'summary_freq: 10000' e.g. 'summary_freq: 50000'

Did not change anything :( 
![image](https://user-images.githubusercontent.com/69884464/197308763-64b281bb-0f9b-4d7b-b80b-2a81d060418f.png)
",increase change anything image,issue,negative,neutral,neutral,neutral,neutral,neutral
1286970131,"My one piece of anecdata is that I generate videos in a similar way very often, and haven't had any such issues. I do it a bit differently though, by using `pyvirtualdisplay` inside of the python code before launching the Unity executable, and take snapshots within Python after each action, so maybe that will work better",one piece generate similar way often bit differently though inside python code unity executable take within python action maybe work better,issue,negative,positive,positive,positive,positive,positive
1283623349,"This is a video outlining the changes we made to the toolkit. Its a proof of concept and would need more work to make it a usable feature.

https://youtu.be/NQ5eASuu-Mw ",video made proof concept would need work make usable feature,issue,negative,neutral,neutral,neutral,neutral,neutral
1282254613,"Hi,
Can you post a video or image of your ""To implement multiple objectives and add weight to those objectives so an Agent could display multiple behaviours without the need to retrain.""?
",hi post video image implement multiple add weight agent could display multiple without need retrain,issue,negative,neutral,neutral,neutral,neutral,neutral
1279058547,"I got to solve the Porblem thx to @LeezPayne on this post : https://github.com/Unity-Technologies/ml-agents/issues/5794

Quote ""It seems that ml-agents depends on the old importlib-metadata library. I solved this issue by pip install importlib-metadata==4.4""",got solve post quote old library issue pip install,issue,negative,positive,neutral,neutral,positive,positive
1278277657,"Same issue: I have tried using python 3.8.10 and ML-Agents v0.29.
Screenshots: I have added the screenshot of the console log.
Is this anything to do with GPU? My system has 1070.
Unity environment with package version 2.2.1-exp.1 and communication version 1.5.0

Environment (please complete the following information):

- Unity Version: Unity 2020.3.33f1
- OS + version: Windows 11
- ML-Agents version: ML-Agents v0.29.0
- Torch version: 1.7.1+cu110
- Environment: Python 3.8.10

![Screenshot 2022-10-14 000440](https://user-images.githubusercontent.com/7978145/195728150-df9d100f-f97a-41df-85b4-4bcc0ed8353a.jpg)
",issue tried python added console log anything system unity environment package version communication version environment please complete following information unity version unity o version version torch version environment python,issue,negative,positive,neutral,neutral,positive,positive
1274014421,"@bbusche I got a working ml-agents on Apple M2 (Monterey) using Python 3.10.7 and PyTorch stable (1.12.1). I had to clone the ml-agents repos and change `ml-agents/ml-agents/setup.py` to:

```
""torch>=1.8.0,<=1.12.0;(platform_system!='Windows' and python_version>='3.9')""
```

Then `pip install -e path/to/clone/of/ml-agents`.",got working apple python stable clone change torch pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
1273411063,"> pip install importlib-metadata==4.4

Thanks you saved me a lot of headache!",pip install thanks saved lot headache,issue,positive,positive,positive,positive,positive,positive
1270296500,"Hi! Thanks for reaching out. Unfortunately, we're unable to reproduce this issue. A couple of things that you could try: a) update to the newer version of ML-Agents v0.29 and/or b) update to python 3.8.10.",hi thanks reaching unfortunately unable reproduce issue couple could try update version update python,issue,negative,negative,negative,negative,negative,negative
1268905406,Please post this on our forums if you still need help. https://forum.unity.com/forums/ml-agents.453/,please post still need help,issue,positive,neutral,neutral,neutral,neutral,neutral
1268899355,"Can you update to the latest version of ML-Agents unity and python packages (2.2.1-exp.1 and 0.28) respectively? Also, We are unable to help reproduce bugs with custom environments. Can you attempt to reproduce your issue with one of the example environments, or provide a minimal patch to one of the environments needed to reproduce the issue?",update latest version unity python respectively also unable help reproduce custom attempt reproduce issue one example provide minimal patch one reproduce issue,issue,positive,negative,neutral,neutral,negative,negative
1268898478,"> This is normal behavior. The ""freezing"" is due to the policy being update. Does training converge? Are you seeing any exceptions in the console window in the editor? If not, then this is normal.

Thanks for the clarification. This freezing was new to me. Back when ML-Agents was using Tensorflow this didn't happen if I remember correctly.",normal behavior freezing due policy update training converge seeing console window editor normal thanks clarification freezing new back happen remember correctly,issue,negative,positive,neutral,neutral,positive,positive
1268894049,"> Is that true that all people work for this project in Unity were fired?

No. There have been changes at Unity that have slowed our development substantially, but we're still alive! :) Thanks for reaching out. I'll have a look at this.",true people work project unity fired unity development substantially still alive thanks reaching look,issue,positive,positive,positive,positive,positive,positive
1268886714,"This is normal behavior. The ""freezing"" is due to the policy being update. Does training converge? Are you seeing any exceptions in the console window in the editor? If not, then this is normal.",normal behavior freezing due policy update training converge seeing console window editor normal,issue,negative,positive,neutral,neutral,positive,positive
1268881322,"This is not a bug. In ML-Agents, the python process, i.e. the UnityEnv, is actually the server, serving decisions as the agents request them. Unity is the client. Agents are free to make decisions independently of each other at different decision frequencies, as well as, have their episodes terminated independently of each other. This sometimes results in a situation where the Unity environment could be sending information about agents , e.g. terminal state info, before other agents in the environment are ready for decisions. Hence that is why you see 0 agents requiring a decision when the other agent is in a terminal state. The agent that is still 'alive' doesn't need a decision yet. The terminal condition likely happened in between normal decision steps, especially if you have the decision requestor period set to 10, for example, as it specifies in the roller ball tutorial. I wager that if you call step and get_steps again after you see the terminal state, you'll get the correct agent requiring a decision. To verify this, you can set the decision frequency to 1, which will force agents to make decisions at every academy step. This is a bit counter intuitive if you're used to the OpenAI gym api style of RL. step and get_steps do different things. You can read more about the low-level API and how step and get_steps works here: https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Python-API.md",bug python process actually server serving request unity client free make independently different decision well independently sometimes situation unity environment could sending information terminal state environment ready hence see decision agent terminal state agent still need decision yet terminal condition likely normal decision especially decision period set example roller ball tutorial wager call step see terminal state get correct agent decision verify set decision frequency force make every academy step bit counter intuitive used gym style step different read step work,issue,positive,positive,neutral,neutral,positive,positive
1268821762,"Either method works. Thanks for the fix! We're considering bumping the minimum supported version of python to 3.8.x, which should fix the issue.",either method work thanks fix considering bumping minimum version python fix issue,issue,negative,positive,positive,positive,positive,positive
1268810857,"Hi @arturosauraa , Thanks for reaching out. Can you please fill out our bug report template?

**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Console logs / stack traces**
Please wrap in [triple backticks (```)](https://help.github.com/en/articles/creating-and-highlighting-code-blocks) to make it easier to read.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Environment (please complete the following information):**
- Unity Version: [e.g. Unity 2020.1f1]
- OS + version: [e.g. Windows 10]
- _ML-Agents version_: (e.g. ML-Agents v0.8, or latest `develop` branch from source)
- _Torch version_: (you can run `pip3 show torch` to get this)
- _Environment_: (which example environment you used to reproduce the error)

**NOTE:** We are unable to help reproduce bugs with custom environments.  Please attempt to reproduce your issue with one of the example environments, or provide a minimal patch to one of the environments needed to reproduce the issue.
",hi thanks reaching please fill bug report template describe bug clear concise description bug reproduce reproduce behavior go click scroll see error console stack please wrap triple make easier read applicable add help explain problem environment please complete following information unity version unity o version latest develop branch source run pip show torch get example environment used reproduce error note unable help reproduce custom please attempt reproduce issue one example provide minimal patch one reproduce issue,issue,positive,positive,neutral,neutral,positive,positive
1268526731,"> It seems that `ml-agents` depends on the old `importlib-metadata` library. I solved this issue by ` pip install importlib-metadata==4.4`

I have the same problem and this is useful for me.",old library issue pip install problem useful,issue,negative,positive,positive,positive,positive,positive
1268218605,"I also found another solution which is upgrading python. I've installed python version 3.8.10, run the hole installation process from the beginning and everything works just fine now!
So I guess we have two different solutions to the problem, that's cool! ",also found another solution python python version run hole installation process beginning everything work fine guess two different problem cool,issue,negative,positive,positive,positive,positive,positive
1268119079,It seems that  `ml-agents` depends on the old `importlib-metadata` library. I solved this issue by ` pip install importlib-metadata==4.4`,old library issue pip install,issue,negative,positive,neutral,neutral,positive,positive
1267690548,"As I mentioned above, I followed the installation guide for the latest release (19). I'm also running this project on a virtual environment with: 

**Unity version**: 2021.3.3f1
**Python version**: 3.9.7 
**Ml agents version**: 0.28.0
**Upgraded pip**: 22.2.2


and everything else as default as the guide provided here. Also I haven't updated the package so currently I'm running on **2.2.1 exp.1** . If more detailed information can help solve this issue tell me and I'll upload more screenshots.



![ML-Agents package](https://user-images.githubusercontent.com/87013739/193944844-51a0b88a-b822-4d04-a206-ca5ef54a6dd4.jpg)
![virtual env](https://user-images.githubusercontent.com/87013739/193945076-1401f918-61d4-4be1-8cec-5aeeedd79363.jpg)
![Project folder](https://user-images.githubusercontent.com/87013739/193945089-e459f467-3a1f-4168-bb80-3f0ce743f290.jpg)

",installation guide latest release also running project virtual environment unity version python version version pip everything else default guide provided also package currently running detailed information help solve issue tell package virtual project folder,issue,positive,positive,positive,positive,positive,positive
1267347572,"Hi,
I’m just starting to learn ML-Agents. Since you are using the same version as me, My question is, did you update anything? E.g. ML-Agents in the Unity Package Manager.
Why haven’t you provided the Environment info as the template states?
",hi starting learn since version question update anything unity package manager provided environment template,issue,negative,neutral,neutral,neutral,neutral,neutral
1267320138,I am also running into the exact same issue.,also running exact issue,issue,negative,positive,positive,positive,positive,positive
1266122481,"I am definitely looking forward to this! I am really glad it has not stagnated for good!

I do already use this for a project and intend to continue using this for lots of game projects.

Such as these cute “Space Algae”: https://giphy.com/gifs/wip-mllearning-spacealgae-74AxRQpca46rGUpYaA",definitely looking forward really glad good already use project intend continue lot game cute space algae,issue,positive,positive,positive,positive,positive,positive
1263458034,"Also the web content there is just a redirect to https://unity.com/products/machine-learning-agents so one option is to just link there directly, but we still need to look into the certificate issue since visitors to unity3d.ai can come from many sources",also web content redirect one option link directly still need look certificate issue since come many,issue,negative,positive,positive,positive,positive,positive
1255451069,"precommit passing on develop, ignoring the failure here",precommit passing develop failure,issue,negative,negative,negative,negative,negative,negative
1252781015,"I fixed by moving from installing on linux using WSL, to using windows for
the install

On Tue, 20 Sep 2022, 19:46 Jason Rupert, ***@***.***> wrote:

> What versions of attrs and cattrs do you have?
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/5755#issuecomment-1252769934>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AFLQW3U4ZL5I5ZUNEOFXOHTV7IA7XANCNFSM5Y67SHWA>
> .
> You are receiving this because you commented.Message ID:
> ***@***.***>
>
",fixed moving install tue wrote reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1251176397,"ML-Agents has not been abandoned. We have had some changes at Unity that have slowed our development, but I'm happy to report that we're planning another release with a bunch of new features in Q4 of this year, sometime in October/November. Stay tuned!",abandoned unity development happy report another release bunch new year sometime stay tuned,issue,negative,positive,positive,positive,positive,positive
1251055734,"+1

There was an earlier thread I think about a re-org... can't find it now though",thread think ca find though,issue,negative,neutral,neutral,neutral,neutral,neutral
1245214883,"""There was layoffs are reportedly focused in the AI and Engineering divisions"" in 28 jun.  Approximately from that date no any tickets assigned. Hope I'm wrong",reportedly ai engineering approximately date assigned hope wrong,issue,negative,negative,negative,negative,negative,negative
1242572778,"This could be fixed by changing the name of the required method to something other than ""Update"" - unless this is intentional, in which case the documentation should probably be updated.

Thanks for this incredibly powerful tool, and keep up the fantastic work!",could fixed name method something update unless intentional case documentation probably thanks incredibly powerful tool keep fantastic work,issue,positive,positive,positive,positive,positive,positive
1241092029,I think the parameters to the VariableLength function are misnamed but the functionality is correct (i.e. just swapping the parameters in the function declaration and where they are used in the function is the correct fix).  I will confirm this.  Thank you for pointing out the problem.,think function functionality correct swapping function declaration used function correct fix confirm thank pointing problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1235668288,"Currently getting the same issue:

```
~/44797/Documents/unityModels   unity 3.7.0 master ❯ mlagents-learn --help                                                         16:58:42
Traceback (most recent call last):
  File ""/home/laurie/.local/bin/mlagents-learn"", line 5, in <module>
    from mlagents.trainers.learn import main
  File ""/home/laurie/.local/lib/python3.8/site-packages/mlagents/trainers/learn.py"", line 2, in <module>
    from mlagents import torch_utils
  File ""/home/laurie/.local/lib/python3.8/site-packages/mlagents/torch_utils/__init__.py"", line 1, in <module>
    from mlagents.torch_utils.torch import torch as torch  # noqa
  File ""/home/laurie/.local/lib/python3.8/site-packages/mlagents/torch_utils/torch.py"", line 6, in <module>
    from mlagents.trainers.settings import TorchSettings
  File ""/home/laurie/.local/lib/python3.8/site-packages/mlagents/trainers/settings.py"", line 5, in <module>
    import cattr
  File ""/home/laurie/.local/lib/python3.8/site-packages/cattr/__init__.py"", line 1, in <module>
    from .converters import Converter, GenConverter, UnstructureStrategy
  File ""/home/laurie/.local/lib/python3.8/site-packages/cattr/converters.py"", line 13, in <module>
    from attr import resolve_types, has as attrs_has
ImportError: cannot import name 'resolve_types' from 'attr' (/usr/lib/python3/dist-packages/attr/__init__.py)
```

How did you solve?",currently getting issue unity help recent call last file line module import main file line module import file line module import torch torch file line module import file line module import file line module import converter file line module import import name solve,issue,positive,positive,neutral,neutral,positive,positive
1217526470,Really appreciate your help! I'll firstly try on the code you provided!,really appreciate help firstly try code provided,issue,positive,positive,positive,positive,positive,positive
1217349831,"@pengzhi1998  I ended changing encoders.py and add some preprocess in the forward method inside ResNetVisualEncoder there.

To forward the image from ml-imagesynthesis, I dont really remember, but adding camera sensor and making the [ImageSynthesis](https://bitbucket.org/Unity-Technologies/ml-imagesynthesis/src/master/) to output its pass to camera 0 should be sufficient
`SetupCameraWithReplacementShader(capturePasses[0].camera, uberReplacementShader, ReplacelementModes.CatergoryId);`",ended add forward method inside forward image dont really remember camera sensor making output pas camera sufficient,issue,negative,positive,positive,positive,positive,positive
1216015750,"Thank you for opening up this issue! I'm also trying to use the [ImageSynthesis](https://bitbucket.org/Unity-Technologies/ml-imagesynthesis/src/master/) tool to obtain semantic segmentation images for my ml-agent but have faced [the same problem](https://github.com/Unity-Technologies/ml-agents/issues/5775). Did you figure out how to do that at last? @darwinharianto
May I have your suggestions as well? @vincentpierre 
Thank you so much!",thank opening issue also trying use tool obtain semantic segmentation faced problem figure last may well thank much,issue,positive,positive,neutral,neutral,positive,positive
1215062588,"I'm so sorry, it seems that I posted the question in the wrong place. But may I have your suggestions? And could you please help me to add a label **help-wanted**? Thank you so much!",sorry posted question wrong place may could please help add label thank much,issue,negative,negative,negative,negative,negative,negative
1205954931,"This seems to be a bug in how ml-agents handles CUDA tensors. I am getting similar errors trying to enable MPS. As a workaround, you can force training to run on the CPU by changing https://github.com/Unity-Technologies/ml-agents/blob/main/ml-agents/mlagents/torch_utils/torch.py#L48 to just be `device_str = ""cpu""`.",bug getting similar trying enable force training run,issue,negative,neutral,neutral,neutral,neutral,neutral
1202328584,Commenting mainly to keep the task alive. @AKemendo any updates on this? Do you think a fix will land anytime soon?,mainly keep task alive think fix land soon,issue,negative,positive,positive,positive,positive,positive
1189076816,Is that true that all people work for this project in Unity were fired?,true people work project unity fired,issue,negative,positive,positive,positive,positive,positive
1187472232,@sunchipsster1 Did the final comment here help you? I'm using Xorg virtual display and the same thing is happening... I don't know what to do as the build shouldn't require pressing play or anything for the game to launch in the virtual display? HELP! 🙏🏽,final comment help virtual display thing happening know build require pressing play anything game launch virtual display help,issue,positive,negative,negative,negative,negative,negative
1180903573,"I still get warning `Fewer observations...`, but it works, not only with one prefab.",still get warning work one prefab,issue,negative,neutral,neutral,neutral,neutral,neutral
1180901988,"I just tried typing the command, `pip3 install torch~=1.7.1 -f https://download.pytorch.org/whl/torch_stable.html`, and it worked",tried command pip install worked,issue,negative,neutral,neutral,neutral,neutral,neutral
1174451635,Has anyone found a fix to this yet? I'm having the same issues as well.,anyone found fix yet well,issue,negative,neutral,neutral,neutral,neutral,neutral
1173136076,"I have met exactly the same problem, I'll add my information:
Using Unity 2021.3.5f1
ml-agent release19 version
PyTorch 1.12.0 with Cuda version=11.6
Unity package for Version 2.2.1-exp.1

Maybe it is worth noting that I didn't follow the Installation Guide to install the PyTorch with command:
pip3 install torch~=1.7.1 -f https://download.pytorch.org/whl/torch_stable.html

Instead, I used the command from PyTorch official site:
conda install PyTorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge

Because I would like to be able to utilize the graphics card for training.
Maybe that has an impact?",met exactly problem add information unity release version unity package version maybe worth follow installation guide install command pip install instead used command official site install would like able utilize graphic card training maybe impact,issue,negative,positive,positive,positive,positive,positive
1173128788,"I'v manage to built  the libgrpc_csharp_ext.dylib for x86_64 arch and rename it to 

`ml-agents/com.unity.ml-agents/Plugins/ProtoBuffer/runtimes/osx/native/libgrpc_csharp_ext.x64.bundle` 

it works now.",manage built arch rename work,issue,negative,neutral,neutral,neutral,neutral,neutral
1172934380,@RedTachyon Thanks for the info! But pyvirtualdisplay doesn't support GPU/Hardware Acceleration which means it will be very slow and the rendering quality sometime wouldn't be good enough.,thanks support acceleration slow rendering quality sometime would good enough,issue,positive,positive,positive,positive,positive,positive
1172904194,"You can do headless rendering with pyvirtualdisplay, integrating it in MLA would be nice, but it's not too hard to make it work yourself ",headless rendering would nice hard make work,issue,negative,positive,positive,positive,positive,positive
1170848424,+1 !! I desperately need to figure how to do this for my project 😇🙏🏽,desperately need figure project,issue,negative,negative,negative,negative,negative,negative
1170096234,"Oh, i forgot to mention that this bug doesn't happen when using heuristics",oh forgot mention bug happen,issue,negative,neutral,neutral,neutral,neutral,neutral
1166227718,"hi @surfnerd , did this native universal lib got test on another x86_64 apple machine? I'v encounter an issue #5759 while try to train a demo Unity project.

it seems the x86_64 version lib depend on libgrpc_csharp_ext.dylib that have absolute RPATH.

```
otool -L /Users/xlla/git/ml-agents/com.unity.ml-agents/Plugins/ProtoBuffer/runtimes/osx/native/libgrpc_csharp_ext.x64.bundle
/Users/xlla/git/ml-agents/com.unity.ml-agents/Plugins/ProtoBuffer/runtimes/osx/native/libgrpc_csharp_ext.x64.bundle:
	/Users/christophergoy/code/ml-agents-grpc/cmake/build/libgrpc_csharp_ext.dylib (compatibility version 0.0.0, current version 0.0.0)
	/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1292.100.5)
	/usr/lib/libresolv.9.dylib (compatibility version 1.0.0, current version 1.0.0)
```

So, could you upload the depend binary lib libgrpc_csharp_ext.dylib for x86_64 darwin platform or tell me how to get source and compile it by myself?

I have dig out ml-agents-grpc repo, but since the libgrpc_csharp_ext.x64.bundle is close source, I don't know which version does it depend on.
 ",hi native universal got test another apple machine encounter issue try train unity project version depend absolute compatibility version current version compatibility version current version compatibility version current version could depend binary platform tell get source compile dig since close source know version depend,issue,negative,positive,neutral,neutral,positive,positive
1166213396,"hi @AKemendo , I am using x86_64 arch apple. I'd like to build that lib from source, but I don't where the source code based on and which version it depend on.",hi arch apple like build source source code based version depend,issue,negative,neutral,neutral,neutral,neutral,neutral
1165940154,"Thank you for the issue. We will look into it and get back with you soon, thanks!",thank issue look get back soon thanks,issue,positive,positive,neutral,neutral,positive,positive
1165939382,"Do you happen to have an M1?

If so you may be able to install libgrpc_csharp_ext.dylib manually with this:
https://github.com/einari/Grpc.Core.M1

If not we'll have to look at different solutions",happen may able install manually look different,issue,negative,positive,positive,positive,positive,positive
1165823099,"I found the reason that libgrpc_csharp_ext.dylib can't be load, it reference to a non-exist lib.

```
otool -L /Users/xlla/git/ml-agents/com.unity.ml-agents/Plugins/ProtoBuffer/runtimes/osx/native/libgrpc_csharp_ext.x64.bundle
/Users/xlla/git/ml-agents/com.unity.ml-agents/Plugins/ProtoBuffer/runtimes/osx/native/libgrpc_csharp_ext.x64.bundle:
	/Users/christophergoy/code/ml-agents-grpc/cmake/build/libgrpc_csharp_ext.dylib (compatibility version 0.0.0, current version 0.0.0)
	/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1292.100.5)
	/usr/lib/libresolv.9.dylib (compatibility version 1.0.0, current version 1.0.0)
```

So, where can I found this lib ""/Users/christophergoy/code/ml-agents-grpc/cmake/build/libgrpc_csharp_ext.dylib"", or how can I build it for use?



",found reason ca load reference compatibility version current version compatibility version current version compatibility version current version found build use,issue,negative,neutral,neutral,neutral,neutral,neutral
1165794989,"hi @jrupert-unity , I am using latest source , 

```
pip3 show mlagents
Name: mlagents
Version: 0.29.0.dev0
Summary: Unity Machine Learning Agents
Home-page: https://github.com/Unity-Technologies/ml-agents
Author: Unity Technologies
Author-email: ML-Agents@unity3d.com
License: 
Location: /Users/xlla/git/ml-agents/ml-agents
Requires: attrs, cattrs, grpcio, h5py, mlagents_envs, numpy, Pillow, protobuf, pyyaml, tensorboard, torch
Required-by: 

```

and I can't change python right now, since I have manual build many package on this version,  tensorflow, pytorch, etc. it's painful to rebuild all those packages.

Finally, I  have manual pick up some code of functools.py from 3.9.9 and apply to 3.9.13, it works now.
",hi latest source pip show name version dev summary unity machine learning author unity license location pillow torch ca change python right since manual build many package version painful rebuild finally manual pick code apply work,issue,negative,positive,positive,positive,positive,positive
1163385087,"Unfortunately when I try to use the following in my YAML
env_settings:
   num_areas: 8
Is not recognised by the ML Agents 1.9 YAML parser it thrwos up an error ""The option num_areas""  is invalid
I tired with env_settings at the top level of my YAML, then under Behaviours. Still does not parse.
Where is the formatting of this option in the YMAL documented ?

Sent from Mail<https://go.microsoft.com/fwlink/?LinkId=550986> for Windows

From: Miguel Alonso ***@***.***>
Sent: 22 June 2022 15:30
To: ***@***.***>
Cc: ***@***.***>; ***@***.***>
Subject: Re: [Unity-Technologies/ml-agents] Training Area Replicator, does not replicate Environments during Training (Issue #5757)


Hi @JulesVerny<https://github.com/JulesVerny> , if you'd like to use the training area replicator in python, you'll have to add the following, for example, to your trainer config file to replicate 8 training areas:

env_settings: num_areas: 8

—
Reply to this email directly, view it on GitHub<https://github.com/Unity-Technologies/ml-agents/issues/5757#issuecomment-1163178344>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AAULPDZRTBILF5T3LLP7GB3VQMPRZANCNFSM5ZPJUTUA>.
You are receiving this because you were mentioned.Message ID: ***@***.***>

",unfortunately try use following parser error option invalid tired top level still parse option sent mail sent june subject training area replicate training issue hi like use training area python add following example trainer file replicate training reply directly view id,issue,negative,negative,neutral,neutral,negative,negative
1163384798,"Unfortunately when I try to use the following in my YAML
env_settings: 
   num_areas: 8
Is not recognised by the ML Agents 1.9 YAML parser it thrwos up an error ""The option num_areas""  is invalid
I tired with env_settings at the top level of my YAML, then under Behaviours. Still does not parse.
Where is the formatting of this option in the YMAL documented ? ",unfortunately try use following parser error option invalid tired top level still parse option,issue,negative,negative,neutral,neutral,negative,negative
1163180234,We currently do not have full support for M1 macs. We'll be looking into adding support over the next few quarters.,currently full support looking support next quarter,issue,positive,positive,positive,positive,positive,positive
1163178344,"Hi @JulesVerny , if you'd like to use the training area replicator in python, you'll have to add the following, for example, to your trainer config file to replicate 8 training areas:

`env_settings:
     num_areas: 8
`",hi like use training area python add following example trainer file replicate training,issue,negative,neutral,neutral,neutral,neutral,neutral
1162095659,"You can resolve this by going to an older version of Python or a new version of ML-Agents (e.g. 0.29.0 or latest source).  You can find more information here:  https://github.com/Unity-Technologies/ml-agents/issues/5689
",resolve going older version python new version latest source find information,issue,negative,positive,positive,positive,positive,positive
1162074530,I have the same issue running more than 1 training instance at the same time. Can you add this feature? It is very important especially when you try a ton of variants of your own learning algorithms.,issue running training instance time add feature important especially try ton learning,issue,negative,positive,positive,positive,positive,positive
1162064489,I've reproduced the environment you describe but do not get the error you are getting.  Can you do a `pip3 list` and share it?,environment describe get error getting pip list share,issue,negative,neutral,neutral,neutral,neutral,neutral
1159609179,"> Two possible fixes to try in the meantime:
> 
> * Remove the version of ML-Agents you have from the Package Manager and the use Install From Disk and install both the com.unity.ml-agents and com.unity.ml-agents.extensions packages that are in ml-agents-release_19.
> * Remove the PushBlockWithInput folder and PushBlockWithInput.meta file from \ml-agents-release_19\Project\Assets\ML-Agents\Examples

The method 2 work for me!After i deleting  files in  \ml-agents-release_19\Project\Assets\ML-Agents\Examples .
It print 
![error2](https://user-images.githubusercontent.com/76671016/174464629-676abbdc-8a83-4f7f-af0e-660e9821a960.jpg)

so i search google and find out how to fix this error:

 https://github.com/jilleJr/Newtonsoft.Json-for-Unity

the last ,3d box works!
![work!](https://user-images.githubusercontent.com/76671016/174464657-351f979a-b1c3-4dce-ab54-b44071001388.jpg)

",two possible try remove version package manager use install disk install remove folder file method work print error search find fix error last box work work,issue,negative,neutral,neutral,neutral,neutral,neutral
1157309943,"TypeError: Invalid first argument to `register()`. typing.Dict[mlagents.trainers.settings.RewardSignalType, mlagents.trainers.settings.RewardSignalSettings] is not a class.
I have this error now,
anyone can help me?
",invalid first argument register class error anyone help,issue,negative,positive,positive,positive,positive,positive
1157101047,"> Two possible fixes to try in the meantime:
> 
> * Remove the version of ML-Agents you have from the package manager and the use Install From Disk and install both the com.unity.ml-agents and com.unity.ml-agents.extensions packages.
> * remove the PushBlockWithInput folder and PushBlockWithInput.meta file from \ml-agents-release_19\Project\Assets\ML-Agents\Examples

thank for replying , i will look on it these day",two possible try remove version package manager use install disk install remove folder file thank look day,issue,negative,neutral,neutral,neutral,neutral,neutral
1156954168,"Two possible fixes to try in the meantime:
- Remove the version of ML-Agents you have from the Package Manager and the use Install From Disk and install both the com.unity.ml-agents and com.unity.ml-agents.extensions packages that are in ml-agents-release_19.
- Remove the PushBlockWithInput folder and PushBlockWithInput.meta file from \ml-agents-release_19\Project\Assets\ML-Agents\Examples
",two possible try remove version package manager use install disk install remove folder file,issue,negative,neutral,neutral,neutral,neutral,neutral
1156898950,"Hi!  I'd like to help resolve this issue, but I haven't been able to reproduce the problem.  Can those who are encountering the error confirm that they've followed these steps or tell me what they've done differently?

1) Download and unzip (or clone) ml-agents-release_19
2) Using in the Unity Hub, use Unity 2021.2.X to Add the ML-Agents Example projects (ml-agents-release_19/Project folder)
3) In the Unity Editor with the Example project open, open the 3DBall scene from Assets/ML-Agents/Examples/3DBall/Scenes/
",hi like help resolve issue able reproduce problem error confirm tell done differently clone unity hub use unity add example folder unity editor example project open open scene,issue,negative,positive,positive,positive,positive,positive
1155972990,"Same issue with versions: Unity Hub 3.1.2, Unity 2021.2.4fq, ML Agents package 2.0.1, ml-agents-release_19
Assets\ML-Agents\Examples\PushBlockWithInput\Scripts\PushBlockActions.cs(274,16): error CS0246: The type or namespace name 'InputAction' could not be found (are you missing a using directive or an assembly reference?)
followed advice on above comment, but not resolved yet.",issue unity hub unity package error type name could found missing directive assembly reference advice comment resolved yet,issue,negative,negative,negative,negative,negative,negative
1149712167,"Hi @andrewcoh, yes; the built-in component;
found it works with smaller demo file sizes; but unfortunately they take longer to load then there worth.

",hi yes component found work smaller file size unfortunately take longer load worth,issue,negative,negative,neutral,neutral,negative,negative
1148929164,I could fix the same Errors  with the new Input Sytem: []https://docs.unity3d.com/Packages/com.unity.inputsystem@1.0/manual/Installation.html,could fix new input,issue,negative,positive,positive,positive,positive,positive
1144615481,"Thanks a lot for your answer!
I used matrix multiplication in place of cumsum and solved this problem, thanks again :)",thanks lot answer used matrix multiplication place problem thanks,issue,negative,positive,positive,positive,positive,positive
1144104563,Thanks for the bug report. We’ve documented your input and will come back to you once we determine how best to support your request.,thanks bug report input come back determine best support request,issue,positive,positive,positive,positive,positive,positive
1143877927,"Hi @Frank-Dz,

Are you looking to use GPU on your Linux-server? Do you have visual observations? from your player logs, it looks like some of the shaders are not compatible with GPU but my gut feeling is it shouldn't affect the `AddFloatObs` in vector sensors. 

you can read more on -nographics and batchmode [here](https://docs.unity3d.com/Manual/EditorCommandLineArguments.html).",hi looking use visual player like compatible gut feeling affect vector read,issue,negative,neutral,neutral,neutral,neutral,neutral
1143749435,"If you step the environment beyond 1 step after the first env.reset(), does the problem persist?",step environment beyond step first problem persist,issue,negative,positive,positive,positive,positive,positive
1143103894,"@andrewcoh 

I think just walljump


```python
import gym
import torch
import numpy as np
from torch import nn
import random
import torch.nn.functional as F
import collections
import time
from torch.optim.lr_scheduler import StepLR
from collections import deque
from mlagents_envs.base_env import ActionTuple
from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel
from mlagents_envs.environment import UnityEnvironment

side_channel = EngineConfigurationChannel()
env = UnityEnvironment(file_name=None, seed=1, side_channels=[side_channel])
side_channel.set_configuration_parameters(time_scale=30)
# env = UnityToGymWrapper(env)
env.reset()
behavior_names = list(env.behavior_specs.keys())
DecisionSteps, TerminalSteps = env.get_steps(behavior_names[0])
print(len(DecisionSteps), len(TerminalSteps)) # normal, output: 1 0
env.reset()
DecisionSteps, TerminalSteps = env.get_steps(behavior_names[0])
print(len(DecisionSteps), len(TerminalSteps)) # output: 0 1

env.reset()
env.reset()
DecisionSteps, TerminalSteps = env.get_steps(behavior_names[0])
print(len(DecisionSteps), len(TerminalSteps)) # output: 0 0
```",think python import gym import torch import torch import import random import import import time import import import import import list print normal output print output print output,issue,negative,negative,negative,negative,negative,negative
1141727145,"It seems to work now.
I'm facing some other issues after the end of the training but it's not related to that.
Thanks again ;)",work facing end training related thanks,issue,negative,positive,neutral,neutral,positive,positive
1141510445,It looks like you are correct and it is not supported.  Repeat this post on the Barracuda [github](https://docs.unity3d.com/Packages/com.unity.barracuda@3.0/manual/SupportedOperators.html).  You might get more info there and they will get the signal that it is desired.  ,like correct repeat post barracuda might get get signal desired,issue,positive,neutral,neutral,neutral,neutral,neutral
1141250134,"I had the same problem today,  `pip3 install --upgrade protobuf==3.20.0` solved that particular error for me",problem today pip install upgrade particular error,issue,negative,positive,positive,positive,positive,positive
1140656883,Have you figured out how to fix this yet? Having trouble with this.,figured fix yet trouble,issue,negative,negative,negative,negative,negative,negative
1133862443,"Or may I know:
During the process of adopting -no graphics -batchmode to Unity3D, what changes will mlagents make based on this?",may know process graphic make based,issue,negative,neutral,neutral,neutral,neutral,neutral
1124545563,"Thank you, I think you are right.  I'll look into it and make any necessary corrections.",thank think right look make necessary,issue,negative,positive,positive,positive,positive,positive
1122581201,"Judging by the number of responses here and on the forum, the answer will be received faster here (thank you).

As I understand it, there are no ready-made answers here how to act in this or that case? Do I need to act by trial and error only (increasing batch-size,buffer memory etc)?
p.s. what the last recommended version unity + MLagents ? my is 2020.3.25f1 and 2.1.0_exp-1?",number forum answer received faster thank understand act case need act trial error increasing buffer memory last version unity,issue,negative,neutral,neutral,neutral,neutral,neutral
1122535529,Thank you @Tyushang we will log this request and get back with you once we have investigated it,thank log request get back,issue,negative,neutral,neutral,neutral,neutral,neutral
1122523487,"Thanks for your questions @mercurion. Your cumulative reward graph isn't showing the hallmarks of catastrophic forgetting however it doesn't seem to be converging and your reward variance seems to be increasing. 

You will most likely get a faster answer with more detail if you ask these in the ML-Agents forum (https://forum.unity.com/forums/ml-agents.453/).

We will log your request for additional example documentation on training.

Thank you.",thanks cumulative reward graph showing catastrophic forgetting however seem converging reward variance increasing likely get faster answer detail ask forum log request additional example documentation training thank,issue,positive,positive,neutral,neutral,positive,positive
1121217197,Thank you @RedTachyon we'll add your input to the ticket as a +1 to the feature request.,thank add input ticket feature request,issue,negative,neutral,neutral,neutral,neutral,neutral
1120312049,"I second this very strongly, sometimes I had to do very weird workaround just to get some simple debug information, e.g. the total reward that the agent has obtained in this step (I ended up having to use reflection, because the relevant attribute is private)",second strongly sometimes weird get simple information total reward agent step ended use reflection relevant attribute private,issue,positive,positive,neutral,neutral,positive,positive
1118757404,"Hi Benedict, Thanks for the suggestion. We’ve documented your feedback and will come back to you once we determine how best to support your request.
",hi benedict thanks suggestion feedback come back determine best support request,issue,positive,positive,positive,positive,positive,positive
1118695126,"This is interesting; when I tried to fork it through github it gave me an error of `max forks allowed for this repository`! Ok, this is just preliminary, I've had a look at the code and made some changes. [PR has been submitted](https://github.com/Unity-Technologies/ml-agents/pull/5741) let me know anything you don't like, including suggestions, coding style, documentation, etc. ",interesting tried fork gave error repository preliminary look code made let know anything like style documentation,issue,negative,positive,positive,positive,positive,positive
1118106551,"Thank you so much for the reply @andrewcoh .

You understand correctly. I'm trying to use a simple MCTS on a match3 game, when I try to restore the env state, deep copy won't work. So I was wondering if there is a way to directly modify the env via the API? 
Or for match3 games, is there any other algorithm recommendation when the env cannot be restore?

Many Thanks!
",thank much reply understand correctly trying use simple match game try restore state deep copy wo work wondering way directly modify via match algorithm recommendation restore many thanks,issue,positive,positive,neutral,neutral,positive,positive
1118098259,"> Hi @fdsafet
> 
> That environment exists on a branch [here](https://github.com/Unity-Technologies/ml-agents/tree/walljump-pushblock/Project/Assets/ML-Agents/Examples/WallJumpAndPushblock) but it may be messy because we never ended up merging it into main.
> 
> Good luck!

thank you!",hi environment branch may messy never ended main good luck thank,issue,negative,positive,positive,positive,positive,positive
1117940172,"Hi @alexge233 

This does sound interesting and we would like to see the PR. I am not totally sure what you mean by

> This repo seems to have reached its maximum forks

Is it possible that is on your end i.e. your person github has reached maximum forks? If I'm not mistaken, it looks like you've successfully forked ML-Agents [here.](https://github.com/alexge233/ml-agents)",hi sound interesting would like see totally sure mean maximum possible end person maximum mistaken like successfully forked,issue,positive,positive,positive,positive,positive,positive
1117937796,"Hi @smgreat 

If I understand correctly, you are trying to replicate envs on the python side i.e. in the LLAPI to perform MCTS like rollouts?

Simply copying a Unity build like this won't work i.e. it wont instantiate an exact copy of the current env at the moment of copy. It's hard to make a recommendation without knowing more because ""rolling back"" a unity simulation to a previous state is not possible in general due to physical non-determinism.

Are you trying to do strict MCTS or do you want to do a MuZero like approach?",hi understand correctly trying replicate python side perform like simply unity build like wo work wont exact copy current moment copy hard make recommendation without knowing rolling back unity simulation previous state possible general due physical trying strict want like approach,issue,positive,negative,neutral,neutral,negative,negative
1117933801,"Hi @fdsafet 

That environment exists on a branch [here](https://github.com/Unity-Technologies/ml-agents/tree/walljump-pushblock/Project/Assets/ML-Agents/Examples/WallJumpAndPushblock) but it may be messy because we never ended up merging it into main.

Good luck!",hi environment branch may messy never ended main good luck,issue,negative,positive,positive,positive,positive,positive
1117569498,"@RedTachyon Yes it is and that is the correct solution here. I confirmed internally that unityagents is fully deprecated

@walchko try:

`pip install mlagents`

and let us know if you have any issues. ",yes correct solution confirmed internally fully try pip install let u know,issue,positive,positive,positive,positive,positive,positive
1117075582,Isn't `unityagents` a very old version that hasn't been updated since 2018? You probably want to install `pip install mlagents` unless I'm missing something,old version since probably want install pip install unless missing something,issue,negative,negative,neutral,neutral,negative,negative
1116811082,"Thank you @walchko I was also able to reproduce this on a different platform so we will look into this asap. 

I logged this internally as https://jira.unity3d.com/browse/MLA-2539. 

Please have a look at our contribution guidelines if you choose to hunt the bug down further: https://github.com/Unity-Technologies/ml-agents/blob/main/com.unity.ml-agents/CONTRIBUTING.md",thank also able reproduce different platform look logged internally please look contribution choose hunt bug,issue,positive,positive,positive,positive,positive,positive
1116374361,new python version mlagents==0.29.0 and mlagents_env==0.29.0 should fix this issue.,new python version fix issue,issue,negative,positive,positive,positive,positive,positive
1115111410,"We are now using the [Unity forums](https://forum.unity.com/forums/ml-agents.453/) to handle [discussions, installation issues, etc] like this. Please create a new thread there, in order to get the community involved in the discussion, and reply with the link to your forum thread. Closing this issue.",unity handle installation like please create new thread order get community involved discussion reply link forum thread issue,issue,positive,positive,positive,positive,positive,positive
1115107035,Thank you. We've logged this internally as https://jira.unity3d.com/browse/MLA-2537. Feel free to submit a PR. Please have a look at our contribution guidelines: https://github.com/Unity-Technologies/ml-agents/blob/main/com.unity.ml-agents/CONTRIBUTING.md,thank logged internally feel free submit please look contribution,issue,positive,positive,positive,positive,positive,positive
1115102622,"We are now using the [Unity forums](https://forum.unity.com/forums/ml-agents.453/) to handle [discussions, installation issues, etc] like this. Please create a new thread there, in order to get the community involved in the discussion, and reply with the link to your forum thread.",unity handle installation like please create new thread order get community involved discussion reply link forum thread,issue,positive,positive,positive,positive,positive,positive
1115102054,"Thanks for your reply. Since this still seems like a concern, I'm re-openning the issue.",thanks reply since still like concern issue,issue,positive,positive,positive,positive,positive,positive
1114678975,Having the same issue. I did a hack by modifying environment.py to force worker_id and it worked but it's not very practical.,issue hack force worked practical,issue,negative,neutral,neutral,neutral,neutral,neutral
1107451664,"> Fix after upgrading pytorch to the latest version
Hello,
to which Version you upgraded?",fix latest version hello version,issue,negative,positive,positive,positive,positive,positive
1104258573,"Hi @mayukhghosh, 

We are now using the [Unity forums](https://forum.unity.com/forums/ml-agents.453/) to handle issues like this with custom environments. Please create a new thread there, in order to get the community involved in the discussion, and reply with the link to your forum thread.

A quick test for your case could be sampling the action from action_spaces or manage the stepping with UnityEnvironment.",hi unity handle like custom please create new thread order get community involved discussion reply link forum thread quick test case could sampling action manage stepping,issue,positive,positive,positive,positive,positive,positive
1104253272,"Hi, thanks for bringing this issue to our attention, I logged it under MLA-2517.",hi thanks issue attention logged,issue,negative,positive,positive,positive,positive,positive
1104183560,"Hi, We have this under our radar to improve the efficiency of our trainers. We also had prototypes for parallelizing the SubprocessEnvManager. Thanks for bringing this to our attention, will prioritize releasing these changes when appropriate.",hi radar improve efficiency also thanks attention appropriate,issue,positive,positive,positive,positive,positive,positive
1104160346,"Hi, Relaese 19 is an experimental package, you would need to 'Enbale Preview Packages' in your project settings to see it in package manager. You can also add the package from disk or git url.",hi experimental package would need preview project see package manager also add package disk git,issue,negative,positive,neutral,neutral,positive,positive
1098213087,"Hey @sunchipsster1, maybe give this script a try:
```sh
#!/bin/bash

port=0
while true; do
  export DISPLAY=:$((++port))
  Xvfb $DISPLAY -noreset &
  sleep 5
  jobs %% && break
done

python run_gym.py
```
The while loop looks for an unused display port so you don't have to hardcode it. Let me know if it works :) ",hey maybe give script try sh true export display sleep break done python loop unused display port let know work,issue,negative,positive,positive,positive,positive,positive
1097184404,"Sorry, we are not currently supporting multi-agent vectorized env for gym wrapper.",sorry currently supporting gym wrapper,issue,negative,negative,negative,negative,negative,negative
1095641609,Are there any updates for this issue? It would be great to see support for Ray's RLlib in ML Agents - particularly multi-agent reinforcement learning.,issue would great see support ray particularly reinforcement learning,issue,positive,positive,positive,positive,positive,positive
1094557458,Hi @maryamhonari any pointers if I want to have a quick hack for this feature? (working on a paper submission....,hi want quick hack feature working paper submission,issue,negative,positive,positive,positive,positive,positive
1092418199,"hi.  I notice that Unity version 2019.4 is not longer supported in Release 19 branch. Could you explain why? What new features are only supported in higher Unity version? What can I do if I want Release 19 branch to work in lower Unity Version,such as 2018.4LTS. Many thanks!",hi notice unity version longer release branch could explain new higher unity version want release branch work lower unity version many thanks,issue,negative,positive,positive,positive,positive,positive
1081912986,Thanks for your feature request! We've logged this feature request internally as MLA-2488.,thanks feature request logged feature request internally,issue,negative,positive,neutral,neutral,positive,positive
1081909102,"Hi! Thanks for reaching out. We're currently investigating adding support for additional sensors, however, we don't have a timeline for the work. More than likely, this will be added to our roadmap for 2023. Thanks again!",hi thanks reaching currently investigating support additional however work likely added thanks,issue,positive,positive,neutral,neutral,positive,positive
1081907196,"Hi! Thanks for reaching out. We do have plans to improve training performance on our roadmap for end of 2022/beginning of 2023. This would include adding support for, among many things, multi-GPU training. Thanks again!",hi thanks reaching improve training performance end would include support among many training thanks,issue,positive,positive,positive,positive,positive,positive
1081210555,please also support turning the timeout off :),please also support turning,issue,positive,neutral,neutral,neutral,neutral,neutral
1072854743,I have the same error on version 2.1 - Any way to solve it?,error version way solve,issue,negative,neutral,neutral,neutral,neutral,neutral
1072732059,Your docker container must be started with GPUs. `xvfb-run` started processes cannot use the GPU for graphics.,docker container must use graphic,issue,negative,neutral,neutral,neutral,neutral,neutral
1067071005,"Hi @chenzhutian, 

This is a useful feature. I logged it internally under [MLA-2465](https://jira.unity3d.com/browse/MLA-2465) and we will prioritize accordingly.",hi useful feature logged internally accordingly,issue,negative,positive,positive,positive,positive,positive
1060678793,"Hey,
I have the same issue.
I think I found a workaround though. In the Package Manager you can press the '+' icon at the top left and click ""Add package from git URL..."" and then paste the following link: ""git+https://github.com/Unity-Technologies/ml-agents.git?path=com.unity.ml-agents#release_19"". Then it will install ""Version 2.2.1-exp.1"" and this is the version that they tell us to install here: https://github.com/Unity-Technologies/ml-agents/releases

But I got over a thousand warnings about signed/unsigned mismatch. But hopefully will work anyways... (haven't tested it yet)
Picture of warnings:
![image](https://user-images.githubusercontent.com/60349722/157041681-a11dc440-4d08-41dc-843e-ebb70028c1c1.png)

Can someone from the team tell me why there are so many warnings and if I can ignore this?

Thanks.
",hey issue think found though package manager press icon top left click add package git paste following link install version version tell u install got thousand mismatch hopefully work anyways tested yet picture image someone team tell many ignore thanks,issue,positive,positive,positive,positive,positive,positive
1058861367,"Hi @jrupert-unity thank you so much for your kind advice :) 

If I interpreted your suggestions correctly: 
for 1. are you suggesting I run  **xvfb-run -a --server-args=""-screen 0 1400x900x24"" python run_gym.py on my Terminal** ? ... Where run_gym is the simple trainer that I displayed above (basically just calling on the env for now).  If this is what you meant, then yes, I have now tried this based on your suggestion, and unfortunately it does not succeed. It gives the same error output as displayed above. 

for 2. I tried this but this did not work as I require the use of graphics. The Server Build ends up ouputting OBS = grey screens, rather than scenes. this is why when I was building the exe file in Unity, I did not click Server Build.

I am a bit puzzled why running xvfb-run ... etc does not seem to work.

Alternatively, I tried out this colab: https://github.com/Unity-Technologies/ml-agents/blob/main/colab/Colab_UnityEnvironment_1_Run.ipynb

This seems to work well in Jupyter. Unfortunately, this solution is not viable for me as it is a jupyter notebook, whereas my university's server can only seem to run python from a  terminal. Is there perhaps a simple way to **translate** the section for installing and running  "" _ rendering dependencies_ "" section in that google collab, into something that can be run by python and terminal? 

Thank you very much for your kind help, I really appreciate it :) 

I hope it can also be informative for others who may find themselves in a similar situation :) ",hi thank much kind advice correctly suggesting run python terminal simple trainer displayed basically calling meant yes tried based suggestion unfortunately succeed error output displayed tried work require use graphic server build grey rather building file unity click server build bit puzzled running seem work alternatively tried work well unfortunately solution viable notebook whereas university server seem run python terminal perhaps simple way translate section running rendering section something run python terminal thank much kind help really appreciate hope also informative may find similar situation,issue,positive,positive,neutral,neutral,positive,positive
1058670785,This isn't supposed to happen and does not happen in the general case.  Knowing the parameters you are running with might help.,supposed happen happen general case knowing running might help,issue,negative,positive,neutral,neutral,positive,positive
1058635573,"There are a couple of things you can try.

1.  Use the server-args parameter explicitly
e.g.  xvfb-run -a --server-args=""-screen 0 1400x900x24"" < call trainer with your env here >
2.  Specify Server Build in your build settings.

Let me know if either of these work for you.",couple try use parameter explicitly call trainer specify server build build let know either work,issue,negative,neutral,neutral,neutral,neutral,neutral
1050876769,"That helped and fixed it.
On rider, choose file --> encoding --> utf-8
Thanks @jrupert-unity ",fixed rider choose file thanks,issue,negative,positive,positive,positive,positive,positive
1050186128,"Your editor might be adding these characters that are giving the ml-agents trainer problems.  Have a look at [this link](https://stackoverflow.com/questions/3255993/how-do-i-remove-%C3%AF-from-the-beginning-of-a-file) and see if it helps you address the problem.
",editor might giving trainer look link see address problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1047824354,The documentation update will be done with the next release. Changing this to a feature request since the remaining work is around auto-sizing observations when using one-hot obs.,documentation update done next release feature request since work around,issue,negative,neutral,neutral,neutral,neutral,neutral
1044794585,"Updating to consolidate notes from above for folks who are on macOS w/M1 and are using miniforge.

Installation instructions: 

```sh
conda create -n unity-mla python=3.9.7 -y
conda activate unity-mla
conda install pytorch=1.8 -y
conda install grpcio h5py -y
pip install mlagents==0.28.0 -q
```

Check install
```sh
mlagents-learn --help
```

![Screen Shot 2022-02-18 at 11 29 49 AM](https://user-images.githubusercontent.com/26209687/154723299-479a5145-e6cb-4a85-9cb7-c746a4f5c7f5.png)
",consolidate installation sh create activate install install pip install check install sh help screen shot,issue,positive,neutral,neutral,neutral,neutral,neutral
1040343134,"> @hvpeteet Function OnActionReceived uses ActionBuffers. I think Acutor is needed.

@315930399 Right you are, thank you. ",function think right thank,issue,negative,positive,positive,positive,positive,positive
1039299082,"Whoops, I didn't mean to hit the close button. Please close this if this solved your issue / helped.",whoop mean hit close button please close issue,issue,negative,negative,negative,negative,negative,negative
1039297303,Thanks for the PR. Does going through the example fail because of a failing import needed from Actuators? I think that the Actuators import is left off because this example doesn't use Actuators but I could be misreading. ,thanks going example fail failing import think import left example use could,issue,negative,negative,neutral,neutral,negative,negative
1039292325,"It seems that Actuators were never available in 1.0.8 and are were added in https://github.com/Unity-Technologies/ml-agents/commit/1ccd8796c11394baf6c391f79e0d16720e71f706 which only targets 1.4.0 and higher. If you need to stick to 1.0.8 then please refer to the usage docs for that version at https://github.com/Unity-Technologies/ml-agents/tree/com.unity.ml-agents_1.0.8, otherwise I would highly recommend upgrading to 2.0.1.",never available added higher need stick please refer usage version otherwise would highly recommend,issue,positive,positive,neutral,neutral,positive,positive
1039277571,"Please note that if you want to get around this for the time being you can upgrade to 2.0.1, I will continue to look into this though since it impacts the verified release.",please note want get around time upgrade continue look though since release,issue,negative,neutral,neutral,neutral,neutral,neutral
1039233642,"Hi, thanks for your interest in ml-agents. We are now using the [Unity forums](https://forum.unity.com/forums/ml-agents.453/) to handle [discussions, installation issues, etc] like this. Please create a new thread there, in order to get the community involved in the discussion, and reply with the link to your forum thread.",hi thanks interest unity handle installation like please create new thread order get community involved discussion reply link forum thread,issue,positive,positive,positive,positive,positive,positive
1039231782,"Thanks for the report, we will update the documentation.",thanks report update documentation,issue,negative,positive,positive,positive,positive,positive
1039228429,"We are now using the [Unity forums](https://forum.unity.com/forums/ml-agents.453/) to handle usage-based discussions like this. Please create a new thread there, in order to get the community involved in the discussion, and reply with the link to your forum thread.",unity handle like please create new thread order get community involved discussion reply link forum thread,issue,positive,positive,positive,positive,positive,positive
1039220542,"~~> Thanks, before digging in deeper can you please check if you have installed the com.unity.ml-agents.extensions extension? 
Pointers:
    1. [InputActuatorComponent.md](https://github.com/Unity-Technologies/ml-agents/blob/release_19/com.unity.ml-agents.extensions/Documentation~/InputActuatorComponent.md#getting-started) 
    2. Packaging [docs](https://github.com/Unity-Technologies/ml-agents/blob/release_19/com.unity.ml-agents.extensions/Documentation~/com.unity.ml-agents.extensions.md#about-ml-agents-extensions-package-comunityml-agentsextensions)~~

Sorry I didn't read your import path correctly and assumed you were using the input system. Taking a look at this issue now",thanks digging please check extension sorry read import path correctly assumed input system taking look issue,issue,positive,negative,negative,negative,negative,negative
1035472494,"Hi, I use exactly the same versions and it turns out the agent can learn something.
However, it take x3 times steps than the config file to make the agent dodge some bullets. And the reward is very unstable.
Do you have any idea about this? Is it because the demo in the blog use another config files with, eg., Curriculum learning or what not?
![Screenshot 2022-02-10 170749](https://user-images.githubusercontent.com/3315274/153491112-7e94d1d8-887a-498a-83f8-f486001afb25.png)
(the rewards)

Thanks!",hi use exactly turn agent learn something however take time file make agent dodge reward unstable idea use another curriculum learning thanks,issue,positive,positive,positive,positive,positive,positive
1034111139,"Thanks!
I will try to use Release 17 and let you know. Thanks again!",thanks try use release let know thanks,issue,positive,positive,positive,positive,positive,positive
1034057256,"Also, please use the version of ML-Agents that is on that branch as opposed to the latest stable version. Since this is not an officially supported environment, we have not tested it with the latest stable version of the toolkit. Your best best to reproduce the results from the blog post is to use the exact setup on that branch.",also please use version branch opposed latest stable version since officially environment tested latest stable version best best reproduce post use exact setup branch,issue,positive,positive,positive,positive,positive,positive
1034055979,"Hmmm, seems like the config for the bullet hell env was not added to that branch. You can try using this one from a previous bullet hell branch: https://github.com/Unity-Technologies/ml-agents/blob/develop-bullet-hell/config/ppo/Bullet.yaml. ",like bullet hell added branch try one previous bullet hell branch,issue,negative,negative,negative,negative,negative,negative
1034052091,"Is this using a custom environment? If so, we are unable to help reproduce bugs. Please attempt to reproduce this issue with one of the example environments, or provide a minimal patch to one of the environments needed to reproduce the issue. This would help us determine track the issue down and determine if it's within the ML-Agents toolkit. If you are using a custom environment, it may be a bug in your environment that is slowing it down over time.",custom environment unable help reproduce please attempt reproduce issue one example provide minimal patch one reproduce issue would help u determine track issue determine within custom environment may bug environment time,issue,positive,negative,negative,negative,negative,negative
1033787235,"I currently use the latest stable version of ml-agents and the code from that branch to train the model. I think it is acceptable if the agent does not perform as good as the one in the blog post. But right now, it seems that the agent learns nothing. ",currently use latest stable version code branch train model think acceptable agent perform good one post right agent nothing,issue,positive,positive,positive,positive,positive,positive
1033702688,"I build with the following environment and everything worked fine now:

macOS Big Sur 11.3.1
Unity Editor = 2020.3.15f2
Unity-Hub = 3.0.1
Python = 3.8 
ML-Agents Release 19
",build following environment everything worked fine big sur unity editor python release,issue,negative,positive,positive,positive,positive,positive
1033379180,"Thanks for your reply! 
I found that I posted the wrong link and actually I am using the branch you provide.
However, there is no config file in the branch you provided. Can you please also share the config file? Thanks!",thanks reply found posted wrong link actually branch provide however file branch provided please also share file thanks,issue,positive,negative,neutral,neutral,negative,negative
1033317594,"Thank you for your replay! But you may not understand what I mean
What I'm trying to say is that I packaged the Unity environment I built and implemented the MADDPG algorithm myself for training. I used [UnityEnvRegistry](https://github.com/Unity-Technologies/ml-agents/blob/d34f3cd6ee078782b22341e4ceb958359069ab60/docs/Unity-Environment-Registry.md) to create an environment and set the time-scale=20:
![image](https://user-images.githubusercontent.com/38696314/153118095-4923e9e9-53cc-41c8-9e64-feb8e3041644.png)
After setting up the environment, I started training with the MADDPG algorithm, but as the training speed slowed down, I didn't change the time-scale at all . The relevant picture is in my original problem description.
I am looking forward to your reply as soon as possible, which is very important to me",thank replay may understand mean trying say unity environment built algorithm training used create environment set image setting environment training algorithm training speed change relevant picture original problem description looking forward reply soon possible important,issue,positive,positive,positive,positive,positive,positive
1033302884,Hi! There's not need to manage the time-scale yourself. The ML-Agents toolkit automatically accelerates the environment during training. You can also set the time scale through the config file: https://github.com/Unity-Technologies/ml-agents/blob/82a1114d83d43fed0b3078ab4cc90943e2804e02/docs/Training-ML-Agents.md.,hi need manage automatically environment training also set time scale file,issue,negative,neutral,neutral,neutral,neutral,neutral
1033301264,"Hi! Thanks for reaching out. Unfortunately, this is not an officially supported ML-Agents example environment. As such, it's not actively maintained and is provided as-is. However, it seems that you're using another branch for bullet hell. The one that was used for the blog post is this one: https://github.com/Unity-Technologies/ml-agents/tree/develop-bullet-hell-buffer. Be sure to use that branch for the unity project, the same versions of ml-agents and ml-agents-envs python packages, as well as, the config Bullet.yaml from that branch.",hi thanks reaching unfortunately officially example environment actively provided however another branch bullet hell one used post one sure use branch unity project python well branch,issue,negative,positive,neutral,neutral,positive,positive
1033293917,"Hi! Thanks for reaching out. Unfortunately, we are unable to help reproduce bugs with custom environments.  Please attempt to reproduce your issue with one of the example environments, or provide a minimal patch to one of the environments needed to reproduce the issue. This would help us determine track the issue down an determine if it's within the ML-Agents toolkit.",hi thanks reaching unfortunately unable help reproduce custom please attempt reproduce issue one example provide minimal patch one reproduce issue would help u determine track issue determine within,issue,positive,negative,negative,negative,negative,negative
1033292794,"Hi @yachty66, are you on an M1 mac by any chance?",hi yachty mac chance,issue,negative,neutral,neutral,neutral,neutral,neutral
1033291146,Hi! Thanks for reaching out. We've logged this request as MLA-2402. ,hi thanks reaching logged request,issue,positive,positive,positive,positive,positive,positive
1033290367,Thanks for reaching out! We've logged this request internally as MLA-2401.,thanks reaching logged request internally,issue,positive,positive,neutral,neutral,positive,positive
1028147783,"@andreialbu03 are you able to train? This is something with what I still struggle. See [https://github.com/Unity-Technologies/ml-agents/issues/5697](url).

> 

",able train something still struggle see,issue,negative,positive,positive,positive,positive,positive
1025251872,"Great suggestion. This is also useful as while debugging in Unity editor, the trainer might disconnect on pause",great suggestion also useful unity editor trainer might disconnect pause,issue,positive,positive,positive,positive,positive,positive
1024940908,"@maryamhonari thanks a lot for solving this problem, it really works! I'm a total beginner, it took me whole day to find out why, I just couldn't figure it out. If I knew there was this issue on Github, I could have saved several god damn hours...",thanks lot problem really work total beginner took whole day find could figure knew issue could saved several god damn,issue,negative,positive,positive,positive,positive,positive
1024800744,"agree that changing the requirements to
""torch>=1.8.0,<=1.10.2;(platform_system!='Windows' and python_version>='3.9')""

it works well on m1, too!",agree torch work well,issue,positive,neutral,neutral,neutral,neutral,neutral
1023501918,"Hey @meiemari, from my little experience with mlagents, I would like to suggest a few potential improvements to your setup that could improve training speed:
-  The use of Docker could be a severe bottleneck in the setup. Even though Docker is considerably lightweight, containerization still adds some significant overhead from my experience. Since you are running your experiments on AWS, why not run mlagents directly on the instance (i.e., installing mlagents through pip/conda on the AWS instance directly)?
- On xvfb -- since you are running your environment in `no-graphics` mode, you do not need xvfb. Xvfb is usually used for when agents in the environment requires visual observation (i.e., rendering). Moreover, even if you plan to train visual-obs based agents in the future, you could also install xvfb directly on the AWS instance, without the need to use Docker.
- It may not be advisable to have the number of parallel environments >> number of cores. There is usually diminishing returns in the number of parallel envs, and too many could hurt performance.
- Might also be advisable to check that GPU is being utilized (e.g., check `nvidia-smi` while training), and perhaps try SAC instead if you were using PPO.

I have limited experience in mlagents, and in machine learning in general, so please take these suggestions with a grain of salt!",hey little experience would like suggest potential setup could improve training speed use docker could severe bottleneck setup even though docker considerably lightweight still significant overhead experience since running run directly instance instance directly since running environment mode need usually used environment visual observation rendering moreover even plan train based future could also install directly instance without need use docker may advisable number parallel number usually number parallel many could hurt performance might also advisable check check training perhaps try sac instead limited experience machine learning general please take grain salt,issue,negative,positive,neutral,neutral,positive,positive
1022959486,"@miguelalonsojr @Woojin-unity 
Thank you very much for your help! I implemented training acceleration in unity executable environment(time-scale=20), but I found another problem:the training speed decreased over time :

![image](https://user-images.githubusercontent.com/38696314/151317706-5ec65460-13b3-4175-9c52-a421c4cee37e.png)
I would like to know what causes this problem and how to solve it. Before that, I also tried to train directly with ML agents. Without packing the environment, the training speed would still decrease.
 
BTW, I found the edit>project settings>time> time scale in the unity editor able to acclerate the simulation speed. Can I speed up both the Unity environment Settings and ML Agents at the same time?  (Theoretically, should get training accelerometers multiplied?)  


 
",thank much help training acceleration unity executable environment found another problem training speed time image would like know problem solve also tried train directly without environment training speed would still decrease found edit project time time scale unity editor able simulation speed speed unity environment time theoretically get training,issue,negative,positive,positive,positive,positive,positive
1021375092,"Hi, Thanks for bringing up this request. Currently we consider a nearly zero entity as an empty observation and it's a fair point that it might be a valid observation. I logged this internally as MLA-2360 and will prioritize accordingly.",hi thanks request currently consider nearly zero entity empty observation fair point might valid observation logged internally accordingly,issue,positive,positive,positive,positive,positive,positive
1020342878,"I did … 3.9.9 works fine

Thanks for looking in to it and for supporting the product

On Jan 24, 2022, at 9:01 AM, Maryam Honari ***@***.***> wrote:

﻿

*Hi* *jimlbeaver* <https://github.com/jimlbeaver>*,*

*Thanks for bringing up this issue. Python 3.9.10 is fairly new and while
we're looking into it, I suggest installing with an older version of python
if possible.*

—
Reply to this email directly, view it on GitHub
<https://github.com/Unity-Technologies/ml-agents/issues/5689#issuecomment-1020326327>,
or unsubscribe
<https://github.com/notifications/unsubscribe-auth/AD6DOWCXCM7Y2EVGI5XMUF3UXWAX5ANCNFSM5MSGHTWQ>
.
Triage notifications on the go with GitHub Mobile for iOS
<https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
or Android
<https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.

You are receiving this because you authored the thread.Message ID:
***@***.***>
",work fine thanks looking supporting product wrote hi thanks issue python fairly new looking suggest older version python possible reply directly view triage go mobile android id,issue,positive,positive,positive,positive,positive,positive
1020326327,"### **Hi** [**jimlbeaver**](https://github.com/jimlbeaver)**,**

**Thanks for bringing up this issue. Python 3.9.10 is fairly new and while we're looking into it, I suggest installing with an older version of python if possible.**",hi thanks issue python fairly new looking suggest older version python possible,issue,negative,positive,positive,positive,positive,positive
1020163589,You can use an asset bundle system to download the latest model from your own CDN.,use asset bundle system latest model,issue,negative,positive,positive,positive,positive,positive
1019324852,"Hey @yachty66 I wasn't able to get it to work. I will try again with that guide as soon as I can, thanks for sharing!",hey yachty able get work try guide soon thanks,issue,negative,positive,positive,positive,positive,positive
1019311013,"call stack:


Python: 3.9.10
Torch: 1.7.1+cu110

Is there anyone that have an idea of what's going on?



(env) C:\Exjobb\ml-agents\ml-agents-main>mlagents-learn config/ppo/3DBall.yaml --run-id=fi

Traceback (most recent call last):

  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.2800.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 197, in _run_module_as_main

    return _run_code(code, main_globals, None,

  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.2800.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 87, in _run_code

    exec(code, run_globals)

  File ""C:\Exjobb\unity\env\Scripts\mlagents-learn.exe\__main__.py"", line 4, in <module>

  File ""C:\Exjobb\unity\env\lib\site-packages\mlagents\trainers\learn.py"", line 2, in <module>

    from mlagents import torch_utils

  File ""C:\Exjobb\unity\env\lib\site-packages\mlagents\torch_utils\__init__.py"", line 1, in <module>

    from mlagents.torch_utils.torch import torch as torch  # noqa

  File ""C:\Exjobb\unity\env\lib\site-packages\mlagents\torch_utils\torch.py"", line 6, in <module>

    from mlagents.trainers.settings import TorchSettings

  File ""C:\Exjobb\unity\env\lib\site-packages\mlagents\trainers\settings.py"", line 644, in <module>

    class TrainerSettings(ExportableSettings):

  File ""C:\Exjobb\unity\env\lib\site-packages\mlagents\trainers\settings.py"", line 667, in TrainerSettings

    cattr.register_structure_hook(

  File ""C:\Exjobb\unity\env\lib\site-packages\cattr\converters.py"", line 207, in register_structure_hook

    self._structure_func.register_cls_list([(cl, func)])

  File ""C:\Exjobb\unity\env\lib\site-packages\cattr\dispatch.py"", line 55, in register_cls_list

    self._single_dispatch.register(cls, handler)

  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.9_3.9.2800.0_x64__qbz5n2kfra8p0\lib\functools.py"", line 855, in register

    raise TypeError(

TypeError: Invalid first argument to `register()`. typing.Dict[mlagents.trainers.settings.RewardSignalType, mlagents.trainers.settings.RewardSignalSettings] is not a class.",call stack python torch anyone idea going recent call last file line return code none file line code file line module file line module import file line module import torch torch file line module import file line module class file line file line file line handler file line register raise invalid first argument register class,issue,negative,positive,neutral,neutral,positive,positive
1016898806,"I think if you push more changes to the file the pull request will update.  Alternatively if Miguel approves we can accept this pull request and I can update with my changes after that.

> Add more commits by pushing to the main branch on DeathVenom54/ml-agents.",think push file pull request update alternatively accept pull request update add pushing main branch,issue,negative,positive,positive,positive,positive,positive
1016835239,"If you open the package manager in unity, does it also say you have version 2.1.0-exp.1 ? This is the com.unity.ml-agents package that is compatible with v0.27.0 according to [https://github.com/Unity-Technologies/ml-agents/releases](url)

Personally I am using the following versions without any issues using anaconda for a python environment:
Unity package: 2.0.1 (Release version listed in the package manager)
Python package: 0.26.0
Python version: 3.8.12
OS: Windows 11",open package manager unity also say version package compatible according personally following without anaconda python environment unity package release version listed package manager python package python version o,issue,negative,neutral,neutral,neutral,neutral,neutral
1016767515,"Ah, I was forgetting to run those commands inside of the cloned ml-agents repo, and I was running them in my venv folder instead. Apologies, it's working like a charm now. 

Thanks!",ah forgetting run inside running folder instead working like charm thanks,issue,positive,positive,positive,positive,positive,positive
1016747900,"> Hi, when you say to ""Install ml-agents locally"" on step 5, what exactly do you mean by that? I tried running `pip3 install mlagents` in my virtual env and I'm getting `ERROR: ResolutionImpossible`.

Did you enter the exact commands from below that?
```
arch -arch x86_64 python3 -m pip install -e ./ml-agents-envs
arch -arch x86_64 python3 -m pip install -e ./ml-agents
```",hi say install locally step exactly mean tried running pip install virtual getting error enter exact arch python pip install arch python pip install,issue,negative,negative,neutral,neutral,negative,negative
1016736486,"Hi, when you say to ""Install ml-agents locally"" on step 5, what exactly do you mean by that? I tried running `pip3 install mlagents` in my virtual env and I'm getting `ERROR: ResolutionImpossible`. ",hi say install locally step exactly mean tried running pip install virtual getting error,issue,negative,negative,negative,negative,negative,negative
1016614960,Hey @andreialbu03. Did you got it to work? I come to similar problems which you described with using M1.,hey got work come similar,issue,negative,neutral,neutral,neutral,neutral,neutral
1016256968,Hey I am very new to OS contributions. I cannot see any options to edit the code in my pull request.,hey new o see edit code pull request,issue,negative,positive,positive,positive,positive,positive
1015954448,@maryamhonari or @jrupert-unity can you have a look at this PR? Seems reasonable since it's a doc update.,look reasonable since doc update,issue,negative,positive,positive,positive,positive,positive
1015936615,@Hunter-Unity Do we still need to merge this? Or is this already included in the dodge ball repo?,still need merge already included dodge ball,issue,negative,neutral,neutral,neutral,neutral,neutral
1015926733,"Closing PR. This should be part of a multi-agent sample environments package, not the ml-agents-env package.",part sample package package,issue,negative,neutral,neutral,neutral,neutral,neutral
1015899225,"hey @simoninithomas , thanks for the contribution! Can you make sure to rebase/merge main, resolve conflicts, and push? That should trigger our latest CI/CD. Once you get to that point, can you update the proposed documentation for the latest version on main? Thanks again!",hey thanks contribution make sure main resolve push trigger latest get point update documentation latest version main thanks,issue,positive,positive,positive,positive,positive,positive
1015807551,"Hi @Ali-khn,

Thank you for being an ML-Agents user. Adding support for external engines in not on the ML-Agents Roadmap right now. We are focused on adding support for custom trainers and expanding our imitation learning features. We love to hear about your feedback on how we can make our users lives easier when it comes to the above features I just mentioned and also when it comes to using ML-Agents in Unity.  
If you want to use ML-Agents but does not have access to Unity you can learn about our free Unity Personal license here ([https://unity3d.com/unity/activation/personal)/)](https://unity3d.com/unity/activation/personal)/))",hi thank user support external right support custom expanding imitation learning love hear feedback make easier come also come unity want use access unity learn free unity personal license,issue,positive,positive,positive,positive,positive,positive
1015798130,"Hi @Woojin-unity , the m_GridSize parameter is computed automatically and is not meant to be exposed as a public property. Closing this PR, as this is a feature, not a bug.",hi parameter automatically meant exposed public property feature bug,issue,negative,neutral,neutral,neutral,neutral,neutral
1014807810,"Hi,

No, unfortunately, that is not it since I took care in specifying the correct behavior type and then double-checked to ensure I did not mess that easy one up. I actually took a screenshot of the incompatibility issue which I still cannot understand, the one I linked in my post above, though by downgrading the ML-Agents to 0.20 seemed to do the trick, which also seems strange ",hi unfortunately since took care correct behavior type ensure mess easy one actually took incompatibility issue still understand one linked post though trick also strange,issue,negative,negative,neutral,neutral,negative,negative
1014403382,"Thanks for the answer.
I will try to look into them.
Its true I have a material change, so i will try to change that.
",thanks answer try look true material change try change,issue,positive,positive,positive,positive,positive,positive
1014399419,"I'm not entirely sure about the issue but I have a few things in mind that you could check:

1. If you used a demonstration recorder , make sure that it's not still active. If your recorder is set to record 3.725.000 steps, it will interrupt the training when its recording has finished.
2. Make sure you don't have a memory leak. I've had an issue in the past when changing materials using `renderer.Material` causing a memory leak and interrupting learning after a large number of steps. If this is something that's used in your code, change it to `renderer.SharedMaterial`

Hope this helps",entirely sure issue mind could check used demonstration recorder make sure still active recorder set record interrupt training recording finished make sure memory leak issue past causing memory leak interrupting learning large number something used code change hope,issue,positive,positive,positive,positive,positive,positive
1014393341,"When you open the example environment, make sure that the right behavior type is selected for the agent. You can find this under the behavior parameters script like the error suggests. This is probably causing your issue

![Screenshot 2022-01-17 115845](https://user-images.githubusercontent.com/18085277/149757650-a6fd18e7-26bf-48de-9726-1bba9d378fe7.png)

",open example environment make sure right behavior type selected agent find behavior script like error probably causing issue,issue,negative,positive,positive,positive,positive,positive
1013959919,"Additional information.
I am using vector observations only, but still training in the cloud is much slower than on my local laptop. Any hints as to how I can solve this riddle are appreciated :)
I am running headless, using xvfb (Linux Server with Docker).
I have read through older posts and it seems like using xvfb could be a bottleneck [https://github.com/Unity-Technologies/ml-agents/issues/1846](url). 

Is there an alternative to xfvb that I can use by now? 
",additional information vector still training cloud much local solve riddle running headless server docker read older like could bottleneck alternative use,issue,positive,positive,neutral,neutral,positive,positive
1011410334,"Hi Woojin,

I’ve answered your question on the GitHub issue. Feel free to follow up with me directly or through GitHub. We also have an ml-agents channel on slack that you can reach out on: #devs-ml-agents.

Cheers,
Miguel




Miguel Alonso Jr, Ph.D.
(he/him)
Manager, Behavior Authoring and Training
***@***.***
unity.com

> On Jan 12, 2022, at 5:43 AM, Woojin-unity ***@***.***> wrote:
> 
> 
> When you are using Python API, The EngineConfiguration side Channel allows you to modify the time-scale.
> 
> See this Python-API <https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Python-API.md>
>  <https://user-images.githubusercontent.com/97542081/149125423-8fbf47f3-f9d2-4e2b-90c0-c96743f425b3.png>
> —
> Reply to this email directly, view it on GitHub <https://github.com/Unity-Technologies/ml-agents/issues/5654#issuecomment-1010903797>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/ASLFC3S4XGKHWAVU5IALBEDUVVLLZANCNFSM5LAV4QZQ>.
> You are receiving this because you were assigned.
> 

",hi question issue feel free follow directly also channel slack reach manager behavior training wrote python side channel modify see reply directly view assigned,issue,positive,positive,positive,positive,positive,positive
1011384581,"Yes, you can set theses settings as follows:

```
    time_scale = 20
    capture_frame_rate = 60
    target_frame_rate = 300
    eng_channel = EngineConfigurationChannel()
    params_channel = EnvironmentParametersChannel()
    env = UnityEnvironment(file_name=file_name, seed=42, side_channels=[eng_channel, params_channel],
                           base_port=5004 + rank, no_graphics=no_graphics)
    engine_config = {""width"": 80,
                     ""height"": 80,
                     ""quality_level"": 0,
                     ""time_scale"": time_scale,
                     ""capture_frame_rate"": capture_frame_rate,
                     ""target_frame_rate"": target_frame_rate
                     }
    eng_channel.set_configuration_parameters(**engine_config)
```",yes set thesis rank width height,issue,negative,negative,negative,negative,negative,negative
1010903797,"When you are using Python API, The EngineConfiguration side Channel allows you to modify the time-scale.

See this [Python-API](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Python-API.md)
![image](https://user-images.githubusercontent.com/97542081/149125423-8fbf47f3-f9d2-4e2b-90c0-c96743f425b3.png)

",python side channel modify see image,issue,negative,neutral,neutral,neutral,neutral,neutral
1010249688,"**miguelalonsojr**, Thank you very much! I had a mess in my python environment, so when I updated mlagents to 0.27, pip did something strange. I have checked it and now it works fine!",thank much mess python environment pip something strange checked work fine,issue,negative,positive,neutral,neutral,positive,positive
1010024010,"I guess BASE_ENVIRONMENT_PORT works only on executable. EDITOR_PORT can't be changed on python side?

```
if base_port is None:
            base_port = (
                self.BASE_ENVIRONMENT_PORT if file_name else self.DEFAULT_EDITOR_PORT
            )
```",guess work executable ca python side none else,issue,negative,neutral,neutral,neutral,neutral,neutral
1009553873,Replying to remove the stale label.  This is still on the list of things to address.,remove stale label still list address,issue,negative,negative,negative,negative,negative,negative
1009211925,"From the video it seems like you're using very old version of the ml-agents and ml-agents-env packages (0.17 for both versions.) 

![image](https://user-images.githubusercontent.com/76960110/148819187-44e5ef82-42e9-452c-9f75-5cc59ee779cb.png)

Version 2.1.0-exp.1 of the ML-Agents Unity package depends on v0.27.0 of the python packages. Also, we no longer support TensorFlow. Please update the python packages, as well as, switch to PyTorch for the best results.",video like old version image version unity package python also longer support please update python well switch best,issue,positive,positive,positive,positive,positive,positive
1008097781,"+1. The [Training Configuration File docs](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md) says epsilon decays to 0 if one enforces an epsilon decay, but actually the code caps its decay to 0.1, as in [optimizer_torch](https://github.com/Unity-Technologies/ml-agents/blob/f469f7c7113d7ac91ee59aaedc858e28f250e830/ml-agents/mlagents/trainers/ppo/optimizer_torch.py#L52). This is inconsistent.
I ran into this issue when expecting epsilon to decay to 0. My use-case is a tricky environment where there is a perfect sequence of actions to yield a +1 reward, and if any action diverges from the optimal in that sequence, then the agent is punished with a -1 reward. The need for a small epsilon increases proportional to the length of that sequence, since the probability of a random action increases in a compounding manner proportional to the number of actions the agent can execute in that sequence.
It would be nice if the docs could be updated to fix the inconsistency of the epsilon decay cap which is apparently 0 in the docs but 0.1 in the code. It would also be nice if the training configuration file enabled specification of this decay cap, since one has to edit the source code to achieve that otherwise.",training configuration file epsilon one epsilon decay actually code decay inconsistent ran issue epsilon decay tricky environment perfect sequence yield reward action optimal sequence agent reward need small epsilon proportional length sequence since probability random action compounding manner proportional number agent execute sequence would nice could fix inconsistency epsilon decay cap apparently code would also nice training configuration file specification decay cap since one edit source code achieve otherwise,issue,positive,positive,positive,positive,positive,positive
1007388807,"Thanks for your fast response! 
Closing this for now.

Edit: I created an issue in the [Barracuda repo](https://github.com/Unity-Technologies/barracuda-release/issues/248).",thanks fast response edit issue barracuda,issue,negative,positive,positive,positive,positive,positive
1007055480,Once this problem is resolved you might want to look at the example [here](https://github.com/Unity-Technologies/ml-agents/blob/main/colab/Colab_UnityEnvironment_2_Train.ipynb) for how to export an ML-Agents model from PyTorch to comply with the expectations of Barracuda for inference.,problem resolved might want look example export model comply barracuda inference,issue,negative,neutral,neutral,neutral,neutral,neutral
1007054225,"The Barracuda ONNX implementation wants the split attribute on the Split operator to be given explicitly.  It looks like the first Split has it, but the second one does not.  You can make the ONNX model have the required split attribute or get Barracuda to handle the case when it isn't present.  

ml-agents\Project\Library\PackageCache\com.unity.barracuda@2.3.1-preview\Barracuda\Runtime\ONNX\ONNXModelConverter.cs  ~Line 1430

You may want to raise this problem with the [Barracuda team](https://github.com/Unity-Technologies/barracuda-release/issues).",barracuda implementation split attribute split operator given explicitly like first split second one make model split attribute get barracuda handle case present may want raise problem barracuda team,issue,negative,positive,neutral,neutral,positive,positive
1006929994,"> > > Were you able to confirm this working locally with the change?
> > 
> > 
> > Yes. I was able to run `/Applications/Unity/Hub/Editor/2018.4.32f1/Unity.app/Contents/MacOS/Unity -projectPath ~/code/ml-agents-01/Project -batchMode -executeMethod Unity.MLAgents.SampleExporter.ExportCuratedSamples -logFile -`
> > and import package locally using Unity Editor.
> > But I have a question regarding the `pack` job
> 
> How can i confirm this job completes before ml-agents Unity SDK release? https://github.com/Unity-Technologies/ml-agents/blob/main/.yamato/com.unity.ml-agents-pack.yml

I think you should be able to see the job in the checks section on Github. E.g. https://unity-ci.cds.internal.unity3d.com/job/10775272. ",able confirm working locally change yes able run import package locally unity editor question regarding pack job confirm job unity release think able see job section,issue,negative,positive,positive,positive,positive,positive
1006921158,"> > Were you able to confirm this working locally with the change?
> 
> Yes. I was able to run `/Applications/Unity/Hub/Editor/2018.4.32f1/Unity.app/Contents/MacOS/Unity -projectPath ~/code/ml-agents-01/Project -batchMode -executeMethod Unity.MLAgents.SampleExporter.ExportCuratedSamples -logFile -`
> 
> and import package locally using Unity Editor.
> 
> But I have a question regarding the `pack` job

How can i confirm this job completes before ml-agents Unity SDK release?
https://github.com/Unity-Technologies/ml-agents/blob/main/.yamato/com.unity.ml-agents-pack.yml",able confirm working locally change yes able run import package locally unity editor question regarding pack job confirm job unity release,issue,negative,positive,positive,positive,positive,positive
1006917716,"> Were you able to confirm this working locally with the change?

Yes. I was able to run 
`/Applications/Unity/Hub/Editor/2018.4.32f1/Unity.app/Contents/MacOS/Unity -projectPath ~/code/ml-agents-01/Project -batchMode -executeMethod Unity.MLAgents.SampleExporter.ExportCuratedSamples -logFile -`

and import package locally using Unity Editor.

But I have a question regarding the `pack` job",able confirm working locally change yes able run import package locally unity editor question regarding pack job,issue,negative,positive,positive,positive,positive,positive
1006890955,"Anyway I think there are solutions:

1) Resume training with updated agent logic.
2) Manually merge PyTorch models.
3) Swap Brains with some logic.

So I am closing this. If you feel like it's still interesting idea to work on, feel free to re-open :)",anyway think resume training agent manually merge swap brain logic feel like still interesting idea work feel free,issue,positive,positive,positive,positive,positive,positive
1006879680,"To merge A with B. Then AB with C and so on. Not completely sure if its good idea, as the network could grow big and become slow? But the mine idea is to add continuous X skills to existing agent and not being limited in some predefined scope. Like today agent learns to walk, tomorrow to jump, maybe later to run, maybe not. ",merge completely sure good idea network could grow big become slow mine idea add continuous agent limited scope like today agent walk tomorrow jump maybe later run maybe,issue,positive,positive,positive,positive,positive,positive
1006873688,You can [manually merge the PyTorch models if you like](https://discuss.pytorch.org/t/merging-two-models/45637).  Is the idea to continue training the merged network on the combined problem?,manually merge like idea continue training network combined problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1006593775,"Seems like you're using a custom environment. We are unable to help reproduce bugs with custom environments.  Please attempt to reproduce your issue with one of the example environments, or provide a minimal patch to one of the environments needed to reproduce the issue.",like custom environment unable help reproduce custom please attempt reproduce issue one example provide minimal patch one reproduce issue,issue,positive,negative,negative,negative,negative,negative
1006591528,"Thanks so much.

I actually got release 18 to work by simply removing the one enviroment (push block with input).

This led to the errors. For future builds you could  install the environment seperately or just keep in mind MLAgents is actually working fine but it's just the environment (game) that has some problems.

I'll use the version I have now until release 19.

Amazing project and thanks for your help.",thanks much actually got release work simply removing one push block input led future could install environment keep mind actually working fine environment game use version release amazing project thanks help,issue,positive,positive,positive,positive,positive,positive
1006517880,"There was an issue with the previous version of the input manager, v1.2.0. v1.3.0 was released this week and **main** has been patched. Give it a shot. We're ramping up for release 19 now, which should be out in the next couple of weeks and will have this fix. For now, **main** should get you going.",issue previous version input manager week main give shot ramping release next couple fix main get going,issue,negative,positive,neutral,neutral,positive,positive
1005449181,"Hi again, I did some test, maybe it helps. I start from a scenary with one single environment NO prefab, that works perfectly with my trained model. It's a custom Walker. 

- I turned it in a prefab without removing from hierarchy -> It doesn't work
- I turned it in a prefab without removing from hierarchy + Save project -> **It works**
- Disable in hierarchy + Enable in hierarchy -> It doesn't work
- Disable in hierarchy + Enable in hierarchy + Save project -> **It works**

- I turned it in a prefab without removing from hierarchy + Save project + Remove it from hierarchy and spawn it again using ""PrefabUtility.InstantiatePrefab""-> It doesn't work
- I turned it in a prefab without removing from hierarchy + Save project + Remove it from hierarchy and spawn it again using ""PrefabUtility.InstantiatePrefab"" + Save project -> It doesn't work
- I turned it in a prefab without removing from hierarchy + Save project + Remove it from hierarchy and spawn it again using ""Instantiate"" -> It doesn't work
- I turned it in a prefab without removing from hierarchy + Save project + Remove it from hierarchy and spawn it again using ""Instantiate"" + Save project -> It doesn't work

It seems two different problems, but maybe they have a relationship with each other.
I hope this help you to fix it.",hi test maybe start scenary one single environment prefab work perfectly trained model custom walker turned prefab without removing hierarchy work turned prefab without removing hierarchy save project work disable hierarchy enable hierarchy work disable hierarchy enable hierarchy save project work turned prefab without removing hierarchy save project remove hierarchy spawn work turned prefab without removing hierarchy save project remove hierarchy spawn save project work turned prefab without removing hierarchy save project remove hierarchy spawn work turned prefab without removing hierarchy save project remove hierarchy spawn save project work two different maybe relationship hope help fix,issue,negative,positive,positive,positive,positive,positive
1005410766,"This happend to me too but with other versions. 
In our case, we instantiate the prefabs and the we train them. In this point, the trained brain work perfectly in this instantiated prefabs, but if we delete them and we instantiate again, or we do a prefab from them (and you dont remove it from scenary and spawn again, you keep in the scenary), they stop to work with this brain.  I suspect about some kind of error in the NNModel initialitation or something else ... are there some workaround?

**Environment**

-  Unity Version: 2020.3.18f1
- OS + version: Windows 10
- ML-Agents version: 2.1.0-exp.1
- Torch Version: TensorFlow version
- Environment: Walker example customized",case train point trained brain work perfectly delete prefab dont remove scenary spawn keep scenary stop work brain suspect kind error something else environment unity version o version version torch version version environment walker example,issue,negative,positive,positive,positive,positive,positive
1005160426,"If you work off the version of the colab on the main branch it will now create a model that will work with the GridWorldColab example scene, also on the main branch.  

These will be in the upcoming release if you would prefer to wait for a released version rather than using the main branch.

The DQN colab notebook can also be used as a reference for how to create a model that names tensors how Barracuda (which does runtime inference in Unity) expects them, and adds several extra tensors that are required to provide configuration information.

Closing this now but please re-open if there are further issues related to this.",work version main branch create model work example scene also main branch upcoming release would prefer wait version rather main branch notebook also used reference create model barracuda inference unity several extra provide configuration information please related,issue,positive,positive,neutral,neutral,positive,positive
1001061176,"> However, when I try to run `mlagents-learn` to test if it works, I receive this error:
> 
> ```
> ImportError: dlopen(/opt/homebrew/Caskroom/miniforge/base/envs/mlagents-env/lib/python3.9/site-packages/grpc/_cython/cygrpc.cpython-39-darwin.so, 0x0002): symbol not found in flat namespace '_CFRelease'
> ```
> 
> Any idea on this error?

Hi @andreialbu03. I ran into the same error, and was able to get things working with Miniforge by running `conda install grpcio`.

Here's a few relevant lines from my `conda list` too, if they help:

```
# Name                    Version                   Build  Channel
grpcio                    1.43.0           py39h18b86b0_0    conda-forge
h5py                      3.6.0           nompi_py39hd982b79_100    conda-forge
hdf5                      1.12.1          nompi_h829dc4f_103    conda-forge
mlagents                  0.28.0.dev0               dev_0    <develop>
mlagents-envs             0.28.0.dev0               dev_0    <develop>
python                    3.9.9           h43b31ca_0_cpython    conda-forge
torch                     1.10.1                   pypi_0    pypi
```
",however try run test work receive error symbol found flat idea error hi ran error able get working running install relevant list help name version build channel dev develop dev develop python torch,issue,negative,positive,positive,positive,positive,positive
1000205321,"Thank you for the help! This is a nice work-around, but being able to import individual small projects like 3DBall and such would be more flexible.",thank help nice able import individual small like would flexible,issue,positive,positive,positive,positive,positive,positive
1000002214,"The way I've always used the examples is by following these steps:
1. From the Unity Hub, ADD a new project.
2. Navigate to the ml-agents folder you got from GitHub
3. Choose the ""Project"" folder there.
4. This should load a project with scenes for all of the Examples, including 3DBall, which you can open by navigating to the corresponding Scene folder for the example you want from the **Project** window.

I'll will follow up to understand why the documented steps do not work, but hopefully this helps in the meantime.",way always used following unity hub add new project navigate folder got choose project folder load project open corresponding scene folder example want project window follow understand work hopefully,issue,negative,positive,neutral,neutral,positive,positive
998892139,"Hi, and thanks for your interest in ml-agents. Our community is very helpful for these kinds of issues and I would recommend posting on our forums where many more people can help. I have removed the `bug` label since we try to reserve it for bugs with our repo rather than bugs in user code. Please feel free to add it back if a further investigation points to something wrong with our repo.",hi thanks interest community helpful would recommend posting many people help removed bug label since try reserve rather user code please feel free add back investigation something wrong,issue,positive,positive,positive,positive,positive,positive
998881713,"Thanks for the report, sadly I haven't been able to replicate it. Our linux builds CI pipeline is looking healthy and is run nightly. I will try to help in this reply but the best (and most responsive) place for this issue is likely the forums where other users with similar use cases can help.

Assuming:
1. you can run Windows builds (not just train from the editor) locally
2. the versions of ml-agents locally and on the server are the same
3. your C# ml-agents version is compatible with the python version

Then it sounds like it may be an issue with your build / upload process. My next suggestions would be to:

1. make sure you are uploading all the required files. Please note that you need all the files produced by the linux build, not just the .x86_64 file. This will include the _data directory and possibly some other outputs.
2. Try running a build manually on the server (not through the ml-agents python package and likely one built in inference mode so it isn't hanging waiting for a connection). If you can do this then I would suggest inspecting the logs, this could help sort out dependency issues you have (maybe you are using libraries only supported on windows, etc...)

Thanks for your interest and I hope this can help",thanks report sadly able replicate pipeline looking healthy run nightly try help reply best responsive place issue likely similar use help assuming run train editor locally locally server version compatible python version like may issue build process next would make sure please note need produced build file include directory possibly try running build manually server python package likely one built inference mode hanging waiting connection would suggest could help sort dependency maybe thanks interest hope help,issue,positive,positive,positive,positive,positive,positive
998873724,"Hi @eugene-goncharov

Thanks for reporting this. We need more information in order to reproduce the issue. What does your network look like, eg. share your config file. Are you able to reproduce this with any of our example environments? It is also helpful to share your ONNX & PyTorch checkpoint.",hi thanks need information order reproduce issue network look like share file able reproduce example also helpful share,issue,positive,positive,positive,positive,positive,positive
997498495,"Hi @353055619 

Alright, no worries. Thanks for your help!",hi alright thanks help,issue,positive,positive,positive,positive,positive,positive
997305383,The easy solution is to upgrade to Unity 2020.3 LTS if this is possible for your situation.,easy solution upgrade unity possible situation,issue,positive,positive,positive,positive,positive,positive
997281629,"Hi , I need help i've find the same errors and i can't fix it, did you find a solution please ?
",hi need help find ca fix find solution please,issue,positive,neutral,neutral,neutral,neutral,neutral
997156271,"@andreialbu03 Hi

I'm sorry that I can't help you. I have no idea of this error.
Here is my python environment(Hope it helps):
![截屏2021-12-18 下午2 47 21](https://user-images.githubusercontent.com/35963306/146632236-4fce456f-523f-
![截屏2021-12-18 下午2 47 37](https://user-images.githubusercontent.com/35963306/146632240-edf126e8-2bcf-4f2d-8bc1-42e167565fb6.png)
417c-9699-5f345b071d9a.png)
![截屏2021-12-18 下午2 47 45](https://user-images.githubusercontent.com/35963306/146632242-c5497973-e12f-4f9e-aee5-44d326ce4485.png)

",hi sorry ca help idea error python environment hope,issue,negative,negative,negative,negative,negative,negative
996997908,"Hi @353055619 

I managed to install both mlagents and mlagents-env in a conda environment. Running `conda list` displays both of them.
```
mlagents                  0.28.0.dev0               dev_0    <develop>
mlagents-envs             0.28.0.dev0               dev_0    <develop>
```

However, when I try to run `mlagents-learn` to test if it works, I receive this error:
```
ImportError: dlopen(/opt/homebrew/Caskroom/miniforge/base/envs/mlagents-env/lib/python3.9/site-packages/grpc/_cython/cygrpc.cpython-39-darwin.so, 0x0002): symbol not found in flat namespace '_CFRelease'
```

Any idea on this error?",hi install environment running list dev develop dev develop however try run test work receive error symbol found flat idea error,issue,negative,negative,neutral,neutral,negative,negative
995928056,"Hi @353055619 ,

To double check you are still using the commands `pip3 install -e ./ml-agents-envs` and `pip3 install -e ./ml-agents` to install the rest of the packages to the conda environment correct? 

Only for hdf5 and h5py, you are using conda to install them.",hi double check still pip install pip install install rest environment correct install,issue,negative,neutral,neutral,neutral,neutral,neutral
995890234,"> Hi @353055619
> 
> I run `conda install hdf5` but afterwards when running `pip3 install -e ./ml-agents` I receive the same error. Should I be using a different command? I am doing this in a venv environment by the way.

I am sorry. Maybe it is helpful to uninstall `hdf5` and `h5py`(if you just deal with the one not both of them), and then

`conda install hdf5, h5py`

I strongly recommend switch to **Miniforge** instead of venv environment. The former can save a lot of effort to manage python environment, especially on m1 Mac",hi run install afterwards running pip install receive error different command environment way sorry maybe helpful deal one install strongly recommend switch instead environment former save lot effort manage python environment especially mac,issue,positive,negative,neutral,neutral,negative,negative
995867018,"Hi @353055619

I run `conda install hdf5` but afterwards when running `pip3 install -e ./ml-agents` I receive the same error. 
Should I be using a different command? I am doing this in a venv environment by the way.",hi run install afterwards running pip install receive error different command environment way,issue,negative,neutral,neutral,neutral,neutral,neutral
995852556,"@andreialbu03 
Hi
I got this problems too. It is easy to handle by **(Miniforge is what I use to manage python environment. I Recommend to switch to it instead of virtual environment)**

`pip uninstall hdf5`

`conda install hdf5`

**By the way:**  Whenever I encounter similar problems, I always try to install that package separately. It always works. Hope to help you.",hi got easy handle use manage python environment recommend switch instead virtual environment pip install way whenever encounter similar always try install package separately always work hope help,issue,positive,positive,positive,positive,positive,positive
995832300,"Hi @353055619,

I am on python 3.9.7. After cloning the main branch, I first ran `pip3 install -e ./ml-agents-envs` which worked. 
However when trying `pip3 install -e ./ml-agents`, I receive an error, specifically related to h5py.

I also tried the following suggestions that I found, but it did not work:
```
$ brew install hdf5
$ export HDF5_DIR=/opt/homebrew/Cellar/hdf5/1.12.0_4
$ pip install --no-binary=h5py h5py
```

I am wondering, how can I get around this issue?

```
Building wheels for collected packages: h5py
  Building wheel for h5py (pyproject.toml) ... error
  ERROR: Command errored out with exit status 1:
   command: /Users/andreialbu/Dev/machineLearning/my_venvs/mlagents-env/bin/python /Users/andreialbu/Dev/machineLearning/my_venvs/mlagents-env/lib/python3.9/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /var/folders/3d/_7r8hqyd6td_gq695rtyvl3w0000gn/T/tmpu3zglxhh
       cwd: /private/var/folders/3d/_7r8hqyd6td_gq695rtyvl3w0000gn/T/pip-install-alc55bt3/h5py_ce185dc0b9fa48f3bcfdfc2f1e86744b
  Complete output (71 lines):
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.macosx-11.5-arm64-3.9
  creating build/lib.macosx-11.5-arm64-3.9/h5py
  copying h5py/h5py_warnings.py -> build/lib.macosx-11.5-arm64-3.9/h5py
  copying h5py/version.py -> build/lib.macosx-11.5-arm64-3.9/h5py
  copying h5py/__init__.py -> build/lib.macosx-11.5-arm64-3.9/h5py
  copying h5py/ipy_completer.py -> build/lib.macosx-11.5-arm64-3.9/h5py
  creating build/lib.macosx-11.5-arm64-3.9/h5py/_hl
  copying h5py/_hl/files.py -> build/lib.macosx-11.5-arm64-3.9/h5py/_hl
  copying h5py/_hl/compat.py -> build/lib.macosx-11.5-arm64-3.9/h5py/_hl
  copying h5py/_hl/__init__.py -> build/lib.macosx-11.5-arm64-3.9/h5py/_hl
  copying h5py/_hl/selections.py -> build/lib.macosx-11.5-arm64-3.9/h5py/_hl
  copying h5py/_hl/dataset.py -> build/lib.macosx-11.5-arm64-3.9/h5py/_hl
  copying h5py/_hl/vds.py -> build/lib.macosx-11.5-arm64-3.9/h5py/_hl
  copying h5py/_hl/selections2.py -> build/lib.macosx-11.5-arm64-3.9/h5py/_hl
  copying h5py/_hl/group.py -> build/lib.macosx-11.5-arm64-3.9/h5py/_hl
  copying h5py/_hl/datatype.py -> build/lib.macosx-11.5-arm64-3.9/h5py/_hl
  copying h5py/_hl/attrs.py -> build/lib.macosx-11.5-arm64-3.9/h5py/_hl
  copying h5py/_hl/dims.py -> build/lib.macosx-11.5-arm64-3.9/h5py/_hl
  copying h5py/_hl/base.py -> build/lib.macosx-11.5-arm64-3.9/h5py/_hl
  copying h5py/_hl/filters.py -> build/lib.macosx-11.5-arm64-3.9/h5py/_hl
  creating build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_dimension_scales.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_attribute_create.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_file_image.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/conftest.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_h5d_direct_chunk.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_h5f.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_dataset_getitem.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_group.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_errors.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_dataset_swmr.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_slicing.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_h5pl.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_attrs.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/__init__.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_attrs_data.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_h5t.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_big_endian_file.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_h5p.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_dims_dimensionproxy.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_h5o.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_datatype.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/common.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_dataset.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_file.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_selections.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_dtype.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_h5.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_file2.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_completions.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_filters.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_base.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  copying h5py/tests/test_objects.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests
  creating build/lib.macosx-11.5-arm64-3.9/h5py/tests/data_files
  copying h5py/tests/data_files/__init__.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests/data_files
  creating build/lib.macosx-11.5-arm64-3.9/h5py/tests/test_vds
  copying h5py/tests/test_vds/test_highlevel_vds.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests/test_vds
  copying h5py/tests/test_vds/test_virtual_source.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests/test_vds
  copying h5py/tests/test_vds/__init__.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests/test_vds
  copying h5py/tests/test_vds/test_lowlevel_vds.py -> build/lib.macosx-11.5-arm64-3.9/h5py/tests/test_vds
  copying h5py/tests/data_files/vlen_string_s390x.h5 -> build/lib.macosx-11.5-arm64-3.9/h5py/tests/data_files
  copying h5py/tests/data_files/vlen_string_dset_utc.h5 -> build/lib.macosx-11.5-arm64-3.9/h5py/tests/data_files
  copying h5py/tests/data_files/vlen_string_dset.h5 -> build/lib.macosx-11.5-arm64-3.9/h5py/tests/data_files
  running build_ext
  Loading library to get build settings and version: libhdf5.dylib
  error: Unable to load dependency HDF5, make sure HDF5 is installed properly
  error: dlopen(libhdf5.dylib, 0x0006): tried: 'libhdf5.dylib' (no such file), '/usr/local/lib/libhdf5.dylib' (no such file), '/usr/lib/libhdf5.dylib' (no such file), '/private/var/folders/3d/_7r8hqyd6td_gq695rtyvl3w0000gn/T/pip-install-alc55bt3/h5py_ce185dc0b9fa48f3bcfdfc2f1e86744b/libhdf5.dylib' (no such file), '/usr/local/lib/libhdf5.dylib' (no such file), '/usr/lib/libhdf5.dylib' (no such file)
  ----------------------------------------
  ERROR: Failed building wheel for h5py
Failed to build h5py
ERROR: Could not build wheels for h5py, which is required to install pyproject.toml-based projects
```",hi python main branch first ran pip install worked however trying pip install receive error specifically related also tried following found work brew install export pip install wondering get around issue building collected building wheel error error command exit status command complete output running running build running build running loading library get build version error unable load dependency make sure properly error tried file file file file file file error building wheel build error could build install,issue,negative,positive,neutral,neutral,positive,positive
995687734,"@andreialbu03 Just simply clone the main breach, and then install locally. **But Remember**, Before installing, setup.py in ""./ml-agents"" must be modified. Here is what I modify:
`        ""torch>=1.8.0,<1.11.0;(platform_system!='Windows' and python_version>='3.9')"",
        ""torch>=1.6.0,<1.11.0;(platform_system!='Windows' and python_version<'3.9')"",`",simply clone main breach install locally remember must modify torch torch,issue,negative,positive,neutral,neutral,positive,positive
995365529,"Thanks a lot, that's exactly what I'm looking for",thanks lot exactly looking,issue,negative,positive,positive,positive,positive,positive
994804348,Hi! Thanks for bringing this up. This is a known issue with InputSystem 1.2.0 package. You can clear `Project/Assets/csc.rsp` file to stop treating warnings as errors or downgrade to InputSystem 1.1.1 if that's an option.,hi thanks known issue package clear file stop treating downgrade option,issue,positive,positive,positive,positive,positive,positive
994466136,"Hi and thanks for the question.  You might be seeing a Swish activation, x*sigmoid(x)?  

There's no way to configure the activation functions without changing the Python code, but if you search for ""Swish"" in the mlagents/trainers/torch/ folder you find where you can do that.",hi thanks question might seeing swish activation sigmoid way configure activation without python code search swish folder find,issue,negative,positive,positive,positive,positive,positive
992705119,"Thank you for the feedback, I will discuss it with the team.",thank feedback discus team,issue,negative,neutral,neutral,neutral,neutral,neutral
992562010,"This is lot of confusion. Unity could:

a) Put a warning, that this is WIP version docs;
b) Handle docs in a way, that the default one corresponds to latest release;",lot confusion unity could put warning version handle way default one latest release,issue,negative,positive,positive,positive,positive,positive
991801161,"This looks like another case where the live (main) documentation is ahead of the latest release.  It reflects the state of the latest main version rather than the last released version.

If you are on Release 18 you can switch to look at that version (or whichever you are on) so that the documentation you are looking at corresponds with the version you are using.
![image](https://user-images.githubusercontent.com/92821409/145694095-fff01f8f-bf6d-458d-9102-adb882aabec4.png)

",like another case live main documentation ahead latest release state latest main version rather last version release switch look version whichever documentation looking version image,issue,negative,positive,positive,positive,positive,positive
991798626,I agree that it is confusing in the (hopefully brief) stretch before Release 19 is out.,agree hopefully brief stretch release,issue,positive,neutral,neutral,neutral,neutral,neutral
991797567,"It isn't a typo, Release 19 just hasn't been cut yet but the documentation has been updated to reflect the state of main branch.  Release 19 will have this feature, and if you [install from a clone of the repo](https://github.com/Unity-Technologies/ml-agents/blob/main/com.unity.ml-agents/CHANGELOG.md) you should have it too.  You can see the changelog for the yet unreleased version [here](https://github.com/Unity-Technologies/ml-agents/blob/main/com.unity.ml-agents/CHANGELOG.md).",typo release cut yet documentation reflect state main branch release feature install clone see yet unreleased version,issue,negative,positive,positive,positive,positive,positive
991215018,I think this is a good suggestion and it has been added to Jira as MLA-2322.,think good suggestion added,issue,negative,positive,positive,positive,positive,positive
991206605,This bug is confirmed to happen with latest versions for Unity (2020.3.21f1) and ML-Agents (2.0.1) and is being investigated (Jira MLA-2321).,bug confirmed happen latest unity,issue,negative,positive,positive,positive,positive,positive
991195403,"Thank you for reporting this, I will reply again after discussing this with the team.

Tracking the issue in Jira as MLA-2320.",thank reply team issue,issue,negative,neutral,neutral,neutral,neutral,neutral
991190118,This is being tracked in Jira as MLA-2319.  Closing as there is a workaround in this case and the issue isn't on an LTS version.  Please open a new issue if anything similar is causing a problem that the proposed workaround does not fix.,tracked case issue version please open new issue anything similar causing problem fix,issue,negative,positive,neutral,neutral,positive,positive
988951220,"commits without changelog:
```
#### com.unity.ml-agents / com.unity.ml-agents.extensions (C#)
- Update gRPC native lib to universal for arm64 and x86_64. This change should enable ml-agents usage on mac M1 (#5283, #5519)
#### ml-agents / ml-agents-envs / gym-unity (Python)
- Harden user PII protection logic and extend TrainingAnalytics to expose detailed configuration parameters. (#5512)
```",without update native universal arm change enable usage mac python harden user protection logic extend expose detailed configuration,issue,negative,positive,positive,positive,positive,positive
988097425,"> I'm wondering if we need all three of UNITY_EDITOR , MLA_UNITY_ANALYTICS_MODULE and ENABLE_CLOUD_SERVICES_ANALYTICS in these checks and if they could be made more consistent. 

👍   was mainly just trying to get a fix in before the release. I feel like this could be pruned down and we could commit to just using [conditional attributes ](https://docs.microsoft.com/en-us/dotnet/api/system.diagnostics.conditionalattribute) (already used in this code alongside the preprocessor directives), but I wasn't sure of all the edge cases with analytics so I played it safe and just added onto the existing directives.",wondering need three could made consistent mainly trying get fix release feel like could could commit conditional already used code alongside sure edge analytics safe added onto,issue,positive,positive,positive,positive,positive,positive
987657135,"HI @X-DDDDD 
the most reliable way to test your custom algorithm is only to use one agent inside one build instance. Being in need of multiple agents/environments, you have to run multiple build instances concurrently.

The Python API is not very intuitive when it comes to handling multiple agents. The sum of DecisionSteps and TerminalSteps is not guaranteed to equal your total agent count, even though if all agents share the same behavior.

@andrewcoh 
Correct me if I'm wrong.",hi reliable way test custom algorithm use one agent inside one build instance need multiple run multiple build concurrently python intuitive come handling multiple sum equal total agent count even though share behavior correct wrong,issue,negative,negative,neutral,neutral,negative,negative
987606534,"> is moving up to a Unity 2020.3 LTS version an option?

Yes for my instance it's an option, luckily, but not sure if everyone can do the same",moving unity version option yes instance option luckily sure everyone,issue,positive,positive,positive,positive,positive,positive
987576945,"I checked 2020.3.20f1 and it didn't have this problem, which I was able to reproduce on Unity 2020.1.9f1.  I'll follow up on that but in the meantime is moving up to a Unity 2020.3 LTS version an option?  ",checked problem able reproduce unity follow moving unity version option,issue,negative,positive,positive,positive,positive,positive
987269919,"Thanks, if you can reproduce a bug like this with any of our environments or a simple example I would love to see it in action.",thanks reproduce bug like simple example would love see action,issue,positive,positive,positive,positive,positive,positive
987230827,"> LGTM. Would you add a line to the change log? thanks!

Done, thanks for the catch.",would add line change log thanks done thanks catch,issue,positive,positive,positive,positive,positive,positive
987149559,"Closing since this seems resolved, please feel free to re-open if this is not the case.",since resolved please feel free case,issue,positive,positive,positive,positive,positive,positive
986992284,"Hi @hvpeteet , thanks for the reply. I am not sure if your suggestion totally prevents race conditions from happening. By setting a flag (and then the reward in some MLA method) we still don't know if the reward will end up in the terminal step as expected, or the next decision step.

We might just have to accept these kind of race conditions and make sure they only result in insignificant noise in the reward function.

Thanks for your help.",hi thanks reply sure suggestion totally race happening setting flag reward method still know reward end terminal step next decision step might accept kind race make sure result insignificant noise reward function thanks help,issue,positive,positive,positive,positive,positive,positive
986869121,"Hi @ykeuter, thanks for the report. After talking to some teammates we would recommend making sure you add rewards in functions that run in the main MLA path rather than in a trigger where race conditions are possible. A reasonable workaround could be to set a flag and update rewards in FixedUpdate, CollectObservations, or some other MLA method. Please let me know if this resolves your issue or if I misunderstood the issue.",hi thanks report talking would recommend making sure add run main path rather trigger race possible reasonable could set flag update method please let know issue misunderstood issue,issue,positive,positive,positive,positive,positive,positive
984695294,"Hi @maryamhonari,

When trying `pip install torch==1.8.0` I receive this error:

```
(my_venv) andreialbu@Andreis-MBP ml-agents % pip install torch==1.8.0

ERROR: Could not find a version that satisfies the requirement torch==1.8.0 (from versions: none)
ERROR: No matching distribution found for torch==1.8.0
``",hi trying pip install receive error pip install error could find version requirement none error matching distribution found,issue,negative,neutral,neutral,neutral,neutral,neutral
984689675,"@andreialbu03 **The documents are a bit outdated you can install pytroch as mentioned** [**here**](https://pytorch.org/get-started/previous-versions/) **with**

```
pip install torch==1.8.0
```

@353055619 Would you mind downgrading to python 3.7 and torch\<1.9.0,>=1.8.0?",bit outdated install pip install would mind python,issue,negative,negative,negative,negative,negative,negative
984601082,"Hi,
I am unable to install mlagents Python package on my Mac to conda environment too.

**Environment**

- M1 pro apple silicon
- Mac OS X (Monterey 12.0.1)
- Python: 3.9.1
- Torch: 1.10.0

I clone main branch to my Mac. 
After I run `pip install -e ./ml-agents-envs `, error messages pop up:

`  
Preparing metadata (setup.py) ... done
WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', timeout('timed out'))': /simple/cloudpickle/
WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', timeout('timed out'))': /simple/cloudpickle/
WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', timeout('timed out'))': /simple/cloudpickle/
WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', timeout('timed out'))': /simple/cloudpickle/
WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', timeout('timed out'))': /simple/cloudpickle/
ERROR: Could not find a version that satisfies the requirement cloudpickle (from mlagents-envs) (from versions: none)
ERROR: No matching distribution found for cloudpickle
`

So, I choose to run `pip install mlagents==0.27.0` directly, but get another error messages:

`
ERROR: Could not find a version that satisfies the requirement torch<1.9.0,>=1.8.0; platform_system != ""Windows"" and python_version >= ""3.9"" (from mlagents) (from versions: 1.9.0, 1.10.0)
ERROR: No matching distribution found for torch<1.9.0,>=1.8.0; platform_system != ""Windows"" and python_version >= ""3.9""
`

It's confusing, when I check up the torch installation that tells me ""macOS is not currently supported for lts"" whose version is ""1.8.2"" satisfying what the error messages require me to do.

So, what should I do to install mlagents on Mac with apple m1 pro.

Additionally
Hi, @maryamhonari. I couldn't find the instruction about installation on Mac on the main branch. Could you tell me where I can find it?",hi unable install python package mac environment environment pro apple silicon mac o python torch clone main branch mac run pip install error pop done warning retry connection broken connect proxy warning retry connection broken connect proxy warning retry connection broken connect proxy warning retry connection broken connect proxy warning retry connection broken connect proxy error could find version requirement none error matching distribution found choose run pip install directly get another error error could find version requirement torch error matching distribution found torch check torch installation currently whose version satisfying error require install mac apple pro additionally hi could find instruction installation mac main branch could tell find,issue,negative,negative,negative,negative,negative,negative
983203068,"The changes on [this branch](https://github.com/Unity-Technologies/ml-agents/tree/dev_curriculum_with_elo) allow you to drive curriculum steps with the Elo score.


Example of environment_parameters I tested it with (added to SoccerTwos.yaml):

```
environment_parameters:
  ball_touch:
    curriculum:
      - name: Lesson0 # The '-' is important as this is a list
        completion_criteria:
          measure: Elo
          behavior: SoccerTwos
          signal_smoothing: false
          min_lesson_length: 100
          threshold: 1250.0
        value: 1.0    
      - name: Lesson1 # The '-' is important as this is a list
        value: 0.0 
```
        
",branch allow drive curriculum score example tested added curriculum name lesson important list measure behavior false threshold value name lesson important list value,issue,positive,positive,positive,positive,positive,positive
982836932,"> These changes look safe to me. Is there info on the issue that is being resolved somewhere?

The training does not converge with the current seed and number of steps which is possible given that it's only one input instance. 

The other tests are failing now because of inputSystem package going from 1.1 to 1.2. more [here](https://jira.unity3d.com/browse/MLA-2311)",look safe issue resolved somewhere training converge current seed number possible given one input instance failing package going,issue,negative,positive,positive,positive,positive,positive
982830727,"Ok, sounds like you were able to solve both of your problems. As for (2), this is a good solution as the sensor component system is intended to be extensible. ",like able solve good solution sensor component system intended extensible,issue,positive,positive,positive,positive,positive,positive
982375430,These changes look safe to me.  Is there info on the issue that is being resolved somewhere?,look safe issue resolved somewhere,issue,positive,positive,positive,positive,positive,positive
982217187,"@andrewcoh 
1. Actually, I want to sorting layer to train my agent (a brain agent and many subagents), so I want to change the model and all parameters(observations, actions, etc) before. After I found it don't support modify at runtime. I use another way to organize my agents.
2. Yeah, I want to preprocessing of the rayperception output before it is sent to python, such as the name of gameobject with special tags. I have written a new component from RayBase to get them.",actually want layer train agent brain agent many want change model found support modify use another way organize yeah want output sent python name special written new component get,issue,positive,positive,positive,positive,positive,positive
981933818,"Hi @maryamhonari,

So I cloned the `main` branch using the command `git clone  https://github.com/Unity-Technologies/ml-agents.git`. 
I then created a virtual environment using the command `python3 -m venv my_venv`, activated it and ensured that it is python 3.7.12. 

Following the ""Advanced: Local Installation for Development"" section, I then tried to install PyTorch using the command `pip3 install torch -f https://download.pytorch.org/whl/torch_stable.html` and I receive this error:
```
(my_venv) andreialbu@Andreis-MBP ml-agents % pip3 install torch -f https://download.pytorch.org/whl/torch_stable.html
Looking in links: https://download.pytorch.org/whl/torch_stable.html
ERROR: Could not find a version that satisfies the requirement torch (from versions: none)
ERROR: No matching distribution found for torch
```",hi main branch command git clone virtual environment command python python following advanced local installation development section tried install command pip install torch receive error pip install torch looking link error could find version requirement torch none error matching distribution found torch,issue,negative,positive,positive,positive,positive,positive
981899737,"Hi **@andreialbu03** 

Ml-agents should work on M1 with recent changes on the `main` branch. You can follow installation in development mode or stay tuned for the next release.",hi work recent main branch follow installation development mode stay tuned next release,issue,negative,positive,neutral,neutral,positive,positive
981867736,"Hi @andrewcoh

That'd be great if you can find out if it has been tested on M1.

I do not have torch installed as I am following the guide and it says to only install it separately for windows. Should I install it separately and if so with which command?

I have tried with python 3.7.12 in a venv virtual environment and I receive this error:
```
(mlAgentsProject) andreialbu@Andreis-MBP Unity % python -m pip install mlagents==0.27.0
Collecting mlagents==0.27.0
  Using cached mlagents-0.27.0-py3-none-any.whl (160 kB)
Collecting importlib-metadata
  Downloading importlib_metadata-4.8.2-py3-none-any.whl (17 kB)
Collecting numpy<2.0,>=1.13.3
  Downloading numpy-1.21.4.zip (10.6 MB)
     |████████████████████████████████| 10.6 MB 3.6 MB/s            
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
ERROR: Exception:
Traceback (most recent call last):
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/cli/base_command.py"", line 164, in exc_logging_wrapper
    status = run_func(*args)
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/cli/req_command.py"", line 205, in wrapper
    return func(self, options, args)
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/commands/install.py"", line 339, in run
    reqs, check_supported_wheels=not options.target_dir
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 93, in resolve
    collected.requirements, max_rounds=try_to_avoid_resolution_too_deep
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 482, in resolve
    state = resolution.resolve(requirements, max_rounds=max_rounds)
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 374, in resolve
    failure_causes = self._attempt_to_pin_criterion(name)
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 214, in _attempt_to_pin_criterion
    criteria = self._get_updated_criteria(candidate)
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 205, in _get_updated_criteria
    self._add_to_criteria(criteria, requirement, parent=candidate)
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 173, in _add_to_criteria
    if not criterion.candidates:
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__
    return bool(self._sequence)
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__
    return any(self)
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>
    return (c for c in iterator if id(c) not in self._incompatible_ids)
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built
    candidate = func()
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link
    version=version,
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 287, in __init__
    version=version,
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 156, in __init__
    self.dist = self._prepare()
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 225, in _prepare
    dist = self._prepare_distribution()
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 292, in _prepare_distribution
    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/operations/prepare.py"", line 482, in prepare_linked_requirement
    return self._prepare_linked_requirement(req, parallel_builds)
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/operations/prepare.py"", line 550, in _prepare_linked_requirement
    self.build_isolation,
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/operations/prepare.py"", line 58, in _get_prepared_distribution
    abstract_dist.prepare_distribution_metadata(finder, build_isolation)
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/distributions/sdist.py"", line 47, in prepare_distribution_metadata
    self._install_build_reqs(finder)
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/distributions/sdist.py"", line 106, in _install_build_reqs
    build_reqs = self._get_build_requires_wheel()
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_internal/distributions/sdist.py"", line 83, in _get_build_requires_wheel
    return backend.get_requires_for_build_wheel()
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_vendor/pep517/wrappers.py"", line 173, in get_requires_for_build_wheel
    'config_settings': config_settings
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_vendor/pep517/wrappers.py"", line 332, in _call_hook
    raise BackendUnavailable(data.get('traceback', ''))
pip._vendor.pep517.wrappers.BackendUnavailable: Traceback (most recent call last):
  File ""/Users/andreialbu/Dev/Unity/mlAgentsProject/lib/python3.7/site-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 89, in _build_backend
    obj = import_module(mod_path)
  File ""/Users/andreialbu/.pyenv/versions/3.7.12/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/private/var/folders/3d/_7r8hqyd6td_gq695rtyvl3w0000gn/T/pip-build-env-nfk8met1/overlay/lib/python3.7/site-packages/setuptools/__init__.py"", line 23, in <module>
    from setuptools.dist import Distribution
  File ""/private/var/folders/3d/_7r8hqyd6td_gq695rtyvl3w0000gn/T/pip-build-env-nfk8met1/overlay/lib/python3.7/site-packages/setuptools/dist.py"", line 34, in <module>
    from setuptools import windows_support
  File ""/private/var/folders/3d/_7r8hqyd6td_gq695rtyvl3w0000gn/T/pip-build-env-nfk8met1/overlay/lib/python3.7/site-packages/setuptools/windows_support.py"", line 2, in <module>
    import ctypes
  File ""/Users/andreialbu/.pyenv/versions/3.7.12/lib/python3.7/ctypes/__init__.py"", line 7, in <module>
    from _ctypes import Union, Structure, Array
ModuleNotFoundError: No module named '_ctypes'
```
",hi great find tested torch following guide install separately install separately command tried python virtual environment receive error unity python pip install zip build done getting build wheel done error exception recent call last file line status file line wrapper return self file line run file line resolve file line resolve state file line resolve name file line criterion candidate file line criterion requirement file line file line return bool file line return self file line return id file line candidate file line file line file line file line file line return file line return file line file line finder file line finder file line file line return file line file line raise recent call last file line file line return name level package level file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file line module import distribution file line module import file line module import file line module import union structure array module,issue,positive,positive,positive,positive,positive,positive
981851459,"Hi @hxx1996 

Did you set the MaxEnvironmentSteps in the agent script? The reported mean reward is the average reward per episode so if the agent episode never terminates, it's possible to obtain a lot of reward.",hi set agent script mean reward average reward per episode agent episode never possible obtain lot reward,issue,positive,negative,negative,negative,negative,negative
981839809,"Hi @andreialbu03 

Would you mind downgrading your version of python to 3.7 and try installing with the venv? We have not tested installation with pipenv.

Additionally, I am not sure of the M1 environment but I believe people internally have tested this and I will inquire. 

Also, which version of torch do you have installed?",hi would mind version python try tested installation additionally sure environment believe people internally tested inquire also version torch,issue,negative,positive,positive,positive,positive,positive
981827167,"Hi @gwc0111 

1. Which part of the Behavior Parameters script do you want to modify at runtime? These parameters all correspond to static components of a neural network so they should remain fixed and NOT modified at runtime.
2. I'm not sure I understand this question. Do you want to do some preprocessing of the rayperception output before it is sent to python?",hi part behavior script want modify correspond static neural network remain fixed sure understand question want output sent python,issue,negative,positive,positive,positive,positive,positive
981813873,Ah thanks a lot for the clarification!,ah thanks lot clarification,issue,negative,positive,positive,positive,positive,positive
981811667,"Hi @Simple2Sample 

The available types for vis_encode_type are documented here: https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md#common-trainer-configurations

The default is 'simple'. It seems that we did not clean up our documentation though, as `fully_connected` is listed as a potential option in the above documentation but is not supported in the code. I will clean up the documentation and add a catch so that specifying an unsupported visual encoder type will raise a warning.

That being said, a visual encoder will only be instantiated if your agent has a camera sensor. Unfortunately, it's not possible to modify the convolution hyperparameters from the yaml, however, if you are comfortable you can modify the encoders directly here: https://github.com/Unity-Technologies/ml-agents/blob/main/ml-agents/mlagents/trainers/torch/encoders.py#L177",hi available default clean documentation though listed potential option documentation code clean documentation add catch unsupported visual type raise warning said visual agent camera sensor unfortunately possible modify convolution however comfortable modify directly,issue,negative,positive,positive,positive,positive,positive
981795581,"@andrewcoh 
YAML
```
trainer_type:   sac
hyperparameters:    
  learning_rate:    0.0003
  learning_rate_schedule:   constant
  batch_size:   256
  buffer_size:  58000
  buffer_init_steps:    10000
  tau:  0.005
  steps_per_update: 4.0
  save_replay_buffer:   False
  init_entcoef: 0.5
  reward_signal_steps_per_update:   10.0
network_settings:   
  normalize:    False
  hidden_units: 256
  num_layers:   1
  vis_encode_type:  simple
  memory:   None
  goal_conditioning_type:   none
reward_signals: 
  extrinsic:    
    gamma:  0.99
    strength:   1.0
    network_settings:   
      normalize:    False
      hidden_units: 128
      num_layers:   2
      vis_encode_type:  simple
      memory:   None
      goal_conditioning_type:   hyper
  gail: 
    gamma:  0.99
    strength:   0.5
    network_settings:   
      normalize:    False
      hidden_units: 128
      num_layers:   2
      vis_encode_type:  simple
      memory:   None
      goal_conditioning_type:   none
    learning_rate:  0.0003
    encoding_size:  None
    use_actions:    True
    use_vail:   False
    demo_path:  ..................../Assets/Demonstrations
init_path:  None
keep_checkpoints:   5
checkpoint_interval:    200000
max_steps:  3000000
time_horizon:   64
summary_freq:   500
threaded:   True
self_play:  None
behavioral_cloning: None
```

For the time intervals, yes, it happens throughout training. To elaborate, printing of summary steps happens in batches after large time intervals. It does not seem to have any relation with the GAIL issue

![image](https://user-images.githubusercontent.com/67644448/143904460-d59674dd-24e3-4eb4-9882-31da95f9dabb.png)
![image](https://user-images.githubusercontent.com/67644448/143904557-7a0f0f21-ecae-4c03-9211-e1851b69fe6a.png)
![image](https://user-images.githubusercontent.com/67644448/143904645-619271da-e6e8-42bd-a362-16bd4b7ab1b7.png)

Have not seen any NaNs anywhere. ",sac constant tau false normalize false simple memory none none extrinsic gamma strength normalize false simple memory none hyper gamma strength normalize false simple memory none none none true false none threaded true none none time yes throughout training elaborate printing summary large time seem relation issue image image image seen anywhere,issue,negative,negative,neutral,neutral,negative,negative
981788537,"Hi @MrOCW 

Can you share your entire yaml file? It looks like the discriminator in GAIL is breaking but it's not clear why just from these curves. I suspect a NaN or something like that because GAIL appears to be working properly until that point around ~500k timesteps.

Are you seeing other large time intervals between summaries or is the screenshot the only time this occurs? Does this screenshot coincide with the degradation of GAIL?

Can you share the other TB curves e.g. policy entropy? Are you seeing any NaNs either in C# or python?",hi share entire file like discriminator breaking clear suspect nan something like working properly point around seeing large time time coincide degradation share policy entropy seeing either python,issue,positive,positive,neutral,neutral,positive,positive
980506220,"> I see. Elo is an existing measure and the learning agent plays against a frozen (non-learning) opponent?

Yes Elo goes up meaning the AI is better but the reward per episode stays the same cause the agent is playing against itself and the game is a zero sum game (like soccer). The opponent is not fully frozen, but a few generations behind or ahead. ",see measure learning agent frozen opponent yes go meaning ai better reward per episode stay cause agent game zero sum game like soccer opponent fully frozen behind ahead,issue,positive,negative,negative,negative,negative,negative
980488832,"I see.  Elo is an existing measure and the learning agent plays against a frozen (non-learning) opponent?
",see measure learning agent frozen opponent,issue,negative,neutral,neutral,neutral,neutral,neutral
980481014,ELO should go up over time as agents increase their skill level when training with self-play.,go time increase skill level training,issue,negative,neutral,neutral,neutral,neutral,neutral
980479240,"The request makes sense, a custom measure for advancing the curriculum training would be useful.  How does Elo rating change with self-play though?",request sense custom measure advancing curriculum training would useful rating change though,issue,negative,positive,positive,positive,positive,positive
980078729,I encountered the same errors with @chamx2 after importing example projects. But these errors disappeared after I upgraded the Input System package to the version 1.1.0-preview3. I guess this [link](https://github.com/Unity-Technologies/ml-agents/blob/d34f3cd6ee078782b22341e4ceb958359069ab60/com.unity.ml-agents.extensions/Documentation~/InputActuatorComponent.md) may also help.,example input system package version guess link may also help,issue,negative,neutral,neutral,neutral,neutral,neutral
967076489,"I think I found the issue. I thought vis_encode_type was convoluting all of the observations, not just the visual observations. I also don't have any visual observations in my project at this time. Does this mean it just defaults to ""simple"" because I have no visual observations?

Follow-up question: Are there any ways to add convolution layers or change the width of individual layers in the NN? I assume the NN is fully connected.",think found issue thought visual also visual project time mean simple visual question way add convolution change width individual assume fully connected,issue,negative,negative,neutral,neutral,negative,negative
963459740,This support question is likely better suited for the official ML-Agents forums found here: [https://forum.unity.com/forums/ml-agents.453/](https://forum.unity.com/forums/ml-agents.453/),support question likely better official found,issue,positive,positive,positive,positive,positive,positive
963273357,"Hi @chamx2 thanks for the question. Can you explain what you are trying to achieve with step 5, I don't think I fully understood the goal? Combining unity projects via copy-paste isn't well defined if I understand correctly. If you want to build on top of the existing project I would recommend you clone and open the existing one and open it as-is. If you are trying to combine the two different Unity projects I would recommend exporting one of them to a project and importing it into another one. https://answers.unity.com/questions/48056/how-can-we-combine-two-unity-projects.html. Please let me know if I misunderstood your question or if this doesn't help.",hi thanks question explain trying achieve step think fully understood goal combining unity via well defined understand correctly want build top project would recommend clone open one open trying combine two different unity would recommend one project another one please let know misunderstood question help,issue,positive,positive,positive,positive,positive,positive
963245266,"Just updating for our internal reference (added the request tag so this shouldn't get marked stale). 

Internal tracking JIRA: https://jira.unity3d.com/browse/MLA-2092",internal reference added request tag get marked stale internal,issue,negative,negative,neutral,neutral,negative,negative
963238494,"For completeness this seems to have been resolved in https://forum.unity.com/threads/academy-fixed-update-stepper-generating-garbage.1130209/ 

@ohernpaul is this sufficiently resolved for us to close this bug here? ",completeness resolved sufficiently resolved u close bug,issue,negative,neutral,neutral,neutral,neutral,neutral
963231712,Closing since this should be resolved as mentioned above. Please re-open if release 19 comes out and you still have this issue.,since resolved please release come still issue,issue,negative,neutral,neutral,neutral,neutral,neutral
963227403,"I realize this has been stale for over a year, but as a side effect of https://github.com/Unity-Technologies/ml-agents/pull/5553 this should not longer occur since it allows socket re-use. Once it is included in releases (release 19+) this should go away. ",realize stale year side effect longer occur since socket included release go away,issue,negative,negative,negative,negative,negative,negative
963223827,Closing since this has been stale for over a year and was just a redirect to the forums.,since stale year redirect,issue,negative,negative,negative,negative,negative,negative
961792411,"Hi, thanks for your interest in ml-agents. This issue looks like an issue with how training is set up and not a bug we can fix on GitHub. For help with a specific environment please post on our forums [https://forum.unity.com/forums/ml-agents.453/](https://forum.unity.com/forums/ml-agents.453/).",hi thanks interest issue like issue training set bug fix help specific environment please post,issue,positive,positive,neutral,neutral,positive,positive
961528012,"> Library 1.7.0 Stops the memory leak during heurstic only

Do you still have this issue? Do you still observe it in more recent versions of ML-Agents?",library memory leak still issue still observe recent,issue,negative,neutral,neutral,neutral,neutral,neutral
961513190,"Hi @armando-fandango, Do you still have this issue with UnityToGymWrapper? Can you provide more detail (possiblye a sample code) as to how you're overriding the `close()` method?",hi still issue provide detail sample code close method,issue,negative,neutral,neutral,neutral,neutral,neutral
961509878,"Yes, ML-agents supports multi-agents training for cooperative and competitive scenarios in a couple of our sample environments. You can find more about it on the [`docs`](https://github.com/Unity-Technologies/ml-agents/blob/3bd0b68bf88aa147c3dfb0b6d3336b8454799750/docs/Learning-Environment-Design-Agents.md#defining-multi-agent-scenarios) and [blog posts](https://blog.unity.com/technology/training-intelligent-adversaries-using-self-play-with-ml-agents).",yes training competitive couple sample find,issue,positive,neutral,neutral,neutral,neutral,neutral
961209971,"1.  The flag only works within unity editor or unity standalone after build. If it's used from python `mlagents-learn`, `--deterministic` should be used.
2.  2.Done, reverted the flag to deterministic inference.

Thanks",flag work within unity editor unity build used python deterministic used flag deterministic inference thanks,issue,negative,positive,positive,positive,positive,positive
961125075,Closing since this does not seem to be a bug based on Vince's comments and there has been no activity for a month.,since seem bug based activity month,issue,negative,neutral,neutral,neutral,neutral,neutral
961121149,"Hi, thanks for your interest in ml-agents. This issue looks like an issue with how training is set up (a hard problem, but not one we can fix on GitHub). For help with a specific environment please post on our forums https://forum.unity.com/forums/ml-agents.453/.",hi thanks interest issue like issue training set hard problem one fix help specific environment please post,issue,positive,negative,neutral,neutral,negative,negative
955998662,"Thanks a lot, I've running the model in binary file. And i got another question. Am I be able to create environment that is able to execute Multi-agent reinforcement learning algorithm??",thanks lot running model binary file got another question able create environment able execute reinforcement learning algorithm,issue,positive,positive,positive,positive,positive,positive
954780651,"While we have not added this feature, we do have a change on main allowing configurable restarts upon environment crashes. https://github.com/Unity-Technologies/ml-agents/pull/5553. Not sure if this fulfills your need but it sounds like it may.",added feature change main upon environment sure need like may,issue,positive,positive,positive,positive,positive,positive
954184971,"I have run into this error previously. I resolved this issue by using a server build rather than a development build. A caveat here is that this would only work for non-vision based environments (i.e., agents do not learn based on observations provided by an in-game camera).",run error previously resolved issue server build rather development build caveat would work based learn based provided camera,issue,negative,negative,negative,negative,negative,negative
953830717,"@luckyShaw Thanks for bringing up this request! We definitely want users to be able to use custom trainers/external codebases with ml-agents. We are working towards a generalizable solution to export ml-agents compatible onnx models which entails a tutorial as well. 

The solution I offered above is a pointer to our code to unblock you at the moment. Alternatively, you can try running the `net.pt` model in python with `--inference` cli command, this way you would not need to export a compatible onnx model.

Hope that helps.",thanks request definitely want able use custom working towards generalizable solution export compatible tutorial well solution pointer code unblock moment alternatively try running model python inference command way would need export compatible model hope,issue,positive,positive,positive,positive,positive,positive
953726738,"@andrewcoh thanks for the alternatives. I think a script that updates an exported ONNX to match a unity-expected ONNX would be great. The second alternative is probably the easiest where I forget a possible import back to unity and run inference on the env with visuals on.

🟩 I have been integrating mlagents gymwrapped environments into a fully functional wandb workflow with start, resume, reinitialize capabilities, which was a lot of things to take into account. 

🟥 Right now I am dealing with an mlagents error mentioned in https://github.com/wandb/client/issues/2750, which is making me lose my mind, but that is a topic outside of this thread. But this is my biggest problem right now when trying to use an MLAgents Environment outside of MLAgents.
->  I think it is critical, based on the paper you guys made which is a huge point of my thesis, to use unity and MLAgents to create new environments/benchmarks and then to have a functional workflow when training with something like SB3 and wandb. I have done * all * this functionality and will pass it to you guys as soon as I finish the thesis but at least this current and hopefully last error is the one that I dread every morning when I wake up after leaving it train overnight in my server. I am in parallel training on my windows but that is a lot slower in compared to a server... If you have any hints on why this error continues to show up please let me know 🙏🏻 

🟥 But then for now my windows-trained models will be divorced from my linux-trained models (mlagents and SB3, respectively)... 

Update: I guess I am going to give up on trying to make this whole `unity_gym + SB3 + wandb` thing work, and I will continue my training with mlagents on the linux server. The limitations of mlagents I guess do not apply that hard to my scope and I can work around them in the document instead of trying to implement this. Figure below is my tensorboard progress from the past 30min.
![image](https://user-images.githubusercontent.com/4501464/139286290-3b4bdaaf-88fe-469d-b18d-80eb2d982c2c.png)

I guess if this will be the end of our exchange, would you have some tips on how to train faster? I am already using a timescale of 50 trying to not break the physics engine calculations, and 5 envs (do envs speed up training?, or is it better to go for a multi-agent environment with copy/paste of agents?)
",thanks think script match would great second alternative probably easiest forget possible import back unity run inference fully functional start resume lot take account right dealing error making lose mind topic outside thread biggest problem right trying use environment outside think critical based paper made huge point thesis use unity create new functional training something like done functionality pas soon finish thesis least current hopefully last error one dread every morning wake leaving train overnight server parallel training lot server error show please let know respectively update guess going give trying make whole thing work continue training server guess apply hard scope work around document instead trying implement figure progress past min image guess end exchange would train faster already trying break physic engine speed training better go environment,issue,positive,positive,neutral,neutral,positive,positive
953219105,"Hi @Abror1997 

Can you open a new issue with more information about your issue so that I can give you better information?

Or @X-DDDDD if you are also having this problem, feel free to open an issue as well.",hi open new issue information issue give better information also problem feel free open issue well,issue,positive,positive,positive,positive,positive,positive
953031355,Ran some experiments with inference and they finished (unlike with 2.3.0). ,ran inference finished unlike,issue,negative,neutral,neutral,neutral,neutral,neutral
952953997,"Hi again @miguelalonsojr, I solved the communication problems changing the build settings in _Player Settings>Player>Other Settings>Configuration>Scripting Backend_ from **IL2CPP** to **Mono**, so I'm able to start a training using the API. 
Now I'm struggling with 1d conv, I didn't understand what have you said about that, can you please repeat it? Do you think it makes sense using 1d convs to manage the RayPerceptionSensor outputs signal? Or it would simply be enough to use a bigger dense net? I'm training a vehicle to park and I'm testing the generalization limit of RL, but I haven't get any good result so far with dense nets, so I thought that 1d conv could have been suitable.",hi communication build player configuration mono able start training struggling understand said please repeat think sense manage signal would simply enough use bigger dense net training vehicle park testing generalization limit get good result far dense thought could suitable,issue,negative,positive,positive,positive,positive,positive
952496601,"I still have no idea how to fix the code so that the transfered onnx model can be used in unity.
I mean, though [https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/release_18_docs/colab/Colab_UnityEnvironment_2_Train.ipynb](url) shows how to train with custom algorithm, the final goal should be applying trained model in unity environment. Maybe a turorial about that part is quite necessary.",still idea fix code model used unity mean though train custom algorithm final goal trained model unity environment maybe part quite necessary,issue,negative,negative,negative,negative,negative,negative
952013234,"The inference is originally set to work with models trained and exported from ml-agents. The error you're seeing is because the input layer is named 'x' and the inference engine can not connect sensors to it. In the mean time, this is our [serialization code](https://github.com/Unity-Technologies/ml-agents/blob/9160d858c36e95edfdb67bbd56fc99735f6c9a1b/ml-agents/mlagents/trainers/torch/model_serialization.py#L158) and it should give you more insight into how the expected tensor from [forward method](https://github.com/Unity-Technologies/ml-agents/blob/9160d858c36e95edfdb67bbd56fc99735f6c9a1b/ml-agents/mlagents/trainers/torch/networks.py#L658-L659) should be named.",inference originally set work trained error seeing input layer inference engine connect mean time serialization code give insight tensor forward method,issue,negative,positive,neutral,neutral,positive,positive
951994581,Thanks for bringing this to our attention. That's a fair enhancement and I logged it in our internal tracker.,thanks attention fair enhancement logged internal tracker,issue,positive,positive,positive,positive,positive,positive
951465506,"Alternatively, you can try running your model in inference with python just as ML-Agents does with the `--inference` CLI command. Then, your onnx model would not need to be compatible with the C# model requirements of ML-Agents.

Is this feasible for you or do you need the model to be part of the game build?",alternatively try running model inference python inference command model would need compatible model feasible need model part game build,issue,negative,negative,negative,negative,negative,negative
951421297,"Hi @Ademord 

Thank you very much for raising this as it is a really important point for us to settle for the future of ML-Agents to be used with external codebases/trainers.  [This is our model serialization code.](https://github.com/Unity-Technologies/ml-agents/blob/main/ml-agents/mlagents/trainers/torch/model_serialization.py) It should give you some insight into the expected tensors and possibly can be used directly with some modification in your use case.

My plan is to follow up with the team on how best to handle this moving forward because we would like to provide for this use case.  An option may be to provide a script that takes one onnx model and exports another with the proper structure for ML-A, though this may be hard to do in general. Additionally, we also have the constraints of which operators Barracuda supports.  ",hi thank much raising really important point u settle future used external model serialization code give insight possibly used directly modification use case plan follow team best handle moving forward would like provide use case option may provide script one model another proper structure though may hard general additionally also barracuda,issue,positive,positive,positive,positive,positive,positive
950647063,"@mmattar anyone who could support on this in the meantime so my block is relieved? I can help you guys with the documentation later when I have it working, but this is essential for my pipeline atm.",anyone could support block relieved help documentation later working essential pipeline,issue,positive,neutral,neutral,neutral,neutral,neutral
950254753,"Confirmed that this now works correctly with Unity Arm64 on the latest main branch, so closing this issue.",confirmed work correctly unity arm latest main branch issue,issue,negative,positive,positive,positive,positive,positive
950244844,"> Hey guys, can you provide some advice on this, I cannot use this API with multiple scenes either?

A lot of people have this problem now.🤣",hey provide advice use multiple either lot people problem,issue,negative,neutral,neutral,neutral,neutral,neutral
950011112,"I was able to run this with Torch 1.10 just by updating the version requirement in the `setup.py` file, and it seems to be working fine.",able run torch version requirement file working fine,issue,negative,positive,positive,positive,positive,positive
949999868,Supporting M1 mac is definitely on our roadmap! I've logged the upgrade to PyTorch in our internal tracker as MLA-2273 and will be prioritized accordingly.,supporting mac definitely logged upgrade internal tracker accordingly,issue,positive,positive,neutral,neutral,positive,positive
949927273,"Hey @Andrealberti , 1d Convs are supported in Barracuda, however, they would have to use the Conv2D operator path, which might make them a bit slow.

Re: the gym wrapper and python API, it seems like there might be an issue the environment itself. If communication to and from Unity hangs, then the python API will timeout. Are you seeing this when training with the editor open or with a build of the environment?",hey barracuda however would use operator path might make bit slow gym wrapper python like might issue environment communication unity python seeing training editor open build environment,issue,negative,negative,negative,negative,negative,negative
949675218,"Glad to hear - closing this issue for now, but feel free to recreate if you have other clarifying questions.",glad hear issue feel free recreate,issue,positive,positive,positive,positive,positive,positive
949363305,"Very useful, thank you very much for the clarification",useful thank much clarification,issue,positive,positive,positive,positive,positive,positive
949171451,"Seconded, especially as the installation instructions for local development have you install the latest stable PyTorch, which is 1.10. Also, I am on an M1 mac and would like to run native ARM code, and a native ARM build of PyTorch is only available for 1.10 or later.",especially installation local development install latest stable also mac would like run native arm code native arm build available later,issue,positive,positive,positive,positive,positive,positive
948764953,Running some CI jobs manually. Will merge before official CI kicks off today,running manually merge official today,issue,negative,neutral,neutral,neutral,neutral,neutral
948697623,"Hi @batu I saw the post you reference and the documentation that was updated based on your `Onnxable` code. Also this code your reference now, is for importing a model for doing inference, but what here is supposed to happen is that MLAgents accepts an externally (SB3, yes stable baselines 3) trained ONNX, as referenced in first post of this thread. It seems like they have hardcoded words into the heads of the networks, so if someone could tell how to export an ONNX properly from torch to be used with ***MLAgents*** and not in general *in Unity*, hence why the Barracuda team moved this issue here and did not keep it there.",hi saw post reference documentation based code also code reference model inference supposed happen externally yes stable trained first post thread like someone could tell export properly torch used general unity hence barracuda team issue keep,issue,positive,positive,neutral,neutral,positive,positive
948009312,Closing this issue due to lack of activity - but please feel free to re-open or post a new one if you're still running into issues. Thanks!,issue due lack activity please feel free post new one still running thanks,issue,positive,positive,positive,positive,positive,positive
947967424,"@Ademord, I am assuming SB3 refers to stable-baselines3. I currently can run my sb3 trained networks in Unity, I might be able to help you out. Here is a relevant conversation on the sb3 repo that might come in handy: https://github.com/DLR-RM/stable-baselines3/issues/383

Here is [my code](https://gist.github.com/batu/d5bc9d90d2cff346e158e06dd1a23665) that I use for inference. It is... janky but works.",assuming currently run trained unity might able help relevant conversation might come handy code use inference work,issue,negative,positive,positive,positive,positive,positive
947905124,"Hey guys, can you provide some advice on this, I cannot use this API with multiple scenes either?",hey provide advice use multiple either,issue,negative,neutral,neutral,neutral,neutral,neutral
947072301,"@TV4Fun In order to get you going before we have a solution for this, please just checkout the repo with the current HEAD only as follows:

export SHA=""commit-hash""
git init
git remote add origin https://github.com/Unity-Technologies/ml-agents.git
git fetch --depth 1 origin $SHA
git checkout FETCH_HEAD


with the commit hash that you want.",order get going solution please current head export git git remote add origin git fetch depth origin sha git commit hash want,issue,positive,negative,neutral,neutral,negative,negative
946139426,"@TV4Fun Thanks for bringing this to our attention. Changing the label to request. The largest folder in the repo is the Unity project folder, which is about 1.3GB on disk. We'll look into a potential solution for avoiding that mega download.",thanks attention label request folder unity project folder disk look potential solution,issue,positive,positive,neutral,neutral,positive,positive
945386826,"> Hi @X-DDDDD
> 
> Thank you for using our Python API. The TerminalSteps contains _only_ information about agents that have reached a termination condition (i.e. EndEpisode() called in C#). The information for the parallel agents that have not reached a termination condition will be contained in the DecisionSteps object.
> 
> Are you seeing behavior that is different from this? Please share as much detail as you can.

I'm sorry I didn't reply to you until now.Thank you for your advice! 

I will communicate with you in time when I make some progress or confusion.",hi thank python information termination condition information parallel termination condition object seeing behavior different please share much detail sorry reply advice communicate time make progress confusion,issue,positive,negative,neutral,neutral,negative,negative
945093961,"@mmattar : Thanks for your reply. Unfortunately, I have been blocked by this only issue for a couple weeks now.

I would highly appreciate it if you could tell me how to adapt this torch-exported ONNX back compatible to Unity. I see there are hardcoded names etc on the network you export, as described above, but I don't know how to add them myself. ",thanks reply unfortunately blocked issue couple would highly appreciate could tell adapt back compatible unity see network export know add,issue,negative,negative,neutral,neutral,negative,negative
944885887,"Hi @mmattar, 
Thanks for your reply, now it is all super clear, I just wanted to have a confirm of what I have seen both in forum and here in the issues. I know the problems of barracuda, but if I am not mistaken, the conv1d layers, which is the one I need, is supported by the library, isn't it? 
Then, talking about ml-agents-env or gym-unity wrappers, yes, at the moment I was exploring these options, but I have found the implementation very hard,  I have communication problems between my UnityEnv and the PythonAPI, here is the error: 

> mlagents_envs.exception.unitytimeoutexception: the unity environment took too long to respond. make sure that : 
> the environment does not need user interaction to launch the agents' 
> behavior parameters > behavior type is set to ""default"" 
> the environment and the python interface have compatible versions.

Anyway, now the challenge is trying add a class 1Dencoder here: https://github.com/Unity-Technologies/ml-agents/blob/003e8aed3be28c56ea6b9b7d784cc142d41e7c19/ml-agents/mlagents/trainers/torch/encoders.py
adding in it Conv1d layers, the aim is to process the output vector of the lidar as an audio signal, in case of good results I will let you know.
Feel free to give me your feedback on my last idea, I will appreciate it.
Thanks a lot.
Regards,
Andrea

",hi thanks reply super clear confirm seen forum know barracuda mistaken one need library talking yes moment exploring found implementation hard communication error unity environment took long respond make sure environment need user interaction launch behavior behavior type set default environment python interface compatible anyway challenge trying add class aim process output vector audio signal case good let know feel free give feedback last idea appreciate thanks lot,issue,positive,positive,positive,positive,positive,positive
944627826,"Hey @escribano89 - to clarify, when an agent dies you will see a Terminal step, but NOT necessarily two Decision Steps. For instance, if an agent dies between the decision steps, then you would only see a Terminal step and no Decision steps because the other two agents have not, yet, requested a decision.

Regarding your question on actions - the number of actions sent at each step has to be equal to the number of decision steps. In the case of the agent dying and no other agents requesting a decision, no action should be sent (i.e. no call to set_actions). 

Hope this helps.",hey clarify agent see terminal step necessarily two decision instance agent decision would see terminal step decision two yet decision regarding question number sent step equal number decision case agent dying decision action sent call hope,issue,positive,positive,neutral,neutral,positive,positive
944621743,"Hi @Andrealberti - unfortunately, we don't provide a mechanism to easily customize our trainers, but it is something we're currently exploring. Part of the challenge here is that we wouldn't be able to guarantee that the model you've trained will be supported by Barracuda (the inference library we use). One option which I think you're already exploring based on another GitHub Issue is to implement your own trainer directly using our ml-agents-env or gym-unity wrappers. Does this make sense?",hi unfortunately provide mechanism easily something currently exploring part challenge would able guarantee model trained barracuda inference library use one option think already exploring based another issue implement trainer directly make sense,issue,negative,positive,positive,positive,positive,positive
944618622,"Thanks for sharing this @Ademord! I've logged your request internally (MLA-2249) so we can prioritize this accordingly. To confirm, are you currently blocked or has your specific issue been resolved?",thanks logged request internally accordingly confirm currently blocked specific issue resolved,issue,negative,positive,neutral,neutral,positive,positive
943687676,"Hey @infinityplusb - we just released a public version of ML-Agents and DOTS. Highly experimental, but check it out: https://github.com/Unity-Technologies/ml-agents-dots",hey public version highly experimental check,issue,negative,positive,neutral,neutral,positive,positive
942341672,"Hi @mmattar,
I have been posting here because I didn't receive any answer to my post in the forum until yesterday morning. This is my request, I am working on MLAgents trying to train a policy in a parking car environment, which I have built. I am using a raycast sensor as a lidar and, because I haven't achieved any results using the ppo implemented policy, I basically want to build a custom trainer with a custom nn (adding a conv1d layer), is it possible to customize a model of neural nets? If the answer is yes, how can I do it? I tryed to work with the Python API following this topic: https://forum.unity.com/threads/q-learning-implementation.915008/ , but without success, in fact, the environment doesn't seem to comunicate with my python script.
Regards,
Andrea",hi posting receive answer post forum yesterday morning request working trying train policy parking car environment built sensor policy basically want build custom trainer custom layer possible model neural answer yes work python following topic without success fact environment seem python script,issue,negative,positive,neutral,neutral,positive,positive
942303647,@ShirelJosef thanks for bringing this to our attention! I'll look into this today.,thanks attention look today,issue,negative,positive,positive,positive,positive,positive
941560019,"Hi @Andrealberti - I'm not sure I understand your question. Can you clarify what you're looking for? Also, if this isn't a Bug or Feature Request - you're much better off posting your expanded question on the Forum (https://forum.unity.com/forums/ml-agents.453/).",hi sure understand question clarify looking also bug feature request much better posting expanded question forum,issue,positive,positive,positive,positive,positive,positive
941555600,"Hey @siddheshgunjal - can you provide the requested information regarding your set-up?

**Environment (please complete the following information):**
- Unity Version: [e.g. Unity 2020.1f1]
- OS + version: [e.g. Windows 10]
- _ML-Agents version_: (e.g. ML-Agents v0.8, or latest `develop` branch from source)
- _Torch version_: (you can run `pip3 show torch` to get this)
- _Environment_: (which example environment you used to reproduce the error)
",hey provide information regarding environment please complete following information unity version unity o version latest develop branch source run pip show torch get example environment used reproduce error,issue,negative,positive,positive,positive,positive,positive
941234814,@Freshchris01 - thanks for letting us know. You are correct that this should be removed. I'll create a PR for it.,thanks u know correct removed create,issue,positive,positive,positive,positive,positive,positive
935614233,"Thank you very much. I didn't find that in the docs section, so it is very useful. I will test it.",thank much find section useful test,issue,positive,positive,positive,positive,positive,positive
934958014,"Thanks for the reply, and for the pointers! Since this problem is unrelated to ML-Agents, I shall close this issue.",thanks reply since problem unrelated shall close issue,issue,negative,positive,positive,positive,positive,positive
934938452,"Given that your log shows it as hanging in grpc's server method, it seems unlikely that this would be ML-Agents related. Looking though grpc's __init__ method called there, it's probably trying to call the C grpc library: https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_server.py#L955

I can suggest from personal experience trying to take a look at your networking -- specifically a look at your hostname and DNS and ensuring that you're able to resolve it/it exists in your /etc/hosts. That's not guaranteed to resolve your issue but you can at least rule it out. Ensure your IPv6 address is in /etc/hosts too.

edit:

A colleague pointed me at this issue which bares some similarities -- you might find the debugging advice in it more helpful: https://github.com/grpc/grpc/issues/24018",given log hanging server method unlikely would related looking though method probably trying call library suggest personal experience trying take look specifically look able resolve resolve issue least rule ensure address edit colleague pointed issue might find advice helpful,issue,positive,negative,neutral,neutral,negative,negative
934869351,"Hi @escribano89 

The DecisionStep also has a `group_reward` field which is separate from the `reward` field. The group rewards given to the Cooperative Pushblock agents should be here.

We apologize that the collab doesn't point this out explicitly and I will make an update to it. ",hi also field separate reward field group given apologize point explicitly make update,issue,positive,neutral,neutral,neutral,neutral,neutral
934860036,"Hi @JesseTG 

This is a problem that has long been on our radar and may be the focus of future projects. We appreciate your suggested solutions as they are aligned with some ideas that we have discussed internally. Thank you for raising this.",hi problem long radar may focus future appreciate internally thank raising,issue,negative,negative,neutral,neutral,negative,negative
934856682,"Hi @JesseTG 

Thank you for your suggestion. I will bubble this up to the team. ",hi thank suggestion bubble team,issue,negative,neutral,neutral,neutral,neutral,neutral
934842595,"Hi @itss-K 

Please provide more information so that I may better understand what is going on. Is this an example environment or your own custom environment? Are you using our trainers or your own custom trainer? Can you share the entire error trace?",hi please provide information may better understand going example environment custom environment custom trainer share entire error trace,issue,positive,positive,positive,positive,positive,positive
934838770,"Hi @X-DDDDD 

Thank you for using our Python API. The TerminalSteps contains _only_ information about agents that have reached a termination condition (i.e. EndEpisode() called in C#).  The information for the parallel agents that have not reached a termination condition will be contained in the DecisionSteps object.

Are you seeing behavior that is different from this? Please share as much detail as you can.

",hi thank python information termination condition information parallel termination condition object seeing behavior different please share much detail,issue,positive,positive,neutral,neutral,positive,positive
929539384,"After more experimentation, I found that a decrease learning rate not solving this issue. It just slows down the event waiting to happen. After some research, the possible cause might be a gradient explosion? 
Here are some more virtual evidence for this problem.

https://www.youtube.com/watch?v=BHb_-0QGQUU
The problem occurs around 7:01:27
The turtle becomes brainless in a second.",experimentation found decrease learning rate issue slows event waiting happen research possible cause might gradient explosion virtual evidence problem problem around turtle becomes brainless second,issue,negative,neutral,neutral,neutral,neutral,neutral
929512302,"The ResNets are created with [create_input_processors](https://github.com/Unity-Technologies/ml-agents/blob/bbdcab1fda1754de4c7dcd5fefc9f5f9efaca506/ml-agents/mlagents/trainers/torch/utils.py#L186) and is called by initializing a [ObservationEncoder](https://github.com/Unity-Technologies/ml-agents/blob/ad7ec2fc284692d29bde783df335a12d2ed19d2a/ml-agents/mlagents/trainers/torch/networks.py#L34). The ObservationEncoder is part of the [NetworkBody](https://github.com/Unity-Technologies/ml-agents/blob/ad7ec2fc284692d29bde783df335a12d2ed19d2a/ml-agents/mlagents/trainers/torch/networks.py#L190) and is the core network of both Actors and Critic in all of our algorithms. For example, in the [SimpleActor](https://github.com/Unity-Technologies/ml-agents/blob/main/ml-agents/mlagents/trainers/policy/torch_policy.py#L57) of the main Policy, the SimpleActor uses the NetworkBody [here](https://github.com/Unity-Technologies/ml-agents/blob/c8d3768e10f724afa6754bf94337db1796a819b1/ml-agents/mlagents/trainers/torch/networks.py#L606).

I hope this helps.",part core network critic example main policy hope,issue,negative,positive,positive,positive,positive,positive
929413691,"Hi @hxx1996,
Can you verify if this does happen in one of our example environments?",hi verify happen one example,issue,negative,neutral,neutral,neutral,neutral,neutral
929391622,"Hi @Kyokanyou,

These type of questions are better suited to to the [ML-Agents Forum](https://forum.unity.com/forums/ml-agents.453/).

`EnvironmentReset()` is invoked to reset an [Environment](https://github.com/Unity-Technologies/ml-agents/blob/d34f3cd6ee078782b22341e4ceb958359069ab60/docs/Learning-Environment-Design.md#organizing-the-unity-scene) which may contain multiple areas or agents. `Agent.OnEpisodeBegin()` is a virtual method that you implement to set up each agent at the beginning of a learning episode, ie. position, velocity, etc. See [Agents doc](https://github.com/Unity-Technologies/ml-agents/blob/d34f3cd6ee078782b22341e4ceb958359069ab60/docs/Learning-Environment-Design-Agents.md#decisions).",hi type better forum reset environment may contain multiple virtual method implement set agent beginning learning episode ie position velocity see doc,issue,negative,positive,positive,positive,positive,positive
928610782,"Sorry, can you tell me where did the resnetblock got initiated and data being passed?
I can't follow what to do if I want to make a new resnet block with a new tapped ""other_data"".",sorry tell got data ca follow want make new block new,issue,negative,negative,neutral,neutral,negative,negative
927816117,It seem like decrease learning rate will help with this problem. I will close issue for now. Thank you.,seem like decrease learning rate help problem close issue thank,issue,positive,neutral,neutral,neutral,neutral,neutral
927555353,"Thanks, modifying resnetblock gives me what I want. 
But the resizing method that is used by camera broke the segmentation map, is there a way to make it not interpolate the image?",thanks want method used camera broke segmentation map way make interpolate image,issue,negative,positive,positive,positive,positive,positive
927393896,"@vincentpierre I'm working on a distributed training and started to explore more into the ML agents python API; plan on this is to run AWS ECS cluster, and I wonder if you would be able to give us some directions on the best approach to handle this one? I appreciate the support. I am looking forward to getting some feedback.",working distributed training explore python plan run cluster wonder would able give u best approach handle one appreciate support looking forward getting feedback,issue,positive,positive,positive,positive,positive,positive
926873932,"You would only need to modify the encoder you want to use, no need to modify all of them. 
If you looks at the forward method of the encoder, it has a ""input_tensor"" argument that is the image. If you need some other data, you will need to modify this api or do the sampling of the ""other_data"" in the encoder.",would need modify want use need modify forward method argument image need data need modify sampling,issue,negative,neutral,neutral,neutral,neutral,neutral
926266438,"Thanks for the reply @vincentpierre 

> https://github.com/Unity-Technologies/ml-agents/blob/main/ml-agents/mlagents/trainers/torch/encoders.py#L248 

This is created as a block multiple times at ResNetVisualEncoder. I assume that means I have to add pretrained GAN model at forward method for every xxVisualEncoder (ResNetVisualEncoder, NatureVisualEncoder, SimpleVisualEncoder, etc)? Since I only want GAN to run one time on the input. Is this right?

I thought that I just have to get the image data before it is forwarded to the model, but I can't find where it is
Something like

```
image_data, other_data = divide_observed_state(observed_state)
image_data = perform_gan_and_augmentation(image_data)
observed_state = combine_state(image_data, other_data)
model.forward(observed_state)
```

",thanks reply block multiple time assume add gan model forward method every since want gan run one time input right thought get image data model ca find something like,issue,negative,positive,positive,positive,positive,positive
925142552,"Hi @darwinharianto 
Very cool stuff. This is the visual encoder we use : https://github.com/Unity-Technologies/ml-agents/blob/main/ml-agents/mlagents/trainers/torch/encoders.py#L248 I think the simplest would maybe be to insert a pretrained GAN model here. Regarding data augmentation, maybe duplicating and processing trajectories should be done in the [agent_processor](https://github.com/Unity-Technologies/ml-agents/blob/main/ml-agents/mlagents/trainers/agent_processor.py) script. I am not sure where it would happen in the C# code, maybe by using the RenderTexture sensor and processing the image between updates. ",hi cool stuff visual use think would maybe insert gan model regarding data augmentation maybe done script sure would happen code maybe sensor image,issue,positive,positive,positive,positive,positive,positive
925115179,"Hi @milant,

Thank you for catching this. Can you please sign the CLA so that I can merge this?",hi thank catching please sign merge,issue,positive,positive,positive,positive,positive,positive
924684022,"Reading your comment about augmentation on images made me do some digging on that particular part.
It seem some people tried to use augmentation on images in gym or atari environment.
Augmentation seems to give better result.

https://arxiv.org/abs/2004.14990
https://openreview.net/pdf?id=GY6-6sTvGaf
  ",reading comment augmentation made digging particular part seem people tried use augmentation gym environment augmentation give better result,issue,negative,positive,positive,positive,positive,positive
924517577,"Hi @vincentpierre 
Thanks for the reply
ah, yes cropping or rotating isn't really my target, but it is close to it.
I would like to perform GAN on the image. something like this
https://www.youtube.com/watch?v=P1IcaBn3ej0

I would like to take the segmentation map, then inputting them into GAN model, which I hope will give me realistic image.


>Segmentation and depth perception are currently logged as task MLA-32,

for the time being I will try to use https://bitbucket.org/Unity-Technologies/ml-imagesynthesis/src/master/
I could use the repo above, which gives me segmentation map and else
but I don't know where to parse the image and preprocess them before sending them to ml agents
",hi thanks reply ah yes rotating really target close would like perform gan image something like would like take segmentation map gan model hope give realistic image segmentation depth perception currently logged task time try use could use segmentation map else know parse image sending,issue,positive,positive,positive,positive,positive,positive
924201434,"Hi @darwinharianto 
I do not see how cropping or rotating the images would be useful. You can always just move the camera around if needed but cropping and rotating is a data augmentation technique that I have not seen being used in RL. If think this falls outside of the scope of ml-agents.
Segmentation and depth perception are currently logged as task MLA-32, we do not know when we will have it available but it is possible today to use a custom shader for rendering the cameras. So if you already have a shader you want to use that does segmentation, you should be able to use that.",hi see rotating would useful always move camera around rotating data augmentation technique seen used think outside scope segmentation depth perception currently logged task know available possible today use custom shader rendering already shader want use segmentation able use,issue,negative,positive,positive,positive,positive,positive
924196877,"Please use the template for submitting issues in the future.
The simplest way to use custom algorithms would be to use our [low-level api](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Python-API.md) for now. There is also a [notebook](https://github.com/Unity-Technologies/ml-agents/tree/main/docs#python-tutorial-with-google-colab) on how to implement Q-Learning with this API in the documentation, I would recommend starting with those.  ",please use template future way use custom would use also notebook implement documentation would recommend starting,issue,positive,neutral,neutral,neutral,neutral,neutral
924195000,"This is the expected behavior if there are multiple vector observations. The gym wrapper will not concatenate the vector observations into a single array, if there are multiple observations (even if they are all vector) you must specify `allow_multiple_obs=True`",behavior multiple vector gym wrapper concatenate vector single array multiple even vector must specify,issue,negative,negative,neutral,neutral,negative,negative
924193749,"Hi @MarcoMeter 
I was unable to reproduce this issue on `main` I tried to change the Decision Period on the decision requester of one of the 3DBall in the Editor but it worked as expected. Can you give some way to reproduce the freezing? How are you changing the Decision Period?",hi unable reproduce issue main tried change decision period decision requester one editor worked give way reproduce freezing decision period,issue,negative,negative,negative,negative,negative,negative
921446174,"Thanks for the reply, I’m still reviewing the code outside this function and trying to run a debugging test to see when this happens

Sent from my iPhone

> On Sep 16, 2021, at 1:47 PM, ykeuter ***@***.***> wrote:
> 
> ﻿
> Hi @Jmumford45 ! Thanks for your message. I guess you mean adding logic to the ResetPlayer function?
> 
> The point is one would expect the buffer to be cleared by OnEnvironmentReset in any case (without need to check). Your suggestion might work as a workaround though (right now I am adding some ""empty steps"" before a reset as a workaround).
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub, or unsubscribe.
> Triage notifications on the go with GitHub Mobile for iOS or Android. 
",thanks reply still code outside function trying run test see sent wrote hi thanks message guess mean logic function point one would expect buffer case without need check suggestion might work though right empty reset reply directly view triage go mobile android,issue,positive,positive,neutral,neutral,positive,positive
921233516,"Hi @Jmumford45 ! Thanks for your message. I guess you mean adding logic to the ResetPlayer function?

The point is one would expect the buffer to be cleared by OnEnvironmentReset in any case (without need to check). Your suggestion might work as a workaround though (right now I am adding some ""empty steps"" before a reset as a workaround).",hi thanks message guess mean logic function point one would expect buffer case without need check suggestion might work though right empty reset,issue,negative,positive,neutral,neutral,positive,positive
920470559,Why not just add a check within this logic to confirm the buffer is null when assigning Vector3.zero in the Awake function?,add check within logic confirm buffer null awake function,issue,negative,neutral,neutral,neutral,neutral,neutral
920182672,"For anyone facing the same problem, I've found temporary work around, just: run train command multiple times, with help of shell script or just copy paste something like this in shell
```
mlagents-learn config\ppo\TRC.yaml --run-id=TRC --force --time-scale 10
mlagents-learn config\ppo\TRC.yaml --run-id=TRC --resume  --time-scale 10
mlagents-learn config\ppo\TRC.yaml --run-id=TRC --resume  --time-scale 10
mlagents-learn config\ppo\TRC.yaml --run-id=TRC --resume  --time-scale 10
mlagents-learn config\ppo\TRC.yaml --run-id=TRC --resume  --time-scale 10
mlagents-learn config\ppo\TRC.yaml --run-id=TRC --resume  --time-scale 10
mlagents-learn config\ppo\TRC.yaml --run-id=TRC --resume  --time-scale 10
 
```

Depending on how soon Full ram is consumed, it will last a little longer, maybe in some cases long enough to finish max steps.",anyone facing problem found temporary work around run train command multiple time help shell script copy paste something like shell force resume resume resume resume resume resume depending soon full ram last little longer maybe long enough finish,issue,negative,positive,neutral,neutral,positive,positive
918840966,Percent wise? Almost above 95 on Both my PC and Colab.. that would be about 14 and 10.5 approx. And it was my custom environment. I can try to reproduce again with demo environment if u want.,percent wise almost would custom environment try reproduce environment want,issue,positive,positive,positive,positive,positive,positive
918579589,Thanks for filing these two issues @ykeuter. We'll investigate and follow up with a fix or additional questions.,thanks filing two investigate follow fix additional,issue,negative,positive,positive,positive,positive,positive
918425123,Thanks for your request @gft-ai. I've logged this request internally (MLA-1782) and will follow up here when we are able to prioritize it.,thanks request logged request internally follow able,issue,negative,positive,positive,positive,positive,positive
918419315,"Thanks for sharing this @Nirav-Madhani. Do you recall how much memory was being utilized? Additionally, I presume you were experiencing this with one of our demo environments or was it your own environment?",thanks recall much memory additionally presume one environment,issue,negative,positive,positive,positive,positive,positive
918417025,"Thanks for your request @i3games. While we do not have such a complete tutorial - you can get started by using any of our basic, single-agent environments (such as 3DBall) with the gym interface. So you can just build any of these scenes and interat with them using the Gym wrapper as described [here](https://github.com/Unity-Technologies/ml-agents/blob/main/gym-unity/README.md). Additionally, for more information on the gym interface, check out this [page](http://gym.openai.com/docs/).

Additionally, I made a note internally (MLA-2215) for us to create a single documentation page. In the meantime, do let us know if you run into any issues getting started with the information I provided. You are also welcome to submit a PR with such documentation.",thanks request complete tutorial get basic gym interface build gym wrapper additionally information gym interface check page additionally made note internally u create single documentation page let u know run getting information provided also welcome submit documentation,issue,positive,positive,positive,positive,positive,positive
917610328,"I Have seen that ML Agents 1.0.0 no longer uses Unity.MLAgents.Actuators or ActionBuffers in its examples. So I can resolve the compilation error.
Will now need to study the float [ ] replacement for Action Buffers, and how to represent discrete Actions.   ",seen longer resolve compilation error need study float replacement action represent discrete,issue,negative,positive,neutral,neutral,positive,positive
917071436,"It's likely a version issue. The folder you cloned is from release_18 that uses ML-Agents v2.2 while you installed 1.0.8, which is incompatible. Please try cloning the repo from the [1.0.8 verified release](https://github.com/Unity-Technologies/ml-agents/tree/com.unity.ml-agents_1.0.8).",likely version issue folder incompatible please try release,issue,negative,neutral,neutral,neutral,neutral,neutral
917010795,"After discussion, we realized that this can ONLY happen when tie horizon is not infinity. mapoca-doublebuffer assumes that the time horizon will always be infinite.
Alternative fixes would be to ignore the time horizon argument when using poca or coma; or raise a better error message what this issue happens.",discussion happen tie horizon infinity time horizon always infinite alternative would ignore time horizon argument coma raise better error message issue,issue,negative,positive,positive,positive,positive,positive
916801429,"@Albert950207 Do you have http_proxy and/or https_proxy environment variables set to something? I just had a similar problem, and I had to remove them for a training to work.",environment set something similar problem remove training work,issue,negative,neutral,neutral,neutral,neutral,neutral
915803307,"Thank you so much!! 

as a wrapping up question, the PPO implementation is based on Schulman 2017 if I did my research correctly? ",thank much wrapping question implementation based research correctly,issue,negative,positive,positive,positive,positive,positive
915607995,"Our SAC implementation is based on the one described in https://arxiv.org/abs/1801.01290 and implemented in https://github.com/hill-a/stable-baselines.

Since this is not a feature request or bug report, I'm closing this now. For general questions, our [forum](https://forum.unity.com/forums/ml-agents.453/) is a better place to get help from.",sac implementation based one since feature request bug report general forum better place get help,issue,positive,positive,positive,positive,positive,positive
912299935,"yes. I used ""3DBall"" environment in ML-Agents.
but when I tested with another PC, it worked.
so I think some of python packages are conflicted.
thanks.",yes used environment tested another worked think python thanks,issue,positive,positive,positive,positive,positive,positive
912173456,">  The sample shows up in version 1.9.1-preview and later. I will call that out in the docs when I get to the task of fixing this issue. Thank you for the feedback!

The sample does *not* show up in 2.1.0-exp.1 Preview on Unity 2020.3 for Mac. Installed through package manager.

<img width=""792"" alt=""Screenshot 2021-09-03 at 02 54 51"" src=""https://user-images.githubusercontent.com/830492/131935182-17d9682d-329d-4cca-adcc-ff2c3ecea31d.png"">
",sample version later call get task fixing issue thank feedback sample show preview unity mac package manager,issue,negative,neutral,neutral,neutral,neutral,neutral
908658233,"@tw0226 - to confirm, did you attempt to use the Python API with one of the environments in ML-Agents?",confirm attempt use python one,issue,negative,neutral,neutral,neutral,neutral,neutral
908619610,"@Albert950207 - to confirm, did you add a DecisionRequester component to your agent as outlined in the [Final Step](https://github.com/Unity-Technologies/ml-agents/blob/release_18_docs/docs/Learning-Environment-Create-New.md#final-agent-setup-in-editor)?

Specifically this step: Add a `Decision Requester` script with the Add Component button. Set the Decision Period to 10. For more information on decisions, see the Agent documentation",confirm add component agent outlined final step specifically step add decision requester script add component button set decision period information see agent documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
907651841,"Sounds like a fun simulator @playztag - best of luck and if you are able to train a more advanced model, let me know!",like fun simulator best luck able train advanced model let know,issue,positive,positive,positive,positive,positive,positive
907648381,"@lukemadera I am building a rocket-league like a drone simulator using MLagents (drones flying and tagging each other by proximity, one of the drones have possession a virtual ball, if this drone is tagged from behind, the possession of the ball transfers to the tagger... whoever with possession flying to the opposite goal will score a point) It seems like the combination of the recent Unity Hummingbird ML agents project with the mixed with soccer example. While there are several strategies to the game, based on this thread, I'm setting my worst-case expectations to be multiple brains for each specific behavior if I can't get a general brain to work. Thanks!",building like drone simulator flying proximity one possession virtual ball drone tagged behind possession ball tagger whoever possession flying opposite goal score point like combination recent unity hummingbird project mixed soccer example several game based thread setting multiple brain specific behavior ca get general brain work thanks,issue,positive,negative,neutral,neutral,negative,negative
907516210,I checked the behavior type and make sure it is default. Does anyone know what happened ? Thanks!,checked behavior type make sure default anyone know thanks,issue,positive,positive,positive,positive,positive,positive
906054108,"Thanks a lot about this,I `m ready to work prefact by **using unity(2019.4.25f1)**! I Love ML-Agents Organization,too!!!",thanks lot ready work unity love organization,issue,positive,positive,positive,positive,positive,positive
905442020,"I solved the issue by 

1. Navigate to this file: ml-agents\com.unity.ml-agents\Tests\Editor\PublicAPI\PublicApiValidation.cs and open it in any text editor.

2. Nativate to: public bool KillswitchEnabled; 

3. just assign it a value either true or false i.e.: public bool KillswitchEnabled = false;

This solved this error for me but I had other issues (scripts were not compiled correctly, hence were not loading) when playing any demo game. Maybe that was just a problem in the version I was using (2019.4.10f1) because the demo project is in 2019.4.25f1.

What i did to solve this issue is I deleted the project. Re-extracted it from the downloaded package (or re-clone the repo if you are using that) and opened it in 2019.4.27f1. Haven't had any issues yet in it.
",issue navigate file open text editor public bool assign value either true false public bool false error correctly hence loading game maybe problem version project solve issue project package yet,issue,negative,negative,negative,negative,negative,negative
905225261,"Same issue here, I followed the steps here: https://github.com/Unity-Technologies/ml-agents/blob/release_18_docs/docs/Getting-Started.md. When I put the trained onnx file as the Model for Behavior Parameters component, the warning ""required constant ""is_continuous_control"" was not found in the model"" shows up. When I try to Play the scene, the agent cubes don't move at all. 

I'm using Mac OS 11.5.2, Python 3.9, ml-agent-release7, and ML Agents version 1.4.0. 

Fixed: 
1. I first updated my ML Agents to 1.9.1 in Package Manager, and imported the 3DBall under Sample tab in the Package Manager (somehow 2.1.0 version doesn't have option to import 3DBall for me)
2. I updated to ml-agent-release18 and trained the model there
3. Update the ML Agents to 2.1.0 in Package Manager, drag the trained model to Model in Behavior Parameters component of the Agent Prefab. Run successfully with the trained model",issue put trained file model behavior component warning constant found model try play scene agent move mac o python version fixed first package manager sample tab package manager somehow version option import trained model update package manager drag trained model model behavior component agent prefab run successfully trained model,issue,negative,positive,positive,positive,positive,positive
904837142,"It is intentional that our policies output actions in the range of -1, 1. The reason for this is keep the scale of the network weights small in order to make training more robust/stable. To get larger values, I recommend mapping the action from -1 ,1 to your desired range in C# through multiplication/addition.",intentional output range reason keep scale network small order make training get recommend action desired range,issue,positive,negative,neutral,neutral,negative,negative
904818995,"I reviewed code and found out the reason. I am converting input to `int` value, on removing which i am getting various floating point inputs. 

But one final problem that remains is why are input values so small, are they meant to range between -1 and 1. I did check other issues and discussions , specifically https://github.com/Unity-Technologies/ml-agents/issues/319 where the author is getting values like 520,630,-580.

Anyway, since this is not the main topic of issue, I am closing it

If you can help regarding getting big inputs directly rather than mathematically clamping them then it would be great!

Anyway, Thank You So Much!",code found reason converting input value removing getting various floating point one final problem remains input small meant range check specifically author getting like anyway since main topic issue help regarding getting big directly rather mathematically would great anyway thank much,issue,positive,positive,positive,positive,positive,positive
904811577,"> Is the behavior type in the Behavior Parameters script set to Default?

Yes.

> Does your agent have a Decision Requester?

No. But I am manually calling `RequestDecision()` Function when required.

While Training for about 9K steps, it did once or twice sent 1 and -1 as inputs.
But the problem still persists, why only 0 most of the time and 1 and -1 rarely? 
Why no other big value like 25 or - 39?
 
I don't mind sharing scripts and scenes, if required please let me know.
",behavior type behavior script set default yes agent decision requester manually calling function training twice sent problem still time rarely big value like mind please let know,issue,negative,neutral,neutral,neutral,neutral,neutral
904778215,Is the `behavior type` in the Behavior Parameters script set to Default? Does your agent have a Decision Requester?,behavior type behavior script set default agent decision requester,issue,negative,neutral,neutral,neutral,neutral,neutral
904512409,"Hi @andrewcoh 

Thanks for your reply. It is a great idea to put the logic in `OnEpisodeBegin`, however, for me, the state 1 is not predefined but the result from one of the runs in state 0, depending on the reward from the python side. Therefore I need a mechanism to tell the agent what state 1 is. That is why I was thinking whether it is possible to implement a function that set the agent state according to input variable from the python side. 

Either way, I have switched to use ROS TCP Endpoint to bulld the connection so this is no longer needed now. Thanks again.",hi thanks reply great idea put logic however state result one state depending reward python side therefore need mechanism tell agent state thinking whether possible implement function set agent state according input variable python side either way switched use connection longer thanks,issue,positive,positive,positive,positive,positive,positive
904288957,"It does work on example environments. 

For my environment,
I checked training_status.json , timers.json and tensorboard as well.
No NaN anywhere.

BTW here is my config file if it can help identify any peoblem
```
behaviors:
  TRC:
    trainer_type: ppo
    hyperparameters:
      batch_size: 256
      buffer_size: 2560
      learning_rate: 0.003
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
      vis_encode_type: resnet
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
      curiosity:
        gamma: 0.99
        strength: 0.02
        network_settings:
          hidden_units: 256
        learning_rate: 0.0003
      gail:
        strength: 0.8
        demo_path: Project\Assets\T-Demos\D1.demo
    keep_checkpoints: 5
    max_steps: 400000
    time_horizon: 64
    behavioral_cloning:
      demo_path: Project\Assets\T-Demos\D1.demo
      strength: 0.7
    summary_freq: 100
```

The command I used:
```
mlagents-learn config\ppo\TRC.yaml --run-id=TRC --resume  --time-scale 5
```

I have 4 camera input and 1 int variable as observation.",work example environment checked well nan anywhere file help identify beta epsilon linear normalize false extrinsic gamma strength curiosity gamma strength strength strength command used resume camera input variable observation,issue,positive,negative,negative,negative,negative,negative
904219517,"Hi @rgulza01 

Are you using one of our camera sensors? Can you please share the sensor setup/observation space of your agent?
",hi one camera please share sensor space agent,issue,positive,neutral,neutral,neutral,neutral,neutral
904023872,"Hi @Nirav-Madhani 

Can you verify that this does not happen on one of our example environments or have you verified that it _does happen?_  Are you seeing NaNs in the tensorboards?",hi verify happen one example happen seeing,issue,negative,neutral,neutral,neutral,neutral,neutral
904021671,"Hi @HainingLuo 

Do you mean after state 0, you enter state 1. Then rollout the trajectory to some termination condition. _And _then__ reset to state 1 and perform another rollout?

If the environment is simple enough, most of this logic can be put in the `OnEpisodeBegin` function. Again, if it's simple enough, you can save whatever you need to (e.g. gameobject velocity, position etc.) in the agent script. However, this is not very scalable. ",hi mean state enter state trajectory termination condition reset state perform another environment simple enough logic put function simple enough save whatever need velocity position agent script however scalable,issue,negative,negative,neutral,neutral,negative,negative
904014806,"Hi @Peamon 

We can only guarantee models will export properly to Barracuda if they are created with our trainers. Please try the Barracuda repo to get further guidance https://github.com/Unity-Technologies/barracuda-release/issues",hi guarantee export properly barracuda please try barracuda get guidance,issue,positive,neutral,neutral,neutral,neutral,neutral
902107432,"We will investigate at some point recoding demonstrations from built executables rather than the editor for imitation learning in Python. Regarding training at runtime, we currently rely on Python to perform training and do not have plans to support training within a built executable in C#. I do not know what this ""separate, even long, background process"" could look like. Maybe this would work if the user was willing to launch a Python learning process but I doubt it. 
Learning from real time data might be excruciatingly slow for a user to manage, I believe a difficulty adjustment setting would provide the same results for a fraction of the complexity.",investigate point built rather editor imitation learning python regarding training currently rely python perform training support training within built executable know separate even long background process could look like maybe would work user willing launch python learning process doubt learning real time data might excruciatingly slow user manage believe difficulty adjustment setting would provide fraction complexity,issue,negative,negative,neutral,neutral,negative,negative
901834615,Is this current possible or still in developing stage?,current possible still stage,issue,negative,neutral,neutral,neutral,neutral,neutral
900003078,"@mahon94 MLAgents is not an open source project that anyone can contribute to? I don't understand why I can't add. There are so many useless test codes in the project(and bug too.), but I don't know why you closed my contribution. Shouldn't you at least ask my opinion or wait for my reply? Anyway the key content of my code is not the example environment itself, but the function used to give observation in the environment. If it is prohibited to add a new environment, I will contribute again by adding it to the existing environment.
Thank you.",open source project anyone contribute understand ca add many useless test project bug know closed contribution least ask opinion wait reply anyway key content code example environment function used give observation environment add new environment contribute environment thank,issue,negative,negative,neutral,neutral,negative,negative
899778399,"@xogur6889 we're not able to help unless you provide full information, which is listed out in the comment above and the issue template. The error can be caused by many reasons and we're not able to look into it without those information.

Besides, we are unable to help reproduce bugs with custom environments since we have no access to your project. Please attempt to reproduce your issue with one of the example environments, or provide a minimal patch to one of the environments needed to reproduce the issue.",able help unless provide full information listed comment issue template error many able look without information besides unable help reproduce custom since access project please attempt reproduce issue one example provide minimal patch one reproduce issue,issue,positive,positive,positive,positive,positive,positive
899273987,"Hi @mahon94

I just installed the version you told me to install in the document and it doesn't seem to be a version problem.
ex) pip3 install torch~=1.7.1 -f https://download.pytorch.org/whl/torch_stable.html

I use Windows and the computer is up to date.

In one environment I use two Behavior Names. (For example, one is red and one is blue). Use the red I want to train as Default. Blue set up the Behavior Type as Inference Only to use the trained model. And the problem arose in this environment.",hi version told install document seem version problem ex pip install use computer date one environment use two behavior example one red one blue use red want train default blue set behavior type inference use trained model problem arose environment,issue,negative,neutral,neutral,neutral,neutral,neutral
898695517,"This is an interesting environment! At the moment, we're not looking for external contribution to the example environments. It would be more suitable to post in the forum to show case your environment.",interesting environment moment looking external contribution example would suitable post forum show case environment,issue,negative,positive,positive,positive,positive,positive
898675213,"Since you're using inference, it might be a Barracuda issue. Can you please provide:

- The full stack trace of the bug in Unity, it helps us to follow where the error comes from
- Barracuda Version: 
- ML-Agents version: (e.g. ML-Agents v0.8, or latest develop branch from source)
- Unity Version: [e.g. Unity 2020.1f1]
- OS + version: 
- Torch version: 
",since inference might barracuda issue please provide full stack trace bug unity u follow error come barracuda version version latest develop branch source unity version unity o version torch version,issue,negative,positive,positive,positive,positive,positive
898095281,"Hi @mahon94

_Please follow the issue template, the steps to reproduce the bug and information on Unity and ML-Agents are necessary for us to better help with your problem._

**Okay. Next time I'll do that.**

_Are you using ML-Agents example environments or your own? if you're using your environment and see these error, please try with one of our environments and see if the error is still there?_

**First of all, I asked the cause of that error, did you misunderstand? I'm using my own environments. not using ML-Agents example. You said try one of your environments but I think it is not helpful to solve my problem. To be more specific about my problem, it usually happens in the process of putting one model in an inference state and learning another when using two models. Usually, it seems to occur once in 5 million steps.**

Thank you.",hi follow issue template reproduce bug information unity necessary u better help next time example environment see error please try one see error still first cause error misunderstand example said try one think helpful solve problem specific problem usually process one model inference state learning another two usually occur million thank,issue,negative,positive,neutral,neutral,positive,positive
897948053,"Hi  @xogur6889, 

Please follow the issue template, the steps to reproduce the bug and information on Unity and ML-Agents are necessary for us to better help with your problem.

Are you using ML-Agents example environments or your own? if you're using your environment and see these error, please try with one of our environments and see if the error is still there?",hi please follow issue template reproduce bug information unity necessary u better help problem example environment see error please try one see error still,issue,negative,positive,positive,positive,positive,positive
896271040,"> Thanks for your contribution and apologies for the delayed review! I left a few comments below and overall it looks good. There are still a few things need to be added before we can merge the PR:
> 
> 1. You need to add a public API for sending immediate messages in `environment.py`. The function should be public, with a clear name of its purpose, and with docstrings.

The process_immediate_message is now renamed to be a public function along with docstrings, would be pushing it with the next push in the next few days.

> 2. Please add some unit tests. The related tests are located under `ml-agents-envs/mlagents_envs/tests/test_envs.py`.

For adding a unit test, a mock environment is needed that can receive a query and return a response. Need some time to implement and hence would be pushing it with the next push in the next few days.",thanks contribution review left overall good still need added merge need add public sending immediate function public clear name purpose public function along would pushing next push next day please add unit related unit test mock environment receive query return response need time implement hence would pushing next push next day,issue,positive,positive,neutral,neutral,positive,positive
895239277,"I had the same error, but in the configuration file under checkpoint_settings, force and resume were set. They were not overwritten via the command line.
I think you need to check your .yaml config file for something like this:
```
checkpoint_settings:
  run_id: ppo
  initialize_from: null
  load_model: false
  resume: true
  force: false
  train_model: false
  inference: false
  results_dir: results
```",error configuration file force resume set via command line think need check file something like null false resume true force false false inference false,issue,negative,negative,negative,negative,negative,negative
894544260,"Hi, 
This issue is not considered a bug. It would be more suitable for the forum since this is an ask for help. The instructions referring to the ML-Agents folder in the Assets folder assume that the github repo was cloned. It corresponds to this folder https://github.com/Unity-Technologies/ml-agents/tree/main/Project/Assets/ML-Agents 
The Unity package does not include the sample environment because they are too large to be part of a package.",hi issue considered bug would suitable forum since ask help folder asset folder assume folder unity package include sample environment large part package,issue,positive,positive,positive,positive,positive,positive
893973626,"> Got all, totally understand you have a small team, licensing is a headache with custom modifications, and you have other priorities besides non-functional code cleanup. Do what you need to do. BTW, not sure if you were aware, but gRPC is [beta testing an official Unity plugin](https://github.com/grpc/grpc/tree/master/src/csharp/experimental#unity).

Thanks, yes we have seen the plugin.  It has been experimental for years and I'm not sure there is any official support going into it.  It is essentially a repackaging of the csharp grpc build via a batch script.",got totally understand small team headache custom besides code cleanup need sure aware beta testing official unity thanks yes seen experimental sure official support going essentially build via batch script,issue,positive,positive,positive,positive,positive,positive
892820784,"I cannot reproduce the issue. I tried to run the following commands on the 3DBall environment : 
```
mlagents-learn config/ppo/3DBall.yaml --env=envs/ball --run-id=issue-5488 && mlagents-learn config/ppo/3DBall.yaml --env=envs/ball --run-id=issue-5488 
```
and I got the correct error : 
```
mlagents.trainers.exception.UnityTrainerException: Previous data from this run ID was found. Either specify a new run ID, use --resume to resume this run, or use the --force parameter to overwrite existing data.
```
My guess is that the config file you are using has the resume command hard coded in the yaml. Can you look through the config to see if there is a resume flag set or try with a brand new config file?",reproduce issue tried run following environment got correct error previous data run id found either specify new run id use resume resume run use force parameter overwrite data guess file resume command hard look see resume flag set try brand new file,issue,negative,negative,neutral,neutral,negative,negative
891821476,"> Thanks for raising this feature request. This is something on our roadmap since we also received requests for ELO as completion criteria previously. We have been re-thinking the interface design and the goal would be providing a more flexible interface to specify completion criteria with any training stats. I will update when we have the feature ready.
> Logged internally as [MLA-2115](https://jira.unity3d.com/browse/MLA-2115).

:pray: :pray: 
---
> As of the bug report you mentioned, this is the expected outcome that when you enable smoothing, the lesson update won't happen the exact time when you first reach the threshold value. You can easily turn it off by setting `signal_smoothing: false`.

I cannot agree with that. In my case, my threshold is -0.7499 and the mean reward is -1, but it still satisfies the criteria. **_No one will expect this result of the curriculum._** If I turn on `signal_smoothing`, to prevent this bug, I cannot set the threshold less than -0.74. So I cannot create a curriculum which threshold is -0.8 (if I turn on `signal_smoothing`). 

The worst thing is, this bug is hard to realize except you do many experiments to find the border (0.75) and see the source code. And nothing about this on the doc. Someone who encounters this may not find the reason. I know I can prevent it easily by turning it off. But people who don't know about this will jump into this bug and cannot find how to solve it. Especially, the example in the document uses `signal_smoothing: true`, which raises the probability for users to encounter this bug. Since it is a complex bug hard to describe, he/she is hard to find this issue. So in my opinion, this should be fixed. Here are three solutions:

### None initial
Set `smoothing ` initial as `None`. When we want to use `smoothing`, if it is `None`, let `smoothing=measure` first. In this way, the initial value of `smoothing` will be the same as `measure`. It is quite reasonable since we never see `measure` before, so we cannot assume it is 0 before.

### Small initial
Set `smoothing` initial as something like -1e7, In this way, the bug will not happen. It may take some epoch to let `smoothing` close to `measure`, but it is ignorable.

### Update every epoch
As the source code, smoothing will be updated only if `min_lesson_length` is satisfied. You can update smoothing every time you enter the `need_increment` function. This will let the value be more smoothing since it takes the epoch before `min_lesson_length` into consideration. But it cannot solve the bug, so it has to work together with `Small initial` or `None initial`. And this method can decrease the negative effect of the other two methods.


---
### One more advice about curriculum
`completion_criteria` will sum `min_lesson_length` epoches, so training summary may greater than threshold while `completion_criteria` is not satisfied. It may be confusing and unclear for users to wait for lesson increases. So I recommend that the summary show the mean reward of `min_lesson_length` epoches of `measure`. Users can clearly know what is the value used for `completion_criteria`. It can be one of the configs and the default is turn on.

---
### Maybe a bug
I find that if you train more than `min_lesson_length` epoches first, then stop training, and then use `--resume` option to start training again. Variable `reward_buffer` will be reset to an empty list, so `lesson_length` will be 0. That means so you have to train at lease more `min_lesson_length` epoches to fit the requirements of `min_lesson_length`. I wonder if that is normal or not.",thanks raising feature request something since also received completion criterion previously interface design goal would providing flexible interface specify completion criterion training update feature ready logged internally pray pray bug report outcome enable smoothing lesson update wo happen exact time first reach threshold value easily turn setting false agree case threshold mean reward still criterion one expect result turn prevent bug set threshold le create curriculum threshold turn worst thing bug hard realize except many find border see source code nothing doc someone may find reason know prevent easily turning people know jump bug find solve especially example document true probability encounter bug since complex bug hard describe hard find issue opinion fixed three none initial set smoothing initial none want use smoothing none let first way initial value smoothing measure quite reasonable since never see measure assume small initial set smoothing initial something like way bug happen may take epoch let smoothing close measure ignorable update every epoch source code smoothing satisfied update smoothing every time enter function let value smoothing since epoch consideration solve bug work together small initial none initial method decrease negative effect two one advice curriculum sum training summary may greater threshold satisfied may unclear wait lesson recommend summary show mean reward measure clearly know value used one default turn maybe bug find train first stop training use resume option start training variable reset empty list train lease fit wonder normal,issue,positive,positive,neutral,neutral,positive,positive
891435023,"Thanks for raising this feature request. This is something on our roadmap since we also received requests for ELO as completion criteria previously. We have been re-thinking the interface design and the goal would be providing a more flexible interface to specify completion criteria with any training stats. I will update when we have the feature ready. 
Logged internally as [MLA-2115](https://jira.unity3d.com/browse/MLA-2115).

As of the bug report you mentioned, this is the expected outcome that when you enable smoothing, the lesson update won't happen the exact time when you first reach the threshold value. You can easily turn it off by setting `signal_smoothing: false`.",thanks raising feature request something since also received completion criterion previously interface design goal would providing flexible interface specify completion criterion training update feature ready logged internally bug report outcome enable smoothing lesson update wo happen exact time first reach threshold value easily turn setting false,issue,positive,positive,neutral,neutral,positive,positive
890539490,"> > Great if this does the job, though I have to ask why you are including pre-built binaries of other open-source libraries in the first place. If it's a hack to get a functioning version out, then maybe it's something worth revisiting, considering the previous versions of this library in the repo are nearly 2 years old. Would it not make more sense to download and build the libraries from source or from their official binary release rather than including a binary of unknown provenance as part of your own source repo? The update to include ARM64 support shows the problems that dropping in native binaries like this can have, and it seems like this patch is just adding on top of existing tech debt.
> 
> @TV4Fun
> I agree with you. I can only speculate on why we decided to add pre-built binaries to the repo. I imagine that this made things work, and there were more important things to work on.
> 
> Additionally, we are building the binaries from source in a private forked repo. We have to change the C# side to get IL2CPP to work on top of the native build for arm64. These C# changes are not appropriate for upstream gRPC in their current form because they are very Unity-specific.
> 
> Another part that isn't obvious to the GitHub consumer of our project is the internal processes we have to release this package in the Unity Editor. There is no gRPC package that exists in the Unity Package registry at the moment. Otherwise, we would gladly depend on that. The team size was initially small and has recently gotten even smaller, making these sorts of changes very low priority for the team.
> 
> Keeping the above in mind, we don't have the team size to update to the latest gRPC, get the licensing in order, and test it thoroughly to ensure that when it's released, we are confident that everything will ""just work."" Newer versions of gRPC have up to as many as 4-6 DLLs that we'd have to keep track of. Updating the native binary to include both x86_64 and arm64 is a purposeful decision based on our team's constraints.
> 
> We very much understand the consequences of dropping native binaries into repos. Again, we made a decision based on an internal cross-benefit analysis that is easy to critique externally. Please understand that we are doing our best to keep up to date with the ever-changing landscape of RL, platform support, etc. and that the team size is tiny.
> 
> We hope that with these changes, you will be able to run natively on arm64.

Got all, totally understand you have a small team, licensing is a headache with custom modifications, and you have other priorities besides non-functional code cleanup. Do what you need to do. BTW, not sure if you were aware, but gRPC is [beta testing an official Unity plugin](https://github.com/grpc/grpc/tree/master/src/csharp/experimental#unity).",great job though ask first place hack get version maybe something worth considering previous library nearly old would make sense build source official binary release rather binary unknown provenance part source update include arm support dropping native like like patch top tech debt agree speculate decided add imagine made work important work additionally building source private forked change side get work top native build arm appropriate upstream current form another part obvious consumer project internal release package unity editor package unity package registry moment otherwise would gladly depend team size initially small recently gotten even smaller making low priority team keeping mind team size update latest get order test thoroughly ensure confident everything work many keep track native binary include arm purposeful decision based team much understand dropping native made decision based internal analysis easy critique externally please understand best keep date landscape platform support team size tiny hope able run natively arm got totally understand small team headache custom besides code cleanup need sure aware beta testing official unity,issue,positive,positive,positive,positive,positive,positive
890133670,I already reviewed #5457 and it's been left there. I'll go merge that.,already left go merge,issue,negative,neutral,neutral,neutral,neutral,neutral
890109076,"Yes, that is just all users want. An auto-named run-id, and we can use the same name to do the same experiment.
Thanks a lot.

Sorry for late reply, I didn't get an e-mail about this issue.
",yes want use name experiment thanks lot sorry late reply get issue,issue,positive,negative,negative,negative,negative,negative
889310142,"> Great if this does the job, though I have to ask why you are including pre-built binaries of other open-source libraries in the first place. If it's a hack to get a functioning version out, then maybe it's something worth revisiting, considering the previous versions of this library in the repo are nearly 2 years old. Would it not make more sense to download and build the libraries from source or from their official binary release rather than including a binary of unknown provenance as part of your own source repo? The update to include ARM64 support shows the problems that dropping in native binaries like this can have, and it seems like this patch is just adding on top of existing tech debt.

@TV4Fun 
I agree with you.  I can only speculate on why we decided to add pre-built binaries to the repo.  I imagine that this made things work, and there were more important things to work on.

Additionally, we are building the binaries from source in a private forked repo.  We have to change the C# side to get IL2CPP to work on top of the native build for arm64.  These C# changes are not appropriate for upstream gRPC in their current form because they are very Unity-specific. 

Another part that isn't obvious to the GitHub consumer of our project is the internal processes we have to release this package in the Unity Editor.  There is no gRPC package that exists in the Unity Package registry at the moment.  Otherwise, we would gladly depend on that.  The team size was initially small and has recently gotten even smaller, making these sorts of changes very low priority for the team.

Keeping the above in mind, we don't have the team size to update to the latest gRPC, get the licensing in order, and test it thoroughly to ensure that when it's released, we are confident that everything will ""just work.""  Newer versions of gRPC have up to as many as 4-6 DLLs that we'd have to keep track of.  Updating the native binary to include both x86_64 and arm64 is a purposeful decision based on our team's constraints.  

We very much understand the consequences of dropping native binaries into repos.  Again, we made a decision based on an internal cross-benefit analysis that is easy to critique externally.  Please understand that we are doing our best to keep up to date with the ever-changing landscape of RL, platform support, etc. and that the team size is tiny.  

We hope that with these changes, you will be able to run natively on arm64. ",great job though ask first place hack get version maybe something worth considering previous library nearly old would make sense build source official binary release rather binary unknown provenance part source update include arm support dropping native like like patch top tech debt agree speculate decided add imagine made work important work additionally building source private forked change side get work top native build arm appropriate upstream current form another part obvious consumer project internal release package unity editor package unity package registry moment otherwise would gladly depend team size initially small recently gotten even smaller making low priority team keeping mind team size update latest get order test thoroughly ensure confident everything work many keep track native binary include arm purposeful decision based team much understand dropping native made decision based internal analysis easy critique externally please understand best keep date landscape platform support team size tiny hope able run natively arm,issue,positive,positive,positive,positive,positive,positive
888816399,"Great if this does the job, though I have to ask why you are including pre-built binaries of other open source libraries in the first place. If it's a hack to get a functioning version out, then maybe it's something worth revisiting, considering the previous versions of this library in the repo are nearly 2 years old. Would it not make more sense to download and build the libraries from source or from their official binary release rather than including a binary of unknown provenance as part of your own source repo? The update to include ARM64 support shows the problems that dropping in native binaries like this can have, and it seems like this patch is just adding on top of existing tech debt.",great job though ask open source first place hack get version maybe something worth considering previous library nearly old would make sense build source official binary release rather binary unknown provenance part source update include arm support dropping native like like patch top tech debt,issue,positive,positive,positive,positive,positive,positive
888517029,"Hi, 

Please follow the issue template so that we can have the information to better help you.
What's the Unity Editor/ML-Agents version you're using?

Also, are you using your own environment or one of our examples? If you're using your custom environment and seeing errors,  please try with any of our examples first and see if the error is still there. ",hi please follow issue template information better help unity version also environment one custom environment seeing please try first see error still,issue,positive,positive,positive,positive,positive,positive
887988545,"> We use 20 for most of our example environments. If you go too fast, the physics gets kind of wonky, and sometimes objects/agents will go through each other.
> 
> The default `mlagents-learn` command will set the time-scale to 20, so that might explain why it seems faster.

Thanks.",use example go fast physic kind wonky sometimes go default command set might explain faster thanks,issue,positive,positive,positive,positive,positive,positive
887798812,"Since this feature would be useful to the rest of the community also, hence attempt to explain in more detail below:

Most of the simulators allow being queried directly. For example, the habitat-sim https://github.com/facebookresearch/habitat-sim. As an example if the sim has an agent or a map, you can directly call functions on the sim anytime, without involving the reinforcement learning loop of ``reset()`` and ``step()``, such as the following query functions:

```
sim.get_agent_state()
sim.get_navgiable_map()
sim.sample_navigable_point()
sim.is_navigable_point()
```
Having said that, we tried implementing it with ``mlagents_env`` using side channels. The process was convoluted as follows:

1. First you have to send a message from python side channel, say get_navigable_map
2. Then in the C# you put this map back n the message queue.
3. Then in the python side you have to call another function to get this message, i.e. the map itself.
4. The worst part of the above 3 step loop is that the message doesn't get back right away. It only gets back after the next step() or reset() function is executed.

The pull request that I have sent, adds a new kind of query called immediate along with step and reset. If this request is merged then the above loop will become as follows:
1. Send a request from the python side, say get_navigable_map
2. in C# send the map back.
3. Get the response right away without waiting for step() or reset() methods.

The function call in python now looks like this : 

```python
def get_navigable_map(self, resolution_x=256, resolution_y=256,
                          cell_occupancy_threshold=0.5) -> np.ndarray:
            self.uenv._process_immediate_message(
            self.map_side_channel.build_immediate_request(""binaryMap"",
                                                          [resolution_x,
                                                           resolution_y,
                                                           cell_occupancy_threshold]))
        return self.map_side_channel.requested_map
```

The map side-channel class:

```python
class MapSideChannel(SideChannel):
    """"""This is the SideChannel for retrieving map data from Unity.
    You can send map requests to Unity using send_request.
    The arguments for a mapRequest are (""binaryMap"", [RESOLUTION_X, RESOLUTION_Y, THRESHOLD])
    """"""
    resolution = None

    def __init__(self) -> None:
        channel_id = uuid.UUID(""24b099f1-b184-407c-af72-f3d439950bdb"")
        super().__init__(channel_id)
        self.requested_map = None

    def on_message_received(self, msg: IncomingMessage) -> np.ndarray:
        if self.resolution is None:
            return None

        raw_bytes = msg.get_raw_bytes()
        self.requested_map = np.unpackbits(raw_bytes)[
                             0:self.resolution[0] * self.resolution[1]]
        self.requested_map = self.requested_map.reshape((self.resolution[1],
                                                         self.resolution[0]))
        return self.requested_map

    def send_request(self, key: str, value: List[float]) -> None:
        """"""Sends a request to Unity
        The arguments for a mapRequest are (""binaryMap"", [RESOLUTION_X, RESOLUTION_Y, THRESHOLD])
        """"""
        self.resolution = value
        msg = OutgoingMessage()
        msg.write_string(key)
        msg.write_float32_list(value)
        super().queue_message_to_send(msg)

    def build_immediate_request(self, key: str,
                                value: List[float]) -> bytearray:
        self.resolution = value
        msg = OutgoingMessage()
        msg.write_string(key)
        msg.write_float32_list(value)

        result = bytearray()
        result += self.channel_id.bytes_le
        result += struct.pack(""<i"", len(msg.buffer))
        result += msg.buffer
        return result
```

However if the capability to send immediate messages is not there then the get_navigable_map would be divided into two functions like this:

```python
 def start_navigable_map(self, resolution_x=256, resolution_y=256,
                               cell_occupancy_threshold=0.5):
        """"""
        Returns:
            Nothing
        """"""

         self.map_side_channel.send_request(""binaryMap"",
                                               [resolution_x, resolution_y,
                                                cell_occupancy_threshold])
 
def get_navigable_map(self) -> np.ndarray:
        """"""
       Returns:
            A numpy array having 0 for non-navigable and 1 for navigable cells
        
        Note:
            This only works if you have called ``reset()`` or ``step()`` on the
            environment at least once after calling start_navigable_map() method.
        """"""

      return self.map_side_channel.requested_map

```",since feature would useful rest community also hence attempt explain detail allow directly example example agent map directly call without reinforcement learning loop reset step following query said tried side process convoluted first send message python side channel say put map back message queue python side call another function get message map worst part step loop message get back right away back next step reset function executed pull request sent new kind query immediate along step reset request loop become send request python side say send map back get response right away without waiting step reset function call python like python self return map class python class map data unity send map unity threshold resolution none self none super none self none return none return self key value list float none request unity threshold value key value super self key value list float value key value result result result result return result however capability send immediate would divided two like python self nothing self array navigable note work reset step environment least calling return,issue,positive,positive,neutral,neutral,positive,positive
887796703,@dongruoping You're welcome. I hesitated to comit this because I though it's intentional since that extra argument has been in there since version 1.,welcome though intentional since extra argument since version,issue,negative,positive,positive,positive,positive,positive
887786436,"Thanks for fixing this, I'll merge it once the tests pass",thanks fixing merge pas,issue,negative,positive,positive,positive,positive,positive
887784921,"To help us understand your goal of this feature request -- can you tell us a bit more about your use case, and why the is the current way of message communication a blocker for your project?",help u understand goal feature request tell u bit use case current way message communication blocker project,issue,negative,neutral,neutral,neutral,neutral,neutral
887517009,I have a PR I am working on that adds the arm64 arch to the native plugin.  It should be ready this week. ,working arm arch native ready week,issue,negative,positive,positive,positive,positive,positive
887440511,sorry it seems like I forgot to bring some files over from the changed version. will open this later.,sorry like forgot bring version open later,issue,negative,negative,negative,negative,negative,negative
887145629,cc: @surfnerd who might have some experience with Apple Silicon,might experience apple silicon,issue,negative,neutral,neutral,neutral,neutral,neutral
887145160,"thank U for your replay,&nbsp;I may have found the wrong place before. Thanks for reminding me. Your reply has helped me solve the problem very well.
------------------&nbsp;原始邮件&nbsp;------------------
发件人:                                                                                                                        ""Unity-Technologies/ml-agents""                                                                                    ***@***.***&gt;;
发送时间:&nbsp;2021年7月27日(星期二) 上午9:42
***@***.***&gt;;
***@***.******@***.***&gt;;
主题:&nbsp;Re: [Unity-Technologies/ml-agents] Speed up the progress of training (#5473)





 
You can run mlagents-learn with --no-graphics to turn off graphics. BTW, for usage questions, the Unity Forums  are usually a better place to ask, as there may be other users who have had the same issue.
 
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or unsubscribe.",thank replay may found wrong place thanks reply solve problem well speed progress training run turn graphic usage unity usually better place ask may issue thread reply directly view,issue,positive,positive,neutral,neutral,positive,positive
887144892,"We use 20 for most of our example environments. If you go too fast, the physics gets kind of wonky, and sometimes objects/agents will go through each other. 

The default `mlagents-learn` command will set the time-scale to 20, so that might explain why it seems faster. ",use example go fast physic kind wonky sometimes go default command set might explain faster,issue,positive,positive,positive,positive,positive,positive
887143403,"You can run mlagents-learn with --no-graphics to turn off graphics. BTW, for usage questions, the [Unity Forums ](https://forum.unity.com/forums/ml-agents.453/) are usually a better place to ask, as there may be other users who have had the same issue. ",run turn graphic usage unity usually better place ask may issue,issue,negative,positive,positive,positive,positive,positive
886450191,"Not entirely sure if this is what you mean but from the DOCS I found this : mlagents-learn <trainer-config-file> --env=<env_name> --run-id=<run-identifier>

""<env_name>(Optional) is the name (including path) of your Unity executable containing the agents to be trained. If <env_name> is not passed, the training will happen in the Editor. Press the Play button in Unity when the message ""Start training by pressing the Play button in the Unity Editor"" is displayed on the screen.""",entirely sure mean found optional name path unity executable trained training happen editor press play button unity message start training pressing play button unity editor displayed screen,issue,positive,positive,neutral,neutral,positive,positive
886256515,"> I am also new to ml agents and I am also having the same problem about the ""is_continuous_control"". I have been able to train the demo environments but when I go to add the brain model it still gives me the same problem.

i have the same problem..has anyone found any solution to that problem? ",also new also problem able train go add brain model still problem problem anyone found solution problem,issue,negative,positive,positive,positive,positive,positive
886148025,"Trying to get this to work. The default gRPC Unity plugin doesn't include an Arm64 build, but I was able to make my own. The Unity editor and player isn't finding the native library to include with the app, but if I build a standalone and manually add the Apple Silicon dylib I built to it, then it works and has a pretty substantial performance improvement. If anyone can help me in getting the editor to find the right Arm64 file, I would greatly appreciate it.",trying get work default unity include arm build able make unity editor player finding native library include build manually add apple silicon built work pretty substantial performance improvement anyone help getting editor find right arm file would greatly appreciate,issue,positive,positive,positive,positive,positive,positive
886111682,Note: This also happens when running inside the Apple Silicon build of Unity Editor. The only way I'm able to train is to use the Intel build.,note also running inside apple silicon build unity editor way able train use build,issue,negative,positive,positive,positive,positive,positive
885867689,Yes it does. Thx. Something within the interaction of _close and close functions is causing this probably ,yes something within interaction close causing probably,issue,negative,neutral,neutral,neutral,neutral,neutral
885423176,"I did some retesting. The clamping did seem to have some effect on the performance. It's unclear to me why that's the case since I normalized using the negative values. That being said, there is still a performance loss when normalising the values. 

Here are the results:
![new testing](https://user-images.githubusercontent.com/18085277/126743192-9d92990c-1924-43fc-a59f-99b6f12be6be.PNG)

To be honest, I have no clue what's causing this difference in performance and might be something inherit to the algorithm. My knowledge of the subject is currently not enough to speculate about why I'm getting these results.

Something that does seem interesting to me is that when normalised between -1 and 1, entropy dropped more significantly than the baseline, but did take about 15% longer to converge. Would this result in a more stable model because of lower entropy?",seem effect performance unclear case since negative said still performance loss new testing honest clue causing difference performance might something inherit algorithm knowledge subject currently enough speculate getting something seem interesting entropy significantly take longer converge would result stable model lower entropy,issue,negative,positive,positive,positive,positive,positive
885064514,"OOh never mind !! Library 1.7.0 Stops the memory leak during heurstic only. When I train I still have the memory leak ... I am training using this YAML : 
behaviors:
  ParkingAssistBehaviour:
    trainer_type: ppo
    hyperparameters:
      batch_size: 512
      buffer_size: 2048
      learning_rate: 0.003
      beta: 0.005
      epsilon: 0.35
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 3
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.9
        strength: 1.0
    keep_checkpoints: 5
    max_steps: 100000000
    time_horizon: 64
    summary_freq: 5000


Also , @ervteng barracuda is 1.1.2",never mind library memory leak train still memory leak training beta epsilon linear normalize false simple extrinsic gamma strength also barracuda,issue,negative,negative,negative,negative,negative,negative
884807632,"Thanks for your reply @ervteng. I see what you're saying. I did normalize the positions using the negative positions available to the agent. I checked the maximum positions that are possible for the ball which were -3 and 3. Then normalized between these values. So -3 would result in 0 and 3 would result in 1.

Like in the code above:
`            sensor.AddObservation(NormalizeVector3(ball.transform.position - gameObject.transform.position, -3, 3));
`

I'll redo my tests without clamping and will report back with the results. ",thanks reply see saying normalize negative available agent checked maximum possible ball would result would result like code redo without report back,issue,negative,positive,neutral,neutral,positive,positive
884790312,"> Hi, you can even increase the time scale to 10 or 20.

Thanks. Do you have any suggestions to increase the simulation speed?",hi even increase time scale thanks increase simulation speed,issue,positive,positive,positive,positive,positive,positive
884781618,"Hi, you can even increase the time scale to 10 or 20.",hi even increase time scale,issue,negative,neutral,neutral,neutral,neutral,neutral
884679015,"@IvailoManolov Sorry, the code with ML-agents was written by my colleague for the indie game of our company, not only several scripts, so I can not share the code ... The indie game we are making is complex far more complex than the official examples, especially the physical system, skin texture and so on, there are many details I don't know because I'm new here. ",sorry code written colleague game company several share code game making complex far complex official especially physical system skin texture many know new,issue,negative,negative,negative,negative,negative,negative
884638580,"@ervteng 

Interestingly, with the built .exe environment file, even without setting the time scale, it still can achieve 100 steps/second. Here is my code.

```python
import tqdm
import mlagents

from mlagents_envs.registry import default_registry

env_id = ""WallJump""


from mlagents_envs.environment import UnityEnvironment
from mlagents_envs.side_channel.environment_parameters_channel import EnvironmentParametersChannel

channel = EnvironmentParametersChannel()

env = UnityEnvironment(file_name=env_id, side_channels=[channel])

# channel.set_float_parameter(""parameter_1"", 1.0)
# env = default_registry[env_id].make()

env.reset()


# We will only consider the first Behavior
behavior_name = list(env.behavior_specs)[0]
print(f""Name of the behavior : {behavior_name}"")
spec = env.behavior_specs[behavior_name]

decision_steps, terminal_steps = env.get_steps(behavior_name)

env.set_actions(behavior_name, spec.action_spec.empty_action(len(decision_steps)))

env.step()

done = False

env.reset()
decision_steps, terminal_steps = env.get_steps(behavior_name)
tracked_agent = -1 # -1 indicates not yet tracking
done = False # For the tracked_agent
episode_rewards = 0 # For the tracked_agent

for i in tqdm.tqdm(range(1000000)):
	# Track the first agent we see if not tracking
	# Note : len(decision_steps) = [number of agents that requested a decision]
	if tracked_agent == -1 and len(decision_steps) >= 1:
		tracked_agent = decision_steps.agent_id[0]

	# Generate an action for all agents
	action = spec.action_spec.random_action(len(decision_steps))

	# Set the actions
	env.set_actions(behavior_name, action)

	# Move the simulation forward
	env.step()

	# Get the new simulation results
	decision_steps, terminal_steps = env.get_steps(behavior_name)
	if tracked_agent in decision_steps: # The agent requested a decision
		episode_rewards += decision_steps[tracked_agent].reward
	if tracked_agent in terminal_steps: # The agent terminated its episode
		episode_rewards += terminal_steps[tracked_agent].reward
		done = True
	if done:
		env.reset()
		decision_steps, terminal_steps = env.get_steps(behavior_name)
		tracked_agent = -1 # -1 indicates not yet tracking
		done = False # For the tracked_agent
		episode_rewards = 0 # For the tracked_agent
```

I think maybe using the `env = default_registry[env_id].make()` can slow down the simulation speed?",interestingly built environment file even without setting time scale still achieve code python import import import import import channel channel consider first behavior list print name behavior spec done false yet done false range track first agent see note number decision generate action action set action move simulation forward get new simulation agent decision agent episode done true done yet done false think maybe slow simulation speed,issue,positive,positive,neutral,neutral,positive,positive
884637587,"> That's very slow. Have you tried increasing the time scale? This can be done through the engine configuration side-channel: https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Python-API.md#engineconfigurationchannel

Hi, I increased the time scale to 2.0, and the speed now is 110 steps/second, much faster. For 1M steps, it will take 2.5 hours to complete.  Does increasing the time scale affect the accuracy of simulation?",slow tried increasing time scale done engine configuration hi time scale speed much faster take complete increasing time scale affect accuracy simulation,issue,negative,negative,neutral,neutral,negative,negative
884600781,That's very slow. Have you tried increasing the time scale? This can be done through the engine configuration side-channel: https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Python-API.md#engineconfigurationchannel,slow tried increasing time scale done engine configuration,issue,negative,negative,negative,negative,negative,negative
884600185,"Duly noted, and we'll add it to the documentation. If you'd like to take a stab at it yourself, feel free to submit a PR. 

Another way to run headless (without --no-graphics) is to do a ""server build"" in Unity (there's a checkbox there). ",duly noted add documentation like take stab feel free submit another way run headless without server build unity,issue,negative,positive,positive,positive,positive,positive
884599620,You can see both the ML-Agents package version and Barracuda version in the Package Manager in the Unity editor,see package version barracuda version package manager unity editor,issue,negative,neutral,neutral,neutral,neutral,neutral
884599465,Does the raw UnityToGymWrapper work on your machine? (just trying to narrow down the issue),raw work machine trying narrow issue,issue,negative,negative,negative,negative,negative,negative
884598859,"Hmm, this isn't something that's currently supported. If I understand correctly, you'd want to constrain the observation in Unity, and have the Box that's returned have those values?

I think it's something worth considering, but to do so in a general way would mean having an explicit range specified on the Unity side. For most of the Sensors (e.g. RayCast) this should be between -1 and 1. We'd have to add something manual to the VectorSensor. ",something currently understand correctly want constrain observation unity box returned think something worth considering general way would mean explicit range unity side add something manual,issue,negative,positive,neutral,neutral,positive,positive
884597818,"This is interesting. I noticed that you clamp everything to 0 and 1. This usually will cause some issues with data being clipped away. For instance, in 3DBall the observations can be both positive and negative. If you ""normalize"" between 0 and 1, you still have negative values, which will be clipped away by the Mathf.Clamp function.",interesting clamp everything usually cause data clipped away instance positive negative normalize still negative clipped away function,issue,negative,negative,neutral,neutral,negative,negative
884593012,"Yes it should be fairly straight forward to implement. For now to use ARGB textures, you can use compressed observations and it should work as-is, as long as you set the correct observation size in your sensor. 

I've logged this request internally as [MLA-2111](https://jira.unity3d.com/browse/MLA-2111).",yes fairly straight forward implement use use compressed work long set correct observation size sensor logged request internally,issue,positive,positive,neutral,neutral,positive,positive
883987019,I am pretty sure for the memory leak as well. Barracuda is their inference library,pretty sure memory leak well barracuda inference library,issue,positive,positive,positive,positive,positive,positive
883957432,"@IvailoManolov 
I train the agent on my onw environment, I am pretty sure their examples have memory leak. And I find my own environment also have the same problem. What is the Barracuda? The version of ML-Agent?
",train agent environment pretty sure memory leak find environment also problem barracuda version,issue,negative,positive,positive,positive,positive,positive
883917273,"@DeDao2333 One issue that comes in my mind is that if you have a memory leak in your own piece of code. It is important to test if the memory leak is not a problem of the ML Lib you are using. Your program is increasing linearly ,this means that you most definitely might have a memory leak. Now the problem is , where it is ? If you share the code we can better tell if the problem is in you. It might be that the GC is kicking quite often and this blocks your thread.I believe that they are still not allowing the GC thread to run concurrently.If you share your script , I think people can help you a little bit more.",one issue come mind memory leak piece code important test memory leak problem program increasing linearly definitely might memory leak problem share code better tell problem might kicking quite often believe still thread run share script think people help little bit,issue,negative,positive,positive,positive,positive,positive
883913693,"@ervteng Okay Gotcha , just how to see the Barracuda version currently ? ",see barracuda version currently,issue,negative,neutral,neutral,neutral,neutral,neutral
883894284,"@IvailoManolov Sorry for late reply ... Yes, I have the same situation as you. In my Game, the MemoryUsed has increased 1 MB per second approximately. But what bothers me most is that when I train my game, for example, it takes longer and longer time to train every 5W step. My solution now is to stop the training halfway, and then restart the training utilizing the resume command.
",sorry late reply yes situation game per second approximately train game example longer longer time train every step solution stop training halfway restart training resume command,issue,negative,negative,negative,negative,negative,negative
883779301,"Hmm, I haven't been able to replicate the memory leak in 3DBall on 2.1.0. What is your Barracuda version?

For longer training - yes, just increase the max steps. ",able replicate memory leak barracuda version longer training yes increase,issue,negative,positive,positive,positive,positive,positive
883768562,"This is definitely something we want to do - there aren't currently fixed APIs with the Trainers as they're not really meant to be public. I've logged it to our internal tracker, will keep you posted if we decided to expose this documentation.

in the meantime, if there's something specific you'd like to build, feel free to reach out to us here or at the Unity forums for some pointers on how to do so.  ",definitely something want currently fixed really meant public logged internal tracker keep posted decided expose documentation something specific like build feel free reach u unity,issue,positive,positive,neutral,neutral,positive,positive
883765704,"Hmm, the main reason we haven't done this is it's not standard on the Python/training side to have the Alpha channel as input to the CNN. Still, we _do_ support more than 3 channels input for matrix observations (it encodes as multiple PNGs), maybe we can make a custom WriteTexture param that also includes the alpha fairly easily. cc: @dongruoping ",main reason done standard side alpha channel input still support input matrix multiple maybe make custom param also alpha fairly easily,issue,positive,positive,positive,positive,positive,positive
883757782,"An easy solution to this would be to just take the action that goes from -1 to 1, add 1, and divide by 2. That will get you a 0-1 centered around 0.5. Only do this for your third action.",easy solution would take action go add divide get centered around third action,issue,positive,positive,positive,positive,positive,positive
883756687,This seems like a PyTorch issue (and not ML-Agents). I'd try asking on the https://discuss.pytorch.org/ and posting your error. ,like issue try posting error,issue,negative,neutral,neutral,neutral,neutral,neutral
883756354,"To second what has been said, it's almost certainly a memory leak. If 3DBall worked fine, it's most likely in your game. I'd check things like disabled gameobjects piling up somewhere. ",second said almost certainly memory leak worked fine likely game check like disabled piling somewhere,issue,positive,positive,neutral,neutral,positive,positive
883755977,"Hi, you can cite this repo using this paper. It's on the Readme for future reference.
```
Juliani, A., Berges, V., Teng, E., Cohen, A., Harper, J., Elion, C., Goy, C., Gao, Y., Henry, H., Mattar, M., Lange, D. (2020). Unity: A General Platform for Intelligent Agents. arXiv preprint arXiv:1809.02627. https://github.com/Unity-Technologies/ml-agents.
```",hi cite paper future reference teng harper goy henry unity general platform intelligent preprint,issue,negative,positive,positive,positive,positive,positive
883145957," @hhhcwb38712
ML-Agent has a bug for sure. If you go to their examples ,3D ball , place the mode on heuristic only . With the time you can see that memory is not stable. If their example has a memory leak , its not your fault .I recommend you downgrading to 1.7.0 , memory is stable there. I would like to ask you a question . How can you train for 7 or 8 hours ? I want to train my agent for a longer period of time . Should I just increase the max steps to 10mil for example ? How can i train for 24hours or 8 or just longer period of time?
",bug sure go ball place mode heuristic time see memory stable example memory leak fault recommend memory stable would like ask question train want train agent longer period time increase mil example train longer period time,issue,positive,positive,positive,positive,positive,positive
882987209,"@IvailoManolov 
Hi，I also met this issue，I use ML-Agent to train my own environment, at first, it took me about 200s to summay 10000 steps. After a night(about 7 or 8 hours), it took me 1000s to summary 10000 steps. And task manager shows the Unity.exe take 16GB memory usage. Is my environment has memory leak or ML-Agent has a bug?",also met use train environment first took night took summary task manager take memory usage environment memory leak bug,issue,negative,positive,positive,positive,positive,positive
882259852,"I think this might be associated with a problem that I have discovered. I speculate a memory leak at least in my machine. Could you start your own training where the FPS dropped and monitor the memory usage of the program . You can simply use the profiler or open Task Manager , and see the memory used there. I simply write a sheet where I say for example  :
7/19/2021 8:55 AM MemoryUsed -> 1GB
7/19/2021 9:23 AM MemoryUsed -> 4GB

If you see this kind of behavior there is a memory leak.Test it out and tell me when you are done.",think might associated problem discovered speculate memory leak least machine could start training monitor memory usage program simply use profiler open task manager see memory used simply write sheet say example see kind behavior memory tell done,issue,negative,positive,neutral,neutral,positive,positive
879708727,"Update:

I've tried the same scenario using the ML-agents 18 release with python package 0.27.0 and am getting similar results. 

This time using one of the provided environments, 3DBall. Below are the results:

![3DBall results](https://user-images.githubusercontent.com/18085277/125590885-b737d51e-a9e7-4643-84ea-97d333868732.PNG)

While the normalized values between 0 and 1 don't have such a dramatic effect on learning as in my custom environment, there is a clear difference in learning speed.

_note: All training runs were performed with the provided config file ""config/ppo/3DBall.yaml"". The only alteration was in the processing of the inputs._

Processing of the inputs looks like this:
```
    public override void CollectObservations(VectorSensor sensor)
    {
        if (useVecObs)
        {
            /*sensor.AddObservation(gameObject.transform.rotation.z);
            sensor.AddObservation(gameObject.transform.rotation.x);
            sensor.AddObservation(ball.transform.position - gameObject.transform.position);
            sensor.AddObservation(m_BallRb.velocity);*/

            /*sensor.AddObservation(Normalize(gameObject.transform.rotation.z, 0, 360));
            sensor.AddObservation(Normalize(gameObject.transform.rotation.x, 0, 360));
            sensor.AddObservation(NormalizeVector3(ball.transform.position - gameObject.transform.position, -3, 3));
            sensor.AddObservation(NormalizeVector3(m_BallRb.velocity, -10, 10));*/

            /*sensor.AddObservation(Normalize(gameObject.transform.rotation.z, 0, 360) * 10);
            sensor.AddObservation(Normalize(gameObject.transform.rotation.x, 0, 360) * 10);
            sensor.AddObservation(NormalizeVector3(ball.transform.position - gameObject.transform.position, -3, 3) * 10);
            sensor.AddObservation(NormalizeVector3(m_BallRb.velocity, -10, 10) * 10);*/

            /*sensor.AddObservation(1 - Normalize(gameObject.transform.rotation.z, 0, 360) * 2);
            sensor.AddObservation(1 - Normalize(gameObject.transform.rotation.x, 0, 360) * 2);
            sensor.AddObservation(Vector3.one - NormalizeVector3(ball.transform.position - gameObject.transform.position, -3, 3) * 2);
            sensor.AddObservation(Vector3.one - NormalizeVector3(m_BallRb.velocity, -10, 10) * 2);*/

            sensor.AddObservation(10 - Normalize(gameObject.transform.rotation.z, 0, 360) * 20);
            sensor.AddObservation(10 - Normalize(gameObject.transform.rotation.x, 0, 360) * 20);
            sensor.AddObservation(Vector3.one*10 - NormalizeVector3(ball.transform.position - gameObject.transform.position, -3, 3) * 20);
            sensor.AddObservation(Vector3.one*10 - NormalizeVector3(m_BallRb.velocity, -10, 10) * 20);

        }
    }

    public static float Normalize(float val, float min, float max)
    {
        return Mathf.Clamp(((val - min) / (max - min)), 0, 1);
    }

    public static Vector3 NormalizeVector3(Vector3 val, float min, float max)
    {
        val.x = Mathf.Clamp(((val.x - min) / (max - min)), 0, 1);
        val.y = Mathf.Clamp(((val.y - min) / (max - min)), 0, 1);
        val.z = Mathf.Clamp(((val.z - min) / (max - min)), 0, 1);
        return val;
    }
```
",update tried scenario release python package getting similar time one provided dramatic effect learning custom environment clear difference learning speed training provided file alteration like public override void sensor normalize normalize normalize normalize normalize normalize normalize normalize public static float normalize float float min float return min min public static vector vector float min float min min min min min min return,issue,positive,positive,neutral,neutral,positive,positive
878438649,"@willawebb Yes, you are correct, ONNX does not support ConvLSTM.
What i was trying to say is, if you have a Keras/TF with ConvLSTM can you export it to ONNX?
That way I can test out locally if it does import :) 
Of course the ConvLSTM will be broken up into different layers, but it might just work without any work from our side.

Let me know",yes correct support trying say export way test locally import course broken different might work without work side let know,issue,negative,negative,negative,negative,negative,negative
875591188,"I gave up on this and moved on (both in terms of machine learning approaches and projects - I'm still working in Unity but not using MLAgents), so I'm mostly out of date on this now, but from what I'm aware, I haven't seen any notable improvements unfortunately @srlowe ",gave machine learning still working unity mostly date aware seen notable unfortunately,issue,negative,positive,positive,positive,positive,positive
875402231,"@lukemadera Thank you for your detailed insights here.  This has helped me a lot with expectations.  Since it's been almost a year since this discussion, please could you tell me - has the situation changed much since then in your opinion?",thank detailed lot since almost year since discussion please could tell situation much since opinion,issue,positive,positive,positive,positive,positive,positive
875216784,"Hi Alex, 

I apologise for the late reply. I also apologise for my own lack of due diligence, which I'm sure that you've already discovered yourself, as apparently the ONNX format itself does not support the ConvLSTM operator. I'll have to see about opening an issue over there first. Sorry for the bother.",hi late reply also lack due diligence sure already discovered apparently format support operator see opening issue first sorry bother,issue,negative,negative,neutral,neutral,negative,negative
873648033,"I am also new to ml agents and I am also having the same problem about the ""is_continuous_control"". I have been able to train the demo environments but when I go to add the brain model it still gives me the same problem.
",also new also problem able train go add brain model still problem,issue,negative,positive,positive,positive,positive,positive
873000191,"hi @andrewcoh (part 2) :D

Still hoping for some news from your team, either as accepted or rejected ",hi part still news team either accepted,issue,negative,neutral,neutral,neutral,neutral,neutral
870839329,@willawebb do you have a onnx model that uses ConvLSTM that you could share with us?,model could share u,issue,negative,neutral,neutral,neutral,neutral,neutral
870212898,"Thanks, closing for now. Feel free to repost if you run into it again.",thanks feel free repost run,issue,positive,positive,positive,positive,positive,positive
870207229,"Yes

On Tue, 29 Jun 2021, 05:36 Ervin T., ***@***.***> wrote:

> Hi @prasengan <https://github.com/prasengan>, have you resolved the issue?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/4313#issuecomment-870127797>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AH3CXDJFTD4ABUZC7LL77NTTVEFAXANCNFSM4PWZMP5Q>
> .
>
",yes tue wrote hi resolved issue reply directly view,issue,positive,positive,neutral,neutral,positive,positive
870163768,"> These curves actually look successful. It is expected for the loss (value/baseline) to go up as the agent learns, as the value of the reward is continuously increasing. You'll see the same for PPO as well. Once the agent converges on a behavior, the loss will go back down.

![image](https://user-images.githubusercontent.com/55425599/123724132-c2e76180-d8c6-11eb-83ce-7a05d7e77acf.png)
This is an example image of your Using-Tensorboard.md.
The reward is increasing but the loss is reduced, is this the opposite of what you said?
",actually look successful loss go agent value reward continuously increasing see well agent behavior loss go back image example image reward increasing loss reduced opposite said,issue,positive,positive,positive,positive,positive,positive
870129033,It should still be 0.26. This issue should be resolved  - feel free to reopen if you're still having issues.,still issue resolved feel free reopen still,issue,positive,positive,positive,positive,positive,positive
870128381,Closing this issue due to inactivity - feel free to reopen if you're still having issues.,issue due inactivity feel free reopen still,issue,positive,positive,positive,positive,positive,positive
870125954,"Closing this issue due to inactivity. If you're still having problems, feel free to post in the [Unity Forums](https://forum.unity.com/forums/ml-agents.453/) or repost the issue.",issue due inactivity still feel free post unity repost issue,issue,positive,positive,positive,positive,positive,positive
870125465,This has been fixed - feel free to repost the issue if you're still having issues. ,fixed feel free repost issue still,issue,positive,positive,positive,positive,positive,positive
870124941,"As of late 2020 (the Pytorch version of ML-Agents), this should be far, far less common. Closing this issue due to inactivity. If you're still having problems, feel free to post in the [Unity Forums](https://forum.unity.com/forums/ml-agents.453/) or repost the issue.",late version far far le common issue due inactivity still feel free post unity repost issue,issue,positive,negative,neutral,neutral,negative,negative
870124069,"Hi @brandonhotdog, closing this issue due to inactivity. If you're still having problems, feel free to repost the issue. ",hi issue due inactivity still feel free repost issue,issue,positive,positive,positive,positive,positive,positive
870123697,"Hi @Ro2Be, I'm closing this issue due to inactivity. If you're still having issues, feel free to post on the Unity Forums. ",hi robe issue due inactivity still feel free post unity,issue,positive,positive,positive,positive,positive,positive
870121170,"Thanks for reporting this bug - i've recorded it in our internal tracker as MLA-2093, and it should be fixed in an upcoming release.",thanks bug internal tracker fixed upcoming release,issue,negative,positive,positive,positive,positive,positive
870118968,"The raycast thing is a known issue, but it's also not a bug. If it wasn't the case, then the rays would never leave the agent! 

You're right, the --force and --resume can get a bit confusing. I've noted it down as a feature request. The intended use is not to use --force and just change the run id. Would it meet your needs to have the repeat behavior you specify above as the default behavior when no flags are specified?",thing known issue also bug case would never leave agent right force resume get bit noted feature request intended use use force change run id would meet need repeat behavior specify default behavior,issue,negative,positive,positive,positive,positive,positive
870117709,"These curves actually look successful. It is expected for the loss (value/baseline) to go up as the agent learns, as the value of the reward is continuously increasing. You'll see the same for PPO as well. Once the agent converges on a behavior, the loss will go back down. ",actually look successful loss go agent value reward continuously increasing see well agent behavior loss go back,issue,positive,positive,positive,positive,positive,positive
869716029,"The new environments look good, will you merge them?",new look good merge,issue,negative,positive,positive,positive,positive,positive
869378101,"Here's my workaround for hiding the inspector:
```
if (Application.isPlaying)
{
    var tracker = UnityEditor.ActiveEditorTracker.sharedTracker;
    var editors = tracker.activeEditors;
    for (int i = 0; i < editors.Length; i++)
    {
        // Can't check type because BehaviorParametersEditor is internal.
        if (editors[i].ToString() == "" (Unity.MLAgents.Editor.BehaviorParametersEditor)"")
        {
            tracker.SetVisible(i, 0);
        }
    }
}
```",inspector tracker ca check type internal,issue,negative,neutral,neutral,neutral,neutral,neutral
868884153,"Yeah, I can see it being super valuable to many Unity-related use-cases! I'll forward this to the Barracuda team and see what they think. ",yeah see super valuable many forward barracuda team see think,issue,positive,positive,positive,positive,positive,positive
868864604,@robinerd Definitely would be interested in taking a look at your implementation. I think it's worth a PR!,definitely would interested taking look implementation think worth,issue,positive,positive,positive,positive,positive,positive
868435744,"Yes, I'd be very interested! Don't worry about making your code look nice, just a gist or something just to see your implementation would be fine.

I have actually implemented this already myself, but it isn't working perfectly, so it would be good to see your solution.

I also have some (crude) python code for implementing this for curriculum of that's interesting for you.",yes interested worry making code look nice gist something see implementation would fine actually already working perfectly would good see solution also crude python code curriculum interesting,issue,positive,positive,positive,positive,positive,positive
868385522,"Hey ervteng,

You're correct in your understanding of their purpose. I don't know the complexity of adding this feature, but I genuinely think it would be a fantastic add. While it may not fit the current use-cases of ML-Agents, some of the most spectacular demos that I've seen of barracuda's capabilities, such as the [style-transfer demo](https://blogs.unity3d.com/2020/11/25/real-time-style-transfer-in-unity-using-deep-neural-networks/), are the sorts of things that could benefit from what a ConvLSTM has to offer. 

The use-case that I wanted to use this for was as part of a saliency model to help actively predict where a user might look next if they're using an eye-tracking VR headset. I can do it with a normal LSTM though it is much less performant and accurate, and without that layer then the predictions can only use the current frame for reference.",hey correct understanding purpose know complexity feature genuinely think would fantastic add may fit current spectacular demo seen barracuda could benefit offer use part model help actively predict user might look next headset normal though much le performant accurate without layer use current frame reference,issue,positive,positive,positive,positive,positive,positive
867347863,"Hey @mmattar , thanks for your reply. I know about parameter randomization, but I don't necessarily want to randomize between certain values. I would like to alter a parameter during training. I'll give an example;

Let's say for instance the wall-jump example.  Instead of having three different heights for the wall as in no wall, low wall and high wall, you could increase the wall height by a small amount every successful episode. I know this is possible to program into the environment itself, which is how I'm doing it right now, but this doesn't allow me to resume training without rebuilding the executable.

Please correct me if I'm wrong and this is already possible with parameter randomization.",hey thanks reply know parameter randomization necessarily want randomize certain would like alter parameter training give example let say instance example instead three different wall wall low wall high wall could increase wall height small amount every successful episode know possible program environment right allow resume training without executable please correct wrong already possible parameter randomization,issue,positive,positive,neutral,neutral,positive,positive
867029552,"Hi @willawebb, it's definitely possible for us to add ConvLSTMs. From my understanding they primarily benefit sequential image-type applications (e.g. video)? The vast majority of ML-Agents use-cases don't use visual observations, so it isn't currently on our roadmap. ",hi definitely possible u add understanding primarily benefit sequential video vast majority use visual currently,issue,positive,positive,neutral,neutral,positive,positive
866242536,"Hi, 
Sorry I am not following. I think it is because I do not know what the problem you are trying to solve is. If you want to know how many episodes were completed, could you use `SUM` for aggregation ? 
Maybe you could make a PR with the suggested change so we can review it with code.
I doubt the cumulative most recent value of all envs will give the correct count of episodes since the most recent could be overwritten (hence you will not have the exact count).",hi sorry following think know problem trying solve want know many could use sum aggregation maybe could make change review code doubt cumulative recent value give correct count since recent could hence exact count,issue,negative,positive,neutral,neutral,positive,positive
866214020,"Hey, if I try to log the current episode count using StatsRecorder, and us AVERAGE as the SummaryMethode, all numbers of all workers reported since the last summary will be averaged and returned. 

As an example let's take a Unity Instance containing 16 independent agents, that try to learn using each there own copy of an Environment. You can collect the cummulative episode count of all those agents in the same unity instance by using a single instance of an own class called recorder.cs that only collects those episode counts from all agents and every 100 fixedUpdate steps sends it to mlagents using StatsRecorder. 

If the summary frequence in the yaml file is set to 10000, and within 100 fixedUpdate steps each agent completes 2 episodes, this would mean after 100 fixedUpdate steps, Recorder.cs would use StatsRecorder to report 32 completed episodes or after 10000 fixedUpdate steps a total count of 3200. 

If StatsRecorder would use the StatAggregationMethod AVERAGE all numbers reported for the episode count would be averaged including the 32 from the beginning, instead of the reporting the correct number of 3200. 

MOST_RECENT would be correct in this case, but if you were to use 16 parallel workers or unity instances the number of MOST_RECENT would only report the number of the first worker. In Tensorboard the resulting graph would suggest that using the same buffer size, the number of episodes contained in the buffer (from which ppo is sampling its learning data) is lower then with one worker. 

This would mean to the untrained eye, that ppo may not have seen data from enough episodes to generalize during learning. If the workers don't preduce the same episode count within the summary frequence, which can happen due to statistical variance in the episode length or hardware limitations, just multiplying the number of MOST_RECENT by the num_envs will lead to a false episode count as well. 

Only the cummulative most recent value of all envs or all Recorder.cs instances will give the correct count of episodes. 

The total episode count of all instances will be helpful in determing the speed up generated by using num_envs > 1 and help to determine the required buffersize. The number of total steps can not provide this information as easily, since the episode length will change during training",hey try log current episode count u average since last summary returned example let take unity instance independent try learn copy environment collect episode count unity instance single instance class episode every summary frequence file set within agent would mean would use report total count would use average episode count would beginning instead correct number would correct case use parallel unity number would report number first worker resulting graph would suggest buffer size number buffer sampling learning data lower one worker would mean untrained eye may seen data enough generalize learning episode count within summary frequence happen due statistical variance episode length hardware multiplying number lead false episode count well recent value give correct count total episode count helpful speed help determine number total provide information easily since episode length change training,issue,positive,negative,neutral,neutral,negative,negative
866128550,"Hey, sorry for the late reply. Why won't average and most recent not give a correct number? According to [this code](https://github.com/Unity-Technologies/ml-agents/blob/main/ml-agents/mlagents/trainers/agent_processor.py#L424) average will be the average across all workers and most recent will be the latest value from worker 0 only (to avoid collisions).
I logged the request for a  Most_Recent_Parallel_Sum StatAggregationMethod.
(For internal tracking : MLA-2062)",hey sorry late reply wo average recent give correct number according code average average across recent latest value worker avoid logged request internal,issue,negative,negative,neutral,neutral,negative,negative
866102249,Thanks @pavel7876 - have you been successful with training any of our demo environments? I'd like to first weed out any installation issues. Thanks.,thanks successful training like first weed installation thanks,issue,positive,positive,positive,positive,positive,positive
866075722,"Hey @kkalera - we support environment randomization. Can you check out this documentation and let us know if it acheives what you're looking for?
- https://github.com/Unity-Technologies/ml-agents/blob/main/docs/ML-Agents-Overview.md#training-robust-agents-using-environment-parameter-randomization
- https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-ML-Agents.md#environment-parameter-randomization",hey support environment randomization check documentation let u know looking,issue,negative,neutral,neutral,neutral,neutral,neutral
864401026,"Hello mmattar! I use 1.6.0 version of ML agents on windows. No, I only trained my own models. I use a YAML file of release 2. ",hello use version trained use file release,issue,negative,neutral,neutral,neutral,neutral,neutral
863606606,"Thanks @robinerd - this is a very reasonable request and I've filed it internally for us to prioritize. For reference - it's tagged under MLA-2056. We also welcome contributions to the project, so feel free to let us know if you'd like to contribute and we can offer guidance and review your PR. In the meantime, a hacky workaround would be to: 

Start training your new scene without `initialize-from` quitting immediately, and then copying that folder (results/<run-id>/<behavior-name>) into the run that is being pointed to in --initialize-from

I realize that this isn't a great short-term solution, but sharing it in case it helps you test an idea or two.",thanks reasonable request internally u reference tagged also welcome project feel free let u know like contribute offer guidance review hacky would start training new scene without immediately folder run pointed realize great solution case test idea two,issue,positive,positive,positive,positive,positive,positive
863587970,"Hi @pavel7876 - can you share a few more pieces of information:
- which ML-Agents version are you using?
- which platform (windows, mac, linux) are you training on?
- Have you tried training ML-Agents on any of our sample environments?
- I presume the environment you shared is a new one that you created. Which YML file are you using? Can you paste its contents?",hi share information version platform mac training tried training sample presume environment new one file paste content,issue,negative,positive,positive,positive,positive,positive
863515901,"I don't think that writing everything in c# is the solution but I would also like to have ability to train custom brain at runtime. So that I can let players train their own custom brains. 
+1 for runtime-learning: MLA-1445",think writing everything solution would also like ability train custom brain let train custom brain,issue,positive,neutral,neutral,neutral,neutral,neutral
860495556,"It is my bad, most of the observations are passed by the PhysicsSensor attached directly to the body parts, that's why CollectObservations only collect a 32 dimensional vector. Sorry !",bad attached directly body collect dimensional vector sorry,issue,negative,negative,negative,negative,negative,negative
860189808,When I add model every ML agent don't do any thing. ,add model every agent thing,issue,negative,neutral,neutral,neutral,neutral,neutral
859313705,"I have made an implementation of the logging part for splitting rewards into categories and charting them in tensorboard. Basically modded Agent to keep rewards separately in a dictionary, not super complicated and nothing in Python. 

If there is any interest in a pull request or sharing my code I can clean it up and share later today or in the weekend. Your idea about curriculum steps sounds great too.",made implementation logging part splitting charting basically agent keep separately dictionary super complicated nothing python interest pull request code clean share later today weekend idea curriculum great,issue,positive,positive,positive,positive,positive,positive
857298532,"Hi @andrewcoh 
This problem occurs when I want to learn with my own deep learning model, not using  ppo, sac given in the ml agents git repo.
In the current release 14 version, custom DQN seems to be impossible in addition to ppo, sac, and imitation learning. I'm a little confused, Is it right I know?
Therefore, I found old versions of projects such as mlagents toolkit v0.4, v0.6 and wanted to use them. Of course, I know it won't work in my excable Unity environment, but I thought communication would be possible if I defaulted on the player.
However, I got only the above error message. 

To sum up, my question is as follows.
1. If I want to learn through custom DQN, should I use an older version such as mlagents toolkit v0.4?
2. According to your development, the network structure that can be changed in config.yaml in Release 14 is limited. Can I change the network structure by any chance?",hi problem want learn deep learning model sac given git current release version custom impossible addition sac imitation learning little confused right know therefore found old use course know wo work unity environment thought communication would possible player however got error message sum question want learn custom use older version according development network structure release limited change network structure chance,issue,negative,negative,neutral,neutral,negative,negative
856902839,"Hi @hyuDev 

This means your environment is not communicating with python. The behavior type on your agent should be set to `default` and also make sure your agent has a decision requester component (or you are calling RequestDecision in the agent script). If these things are alright, please send more information e.g. any console output from the editor and terminal.",hi environment communicating python behavior type agent set default also make sure agent decision requester component calling agent script alright please send information console output editor terminal,issue,positive,positive,positive,positive,positive,positive
856900905,"Yes, you are correct. Thank you for pointing this out. It was an error introduced when shifting soccer to the group manager. I have fixed this here https://github.com/Unity-Technologies/ml-agents/pull/5424",yes correct thank pointing error shifting soccer group manager fixed,issue,negative,positive,neutral,neutral,positive,positive
856321082,"hey @ohernpaul this does sounds like a strange issue. Can you share more about your scene and sensor setup? Grid sensor settings (size/tags/compression)? Did you made your own custom grid sensor or you're using the original one? What happens when the agent dies, which might be triggering the error?

Lifting the agent up off the ground or detecting nothing shouldn't be a problem, and it'll be just a zero observation. Does it happen constantly (i.e. when it detects nothing it throws errors)? 

I tried playing a bit with the grid sensor but couldn't find a way to reproduce the error. Since this does look concerning, it would be very helpful if you can describe your scene with more details or upload a minimal project/model that can repro the errors, thanks.",hey like strange issue share scene sensor setup grid sensor made custom grid sensor original one agent might error lifting agent ground nothing problem zero observation happen constantly nothing tried bit grid sensor could find way reproduce error since look concerning would helpful describe scene minimal thanks,issue,positive,positive,neutral,neutral,positive,positive
856298478,"No I'm not disabling the agents, learned the hard way on that a few months back.

The only bits of additional information I can provide right now is that:

I confirmed that the issues source is grid sensor -> removed all observations from agent and trained for a few thousand steps only with grid sensor.

The issue also happens when I load the model into the agent (inference) and lift the agent up off the ground so that the two detectable tags (road, obstacles) are not visible. The moment the grid has nothing to read it breaks and throws the off axis error.

 It's very strange - I tried to reproduce this with the food collector example by training from scratch for 100,000 steps (arbitrary amount). I loaded the model into one of the agents and lifted it off the ground and it did not throw the off axis error.

I have since given up on using the grid sensor even though the version in release 17 is much better than it's ever been (major props). 

Want to say that I appreciate what you all have made and that you continue to develop this project so rapidly.",learned hard way back additional information provide right confirmed source grid sensor removed agent trained thousand grid sensor issue also load model agent inference lift agent ground two detectable road visible moment grid nothing read axis error strange tried reproduce food collector example training scratch arbitrary amount loaded model one ground throw axis error since given grid sensor even though version release much better ever major prop want say appreciate made continue develop project rapidly,issue,positive,positive,positive,positive,positive,positive
856243536,note: edited the original post to use triple-backticks,note original post use,issue,negative,positive,positive,positive,positive,positive
856241418,"Hi @ohernpaul 

When the agent dies, are you disabling it i.e. calling `SetActive(false)`? Can you describe the other scenarios where this happens?",hi agent calling false describe,issue,negative,negative,negative,negative,negative,negative
855457036,"@julienroyd I tried to register my environment following your code but i get an error when trying to use vectorized environments
```
Traceback (most recent call last):
  File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap
    self.run()
  File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/ros/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py"", line 24, in _worker
    env = env_fn_wrapper.var()
  File ""unity_test_vectrorized.py"", line 19, in _init
    env = gym.make(env_id)
  File ""/home/ros/.local/lib/python3.8/site-packages/gym/envs/registration.py"", line 145, in make
    return registry.make(id, **kwargs)
  File ""/home/ros/.local/lib/python3.8/site-packages/gym/envs/registration.py"", line 89, in make
    spec = self.spec(path)
  File ""/home/ros/.local/lib/python3.8/site-packages/gym/envs/registration.py"", line 131, in spec
    raise error.UnregisteredEnv('No registered env with id: {}'.format(id))
gym.error.UnregisteredEnv: No registered env with id: BirdSingleAgentNoStackedNoVisual-v1
```",tried register environment following code get error trying use recent call last file line file line run file line file line file line make return id file line make spec path file line spec raise registered id id registered id,issue,negative,neutral,neutral,neutral,neutral,neutral
855315485,"For log continuuity:
I replied to Ademord on an [issue](https://github.com/laszukdawid/ai-traineree/issues/10) they created in my deep [reinforcement learning repository](https://github.com/laszukdawid/ai-traineree/). I'm happy to assist with things I can assist.",log issue deep reinforcement learning repository happy assist assist,issue,positive,positive,positive,positive,positive,positive
855256935,"Glad to see a lot of feedback in this thread. I'd add a couple more things. 

If the gym API is too slow, one thing to consider is to use the vectorize environment API. This is an approach that is being done by [procgen](https://github.com/openai/procgen), [gym-microrts](https://github.com/vwxyzjn/gym-microrts) and others. 

So using the gym API with SB3, it looks like the following

```python
import gym
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import VecMonitor, VecVideoRecorder, DummyVecEnv
env = DummyVecEnv([lambda: gym.make(""procgen:starpilot"")])
# Record the video starting at the first step
env = VecVideoRecorder(env, 'logs/videos/',
                       record_video_trigger=lambda x: x == 0, video_length=100)
# Wrap with a VecMonitor to collect stats and avoid errors
env = VecMonitor(env=env)
model = PPO(""MultiInputPolicy"", env, verbose=1)
model.learn(10000)
```

Whereas using the vectorized environment API, it looks like this

```diff
- import gym
+ from procgen import ProcgenEnv
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import VecMonitor, VecVideoRecorder, DummyVecEnv
# ProcgenEnv is already vectorized
- env = DummyVecEnv([lambda: gym.make(""procgen:starpilot"")])
+ env = ProcgenEnv(num_envs=2, env_name='starpilot')
# Record the video starting at the first step
env = VecVideoRecorder(env, 'logs/videos/',
                       record_video_trigger=lambda x: x == 0, video_length=100)
# Wrap with a VecMonitor to collect stats and avoid errors
env = VecMonitor(env=env)
model = PPO(""MultiInputPolicy"", env, verbose=1)
model.learn(10000)
```

## DQN compatibility

When setting `num_envs=1`, this vectorize environment would also work with DQN from SB3.

## Potential API design


```python
import gym_unity
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import VecMonitor, VecVideoRecorder
# ProcgenEnv is already vectorized
env = gym_unity.VecEnv(num_envs=2, env_name='GridWorldPixels')
# Record the video starting at the first step
env = VecVideoRecorder(env, 'logs/videos/',
                       record_video_trigger=lambda x: x == 0, video_length=100)
# Wrap with a VecMonitor to collect stats and avoid errors
env = VecMonitor(env=env)
model = PPO(""MultiInputPolicy"", env, verbose=1)
model.learn(10000)
```
",glad see lot feedback thread add couple gym slow one thing consider use environment approach done gym like following python import gym import import lambda record video starting first step wrap collect avoid model whereas environment like import gym import import import already lambda record video starting first step wrap collect avoid model compatibility setting environment would also work potential design python import import import already record video starting first step wrap collect avoid model,issue,negative,positive,neutral,neutral,positive,positive
855255613,"@andrewcoh we should have a collab for adapting an external algorithm into the UE API if there will be no efforts to push the gym_wrapper to be fully compatible with gym. And special emphasis on not using a multi-branch approach when creating an environment should be done *somewhere*, since adapting an external algorithm to use MutliDiscrete is too much overhead, and flatten_branched is returning 27 instead of 9 for me (3 discrete branches with 3 possible values). ",external algorithm push fully compatible gym special emphasis approach environment done somewhere since external algorithm use much overhead instead discrete possible,issue,negative,positive,positive,positive,positive,positive
855254795,I insist on pushing the gym wrapper to be fully compatible with gym so we can all use the algorithms that are already implemented outside. It takes too much overhead adapting every single one of them TO the UnityEnvironment API. ,insist pushing gym wrapper fully compatible gym use already outside much overhead every single one,issue,negative,positive,neutral,neutral,positive,positive
855254316,I agree on making the UnityEnvironment API *fully* compatible with GymWrapper. ,agree making fully compatible,issue,negative,neutral,neutral,neutral,neutral,neutral
855253869,@laszukdawid can you provide a simple collab to learn how to use your wrapper? i am in the dark with the Python API and Gym Wrapper's outdated documentation.,provide simple learn use wrapper dark python gym wrapper outdated documentation,issue,negative,negative,negative,negative,negative,negative
855253516,"I also come here to bump that the Python API and the Gym Wrapper don't get enough love. Baselines isn't supporting TF2 and the docs are obviously out of date (should update to stable-baselines3, for example). ",also come bump python gym wrapper get enough love supporting obviously date update example,issue,positive,positive,positive,positive,positive,positive
855155579,My bad. The document I tried to reference is here : https://github.com/Unity-Technologies/ml-agents/blob/main/ml-agents-envs/mlagents_envs/side_channel/stats_side_channel.py,bad document tried reference,issue,negative,negative,negative,negative,negative,negative
854907571,"Hi @vincentpierre,

I would like to try this, but currently when i follow this link i only get a 404 error. Maybe the link is incorrect or you are referrencing a different branch. I tried to see if i can find a file in the colab folder, but i couldn't find one. I hope you can help me find the file you referrenced.",hi would like try currently follow link get error maybe link incorrect different branch tried see find file folder could find one hope help find file,issue,positive,neutral,neutral,neutral,neutral,neutral
853481374,"I'm having the same issue - do I have to download the main (unstable) release to get access to this, or do I have to upgrade through the package manager from release 17's folder?

EDIT: you have to go to the package manager -> unity registry -> find barracuda -> dropdown -> select the version and upgrade",issue main unstable release get access upgrade package manager release folder edit go package manager unity registry find barracuda select version upgrade,issue,negative,positive,positive,positive,positive,positive
853420896,"@wesley
edit: sorry, tagged the wrong person :)",edit sorry tagged wrong person,issue,negative,negative,negative,negative,negative,negative
853161072,"Hi @jziegle4 
You can receive the stats using this [side channel](https://github.com/Unity-Technologies/ml-agents/blob/main/ml-agents-envs/mlagents_envs/side_channel/stats_side_channel.py) when you register it with your UnityEnvironment it will receive the messages from the Unity side StatsReporter",hi receive side channel register receive unity side,issue,negative,neutral,neutral,neutral,neutral,neutral
853056173,"I will try that, but how can i connect the StatsRecorder on Unity side to a custom side channel on Python side?
As i see in [https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Custom-SideChannels.md](url) i have to reference the side channel using a ID. So far I could not find out how to connect StatsRecorder with a sidechannel. I hope you can help me with that?",try connect unity side custom side channel python side see reference side channel id far could find connect hope help,issue,positive,positive,neutral,neutral,positive,positive
852599210,"I have already increased the time scale of the executable.
While it has increased, the environment is still a tad slow on env.reset() and env.step(). Is there a way to enable more threads for the server build? 

Also does it affect that I am running the UnityEnvironment on Linux?",already time scale executable environment still tad slow way enable server build also affect running,issue,negative,negative,negative,negative,negative,negative
852468119,"I think the simplest way to solve your problem is to use a [StatsReporter](https://github.com/Unity-Technologies/ml-agents/blob/main/ml-agents/mlagents/trainers/stats.py#L289) with a [TensorboardWriter](https://github.com/Unity-Technologies/ml-agents/blob/main/ml-agents/mlagents/trainers/stats.py#L213) and call add_stats and write stats when the side channel receives information. Putting this logic inside of UnityEnvironment is challenging, because not all users will want their stats reported at the same time or in the same manner.",think way solve problem use call write side channel information logic inside want time manner,issue,negative,neutral,neutral,neutral,neutral,neutral
852398147,"Hi @vincentpierre

I'd like to use the same executable with mlagents PPO variant and with DQN like the Rainbow variant in Ray RLlib. Currently its easy for me to use Statsrecorder to gather statistical data abount the agent and directly plot that in Tensorboard without any changes in the python code. But when i use my own DQN i have to change that, because i can't log the data into tensorboard by default. In this case i have to either disable StatsRecorder or implement an own custom sidechannel both in C# as well as in python. In libaries like Ray RLlib most of the tensorboard logging is handled internally so its harder to access and adapt. With that in mind I'm searching for a way to easily use Statsrecorder to log my data into Tensorboars while using other RL libaries that are harder to adapt. If there is a way to make that happen using the existing Side Channel APi i'd be happy to use that otherwise it would be nice change the API of mlagents_env so it can handle StatsRecorder to Tensorboard",hi like use executable variant like rainbow variant ray currently easy use gather statistical data agent directly plot without python code use change ca log data default case either disable implement custom well python like ray logging handled internally harder access adapt mind searching way easily use log data harder adapt way make happen side channel happy use otherwise would nice change handle,issue,positive,positive,positive,positive,positive,positive
852353331,"Hi @jziegle4 
I was able to reproduce this issue. I have a potential fix here #5410
Let us know if it resolves your issue.",hi able reproduce issue potential fix let u know issue,issue,negative,positive,positive,positive,positive,positive
852298892,"One way to speed up the UnityEnvironment is to increase the time scale of the executable. This could result in instabilities of the environment, so you need to use it carefully.
When creating the UnityEnvironment, you need to specify an [Engine configuration side channel](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Python-API.md#engineconfigurationchannel) to modify the engine configuration (time scale and others)",one way speed increase time scale executable could result environment need use carefully need specify engine configuration side channel modify engine configuration time scale,issue,negative,negative,neutral,neutral,negative,negative
852297033,"Hi @jziegle4 
I am not sure I understand the request. mlagents_envs does not send any information to TensorBoard on purpose because a lot of users do not need to use TensorBoard and don't even want to import it. Adding a dependency on TensorBoard in both mlagents_envs and mlagents would increase complexity. Using side channels allows for more flexibility since the data can be logged into anything from mlagents_envs and not just into TensorBoard. 
Is there an issue with the side channel API? Can you suggest some changes it its API to make it easier to use?  ",hi sure understand request send information purpose lot need use even want import dependency would increase complexity side flexibility since data logged anything issue side channel suggest make easier use,issue,positive,positive,positive,positive,positive,positive
852290758,"Hi @sarmientoj24 
This is not using the proper template for feature requests and I think this question is more appropriate for the [forum](https://forum.unity.com/forums/ml-agents.453/)
You can change the port when using the `mlagents-learn` command using the command line argument `--base-port`. See [here](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-ML-Agents.md#training-using-concurrent-unity-instances) for more information. ",hi proper template feature think question appropriate forum change port command command line argument see information,issue,negative,positive,positive,positive,positive,positive
852288581,"Hi @sarmientoj24 
This does not seem to be a bug, please use the appropriate template for either bugs or feature requests. Other issues should go on the forum.
To solve you problem, you can try to create a [custom side channel](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Custom-SideChannels.md) or you could use the `FloatPropertiesSideChannel` (different from `EnvironmentParametersChannel`) since this one allows for floats to also go from Unity to Python (allowing Python to know what properties are expected)",hi seem bug please use appropriate template either feature go forum solve problem try create custom side channel could use different since one also go unity python python know,issue,negative,positive,positive,positive,positive,positive
852283739,"Hi @nathan60107 
This is intended behavior. An episode is a sequence of steps that ended (either because the Agent was done or because it was interrupted). The min_lesson_length is in number of episodes (not number of steps) because we allow to use cumulative reward as a measure. Since the new lesson shows up at step 20,000 when the min_lesson_length is 1,000 it means that your average episode length is around 20 (you can double check on TensorBoard).",hi intended behavior episode sequence ended either agent done interrupted number number allow use cumulative reward measure since new lesson step average episode length around double check,issue,positive,negative,neutral,neutral,negative,negative
849157180,"There appears to be a bug in `cattrs` 1.7. Needs further investigation, cap is probably the best call for now. ",bug need investigation cap probably best call,issue,positive,positive,positive,positive,positive,positive
848234843,"Hey @TheTrope, the Barracuda package has been updated to 2.0.0-pre4, which should fix this issue. Thanks for reporting! ",hey barracuda package fix issue thanks,issue,negative,positive,positive,positive,positive,positive
847548832,"The training time is spent on two parts, running inference and updating model. Running more parallel instance helps speed up the inference part. As you speed up more and more on one part, the other will become the main bottleneck and you'll no longer getting more speed up. You can look at the `timers.json` file in your results folder to see where the training time is bottlenecked on. 

As for the CPU utilization, it really varies with different machine specifications and the environment you run. We are actively working on looking into resource utilizations and how to improve that, in order to give a better guide on how to improve training speed.

Since your original issue has been resolved, I'll close this issue. If you have any more questions, [our forum](https://forum.unity.com/forums/ml-agents.453/) is a better place for asking questions and general discussions. I would suggest you open a post there if you have more questions regarding your project, thanks.",training time spent two running inference model running parallel instance speed inference part speed one part become main bottleneck longer getting speed look file folder see training time utilization really different machine environment run actively working looking resource improve order give better guide improve training speed since original issue resolved close issue forum better place general would suggest open post regarding project thanks,issue,positive,positive,positive,positive,positive,positive
847492897,"Oh, sorry for not using the correct templete! 
But I find it works successfully now! Maybe cause I reboot the computer.
Anyway, thank for your reply! 
But I also find some problems in efficiency. The following pics are corresponding to the --num-envs equals 1,2,3,4:
![image](https://user-images.githubusercontent.com/47738954/119432012-192f1680-bd03-11eb-8844-5a92f7e70307.png)
![image](https://user-images.githubusercontent.com/47738954/119432024-1f24f780-bd03-11eb-8c64-5fc36b21629b.png)
![image](https://user-images.githubusercontent.com/47738954/119432031-23e9ab80-bd03-11eb-9876-6aca9e0f474a.png)
![image](https://user-images.githubusercontent.com/47738954/119432044-2946f600-bd03-11eb-8533-41d6b1608a90.png)

Well....I just want to ask why their training efficiency seem not to improve obviously? Can you plz give some advice for improve training speed in --num-envs?
Also, I find whatever num I choose, the CPU working rate seems to not change (I use CPU for training). Does the training can only use limited CPU resources?
![image](https://user-images.githubusercontent.com/47738954/119433268-72984500-bd05-11eb-9331-aa75346b3285.png)


My Unity version is 2020.2.6f1c1, OS is win10 and the ml-agents versions is 0.26.0.
Thanks your relpy again. : )
",oh sorry correct find work successfully maybe cause computer anyway thank reply also find efficiency following corresponding image image image image well want ask training efficiency seem improve obviously give advice improve training speed also find whatever choose working rate change use training training use limited image unity version o win thanks,issue,positive,positive,positive,positive,positive,positive
847290971,"Some update notes after talking to other folks on the team: We don't currently support display rendering for Gym Wrapper. This is something we're aware of but haven't gotten to it yet. 

To fix this it will require a rather bigger change in the code and we don't have a timeline on that. But for now I'll update the documents to clear the confusion.",update talking team currently support display rendering gym wrapper something aware gotten yet fix require rather bigger change code update clear confusion,issue,negative,positive,neutral,neutral,positive,positive
847286837,"We've fixed the installation links, it should be working now.",fixed installation link working,issue,negative,positive,neutral,neutral,positive,positive
847247723,Hi @mattdeitke Thanks for raising this - we're looking into this and will fix it as soon as possible.,hi thanks raising looking fix soon possible,issue,negative,positive,neutral,neutral,positive,positive
847230903,"@BYJRK @vinicassol Please make sure that the versions of the python package and Unity plugin are compatible with each other. In particular, the v0.26.0 python release (and later) will generate model files that are incompatible with 1.x versions of `com.unity.ml-agents`. If you can't upgrade to `com.unity.ml-agents` 2.0, you'll need to downgrade your version of the python packages to something between 0.16.1 and 0.20.0.

These new errors are separate from the original issue; I'm going to close and lock this issue. Please submit a new bug report (with all the relevant version information) if you're still having problems.",please make sure python package unity compatible particular python release later generate model incompatible ca upgrade need downgrade version python something new separate original issue going close lock issue please submit new bug report relevant version information still,issue,positive,positive,positive,positive,positive,positive
847219809,"Hi,

Please follow our issue template to report an issue, since we need the information to help look into your issue.
Can you provide your ML-Agents version and the environment you are using (our example environment/your own project)?

Also are you using visual observations? It will be helpful if you can provide the details and step by step command for us to reproduce the problem.

When you say env.render() doesn't really do anything, are you talking about the scene rendering on your screen or env.render() didn't return what you want?",hi please follow issue template report issue since need information help look issue provide version environment example project also visual helpful provide step step command u reproduce problem say really anything talking scene rendering screen return want,issue,positive,positive,neutral,neutral,positive,positive
847207845,"Hi,

Please follow our issue template to report an issue, since we need the information to help look into your issue.

Can you provide your Unity Version, OS, and ML-Agents version?

Also just to clarify, you are using our 3dball example, and the same command works with num-envs=1 but doesn't work with --num-envs=2, is that correct?
When it's blocked, did you see two Unity game instances being launched?",hi please follow issue template report issue since need information help look issue provide unity version o version also clarify example command work work correct blocked see two unity game,issue,negative,negative,negative,negative,negative,negative
847179929,"Hi, any updates about the NullReferenceException? Just trained an agent today but I'm not able to drag the .onx.

```
NullReferenceException: Object reference not set to an instance of an object
Unity.MLAgents.Inference.BarracudaModelParamLoader.CheckModel (Unity.Barracuda.Model model, Unity.MLAgents.Policies.BrainParameters brainParameters, Unity.MLAgents.Sensors.SensorComponent[] sensorComponents, Unity.MLAgents.Policies.BehaviorType behaviorType) (at Library/PackageCache/com.unity.ml-agents@1.0.7/Runtime/Inference/BarracudaModelParamLoader.cs:152)
Unity.MLAgents.Editor.BehaviorParametersEditor.DisplayFailedModelChecks () (at Library/PackageCache/com.unity.ml-agents@1.0.7/Editor/BehaviorParametersEditor.cs:108)
Unity.MLAgents.Editor.BehaviorParametersEditor.OnInspectorGUI () (at Library/PackageCache/com.unity.ml-agents@1.0.7/Editor/BehaviorParametersEditor.cs:68)
UnityEditor.UIElements.InspectorElement+<>c__DisplayClass59_0.<CreateIMGUIInspectorFromEditor>b__0 () (at /Users/bokken/buildslave/unity/build/External/MirroredPackageSources/com.unity.ui/Editor/Inspector/InspectorElement.cs:546)
UnityEngine.GUIUtility:ProcessEvent(Int32, IntPtr, Boolean&) (at /Users/bokken/buildslave/unity/build/Modules/IMGUI/GUIUtility.cs:189)
```
",hi trained agent today able drag object reference set instance object model,issue,negative,positive,positive,positive,positive,positive
846457543,"Thanks for pointing this out, it was mistakenly duplicated.

We'll fix it soon, thanks!",thanks pointing mistakenly fix soon thanks,issue,positive,positive,positive,positive,positive,positive
844756347,"Sorry I don't understand Japanese, but from what I see there's something missing.

You'll need to add the Agent component first, and then add the behavior parameters.
The Agent component is the script inherit from the Agent class that implements your agent. I think the RollerAgent in your tutorial is just a game object named ""RollerAgent"".

I think the website is coming from our official roller ball tutorial. Here is the most up-to-date version (in English) that you can follow step by step: https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Create-New.md",sorry understand see something missing need add agent component first add behavior agent component script inherit agent class agent think tutorial game object think coming official roller ball tutorial version follow step step,issue,negative,negative,negative,negative,negative,negative
844664995,"Thanks for the reply.
I was not able to solve it.

I'm sorry to say that this is a Japanese site, but I made it based on the following site.
https://note.com/npaka/n/n660d5e9f7c43#tgqfT
I am using unity 2018 in this site.
I am using unity 2020.3.8f, ML-agents 2.0.0-pre3 (release 17).
I added the RollerAgent from the Hierarchy and Add Component from the Inspector just like the RigitBody.",thanks reply able solve sorry say site made based following site unity site unity release added hierarchy add component inspector like,issue,positive,positive,neutral,neutral,positive,positive
844631982,"> Hi,
> 
> Have you make sure your behavior parameter component is attached to the right game object? It should be attached to the same object as your Agent.

返信ありがとうございます．
私はそれを解決することができませんでした．

日本語のサイトで申し訳ないですが，以下のサイトを参考に作成しています．
https://note.com/npaka/n/n660d5e9f7c43#tgqfT
このサイトではunity 2018を使用しています．
私はunity 2020.3.8f，ML-agents 2.0.0-pre3(リリース17)を使用しています．
HierarchyからRollerAgentを追加し，RigitBodyと同じようにInspectorからAdd Componentしています．",hi make sure behavior parameter component attached right game object attached object agent,issue,negative,positive,positive,positive,positive,positive
843326008,"Imitation is still supported as GAIL and Behavior Cloning. Check out the docs here: https://github.com/Unity-Technologies/ml-agents/blob/release_17_docs/docs/ML-Agents-Overview.md#imitation-learning

Also, the [forums](https://forum.unity.com/forums/ml-agents.453/) are a good spot for general questions - other users might be able to help out there. ",imitation still behavior check also good spot general might able help,issue,positive,positive,positive,positive,positive,positive
842560135,"@ervteng 
Thanks, will try it out as soon as available",thanks try soon available,issue,negative,positive,positive,positive,positive,positive
842547657,"Hi @TheTrope, the issue is with the Barracuda package. It will be fixed in a following release of Barracuda - stay tuned!",hi issue barracuda package fixed following release barracuda stay tuned,issue,negative,positive,neutral,neutral,positive,positive
842492669,Feel free to merge #5369 into this branch when the tests pass.,feel free merge branch pas,issue,positive,positive,positive,positive,positive,positive
841813886,"I don't mind messing about with the python code. The main thing I'm worried about is causing potential issues with mismatches between unity code and the python mlagents package.

I wasn't aware of custom tensorboard stats, maybe I can get it all working entirely on the Unity side, in which case the pyhthon packages is less of a worry. Still, it would be nice to get a proper integration working.",mind messing python code main thing worried causing potential unity code python package aware custom maybe get working entirely unity side case le worry still would nice get proper integration working,issue,negative,positive,positive,positive,positive,positive
841391545,"You'll need to look in the player logs for more details; generally there will be a stack trace. This line tells you where they are saved:
```
2021-05-14 09:55:25 DEBUG [env_utils.py:106] Running with args ['--mlagents-port', '5005', '-logFile', '/Users/B/Downloads/ml-agents-release_14/python/results/push_051/run_logs/Player-0.log']
```",need look player generally stack trace line saved running,issue,negative,positive,neutral,neutral,positive,positive
841343000,I was able to replicate the issue - we're taking a look at it. Thanks for reporting!,able replicate issue taking look thanks,issue,negative,positive,positive,positive,positive,positive
841332081,"The python package on pypi is built and uploaded from release tag or python package tag ? If python package tag, then would you be pushing a new version to the pypi repo as 0.27 ?",python package built release tag python package tag python package tag would pushing new version,issue,negative,positive,positive,positive,positive,positive
841329312,The release tag is correct - we'll fix the Python tag. Thanks for finding this!,release tag correct fix python tag thanks finding,issue,negative,positive,positive,positive,positive,positive
841317473,"This is actually something of interest to us - and the weighting as well. One way you could do it with the current APIs is to have the weights as Environment Parameters, and use [custom tensorboard stats](https://github.com/Unity-Technologies/ml-agents/blob/745bf7b20c72b4567104a6d3809e6f4d18137400/docs/Using-Tensorboard.md) to send the rewards to Tensorboard. You might even be able to build this feature on top of the custom tensorboard feature and it won't have to hit the Python code at all. 
We don't have an ETA for something like this, but hope you are able to get it working!",actually something interest u weighting well one way could current environment use custom send might even able build feature top custom feature wo hit python code eta something like hope able get working,issue,positive,positive,positive,positive,positive,positive
840925278,"@chriselion 
Hi I know already that SIGABRT (from one of your other messages) means the environment crashed.
I don't know why the environment crashed, because I didn't fix anything, and all the settings are in the state where the project is saved. 
If my graphic setting(or setup) is a problem, where and how should I fix it? the problem happened all os( window, ubuntu, mac os) What information do you think it will be easy for me to solve this problem? 

Just in case, I'm leaving behind the command and debug results.

(ml-agents) BBookPro:python B$ mlagents-learn /Users/B/Downloads/ml-agents-release_14/config/ppo/VisualPushBlock.yaml  --env=""vi14_0513"" --run-id=push_051 --debug


                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓

        
 Version information:
  ml-agents: 0.24.1,
  ml-agents-envs: 0.24.1,
  Communicator API: 1.4.0,
  PyTorch: 1.7.1
2021-05-14 09:55:25 DEBUG [learn.py:220] Configuration for this run:
2021-05-14 09:55:25 DEBUG [learn.py:221] {
    ""default_settings"": null,
    ""behaviors"": {
        ""VisualPushBlock"": {
            ""trainer_type"": ""ppo"",
            ""hyperparameters"": {
                ""batch_size"": 64,
                ""buffer_size"": 1024,
                ""learning_rate"": 0.0003,
                ""beta"": 0.01,
                ""epsilon"": 0.2,
                ""lambd"": 0.95,
                ""num_epoch"": 3,
                ""learning_rate_schedule"": ""linear""
            },
            ""network_settings"": {
                ""normalize"": false,
                ""hidden_units"": 128,
                ""num_layers"": 1,
                ""vis_encode_type"": ""simple"",
                ""memory"": {
                    ""sequence_length"": 32,
                    ""memory_size"": 128
                }
            },
            ""reward_signals"": {
                ""extrinsic"": {
                    ""gamma"": 0.99,
                    ""strength"": 1.0
                }
            },
            ""init_path"": null,
            ""keep_checkpoints"": 5,
            ""checkpoint_interval"": 500000,
            ""max_steps"": 3000000,
            ""time_horizon"": 64,
            ""summary_freq"": 60000,
            ""threaded"": true,
            ""self_play"": null,
            ""behavioral_cloning"": null
        }
    },
    ""env_settings"": {
        ""env_path"": ""vi14_0513"",
        ""env_args"": null,
        ""base_port"": 5005,
        ""num_envs"": 1,
        ""seed"": -1
    },
    ""engine_settings"": {
        ""width"": 84,
        ""height"": 84,
        ""quality_level"": 5,
        ""time_scale"": 20,
        ""target_frame_rate"": -1,
        ""capture_frame_rate"": 60,
        ""no_graphics"": false
    },
    ""environment_parameters"": null,
    ""checkpoint_settings"": {
        ""run_id"": ""push_051"",
        ""initialize_from"": null,
        ""load_model"": false,
        ""resume"": false,
        ""force"": true,
        ""train_model"": false,
        ""inference"": false,
        ""results_dir"": ""results""
    },
    ""torch_settings"": {
        ""device"": null
    },
    ""debug"": true
}
2021-05-14 09:55:25 INFO [learn.py:245] run_seed set to 7444
2021-05-14 09:55:25 INFO [torch.py:58] default Torch device: cpu
2021-05-14 09:55:25 DEBUG [stats_writer.py:59] Initializing StatsWriter plugins: default
2021-05-14 09:55:25 DEBUG [stats_writer.py:63] Found 3 StatsWriters for plugin default
2021-05-14 09:55:25 DEBUG [env_utils.py:33] The true file name is vi14_0513
2021-05-14 09:55:25 DEBUG [env_utils.py:105] The launch string is /Users/B/Downloads/ml-agents-release_14/python/vi14_0513.app/Contents/MacOS/UnityEnvironment
2021-05-14 09:55:25 DEBUG [env_utils.py:106] Running with args ['--mlagents-port', '5005', '-logFile', '/Users/B/Downloads/ml-agents-release_14/python/results/push_051/run_logs/Player-0.log']
2021-05-14 09:55:26 INFO [environment.py:112] Connected to Unity environment with package version 1.8.1-preview and communication version 1.4.0
2021-05-14 09:55:32 INFO [subprocess_env_manager.py:220] UnityEnvironment worker 0: environment stopping.
2021-05-14 09:55:32 DEBUG [subprocess_env_manager.py:234] UnityEnvironment worker 0 closing.
2021-05-14 09:55:32 INFO [trainer_controller.py:188] Learning was interrupted. Please wait while the graph is generated.
2021-05-14 09:55:32 INFO [trainer_controller.py:81] Saved Model
2021-05-14 09:55:32 DEBUG [subprocess_env_manager.py:368] SubprocessEnvManager closing.
2021-05-14 09:55:32 INFO [environment.py:426] Environment shut down with return code -6 (SIGABRT).
2021-05-14 09:55:32 DEBUG [subprocess_env_manager.py:237] UnityEnvironment worker 0 done.
Traceback (most recent call last):
  File ""/Users/B/opt/anaconda3/envs/ml-agents/bin/mlagents-learn"", line 33, in <module>
    sys.exit(load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')())
  File ""/Users/B/Downloads/ml-agents-release_14/ml-agents/mlagents/trainers/learn.py"", line 250, in main
    run_cli(parse_command_line())
  File ""/Users/B/Downloads/ml-agents-release_14/ml-agents/mlagents/trainers/learn.py"", line 246, in run_cli
    run_training(run_seed, options)
  File ""/Users/B/Downloads/ml-agents-release_14/ml-agents/mlagents/trainers/learn.py"", line 125, in run_training
    tc.start_learning(env_manager)
  File ""/Users/B/Downloads/ml-agents-release_14/ml-agents-envs/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/Users/B/Downloads/ml-agents-release_14/ml-agents/mlagents/trainers/trainer_controller.py"", line 197, in start_learning
    raise ex
  File ""/Users/B/Downloads/ml-agents-release_14/ml-agents/mlagents/trainers/trainer_controller.py"", line 173, in start_learning
    self._reset_env(env_manager)
  File ""/Users/B/Downloads/ml-agents-release_14/ml-agents-envs/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/Users/B/Downloads/ml-agents-release_14/ml-agents/mlagents/trainers/trainer_controller.py"", line 105, in _reset_env
    env_manager.reset(config=new_config)
  File ""/Users/B/Downloads/ml-agents-release_14/ml-agents/mlagents/trainers/env_manager.py"", line 68, in reset
    self.first_step_infos = self._reset_env(config)
  File ""/Users/B/Downloads/ml-agents-release_14/ml-agents/mlagents/trainers/subprocess_env_manager.py"", line 333, in _reset_env
    ew.previous_step = EnvironmentStep(ew.recv().payload, ew.worker_id, {}, {})
  File ""/Users/B/Downloads/ml-agents-release_14/ml-agents/mlagents/trainers/subprocess_env_manager.py"", line 98, in recv
    raise env_exception
mlagents_envs.exception.UnityEnvironmentException: Environment shut down with return code -6 (SIGABRT).


",hi know already one environment know environment fix anything state project saved graphic setting setup problem fix problem o window mac o information think easy solve problem case leaving behind command python version information communicator configuration run null beta epsilon linear normalize false simple memory extrinsic gamma strength null threaded true null null null seed width height false null null false resume false force true false inference false device null true set default torch device default found default true file name launch string running connected unity environment package version communication version worker environment stopping worker learning interrupted please wait graph saved model environment shut return code worker done recent call last file line module file line main file line file line file line wrapped return file line raise ex file line file line wrapped return file line file line reset file line file line raise environment shut return code,issue,positive,negative,neutral,neutral,negative,negative
840912158,"Please don't spam the same question in [multiple](https://github.com/Unity-Technologies/ml-agents/issues/5349#issuecomment-840347064) [places](https://github.com/Unity-Technologies/ml-agents/pull/4880#issuecomment-840249346).

SIGABRT (from one of your other messages) means the environment crashed. You'll need to look in the player logs for a better idea of what caused that.

If you're running VisualPushBlock, it's likely a problem with your graphics setup.",please question multiple one environment need look player better idea running likely problem graphic setup,issue,negative,positive,positive,positive,positive,positive
840866037,This is (apparently) not possible on packages right now. See jira for more discussion.,apparently possible right see discussion,issue,negative,positive,positive,positive,positive,positive
840347064,"@surfnerd hey could you help me?
I got a error msg in ubuntu18.04,unity 2019.4.18fi,tensorflow gpu 1.15.0.
the msg is mlagents_envs.exception.UnityEnvironmentException: Environment shut down with return code -6 (SIGABRT).
my behavior type is Default and all setting follows /docs/Learning-Environment-Executable.md

why I get that message??


",hey could help got error unity fi environment shut return code behavior type default setting get message,issue,negative,neutral,neutral,neutral,neutral,neutral
840249346,"HI I got a error msg( mlagents_envs.exception.UnityEnvironmentException: Environment shut down with return code -6 (SIGABRT).)

the visualpushblock example runs macos but not running windows and ubuntu.
all setting is same, only gpu diffierent.
visual example run only cpu env?
",hi got error environment shut return code example running setting visual example run,issue,negative,neutral,neutral,neutral,neutral,neutral
840024212,"Hi @TheTrope, could you post the broken ONNX file?

There is also a known issue with the ResNet visual encoder type that should be fixed in an upcoming patch. ",hi could post broken file also known issue visual type fixed upcoming patch,issue,negative,negative,negative,negative,negative,negative
839194470,"Yes, please create a new issue.",yes please create new issue,issue,positive,positive,positive,positive,positive,positive
839179763,"@surfnerd 
So what's fixed now is the division/zero error handling right ?

However the cause of this error (the grid dimensions < 20) is still there, what should we do about it, start a new issue?
Thanks",fixed error handling right however cause error grid still start new issue thanks,issue,negative,positive,positive,positive,positive,positive
839132574,"My apologies, the issue wasn't fixed, but you will not receive a better error message about the dimensions of your grid.",issue fixed receive better error message grid,issue,negative,positive,positive,positive,positive,positive
837658517,"@surfnerd yup! just 1 CameraSensor with width and height unchanged

These errors are seen when inference is done on GPU.
On CPU, the only issue is:  `AssertionException: Assertion failure. Values are not equal.`",width height unchanged seen inference done issue assertion failure,issue,negative,negative,negative,negative,negative,negative
837008484,"Thanks. Are you sure that the inputs to your trained model are the same as what you are currently using in your project.  I'd like to make sure my understanding is correct.

The issue that you seem to be having is that inference on CPU works, but on GPU it doesn't.  And these errors you are seeing are when you are running inference on GPU.  Is that correct?",thanks sure trained model currently project like make sure understanding correct issue seem inference work seeing running inference correct,issue,positive,positive,positive,positive,positive,positive
836943288,~A fix for the original issue has been merged into `main` from #5345.  Could you please give it a try to see if it fixes your original problem?  Thanks.~,fix original issue main could please give try see original problem,issue,negative,positive,positive,positive,positive,positive
836942081,"> Another follow up:
> 
> When importing (or selection ) an onnx trained with the grid sensor, I have this error (x8 times)
> 
> ```
> Unexpected error while evaluating model output version_number. System.ArgumentException: Cannot reshape array of size 32768 into shape with multiple of 768 elements
>   at Unity.Barracuda.TensorExtensions.Reshape (Unity.Barracuda.TensorShape shape, System.Int32[] size4Dor8D) [0x0018c] in C:\Users\alexi\Dev\MyProject\Library\PackageCache\com.unity.barracuda@1.4.0-preview\Barracuda\Runtime\Core\TensorExtensions.cs:495 
>   at Unity.Barracuda.ModelAnalyzer.ListTemporaryTensorShapes (Unity.Barracuda.Model model, System.Collections.Generic.IDictionary`2[TKey,TValue] inputShapes, System.Collections.Generic.IDictionary`2[System.String,System.Nullable`1[Unity.Barracuda.TensorShape]]& shapesByName) [0x00c2f] in C:\Users\alexi\Dev\MyProject\Library\PackageCache\com.unity.barracuda@1.4.0-preview\Barracuda\Runtime\Core\Backends\ModelAnalyzer.cs:410 
>   at Unity.Barracuda.ModelAnalyzer.TryGetOutputTensorShape (Unity.Barracuda.Model model, System.Collections.Generic.IDictionary`2[TKey,TValue] inputShapes, System.String output, Unity.Barracuda.TensorShape& shape) [0x00007] in C:\Users\alexi\Dev\MyProject\Library\PackageCache\com.unity.barracuda@1.4.0-preview\Barracuda\Runtime\Core\Backends\ModelAnalyzer.cs:586 
>   at Unity.Barracuda.Editor.NNModelEditor+<>c__DisplayClass33_0.<OnEnable>b__2 (System.String i) [0x00006] in C:\Users\alexi\Dev\MyProject\Library\PackageCache\com.unity.barracuda@1.4.0-preview\Barracuda\Editor\ONNXModelImporterEditor.cs:209 
> UnityEngine.Debug:LogError (object)
> Unity.Barracuda.D:LogError (object) (at Library/PackageCache/com.unity.barracuda@1.4.0-preview/Barracuda/Runtime/Core/Internals/Debug.cs:131)
> Unity.Barracuda.Editor.NNModelEditor/<>c__DisplayClass33_0:<OnEnable>b__2 (string) (at Library/PackageCache/com.unity.barracuda@1.4.0-preview/Barracuda/Editor/ONNXModelImporterEditor.cs:214)
> System.Linq.Enumerable:ToList<string> (System.Collections.Generic.IEnumerable`1<string>)
> Unity.Barracuda.Editor.NNModelEditor:OnEnable () (at Library/PackageCache/com.unity.barracuda@1.4.0-preview/Barracuda/Editor/ONNXModelImporterEditor.cs:204)
> ```
> 
> Is it related? or is it a separate issue?

This is a different issue, please create another issue for the above error.",another follow selection trained grid sensor error time unexpected error model output reshape array size shape multiple shape model model output shape object object string string string related separate issue different issue please create another issue error,issue,negative,positive,neutral,neutral,positive,positive
836938775,"Hi,
The GitHub issues are reserved for feature requests and bug reports.  Please take this discussion to the [ML-Agents forums](https://forum.unity.com/forums/ml-agents.453/).  If we decide that there is a bug here, we can open another issue for that.  ",hi reserved feature bug please take discussion decide bug open another issue,issue,negative,neutral,neutral,neutral,neutral,neutral
836936761,"Hi, this question is suited better to the[ ML-Agents forums](https://forum.unity.com/forums/ml-agents.453/).  In the future please only use the github issues for bug reports or feature requests.  Thanks.",hi question better future please use bug feature thanks,issue,positive,positive,positive,positive,positive,positive
836933903,"Hmm, accidentally pushed an earlier version directly to the branch https://github.com/Unity-Technologies/ml-agents/commit/6d0edbbf3af637a19a804f5ed432e4c0a8203a43 :/",accidentally version directly branch,issue,negative,positive,neutral,neutral,positive,positive
836546215,"Another follow up: 

When importing (or selection ) an onnx trained with the grid sensor, I have this error (x8 times)

```
Unexpected error while evaluating model output version_number. System.ArgumentException: Cannot reshape array of size 32768 into shape with multiple of 768 elements
  at Unity.Barracuda.TensorExtensions.Reshape (Unity.Barracuda.TensorShape shape, System.Int32[] size4Dor8D) [0x0018c] in C:\Users\alexi\Dev\MyProject\Library\PackageCache\com.unity.barracuda@1.4.0-preview\Barracuda\Runtime\Core\TensorExtensions.cs:495 
  at Unity.Barracuda.ModelAnalyzer.ListTemporaryTensorShapes (Unity.Barracuda.Model model, System.Collections.Generic.IDictionary`2[TKey,TValue] inputShapes, System.Collections.Generic.IDictionary`2[System.String,System.Nullable`1[Unity.Barracuda.TensorShape]]& shapesByName) [0x00c2f] in C:\Users\alexi\Dev\MyProject\Library\PackageCache\com.unity.barracuda@1.4.0-preview\Barracuda\Runtime\Core\Backends\ModelAnalyzer.cs:410 
  at Unity.Barracuda.ModelAnalyzer.TryGetOutputTensorShape (Unity.Barracuda.Model model, System.Collections.Generic.IDictionary`2[TKey,TValue] inputShapes, System.String output, Unity.Barracuda.TensorShape& shape) [0x00007] in C:\Users\alexi\Dev\MyProject\Library\PackageCache\com.unity.barracuda@1.4.0-preview\Barracuda\Runtime\Core\Backends\ModelAnalyzer.cs:586 
  at Unity.Barracuda.Editor.NNModelEditor+<>c__DisplayClass33_0.<OnEnable>b__2 (System.String i) [0x00006] in C:\Users\alexi\Dev\MyProject\Library\PackageCache\com.unity.barracuda@1.4.0-preview\Barracuda\Editor\ONNXModelImporterEditor.cs:209 
UnityEngine.Debug:LogError (object)
Unity.Barracuda.D:LogError (object) (at Library/PackageCache/com.unity.barracuda@1.4.0-preview/Barracuda/Runtime/Core/Internals/Debug.cs:131)
Unity.Barracuda.Editor.NNModelEditor/<>c__DisplayClass33_0:<OnEnable>b__2 (string) (at Library/PackageCache/com.unity.barracuda@1.4.0-preview/Barracuda/Editor/ONNXModelImporterEditor.cs:214)
System.Linq.Enumerable:ToList<string> (System.Collections.Generic.IEnumerable`1<string>)
Unity.Barracuda.Editor.NNModelEditor:OnEnable () (at Library/PackageCache/com.unity.barracuda@1.4.0-preview/Barracuda/Editor/ONNXModelImporterEditor.cs:204)
```

Is it related? or is it a separate issue?",another follow selection trained grid sensor error time unexpected error model output reshape array size shape multiple shape model model output shape object object string string string related separate issue,issue,negative,positive,neutral,neutral,positive,positive
835376360,"Had to install compatible Unity and Python package
",install compatible unity python package,issue,negative,neutral,neutral,neutral,neutral,neutral
834657032,"Hi @hhhcwb38712,
There is a [`Match3Actuator`](https://github.com/Unity-Technologies/ml-agents/blob/950314f6906641155c33bae65963485cb294e986/com.unity.ml-agents/Runtime/Integrations/Match3/Match3Actuator.cs#L124) that implements the `Heuristic` method to solve the game when there is no neural network present.",hi heuristic method solve game neural network present,issue,negative,negative,negative,negative,negative,negative
834652108,"Hi @MrOCW,
Thanks for bringing this conversation to our GitHub issues.  Could you provide the version of the mlagents pip package you are using?",hi thanks conversation could provide version pip package,issue,negative,positive,positive,positive,positive,positive
834189418,"That's the all code of Match3Agent:
using System;
using UnityEngine;
using Unity.MLAgents;
using Unity.MLAgents.Actuators;
using Unity.MLAgents.Extensions.Match3;

namespace Unity.MLAgentsExamples
{

    /// <summary>
    /// State of the ""game"" when showing all steps of the simulation. This is only used outside of training.
    /// The state diagram is
    ///
    ///      | <--------------------------------------- ^
    ///      |                                          |
    ///      v                                          |
    ///  +--------+      +-------+      +-----+      +------+
    ///  |Find    | ---> |Clear  | ---> |Drop | ---> |Fill  |
    ///  |Matches |      |Matched|      |     |      |Empty |
    ///  +--------+      +-------+      +-----+      +------+
    ///
    ///    |     ^
    ///    |     |
    ///    v     |
    ///
    ///  +--------+
    ///  |Wait for|
    ///  |Move    |
    ///  +--------+
    ///
    /// The stats advances each ""MoveTime"" seconds.
    /// </summary>
    enum State
    {
        /// <summary>
        /// Guard value, should never happen.
        /// </summary>
        Invalid = -1,

        /// <summary>
        /// Look for matches. If there are matches, the next state is ClearMatched, otherwise WaitForMove.
        /// </summary>
        FindMatches = 0,

        /// <summary>
        /// Remove matched cells and replace them with a placeholder value.
        /// </summary>
        ClearMatched = 1,

        /// <summary>
        /// Move cells ""down"" to fill empty space.
        /// </summary>
        Drop = 2,

        /// <summary>
        /// Replace empty cells with new random values.
        /// </summary>
        FillEmpty = 3,

        /// <summary>
        /// Request a move from the Agent.
        /// </summary>
        WaitForMove = 4,
    }

    public class Match3Agent : Agent
    {
        [HideInInspector]
        public Match3Board Board;

        public float MoveTime = 1.0f;
        public int MaxMoves = 500;


        State m_CurrentState = State.WaitForMove;
        float m_TimeUntilMove;
        private int m_MovesMade;

        private const float k_RewardMultiplier = 0.01f;
        void Awake()
        {
            Board = GetComponent<Match3Board>();
        }

        public override void OnEpisodeBegin()
        {
            base.OnEpisodeBegin();

            Board.InitSettled();
            m_CurrentState = State.FindMatches;
            m_TimeUntilMove = MoveTime;
            m_MovesMade = 0;
        }

        private void FixedUpdate()
        {
            if (Academy.Instance.IsCommunicatorOn)
            {
                FastUpdate();
            }
            else
            {
                AnimatedUpdate();
            }

            // We can't use the normal MaxSteps system to decide when to end an episode,
            // since different agents will make moves at different frequencies (depending on the number of
            // chained moves). So track a number of moves per Agent and manually interrupt the episode.
            if (m_MovesMade >= MaxMoves)
            {
                EpisodeInterrupted();
            }
        }

        void FastUpdate()
        {
            while (true)
            {
                var hasMatched = Board.MarkMatchedCells(); // 匹配当前是否有满足消除条件的元素
                if (!hasMatched)
                {
                    break;
                }
                var pointsEarned = Board.ClearMatchedCells();
                AddReward(k_RewardMultiplier * pointsEarned);
                Board.DropCells();
                Board.FillFromAbove();
            }

            while (!HasValidMoves())
            {
                // Shuffle the board until we have a valid move.
                Board.InitSettled();
            }
            RequestDecision();
            m_MovesMade++;
        }

        void AnimatedUpdate()
        {
            m_TimeUntilMove -= Time.deltaTime;
            if (m_TimeUntilMove > 0.0f)
            {
                return;
            }

            m_TimeUntilMove = MoveTime;

            State nextState;
            switch (m_CurrentState)
            {
                case State.FindMatches:
                    var hasMatched = Board.MarkMatchedCells();
                    nextState = hasMatched ? State.ClearMatched : State.WaitForMove;
                    if (nextState == State.WaitForMove)
                    {
                        m_MovesMade++;
                    }
                    break;
                case State.ClearMatched:
                    var pointsEarned = Board.ClearMatchedCells();
                    AddReward(k_RewardMultiplier * pointsEarned);
                    nextState = State.Drop;
                    break;
                case State.Drop:
                    Board.DropCells();
                    nextState = State.FillEmpty;
                    break;
                case State.FillEmpty:
                    Board.FillFromAbove();
                    nextState = State.FindMatches;
                    break;
                case State.WaitForMove:
                    while (true)
                    {
                        // Shuffle the board until we have a valid move.
                        // 判断当前局面是否有可以移动的元素
                        bool hasMoves = HasValidMoves();
                        if (hasMoves)
                        {
                            break;
                        }
                        Board.InitSettled();
                    }
                    // 执行到这说明当前有可以移动的元素，向智能体请求决策
                    RequestDecision();

                    nextState = State.FindMatches;
                    break;
                default:
                    throw new ArgumentOutOfRangeException();
            }

            m_CurrentState = nextState;
        }

        bool HasValidMoves()
        {
            foreach (var unused in Board.ValidMoves())
            {
                return true;
            }

            return false;
        }

    }

}",code system summary state game showing simulation used outside training state diagram state summary guard value never happen invalid summary look next state otherwise summary remove replace value summary move fill empty space drop summary replace empty new random summary request move agent public class agent public matchboard board public float public state float private private float void awake board matchboard public override void private void else ca use normal system decide end episode since different make different depending number chained track number per agent manually interrupt episode void true break shuffle board valid move void return state switch case break case break case break case break case true shuffle board valid move bool break break default throw new bool unused return true return false,issue,positive,negative,neutral,neutral,negative,negative
834188507,"And I search the other games agent source code, I find the CollectObservations() method, OnActionReceived() method, Heuristic() method which are not exit in Match3Agent C# file.",search agent source code find method method heuristic method exit file,issue,negative,neutral,neutral,neutral,neutral,neutral
833385204,"This actually seems to solve the issue, great! I specified an individual `--base-port` for each job i submitted. Thanks @vincentpierre!",actually solve issue great individual job thanks,issue,positive,positive,positive,positive,positive,positive
833306572,Has anyone found the answer to solve this NullReferenceException problem? I'm still encountering this problem when trying to drag the .onnx to Model in 2021.,anyone found answer solve problem still problem trying drag model,issue,negative,negative,neutral,neutral,negative,negative
832897306,I do not know what is causing this. It could. be that both executables are trying to communicate on the same port and there is collision going on. you should try to play with the `--base-port` argument and make it different on both jobs. ,know causing could trying communicate port collision going try play argument make different,issue,negative,neutral,neutral,neutral,neutral,neutral
832408468,"First of all, `MaxStep` field is currently set to `0`. According to the [official document](https://docs.unity3d.com/Packages/com.unity.ml-agents@1.0/api/Unity.MLAgents.Agent.html), this means `max_step` will never affect on calling `EndEpisode` by itself.

I will check about other settings..",first field currently set according official document never affect calling check,issue,negative,positive,positive,positive,positive,positive
832392014,"@vincentpierre Thank you for the reply. I will go check settings you mentioned above, and make a test to reproduce on Dungeon escape or push block environment. After the test is done, then I will comment about the result.",thank reply go check make test reproduce dungeon escape push block environment test done comment result,issue,negative,neutral,neutral,neutral,neutral,neutral
832180538,It is also possible that there is an issue with registering and deregistering Agents from the agentGroup. Can you try to reproduce your issue with Dungeon escape or colab push block ?,also possible issue try reproduce issue dungeon escape push block,issue,negative,neutral,neutral,neutral,neutral,neutral
832153947,"I cannot reproduce this issue with the description provided. Can you send a working example or tell us how to modify the existing example environments to reproduce the bug? It is possible that Agent.OnEpisodeBegin is called even if no agent terminated an episode, it is called for example the first time an agent is spawned. If some Agents have max_step setting, it could also affect their episode terminations.
I also believe that doing `episode_done = (terminal_steps.group_reward.shape[0] > 0)` is equivalent to doing `episode_done = len(terminal_steps)>0` can you confirm?",reproduce issue description provided send working example tell u modify example reproduce bug possible even agent episode example first time agent setting could also affect episode also believe equivalent confirm,issue,negative,positive,positive,positive,positive,positive
831767273,"This is the output I get in the standard output and the results/simulation/Player-0.log, when I start two simulations on the same node. When they are startet simultanuously, both seem to fail to start.

For the first job

**Player-0.log**

```
Mono path[0] = '/cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/1/ecosim_Data/Managed'
Mono config path = '/cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/1/ecosim_Data/MonoBleedingEdge/etc'
Preloaded 'lib_burst_generated.so'
Preloaded 'libgrpc_csharp_ext.x64.so'
Initialize engine version: 2020.1.9f1 (145f5172610f)
[Subsystems] Discovering subsystems at path /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/1/ecosim_Data/UnitySubsystems
Forcing GfxDevice: Null
GfxDevice: creating device client; threaded=0
NullGfxDevice:
    Version:  NULL 1.0 [1.0]
    Renderer: Null Device
    Vendor:   Unity Technologies
Begin MonoManager ReloadAssembly
- Completed reload, in  1.909 seconds
ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)
ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)
ERROR: Shader GUI/Text Shader shader is not supported on this GPU (none of subshaders/fallbacks are suitable)
ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)
WARNING: Shader Unsupported: 'Standard (Specular setup)' - All subshaders removed
WARNING: Shader Did you use #pragma only_renderers and omit this platform?
WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?
ERROR: Shader Standard (Specular setup) shader is not supported on this GPU (none of subshaders/fallbacks are suitable)
WARNING: Shader Unsupported: 'Standard (Specular setup)' - All subshaders removed
WARNING: Shader Did you use #pragma only_renderers and omit this platform?
WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?
ERROR: Shader Legacy Shaders/Particles/Alpha Blended Premultiply shader is not supported on this GPU (none of subshaders/fallbacks are suitable)
ERROR: Shader FX/Water (Basic) shader is not supported on this GPU (none of subshaders/fallbacks are suitable)
WARNING: Shader Unsupported: 'Standard' - All subshaders removed
WARNING: Shader Did you use #pragma only_renderers and omit this platform?
WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?
ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)
WARNING: Shader Unsupported: 'Standard' - All subshaders removed
WARNING: Shader Did you use #pragma only_renderers and omit this platform?
WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?
UnloadTime: 0.759724 ms
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/1/ecosim_Data/Mono/libcoreclr.so
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/1/ecosim_Data/Mono/libcoreclr.so
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/1/ecosim_Data/Mono/libcoreclr.so
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/1/ecosim_Data/Mono/libSystem.dylib
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/1/ecosim_Data/Mono/libSystem.dylib.so
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/1/ecosim_Data/Mono/libSystem.dylib
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/1/ecosim_Data/Mono/libcoreclr.so
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/1/ecosim_Data/Mono/libcoreclr.so
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/1/ecosim_Data/Mono/libcoreclr.so
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/1/ecosim_Data/Mono/libSystem.dylib
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/1/ecosim_Data/Mono/libSystem.dylib.so
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/1/ecosim_Data/Mono/libSystem.dylib
WARNING: The communication API versions between Unity and python differ at the minor version level. Python API: 1.3.0, Unity API: 1.0.0 Python Library Version: 0.23.0 .
This means that some features may not work unless you upgrade the package with the lower version.Please find the versions that work best together from our release page.
https://github.com/Unity-Technologies/ml-agents/releases
Setting up 16 worker threads for Enlighten.
  Thread -> id: 2aec5c7fb700 -> priority: 1 
  Thread -> id: 2aec5c9fc700 -> priority: 1 
  Thread -> id: 2aec5cbfd700 -> priority: 1 
  Thread -> id: 2aec5cdfe700 -> priority: 1 
  Thread -> id: 2aec5cfff700 -> priority: 1 
  Thread -> id: 2aec5d200700 -> priority: 1 
  Thread -> id: 2aec5d401700 -> priority: 1 
  Thread -> id: 2aec5d602700 -> priority: 1 
  Thread -> id: 2aec5d803700 -> priority: 1 
  Thread -> id: 2aec5da04700 -> priority: 1 
  Thread -> id: 2aec5dc05700 -> priority: 1 
  Thread -> id: 2aec5de06700 -> priority: 1 
  Thread -> id: 2aec5e007700 -> priority: 1 
  Thread -> id: 2aec5e208700 -> priority: 1 
  Thread -> id: 2aec5e409700 -> priority: 1 
  Thread -> id: 2aec5e60a700 -> priority: 1 
```

**Standard output:**
```
2021-05-04 10:01:32.186033: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
WARNING:tensorflow:From /cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2021-05-04 10:02:31 INFO [learn.py:275] run_seed set to 5767


                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓

        
 Version information:
  ml-agents: 0.23.0,
  ml-agents-envs: 0.23.0,
  Communicator API: 1.3.0,
  PyTorch: 1.7.1
Found path: /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/1/ecosim.x86_64
2021-05-04 10:02:35 INFO [environment.py:110] Connected to Unity environment with package version 1.0.6 and communication version 1.0.0
2021-05-04 10:02:35 INFO [environment.py:271] Connected new brain:
Deer?team=0
2021-05-04 10:02:35 INFO [environment.py:271] Connected new brain:
Wolf?team=0
2021-05-04 10:02:35 INFO [stats.py:145] Hyperparameters for behavior name Deer: 
	trainer_type:	ppo
	hyperparameters:	
	  batch_size:	256
	  buffer_size:	10240
	  learning_rate:	0.0003
	  beta:	0.005
	  epsilon:	0.2
	  lambd:	0.95
	  num_epoch:	5
	  learning_rate_schedule:	linear
	network_settings:	
	  normalize:	False
	  hidden_units:	256
	  num_layers:	2
	  vis_encode_type:	simple
	  memory:	None
	reward_signals:	
	  extrinsic:	
	    gamma:	0.99
	    strength:	1.0
	init_path:	../../../ecosimdata/results/e1_large/Deer
	keep_checkpoints:	5
	checkpoint_interval:	500000
	max_steps:	1000000000
	time_horizon:	1024
	summary_freq:	10
	threaded:	True
	self_play:	None
	behavioral_cloning:	None
	framework:	pytorch
2021-05-04 10:02:38 INFO [torch_model_saver.py:96] Starting training from step 0 and saving to results/dynamics_1/Deer.
2021-05-04 10:02:38 INFO [stats.py:145] Hyperparameters for behavior name Wolf: 
	trainer_type:	ppo
	hyperparameters:	
	  batch_size:	256
	  buffer_size:	10240
	  learning_rate:	0.0003
	  beta:	0.005
	  epsilon:	0.2
	  lambd:	0.95
	  num_epoch:	5
	  learning_rate_schedule:	linear
	network_settings:	
	  normalize:	False
	  hidden_units:	256
	  num_layers:	2
	  vis_encode_type:	simple
	  memory:	None
	reward_signals:	
	  extrinsic:	
	    gamma:	0.99
	    strength:	1.0
	init_path:	../../../ecosimdata/results/e1_large/Wolf
	keep_checkpoints:	5
	checkpoint_interval:	500000
	max_steps:	1000000000
	time_horizon:	1024
	summary_freq:	10
	threaded:	True
	self_play:	None
	behavioral_cloning:	None
	framework:	pytorch
2021-05-04 10:02:38 INFO [torch_model_saver.py:96] Starting training from step 0 and saving to results/dynamics_1/Wolf.
2021-05-04 10:02:38 ERROR [_server.py:445] Exception calling application: Ran out of input
Traceback (most recent call last):
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/grpc/_server.py"", line 435, in _call_behavior
    response_or_iterator = behavior(argument, context)
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents_envs/rpc_communicator.py"", line 30, in Exchange
    return self.child_conn.recv()
  File ""/apps/Alvis/software/Compiler/GCCcore/10.2.0/Python/3.8.6/lib/python3.8/multiprocessing/connection.py"", line 251, in recv
    return _ForkingPickler.loads(buf.getbuffer())
EOFError: Ran out of input
2021-05-04 10:02:38 ERROR [_server.py:445] Exception calling application: invalid load key, '\x04'.
Traceback (most recent call last):
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/grpc/_server.py"", line 435, in _call_behavior
    response_or_iterator = behavior(argument, context)
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents_envs/rpc_communicator.py"", line 30, in Exchange
    return self.child_conn.recv()
  File ""/apps/Alvis/software/Compiler/GCCcore/10.2.0/Python/3.8.6/lib/python3.8/multiprocessing/connection.py"", line 251, in recv
    return _ForkingPickler.loads(buf.getbuffer())
_pickle.UnpicklingError: invalid load key, '\x04'.
2021-05-04 10:03:38 INFO [subprocess_env_manager.py:186] UnityEnvironment worker 0: environment stopping.
2021-05-04 10:03:38 INFO [environment.py:407] Environment shut down with return code 0.
2021-05-04 10:03:38 INFO [model_serialization.py:104] Converting to results/dynamics_1/Deer/Deer-0.onnx
2021-05-04 10:03:38 INFO [model_serialization.py:116] Exported results/dynamics_1/Deer/Deer-0.onnx
2021-05-04 10:03:38 INFO [torch_model_saver.py:116] Copied results/dynamics_1/Deer/Deer-0.onnx to results/dynamics_1/Deer.onnx.
2021-05-04 10:03:39 INFO [model_serialization.py:104] Converting to results/dynamics_1/Wolf/Wolf-0.onnx
2021-05-04 10:03:39 INFO [model_serialization.py:116] Exported results/dynamics_1/Wolf/Wolf-0.onnx
2021-05-04 10:03:39 INFO [torch_model_saver.py:116] Copied results/dynamics_1/Wolf/Wolf-0.onnx to results/dynamics_1/Wolf.onnx.
2021-05-04 10:03:39 INFO [trainer_controller.py:85] Saved Model
Traceback (most recent call last):
  File ""/cephyr/users/tobiaka/Alvis/.local/bin/mlagents-learn"", line 8, in <module>
    sys.exit(main())
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents/trainers/learn.py"", line 280, in main
    run_cli(parse_command_line())
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents/trainers/learn.py"", line 276, in run_cli
    run_training(run_seed, options)
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents/trainers/learn.py"", line 153, in run_training
    tc.start_learning(env_manager)
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents/trainers/trainer_controller.py"", line 176, in start_learning
    n_steps = self.advance(env_manager)
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents/trainers/trainer_controller.py"", line 234, in advance
    new_step_infos = env_manager.get_steps()
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents/trainers/env_manager.py"", line 113, in get_steps
    new_step_infos = self._step()
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents/trainers/subprocess_env_manager.py"", line 276, in _step
    raise env_exception
mlagents_envs.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that :
	 The environment does not need user interaction to launch
	 The Agents' Behavior Parameters > Behavior Type is set to ""Default""
	 The environment and the Python interface have compatible versions.
```

**Second job:**
Player-0.log
```
Mono path[0] = '/cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/2/ecosim_Data/Managed'
Mono config path = '/cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/2/ecosim_Data/MonoBleedingEdge/etc'
Preloaded 'lib_burst_generated.so'
Preloaded 'libgrpc_csharp_ext.x64.so'
Initialize engine version: 2020.1.9f1 (145f5172610f)
[Subsystems] Discovering subsystems at path /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/2/ecosim_Data/UnitySubsystems
Forcing GfxDevice: Null
GfxDevice: creating device client; threaded=0
NullGfxDevice:
    Version:  NULL 1.0 [1.0]
    Renderer: Null Device
    Vendor:   Unity Technologies
Begin MonoManager ReloadAssembly
- Completed reload, in  2.447 seconds
ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)
ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)
ERROR: Shader GUI/Text Shader shader is not supported on this GPU (none of subshaders/fallbacks are suitable)
ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)
WARNING: Shader Unsupported: 'Standard (Specular setup)' - All subshaders removed
WARNING: Shader Did you use #pragma only_renderers and omit this platform?
WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?
ERROR: Shader Standard (Specular setup) shader is not supported on this GPU (none of subshaders/fallbacks are suitable)
WARNING: Shader Unsupported: 'Standard (Specular setup)' - All subshaders removed
WARNING: Shader Did you use #pragma only_renderers and omit this platform?
WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?
ERROR: Shader Legacy Shaders/Particles/Alpha Blended Premultiply shader is not supported on this GPU (none of subshaders/fallbacks are suitable)
ERROR: Shader FX/Water (Basic) shader is not supported on this GPU (none of subshaders/fallbacks are suitable)
WARNING: Shader Unsupported: 'Standard' - All subshaders removed
WARNING: Shader Did you use #pragma only_renderers and omit this platform?
WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?
ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)
WARNING: Shader Unsupported: 'Standard' - All subshaders removed
WARNING: Shader Did you use #pragma only_renderers and omit this platform?
WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?
UnloadTime: 0.971674 ms
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/2/ecosim_Data/Mono/libcoreclr.so
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/2/ecosim_Data/Mono/libcoreclr.so
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/2/ecosim_Data/Mono/libcoreclr.so
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/2/ecosim_Data/Mono/libSystem.dylib
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/2/ecosim_Data/Mono/libSystem.dylib.so
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/2/ecosim_Data/Mono/libSystem.dylib
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/2/ecosim_Data/Mono/libcoreclr.so
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/2/ecosim_Data/Mono/libcoreclr.so
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/2/ecosim_Data/Mono/libcoreclr.so
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/2/ecosim_Data/Mono/libSystem.dylib
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/2/ecosim_Data/Mono/libSystem.dylib.so
Fallback handler could not load library /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/2/ecosim_Data/Mono/libSystem.dylib
Couldn't connect to trainer on port 5005 using API version 1.0.0. Will perform inference instead.
```


Standard output:
```
2021-05-04 10:01:39.159955: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
WARNING:tensorflow:From /cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2021-05-04 10:02:31 INFO [learn.py:275] run_seed set to 6525


                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓

        
 Version information:
  ml-agents: 0.23.0,
  ml-agents-envs: 0.23.0,
  Communicator API: 1.3.0,
  PyTorch: 1.7.1
Found path: /cephyr/users/tobiaka/Alvis/ecosim-predator-prey/builds/dynamics_e1/2/ecosim.x86_64
2021-05-04 10:03:31 INFO [environment.py:409] Environment timed out shutting down. Killing...
2021-05-04 10:03:31 INFO [subprocess_env_manager.py:186] UnityEnvironment worker 0: environment stopping.
2021-05-04 10:03:31 INFO [trainer_controller.py:85] Saved Model
Traceback (most recent call last):
  File ""/cephyr/users/tobiaka/Alvis/.local/bin/mlagents-learn"", line 8, in <module>
    sys.exit(main())
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents/trainers/learn.py"", line 280, in main
    run_cli(parse_command_line())
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents/trainers/learn.py"", line 276, in run_cli
    run_training(run_seed, options)
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents/trainers/learn.py"", line 153, in run_training
    tc.start_learning(env_manager)
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents/trainers/trainer_controller.py"", line 174, in start_learning
    self._reset_env(env_manager)
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents/trainers/trainer_controller.py"", line 109, in _reset_env
    env_manager.reset(config=new_config)
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents/trainers/env_manager.py"", line 67, in reset
    self.first_step_infos = self._reset_env(config)
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents/trainers/subprocess_env_manager.py"", line 299, in _reset_env
    ew.previous_step = EnvironmentStep(ew.recv().payload, ew.worker_id, {}, {})
  File ""/cephyr/users/tobiaka/Alvis/.local/lib/python3.8/site-packages/mlagents/trainers/subprocess_env_manager.py"", line 95, in recv
    raise env_exception
mlagents_envs.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that :
	 The environment does not need user interaction to launch
	 The Agents' Behavior Parameters > Behavior Type is set to ""Default""
	 The environment and the Python interface have compatible versions.
```

What is interesting is the last line in the player log for the second simulation, ""Couldn't connect to trainer on port 5005 using API version 1.0.0. Will perform inference instead."". This is probably the cause of the error. The second job cant connect to the trainer and assumes inference only instead and this somehow also makes the first job fail.",output get standard output start two node seem fail start first job mono path mono path initialize engine version path forcing null device client version null renderer null device vendor unity begin reload error shader shader none suitable error shader shader none suitable error shader shader shader none suitable error shader legacy shader none suitable warning shader unsupported specular setup removed warning shader use omit platform warning shader removal intentional may forgotten turning fallback error shader standard specular setup shader none suitable warning shader unsupported specular setup removed warning shader use omit platform warning shader removal intentional may forgotten turning fallback error shader legacy blended premultiply shader none suitable error shader basic shader none suitable warning shader unsupported removed warning shader use omit platform warning shader removal intentional may forgotten turning fallback error shader standard shader none suitable warning shader unsupported removed warning shader use omit platform warning shader removal intentional may forgotten turning fallback fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library warning communication unity python differ minor version level python unity python library version may work unless upgrade package lower find work best together release page setting worker enlighten thread id priority thread id priority thread id priority thread id priority thread id priority thread id priority thread id priority thread id priority thread id priority thread id priority thread id priority thread id priority thread id priority thread id priority thread id priority thread id priority standard output successfully dynamic library warning removed future version long term set version information communicator found path connected unity environment package version communication version connected new brain deer connected new brain wolf behavior name deer beta epsilon linear normalize false simple memory none extrinsic gamma strength threaded true none none framework starting training step saving behavior name wolf beta epsilon linear normalize false simple memory none extrinsic gamma strength threaded true none none framework starting training step saving error exception calling application ran input recent call last file line behavior argument context file line exchange return file line return ran input error exception calling application invalid load key recent call last file line behavior argument context file line exchange return file line return invalid load key worker environment stopping environment shut return code converting copied converting copied saved model recent call last file line module main file line main file line file line file line wrapped return file line file line wrapped return file line advance file line file line raise unity environment took long respond make sure environment need user interaction launch behavior behavior type set default environment python interface compatible second job mono path mono path initialize engine version path forcing null device client version null renderer null device vendor unity begin reload error shader shader none suitable error shader shader none suitable error shader shader shader none suitable error shader legacy shader none suitable warning shader unsupported specular setup removed warning shader use omit platform warning shader removal intentional may forgotten turning fallback error shader standard specular setup shader none suitable warning shader unsupported specular setup removed warning shader use omit platform warning shader removal intentional may forgotten turning fallback error shader legacy blended premultiply shader none suitable error shader basic shader none suitable warning shader unsupported removed warning shader use omit platform warning shader removal intentional may forgotten turning fallback error shader standard shader none suitable warning shader unsupported removed warning shader use omit platform warning shader removal intentional may forgotten turning fallback fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library could connect trainer port version perform inference instead standard output successfully dynamic library warning removed future version long term set version information communicator found path environment timed shutting killing worker environment stopping saved model recent call last file line module main file line main file line file line file line wrapped return file line file line wrapped return file line file line reset file line file line raise unity environment took long respond make sure environment need user interaction launch behavior behavior type set default environment python interface compatible interesting last line player log second simulation could connect trainer port version perform inference probably cause error second job cant connect trainer inference instead somehow also first job fail,issue,negative,positive,positive,positive,positive,positive
831734958,"I will check the logs, I can't access them right now though due to the compute-server being down. Do you mean the log in run_logs that mlagents put in the results folder during a run?
Do you have any idea what possible causes there might be in cases when one environment prevents two from running at the same time? It seems like the second possibility is less likely due to the huge amount of resources available and that the environment runs fine if it is the only Unity simulation running on the node (there can be other jobs running and it is fine, but not Unity jobs).",check ca access right though due mean log put folder run idea possible might one environment two running time like second possibility le likely due huge amount available environment fine unity simulation running node running fine unity,issue,positive,positive,positive,positive,positive,positive
831500832,"It looks like the environment stopped responding. I see two possibilities : Either one of the two executables never launched (There is something in the environment that prevents two environments to run at the same time on the same node) OR one of the environment somehow crashed (ran out of resources for example).
Since the error seems to come from inside the Unity Environment, can you look into the executable [logs](https://docs.unity3d.com/Manual/LogFiles.html)? They might give some hints as to why this happens. ",like environment stopped see two either one two never something environment two run time node one environment somehow ran example since error come inside unity environment look executable might give,issue,negative,neutral,neutral,neutral,neutral,neutral
831254616,"Hey,
We are a small team and aren’t always able to respond immediately to
threads.  Please have some patience with us.  I’m sure he will get back to
you soon.
Cheers,
Chris

On Mon, May 3, 2021 at 01:45 Nathan ***@***.***> wrote:

> HI @surfnerd <https://github.com/surfnerd>,
> In this thread
> https://forum.unity.com/threads/some-advice-and-suggestions-python-status-maxstep-vs-maxenvironmentsteps-and-ray-sensor-useage.1097416/
>
> I receive a reply and then send the next advice. To prevent him from
> didn't get a notice I tag him again. But it seems he still doesn't see my
> next advice. Should I open a new thread or did I do something wrong?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/5243#issuecomment-831117677>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAKLGSEQVWY5WGUZIMA6S23TLZPEDANCNFSM42YYFMKQ>
> .
>
-- 
*Chris Goy*
Senior Software Developer | ML Agents
San Francisco
<https://unity3d.com/?utm_source=unity3d&utm_medium=email&utm_campaign=company-information-2016-Global-Unity-Email-Sig>
",hey small team always able respond immediately please patience u sure get back soon mon may wrote hi thread receive reply send next advice prevent get notice tag still see next advice open new thread something wrong reply directly view goy senior developer san,issue,negative,positive,neutral,neutral,positive,positive
831117677,"HI @surfnerd,
In this thread [https://forum.unity.com/threads/some-advice-and-suggestions-python-status-maxstep-vs-maxenvironmentsteps-and-ray-sensor-useage.1097416/](https://forum.unity.com/threads/some-advice-and-suggestions-python-status-maxstep-vs-maxenvironmentsteps-and-ray-sensor-useage.1097416/)

I receive a reply and then send the next advice. To prevent him from didn't get a notice I tag him again. But it seems he still doesn't see my next advice. Should I open a new thread or did I do something wrong?",hi thread receive reply send next advice prevent get notice tag still see next advice open new thread something wrong,issue,negative,negative,neutral,neutral,negative,negative
830678356,"I cannot open this gameassets.zip file. Can you try using a `.unitypackage` format?
As long as you cannot make the agent move using heuristic mode, the agent will stand no chance. Does this line 
```
print(""onaction x: "" + controlSignal.x);
```
get called? Are you sure OnActionReceived is called? Can you try manually applying a force to the object until the object moves? What is the mass of the rigidbody of the Agent (If it is too high, it might prevent it from moving)?

You seem to be able to run the example environment properly, so I think the issue is in your code. Either there is something preventing the agent from moving or not enough force is applied to it.",open file try format long make agent move heuristic mode agent stand chance line print get sure try manually force object object mass agent high might prevent moving seem able run example environment properly think issue code either something agent moving enough force applied,issue,positive,positive,positive,positive,positive,positive
830676411,"My mistake was not enabling preview packages in Unity's package manager, which was why everything was out of sync as you say!",mistake preview unity package manager everything sync say,issue,negative,neutral,neutral,neutral,neutral,neutral
830676157,"hi, thanks for the response.

I updated my code and tried it too. Still the same result. The cube is not moving at all.
[gameassets.zip](https://github.com/Unity-Technologies/ml-agents/files/6410114/gameassets.zip)

",hi thanks response code tried still result cube moving,issue,negative,positive,positive,positive,positive,positive
830668106,"> Integrating our own implementation of DDPG would be a relatively important amount of work without a clear benefit to our users. What are some benefits you see in having an implementation of DDPG in addition to PPO and SAC?

I'm working on a project which I hope to be able to compare several RL algorithms on top of PPO and SAC but of course, no complaints with what ML-Agents is already offering, just being a little greedy here :P 

Thanks so much @vincentpierre !
",implementation would relatively important amount work without clear benefit see implementation addition sac working project hope able compare several top sac course already offering little greedy thanks much,issue,negative,positive,positive,positive,positive,positive
830667128,"Hi @lukebandy 

It looks like the version of the trainers you use and the C# package you use are several versions appart. According to [this page](https://docs.unity3d.com/Packages/com.unity.ml-agents@1.0/manual/index.html) the version 1.0.7 of the ml-agents package only works with trainers on versions 0.16.1 to 0.20.0. You could either downgrade your trainers to version 0.20.0 or upgrade your Unity package to a newer version.",hi like version use package use several according page version package work could either downgrade version upgrade unity package version,issue,negative,neutral,neutral,neutral,neutral,neutral
830666415,"> The cube is not moving even in the Heuristic only mode.

Well this is your issue. If you cannot move the cube at all, then the agent would be unable to as well. This is probably why the agent always had a reward of 0.

>These are the codes I just used from the Creating new environment section of unity docs.

```
this.rBody.velocity= new Vector3(controlSignal.x, 0, controlSignal.z);
```
This does not look like the proper way to modify the velocity of a rigidbody, I think you need to use `AddForce`

```
public float forceMultiplier = 10;
```
This float is not used anywhere in your code, I think this might be one of the causes of the problem.

Let me know if you manage to get it to work. This tutorial is central for our users, so I need to make sure it actually works.",cube moving even heuristic mode well issue move cube agent would unable well probably agent always reward used new environment section unity new vector look like proper way modify velocity think need use public float float used anywhere code think might one problem let know manage get work tutorial central need make sure actually work,issue,positive,positive,neutral,neutral,positive,positive
830665403,"Hi @MrOCW I am glad you were able to run PPO2 !

> What is that 00001 file

I do not know, I think this is a question more suitable for the baseline authors. My guess is that this is where the weights of the models are being saved.

> how do I use it for inference

We do not support running inference on models that have not been trained with our trainers. I think it would be possible if you converted the model to ONNX and renamed all the inputs and outputs of the models in our format but we do not support this use case.

> Could you also provide an example of using DDPG as well? or even better, integrate it into mlagents

Integrating our own implementation of DDPG would be a relatively important amount of work without a clear benefit to our users. What are some benefits you see in having an implementation of DDPG in addition to PPO and SAC? ",hi glad able run file know think question suitable guess saved use inference support running inference trained think would possible converted model format support use case could also provide example well even better integrate implementation would relatively important amount work without clear benefit see implementation addition sac,issue,positive,positive,positive,positive,positive,positive
830663967,"I think this line is problematic : `actionSegment[0] = Mathf.Clamp(Mathf.RoundToInt(look.x),-24,24);`
Discrete actions with a branch size of 50 will expect integers between 0 and 49, not between -24 and 24.",think line problematic discrete branch size expect,issue,negative,neutral,neutral,neutral,neutral,neutral
830508268,"@vincentpierre,

I could able to train the example environments. I have just now trained 3DBall example. It trains fine with increasing mean and standard rewards.
![error5](https://user-images.githubusercontent.com/6459572/116769538-8e882e00-aa5a-11eb-944b-89da7df40c94.png)
",could able train example trained example fine increasing mean standard error,issue,negative,positive,positive,positive,positive,positive
830495234,"Thanks for the Response.

The cube is not moving even in the Heuristic only mode. These are the codes I just used from the *Creating new environment* section of unity docs. 

",thanks response cube moving even heuristic mode used new environment section unity,issue,negative,positive,positive,positive,positive,positive
830276008,"Hi @surfnerd 

Thanks for the very fast reply.  That fixed my issue.

I've included another screenshot with the 3D ball import option for others who stumble across this thread:
![Capture2](https://user-images.githubusercontent.com/83470418/116737159-b79ab700-a9a5-11eb-98aa-dbb99e501609.PNG)
",hi thanks fast reply fixed issue included another ball import option stumble across thread capture,issue,negative,positive,positive,positive,positive,positive
830272506,"Hey @Jconrad15,
The sample shows up in version 1.9.1-preview and later.  I will call that out in the docs when I get to the task of fixing this issue. Thank you for the feedback!",hey sample version later call get task fixing issue thank feedback,issue,negative,neutral,neutral,neutral,neutral,neutral
830270661,"Hi @surfnerd,

I am having a similar problem as joslat.  

I followed through the installation section here: 
       https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Installation.md

The end of the installation section includes a Next Step of working though the Getting Started Guide here: 
       https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Getting-Started.md

During the Getting Started Guide I navigate to the ML Agents package in the Package Manager (per steps 1 and 2), as shown in: 
![Capture1](https://user-images.githubusercontent.com/83470418/116736106-4a3a5680-a9a4-11eb-94e8-4849e63cdbba.PNG)

However, there is no option to import any samples.  This is shown in the included screenshot: 
![Capture](https://user-images.githubusercontent.com/83470418/116735935-12331380-a9a4-11eb-9af7-7aa4529f25ab.PNG)

I am unsure why the samples are not showing in the ML Agents package. 

Thanks.",hi similar problem installation section end installation section next step working though getting guide getting guide navigate package package manager per shown capture however option import shown included capture unsure showing package thanks,issue,negative,positive,neutral,neutral,positive,positive
830261562,"Hi @invincibledinku,
Are you able to train on the example environments? If yes, this type of issue is more suitable for a forum question.
Are you able to solve the task when using the mode `Heuristic Only` in the `Behavior Parameters` window? I suspect your issue is that the force applied to the agent is not high enough to move the cube and this can be diagnosed with the Heuristic Only mode. It is also possible that in order to move towards the goal, the agent has to maintain a direction for a long time before moving. This type of behavior can be very hard for an agent to solve. 
How long have you been running the training for? This could be an exploration problem and it might be solved with some intermediate rewards.",hi able train example yes type issue suitable forum question able solve task mode heuristic behavior window suspect issue force applied agent high enough move cube heuristic mode also possible order move towards goal agent maintain direction long time moving type behavior hard agent solve long running training could exploration problem might intermediate,issue,negative,positive,positive,positive,positive,positive
829928163,"Thanks @vincentpierre , following your code, PPO2 is running on my own environment with 2 continuous actions and 1 CameraSensor. However, while running, i dont seem to get any logs of the results while training. I tried setting this as the arguments,
```
ppo2.learn(
        network=""cnn"",
        env=env,
        total_timesteps=5000,
        lr=1e-3,
        log_interval=1000,
        save_interval=2500
```

Only at the end of training, I get this
```
--------------------------------------
| eplenmean               | 25.5     |
| eprewmean               | 1.17     |
| fps                     | 42       |
| loss/approxkl           | 0.00469  |
| loss/clipfrac           | 0.0546   |
| loss/policy_entropy     | 2.84     |
| loss/policy_loss        | -0.0039  |
| loss/value_loss         | 2.75     |
| misc/explained_variance | -0.0158  |
| misc/nupdates           | 1        |
| misc/serial_timesteps   | 2.05e+03 |
| misc/time_elapsed       | 96.2     |
| misc/total_timesteps    | 4.1e+03  |
--------------------------------------

Saving to ./logs/checkpoints/00001
```
What is that 00001 file and how do I use it for inference?

Could you also provide an example of using DDPG as well? or even better, integrate it into mlagents",thanks following code running environment continuous however running dont seem get training tried setting end training get saving file use inference could also provide example well even better integrate,issue,positive,positive,positive,positive,positive,positive
829892965,"I'm trying to make the AI shoot a target
To Reproduce:
1. I'm Following the tutorial on youtube to set up the environment and the imitation learning (by CodeMonkey)
2. Record the demo
3. then put in on the yaml file
4. run in cmd (mlagents-learn config/Shooting.yaml --force --time-scale 1)
5. and After 10000-20000 steps it's freeze and the error showing.

```
public override void OnActionReceived(ActionBuffers actions)
    {
        int directionX = actions.DiscreteActions[0];
        int directionY = actions.DiscreteActions[1];
        
        
        rotationTr.LookAt (new Vector3 (directionX,directionY,transform.position.z));
        int shoot = actions.DiscreteActions[2];
        if (shoot == 1 && timeToShoot <= Time.time)
        {
            Shoot ();
            timeToShoot = Time.time + (1f / fireRate);
        }
        AddReward (-1f / MaxStep);
    }

    public override void Heuristic(in ActionBuffers actionsOut)
    {
        ActionSegment<int> actionSegment = actionsOut.DiscreteActions;
        if (Input.GetKey (KeyCode.Space))
        {
            actionSegment[2] = 1;
        }
        else
        {
            actionSegment[2] = 0;
        }
        Ray ray = Camera.main.ScreenPointToRay (Input.mousePosition);
        RaycastHit hit;
        if (Physics.Raycast (ray, out hit))
        {
            Vector3 look = hit.point;
            look.z = 0;
            rotationTr.LookAt (look);
            actionSegment[0] = Mathf.Clamp(Mathf.RoundToInt(look.x),-24,24);
            actionSegment[1] = Mathf.Clamp (Mathf.RoundToInt (look.y), -24, 24);
        }
    }
```
![image](https://user-images.githubusercontent.com/59899986/116661327-d9586600-a9c6-11eb-806d-2c7c5686c86f.png)
",trying make ai shoot target reproduce following tutorial set environment imitation learning record put file run force freeze error showing public override void new vector shoot shoot shoot public override void heuristic else ray ray hit ray hit vector look look image,issue,negative,positive,neutral,neutral,positive,positive
829442640,"Cool, glad you were able to figure it out.",cool glad able figure,issue,positive,positive,positive,positive,positive,positive
829423293,"> Error from DQN Baseline

Looks like you were able to fix the error

> Based on source code (init.py), i removed 0 and the new error is

It looks like DQN is expecting a discrete action space (illustrated by the `.n`) but the gym environment provided a continuous action space (Box). I. think you need either make your environment discrete or somehow discretize your action space to be able to use DQN. I am able to train with an environment that has a single discrete action

Regarding the error with PPO2, I do not know what this error is. It looks like the observation is of the wrong dimension. The training seemed to start properly for me with an environment with a single visual observation and 4 discrete actions (on a single branch). But I had to change the environment creation method : 
```
    def make_env(rank, use_visual=True): # pylint: disable=C0111
        def _thunk():
            unity_env = UnityEnvironment(env_directory, base_port=5000 + rank)
            env = UnityToGymWrapper(unity_env, uint8_visual=True)
            env = Monitor(env, logger.get_dir() and os.path.join(logger.get_dir(), str(rank)))
            return env
```
",error like able fix error based source code removed new error like discrete action space gym environment provided continuous action space box think need either make environment discrete somehow action space able use able train environment single discrete action regarding error know error like observation wrong dimension training start properly environment single visual observation discrete single branch change environment creation method rank rank monitor rank return,issue,negative,negative,neutral,neutral,negative,negative
829389000,"I think we need to change 
https://github.com/Unity-Technologies/ml-agents/blob/4e627782cee6d2e16703806650eed44c1ff7f69b/.yamato/test_versions.metafile#L10
too (OK if that's in another PR though)",think need change another though,issue,negative,neutral,neutral,neutral,neutral,neutral
829375130,I cannot know for sure without a way to reproduce it. All I know is that the error is happening when the trainer is processing discrete demo actions. I tried to reproduce with invalid actions but I was unable to. Please share a way to reproduce this issue.,know sure without way reproduce know error happening trainer discrete tried reproduce invalid unable please share way reproduce issue,issue,negative,neutral,neutral,neutral,neutral,neutral
829274607,"Thanks for your reply. While I see your argument that maybe you want to switch to another branch later, I don't think it's a good practice to let every user download a 2GB+ repo (that will keep increasing) while most of the time they don't need it.

I did try to clone all branches shallowly (with `--depth 1 --no-single-branch`), it saves a bit but not that much unfortunatly.

| Comparison | Full clone | All branches shallow clone | Single branch shallow clone |
|------------|-----------:|---------------------------:|----------------------------:|
| Download   |   1.92 GiB |                   1.61 GiB |                    94.6 MiB |
| Objects    |      77402 |                      28394 |                        2233 |
| Size       |    2.08 GB |                    1.76 GB |                      244 MB |

I feel that at least we could add a message somewhere to consider a shallow clone, since the overwhelming majority will build directly or only check the documentation, for which only a shallow, single branch is needed.",thanks reply see argument maybe want switch another branch later think good practice let every user keep increasing time need try clone shallowly depth bit much comparison full clone shallow clone single branch shallow clone gib gib mib size feel least could add message somewhere consider shallow clone since overwhelming majority build directly check documentation shallow single branch,issue,positive,negative,neutral,neutral,negative,negative
828972443,"Thanks for the suggestion, the issue solved. I tried the memory profiler and found out that there is an old script I used to store log info has not setup a size limitation during training, which caused memory leak issue.",thanks suggestion issue tried memory profiler found old script used store log setup size limitation training memory leak issue,issue,negative,positive,positive,positive,positive,positive
828943296,"you right, when I try using different learning scenario and different demo in the same environment it's working just fine.
is the learning scenario that causes this issue? because when I using the old learning scenario(Which causing the problem) and using the different demo it keeps crashing.",right try different learning scenario different environment working fine learning scenario issue old learning scenario causing problem different,issue,negative,positive,positive,positive,positive,positive
828885142,"> * It seems to be just a markdownification of the docstrings of the mlagents_envs package. I do not see how this is useful.

Perhaps. But at least one user/customer does find it useful, and likely more do as well. Also, I can think of several reasons why this would be useful:

1. SEO for when users are searching for API info on the ML Agents Python API.
2. Removes a pain point for users/customers to have to generate these themselves.
3. Can easily be setup to be hosted on something like readthedocs.io

> * I assume (hope) you used a tool to generate this documentation and I think it would be a lot more useful if this PR contained the code to generate this documentation. 

Yes, of course this was automatically generated. I used pydoc-markdown to generate the api docs. I can include the yaml config file for it in the mlagents_envs package. Great idea. I'll add this in.

> *You could also have a pre-commit hook to detect changes on the mlagents_envs package and regenerate the markdown when needed. This would be much more future-proof!

Great suggestion. I was originally going to do that, but I wasn't sure what the team's best practices are/were. I'll add this in.

",package see useful perhaps least one find useful likely well also think several would useful searching python pain point generate easily setup something like assume hope used tool generate documentation think would lot useful code generate documentation yes course automatically used generate include file package great idea add could also hook detect package regenerate markdown would much great suggestion originally going sure team best add,issue,positive,positive,positive,positive,positive,positive
828878444,"We are on the same page there. What I wish for is: ""decisions steps + terminal steps to be constant in a single executable,""

> In most environment, there is no reason to assume all agents will make decisions at the same time. We allow agents to spawn and to be destroyed at anytime during the simulation, which is something gym cannot do.

I agree with you that there is no inherent reason why the decisions of the agents should be synced up in an executable. Honestly, this is more of an unfortunate side-effect of a really valid design decision of UnityML not playing nicely with the GYM paradigm. 

Unfortunately, this fact also makes it difficult to work with other learning systems that expects the gym style of same number of observations every step.

> Unless you create your own environment where you can control exactly the timing where the agents request decisions and are reset, you will have a variable number of decisions and terminal steps.

Now that you mention it this might be the way to go! I am working on a simple navigation environment, and call ```Done()``` when the agent reaches a goal. I can make it so that the agent waits until the next ``` if (academyStepCount % DecisionPeriod == 0)``` to call done. Meanwhile the agent can just... wait I guess, it is just a few frames anyway.

A simple API enhancement (I don't know how many people would really use it, it feels niche) might be to have something like a ```bool SyncDonesWithDecisionRequest``` on the DecisionRequester component which implements the above idea.

Thank you @vincentpierre, this conversation is very helpful!

PS: I felt like I was going crazy when I was trying to update the C# code. I would make a change then poof! Hah :D 
![image](https://user-images.githubusercontent.com/6670976/116490623-aee29d80-a84c-11eb-954e-d8bc6f57e539.png)",page wish terminal constant single executable environment reason assume make time allow spawn simulation something gym agree inherent reason executable honestly unfortunate really valid design decision nicely gym paradigm unfortunately fact also difficult work learning gym style number every step unless create environment control exactly timing request reset variable number terminal mention might way go working simple navigation environment call done agent goal make agent next call done meanwhile agent wait guess anyway simple enhancement know many people would really use niche might something like bool component idea thank conversation helpful felt like going crazy trying update code would make change poof hah image,issue,positive,negative,neutral,neutral,negative,negative
828869752,"I think there is some confusion with what we mean by environment. In the Blog post you shared, environment refers to a whole executable. When calling `env.step()` there is only one executable (maybe with several agents).
I think you would want to have the number of decisions steps + terminal steps to be constant in a single executable, but in all honesty, I do not know how to make this happen.
In most environment, there is no reason to assume all agents will make decisions at the same time. We allow agents to spawn and to be destroyed at anytime during the simulation, which is something gym cannot do.
Unless you create your own environment where you can control exactly the timing where the agents request decisions and are reset, you will have a variable number of decisions and terminal steps.
Do you have suggestions on how to improve the API?",think confusion mean environment post environment whole executable calling one executable maybe several think would want number terminal constant single executable honesty know make happen environment reason assume make time allow spawn simulation something gym unless create environment control exactly timing request reset variable number terminal improve,issue,positive,positive,neutral,neutral,positive,positive
828866771,"I would need from you a minimal way to reproduce it (like the issue template asks) so I can help.
My current guess is that the demo file contains an illegal action and when trying to encode it into onehot, it causes this issue. ",would need minimal way reproduce like issue template help current guess file illegal action trying encode issue,issue,negative,negative,negative,negative,negative,negative
828863232,"Hi @EwoutH 

Thank you so much for your PR. Unfortunately, I think using the `--depth 1` argument will make it impossible to later pull and go to another branch like the main branch.

Regarding the release_17 tag. This is intended, the tag points to the commit the release was made on. We usually do not push to the release branch after that.",hi thank much unfortunately think depth argument make impossible later pull go another branch like main branch regarding tag intended tag commit release made usually push release branch,issue,negative,negative,negative,negative,negative,negative
828604873,"`Max Step` in the Agent parameters corresponds to the number of Decisions the Agent will make before it will begin a new episode (regardless of wether or not EndEpisode() was called). This is to make sure the Agent will nit stuck itself and never be able to complete the task. Think of it as a timeout. See [here](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Design-Agents.md#agent-properties) for more information.

`Max Step` in the yaml config file corresponds to the total number of experience points collected for training. See [here](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md#common-trainer-configurations) for more information. (The choice of names for these two is unfortunate)

This does not look like a proper feature request issue, so I will close it. I encourage you to as in the forums if you have general questions about ML-Agents and to post a bug issue if you encounter a bug.",step agent number agent make begin new episode regardless wether make sure agent nit stuck never able complete task think see information step file total number experience collected training see information choice two unfortunate look like proper feature request issue close encourage general post bug issue encounter bug,issue,positive,positive,neutral,neutral,positive,positive
828324645,"> Hi @shohanulka,
> This question is better suited for the [ML-Agents forums](https://forum.unity.com/forums/ml-agents.453/) - can you please re-ask it there? I'll reply there when you do.
> 
> Thanks!

thank you for your kind reply, I have working for a project for a showcase with unity-ml agent, been working with this around 2 months but still unable to get desired result. here is the forum link please guid me how can I train my agent properly. 

https://forum.unity.com/threads/behavioural-cloning-with-gail-agent-behaving-wired.1100698/",hi question better please reply thanks thank kind reply working project showcase agent working around still unable get desired result forum link please train agent properly,issue,positive,positive,positive,positive,positive,positive
828029977,"Hi @shohanulka,
This question is better suited for the [ML-Agents forums](https://forum.unity.com/forums/ml-agents.453/) - can you please re-ask it there? I'll reply there when you do.

Thanks!",hi question better please reply thanks,issue,positive,positive,positive,positive,positive,positive
828000755,"I think you need to attach a debugger or add some extra logging to your environment, and find out what's causing it to stop responding. A common case of this is an infinite loop in user code.",think need attach add extra logging environment find causing stop common case infinite loop user code,issue,negative,negative,negative,negative,negative,negative
827965999,"Also I noticed that the `release_17` branch is cloned, which points to the tag, and not the `release_17_branch`, which points to the maintenance branch. Is this intended?",also branch tag maintenance branch intended,issue,negative,neutral,neutral,neutral,neutral,neutral
826163702,"![snapshot](https://user-images.githubusercontent.com/42013223/115974892-f4cdf780-a514-11eb-8584-5adef2126fb5.png)
Thanks for replying, for steps less than 10,000, it terminate in this way as shows in the pic, ""The unity environment took too long to respond."" For steps more than 10,000, it simply freezes half way during the training, no callstack shows.",snapshot thanks le terminate way pic unity environment took long respond simply half way training,issue,negative,negative,neutral,neutral,negative,negative
826057275,"Hi ,

Sorry ,This is a issue from my side , the python package was way ahead of 0.16.1 recommended for release 1.0.7.

Please close this issue.

Thanks
 ",hi sorry issue side python package way ahead release please close issue thanks,issue,positive,negative,negative,negative,negative,negative
826031803,"It seems that that is the problem. No error appeared after I fixed the trigger and increased my summary freq to 10k.
![image](https://user-images.githubusercontent.com/27879091/115946980-ee2c8980-a4ee-11eb-8208-494c0843933d.png)

My agent is still not behaving how I wished it to be but I think that's another problem on it's own",problem error fixed trigger summary image agent still wished think another problem,issue,negative,positive,neutral,neutral,positive,positive
826026402,"Hi ,

I don't think its instillation issue ,  the  3DBall runs just fine.

2021-04-24 08:33:01 INFO [stats.py:139] 3DBall. Step: 50000. Time Elapsed: 227.826 s. Mean Reward: 1.331. Std of Reward: 0.829. Training.
2021-04-24 08:36:46 INFO [stats.py:139] 3DBall. Step: 100000. Time Elapsed: 452.419 s. Mean Reward: 2.272. Std of Reward: 1.512. Training.
2021-04-24 08:40:14 INFO [stats.py:139] 3DBall. Step: 150000. Time Elapsed: 660.560 s. Mean Reward: 4.112. Std of Reward: 3.658. Training.

**Only training my agent causes the issue , that too after some successful >5000 steps of training , I can see clearly that training started  in GUI** 

Yes I have one agent with Behavior type set to ""Default"" , Yes I have Decision Requestor. 

No , there is no other Information printed other than mentioned in above/below comment after this training crashes and exits.

c:\users\dell\anaconda3\envs\ml-agents_rel1\lib\site-packages\mlagents\trainers\torch\utils.py:267: UserWarning: This overload of nonzero is deprecated:
        nonzero()
Consider using one of the following signatures instead:
        nonzero(*, bool as_tuple) (Triggered internally at  ..\torch\csrc\utils\python_arg_parser.cpp:882.)
  res += [data[(partitions == i).nonzero().squeeze(1)]]



Let me know if I can provide any specific information from any log.



",hi think instillation issue fine step time mean reward reward training step time mean reward reward training step time mean reward reward training training agent issue successful training see clearly training yes one agent behavior type set default yes decision information printed comment training overload nonzero nonzero consider one following instead nonzero bool triggered internally data let know provide specific information log,issue,positive,positive,neutral,neutral,positive,positive
826010580,"I kept the max step as 0 as I've set the EndEpisode() to trigger after a set of time with this script. ""timeout"" being the one that trigger the EndEpisode() and restart that reset agent position and everything.

```
using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using UnityEngine.SceneManagement;

public class scRestart : MonoBehaviour
{
    // Start is called before the first frame update
    public float maxGameTime;
    public float elapsedTime;

    void Start()
    {
        maxGameTime = 60f;
    }


    void FixedUpdate()
    {
        if(elapsedTime>maxGameTime){
            EventManager.TriggerEvent(""timeout"", new EventParam());
            elapsedTime = 0f;
        }else{
            elapsedTime += Time.deltaTime;
        }
    }

    void Update() {
        
        if (Input.GetKeyDown(KeyCode.R))
        {
            EventManager.TriggerEvent(""restart1"", new EventParam());
            EventManager.TriggerEvent(""restart2"", new EventParam());
        }
    }
    void OnMouseClick(){
        //SceneManager.LoadScene(SceneManager.GetActiveScene().buildIndex);
    }
}
```

I'm training symmetrical adverserial agent, does that count as a group. There are only 2 agent in total, one in each team. I didn't set the parameter for environment number so I assume it would be only 1.

As seen on the graphs, the episode length is +-350steps and +-700 steps (after change). I think this should be enough for the summary to catch up considering there's only two agent. And if they didn't it's weird that suddenly there's suddenly no episode summary after hundreds of episode and consistently none after. But, i will keep this in mind and try to increase the summary size.

My agent is learning, but it's still not up to what I intended them to do. I'm afraid, this error means that after a certain episode, my agent stop learning and the model just went haywire. Considering I'm planning to create my own custom learning algorithm, I wish I could get the agent learn with the already built algorithm. So I could minimize the window of where things went wrong.




Below is the screenshot of a new training session I did with some tweak to the hyperparameter and my agent action output. While tweaking, I tried to run the training, and it throws the No Episode, then I tweak again but still error, but then it somehow didn't in this version. I didn't change the behaviour name in the config nor unity editor.
![image](https://user-images.githubusercontent.com/27879091/115941300-068cac00-a4cf-11eb-8aba-05385cd1dfdd.png)

Also, this time it throws no episode from the start of training. Another thing I would note is, in this version the episode length is +-1400 steps and summary is 2000, so summary being less than episode length might be the case.
![image](https://user-images.githubusercontent.com/27879091/115941966-f0341f80-a4d1-11eb-95d6-94991dd430c7.png)


This is the training config
![image](https://user-images.githubusercontent.com/27879091/115941860-83208a00-a4d1-11eb-8c8c-408a60a699d4.png)


I will update on this with increased summary freq



Added : While writing this comment, I decided to review my agent code and tried to comment out the name of the agent when it end it's episode. Apparently only one agent actually end the episode. This seems to be caused by the other agent somehow doesn't receive the timeout trigger. I'm ashamed that I didn't thought of fixing the timeout trigger when I fixed my restart trigger. I will update on this after i fixed it.",kept step set trigger set time script one trigger restart reset agent position everything public class start first frame update public float public float void start void new else void update restart new restart new void training symmetrical agent count group agent total one team set parameter environment number assume would seen episode length change think enough summary catch considering two agent weird suddenly suddenly episode summary episode consistently none keep mind try increase summary size agent learning still intended afraid error certain episode agent stop learning model went haywire considering create custom learning algorithm wish could get agent learn already built algorithm could minimize window went wrong new training session tweak agent action output tried run training episode tweak still error somehow version change behaviour name unity editor image also time episode start training another thing would note version episode length summary summary le episode length might case image training image update summary added writing comment decided review agent code tried comment name agent end episode apparently one agent actually end episode agent somehow receive trigger ashamed thought fixing trigger fixed restart trigger update fixed,issue,negative,positive,neutral,neutral,positive,positive
825988992,"It sounds like your environment is crashing or hanging, which is not a bug with ML-Agents. Are there any logs from it, or are you able to get a callstack from it when the python side stops? ",like environment hanging bug able get python side,issue,negative,positive,positive,positive,positive,positive
825988146,"There's not enough information to go on here. Can you try connecting with one of our example scenes (3DBall is the easiest)? Do you have at least one Agent added to an active GameObject, and does it have the Behavior Type set to ""Default""? Are there any other messages in the editor console or player log?",enough information go try one example easiest least one agent added active behavior type set default editor console player log,issue,positive,negative,negative,negative,negative,negative
825985816,"The ""No episode was completed since last summary"" message is not necessarily an error, it just information that since the last summary (which is `summary_freq` agent steps, in your case 2000) no episodes were ended. You should make sure that the episodes are actually ending, by either
* setting Agent.MaxStep to a non-zero value
* calling Agent.EndEpisode() after a certain number of steps
* calling SimpleMultiAgentGroup.EndGroupEpisode() (if you're using groups).
Also note that `summary_freq` refers to the total number of agents' steps, so if you add more agents (for example, by running with --num-envs=8) you'll get to `summary_freq` faster, possibly before any agent reaches their `MaxStep`.

Aside from the error, it's not clear to me what problem you're having? Does the agent look like it's behaving well? ",episode since last summary message necessarily error information since last summary agent case ended make sure actually ending either setting value calling certain number calling also note total number add example running get faster possibly agent aside error clear problem agent look like well,issue,positive,positive,neutral,neutral,positive,positive
825899520,"Hmm, I think you might have some weird combination of using the branch's code and an installed PyPI package. Line 234 on the branch is something different:
https://github.com/Unity-Technologies/ml-agents/blob/b394d2a2d6a4cb059833bc8faaf0e15fa3e4d17a/gym-unity/gym_unity/envs/__init__.py#L233-L235
",think might weird combination branch code package line branch something different,issue,negative,negative,negative,negative,negative,negative
825893609,"I also would like to put in another usecase:

The variable length of `len(termination_steps) + len(decision_steps)` makes it difficult to work with learning frameworks that expect the same number of agents actions in each ""step"". ",also would like put another variable length difficult work learning expect number step,issue,negative,negative,negative,negative,negative,negative
825887981,"@chriselion, it works with your workaround (setting env.action_space.seed() manually) but when I run the same example above on the PR branch I get this error:

```
line 13, in <module>
    env = UnityToGymWrapper(unity_env, uint8_visual=False, flatten_branched=False, allow_multiple_obs=False)
  File ""/home/user1/anaconda3/envs/cbp/lib/python3.7/site-packages/gym_unity/envs/__init__.py"", line 77, in __init__
    if self._get_n_vis_obs() == 0 and self._get_vec_obs_size() == 0:
  File ""/home/user1/anaconda3/envs/cbp/lib/python3.7/site-packages/gym_unity/envs/__init__.py"", line 234, in _get_n_vis_obs
    for shape in self.group_spec.observation_shapes:
AttributeError: 'BehaviorSpec' object has no attribute 'observation_shapes'
```

Not sure what is going on here :/",work setting manually run example branch get error line module file line file line shape object attribute sure going,issue,negative,positive,positive,positive,positive,positive
825579384,"> Is there any sensor can show where is occupied (output an 2d array like a map) or how can I add my own script as a new ml-agents sensor?

> If that doesn't fit your use case and you want to write your own, you should to derive your custom sensor form the ISensor interface and implement all the interface methods. Also you'll need to make your own SensorComponent to create the custom sensor and attach this SensorComponent to your agent.

@TimothyHo584 I've attempted something like this with my custom grid sensor https://github.com/mbaske/grid-sensor 
You can ignore all the gameobject specific stuff and just check out the basic sensor implementation, which seems to do what you are looking for.
",sensor show output array like map add script new sensor fit use case want write derive custom sensor form interface implement interface also need make create custom sensor attach agent something like custom grid sensor ignore specific stuff check basic sensor implementation looking,issue,positive,positive,positive,positive,positive,positive
825047713,"@julienroyd - here's the PR: https://github.com/Unity-Technologies/ml-agents/pull/5303
Do you want to try that out and see if it solves the problem for you?",want try see problem,issue,negative,neutral,neutral,neutral,neutral,neutral
825026211,"> Do you think ""RootReference"" is still a good user-facing name for this? Maybe something like ""AgentGameObject"" or ""GameObjectToIgnore"" of ""FilteredGameObject""?

agreed, renamed it to AgentGameObject",think still good name maybe something like agreed,issue,positive,positive,positive,positive,positive,positive
824862714,"This is my training config.
![Screenshot 2021-04-22 205334](https://user-images.githubusercontent.com/27879091/115726351-d9e07380-a3ac-11eb-8c1d-894aac476dde.jpg)

After the agent reached 500k step, I increased the max step to 700k
",training agent step step,issue,negative,neutral,neutral,neutral,neutral,neutral
824860494,"I changed the match max time from 30s to 60s mid training. I changed it after the error happened so I doubt that was the problem as indicated by the graph (changed max time at around step 200k error happened at 150k)

Error also happened after my monitor screen blacked, but it wasn't sleeping. I'm not sure whether this is the reason as the third time the error happened (step 500k) it happened right at the start of the training",match time mid training error doubt problem graph time around step error error also monitor screen sleeping sure whether reason third time error step right start training,issue,negative,positive,positive,positive,positive,positive
824462286,"Do you think ""RootReference"" is still a good user-facing name for this? Maybe something like ""AgentGameObject"" or ""GameObjectToIgnore"" of ""FilteredGameObject""?",think still good name maybe something like,issue,positive,positive,positive,positive,positive,positive
824249496,"Hi @julienroyd,
`env.action_space` is a gym.Shape, so it has its own seeding scheme. It looks like they don't provide any way to set a global seed (I think it falls through to [here](https://github.com/openai/gym/blob/a5a6ae6bc0a5cfc0ff1ce9be723d59593c165022/gym/utils/seeding.py#L55) if a seed isn't specified).

We can change the `UnityToGymWrapper` code to set the `action_space` seeds after creation using the provided seed. In the meantime, I think you'll need to call `env.action_space.seed(SEED)`.

The internal tracker ID for this is MLA-1952; I'll try to get a fix today.",hi scheme like provide way set global seed think seed change code set creation provided seed think need call seed internal tracker id try get fix today,issue,positive,neutral,neutral,neutral,neutral,neutral
822654911,"@RedTachyon The variable length observation feature was added in the previous release. 

@robinerd There are some information about the implementation in
https://github.com/Unity-Technologies/ml-agents/blob/release_16_docs/docs/ML-Agents-Overview.md#learning-from-variable-length-observations-using-attention
and 
https://github.com/Unity-Technologies/ml-agents/blob/release_16_docs/docs/Learning-Environment-Design-Agents.md#variable-length-observations",variable length observation feature added previous release information implementation,issue,negative,negative,negative,negative,negative,negative
822653691,"> Looks good, just not sure if it should go in the extensions documentation or the general docs.

Since it mentioned CountingGridSensor and limitations around normalization and compression, I think it would be better to keep in the extension. ",good sure go documentation general since around normalization compression think would better keep extension,issue,positive,positive,positive,positive,positive,positive
822648847,"Also for general ML-Agents questions (not bug report or feature request), our official forums are a preferred place for asking questions and getting discussions: https://forum.unity.com/forums/ml-agents.453//, thanks.",also general bug report feature request official preferred place getting thanks,issue,negative,positive,positive,positive,positive,positive
822645470,"Hi,

The closest thing to what you describe is the Grid Sensor in ml-agents extension package. The sensor is a grid centered around the agent and it will detect objects with tags that are defined as ""detectable tags"". The output will be 3D array of the size `width` x `height` x `channel` with `channel` depending on how many tags you want to detect. 

If that doesn't fit your use case and you want to write your own, you should to derive your custom sensor form the `ISensor` interface and implement all the interface methods. Also you'll need to make your own `SensorComponent` to create the custom sensor and attach this `SensorComponent` to your agent.",hi thing describe grid sensor extension package sensor grid centered around agent detect defined detectable output array size width height channel channel depending many want detect fit use case want write derive custom sensor form interface implement interface also need make create custom sensor attach agent,issue,positive,positive,positive,positive,positive,positive
822637968,"Add one hot observation for every grid square, make each square state be represented by a numeric (0 = empty, 1 = full, etc) as part of a larger structure like an Enum. You could then do a foreach loop (depending on how you set it up), observing the state of each and returning a square if it meets x condition (in your case, the learning algorithm would recognize 1 as occupied and 0 as unoccupied). As for how you are adding/removing occupants to a grid square, I cannot advise you further, but you should be changing the state either at a point in code where an addition/subtraction is made to a square or having each square observe objects within it and returning 0 or 1 to reflect that check.",add one hot observation every grid square make square state empty full part structure like could loop depending set observing state square condition case learning algorithm would recognize unoccupied grid square advise state either point code made square square observe within reflect check,issue,negative,positive,positive,positive,positive,positive
822572912,"Hey @nathan60107,
The forums are the preferred way to get advice about ml-agents.  And thank you for asking :)
Cheers,
Chris",hey preferred way get advice thank,issue,negative,neutral,neutral,neutral,neutral,neutral
822332475,"HI @surfnerd,
Can I keep send some advice or questions about ml-agents here? Or should I go to [the forums](https://forum.unity.com/forums/ml-agents.453/)?",hi keep send advice go,issue,negative,neutral,neutral,neutral,neutral,neutral
822032847,"Yes! 

Thank you, adding `--capture-frame-rate=0` to my arguments brings FixedUpdateSteps/s close to 100. Thank you!



",yes thank close thank,issue,positive,neutral,neutral,neutral,neutral,neutral
821931964,"Ok I think I understand your question better now. The reason that you're seeing the difference between hitting play and connecting to the Editor via `mlagents-learn` is that the python trainer sets the engine parameters differently. Specifically it sets `Time.captureFrameRate` to 60 by default which is default as 0 in the editor. `Time.captureFrameRate` does not change the speed of the simulation at all, but it might affect the frequency on calling `Updates` and `FixedUpdates`. So you're seeing your measurements are changing but the physics are not moving x times faster compared to inference. If you set to 0 I would expect you to get similar results as in inference. For all the engine configuration set by `mlagents-learn` please see [this doc](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Python-API.md#engineconfigurationchannel).

FYI, [this graph](https://docs.unity3d.com/ScriptReference/Time.html) might give you more information about how time works in Unity. `FixedUpdate` guarantee that physics are calculated within a fixed time - which refer to Time.fixedDeltaTime and not necessary maps to Time.realtimeSinceStartup. ",think understand question better reason seeing difference play editor via python trainer engine differently specifically default default editor change speed simulation might affect frequency calling seeing physic moving time faster inference set would expect get similar inference engine configuration set please see doc graph might give information time work unity guarantee physic calculated within fixed time refer necessary,issue,positive,positive,positive,positive,positive,positive
821904201,"> The tricky thing I see from your description is that you're measuring ""FixedUpdates per second"" using Time.realtimeSinceStartup. 

I disagree. From what I've read having a constant number of ""FixedUpdates per Second"" is exactly why it's there in the first place: to guarantee that physics can be calculated with a fixed time in between them - even if the FPS-numbers (rendered frames) are inconsistent. 
The fact that I get close to 100 ""FixedUpdates Per Second"" during inference shows me that FixedUpdate() is called every 10ms (as specified by fixedDeltaTime).
But during training the example above with its 100.000 steps would finish after around 300s.
If I had any objects in my scene they would move 3x as fast as during inference. So not at all at a ""timescale of 1"".

> Btw everything you described isn't specific to ML-Agents - you can make an arbitrary MonoBehavior script and get the same results without involving an Agent.

The described behavior only happens when I'm connecting to the Editor via mlagents-learn. I suspect that mlagents is somehow triggering FixedUpdate to be called faster. Which is nice for training speedups and wouldn't normally be a problem. But I'm *wanting* it to run at timescale=1 to benchmark some timings. ",tricky thing see description measuring per second disagree read constant number per second exactly first place guarantee physic calculated fixed time even inconsistent fact get close per second inference every training example would finish around scene would move fast inference everything specific make arbitrary script get without agent behavior editor via suspect somehow faster nice training would normally problem wanting run,issue,negative,positive,positive,positive,positive,positive
821900265,"Hi,

The tricky thing I see from your description is that you're measuring ""FixedUpdates per second"" using `Time.realtimeSinceStartup`. How `FixedUpdate()` being called is configured by `Time.FixedDaltaTime` which is relative to the ""in-game"" time (see [here](https://docs.unity3d.com/ScriptReference/Time-fixedDeltaTime.html)). So it's likely to get unexpected result if you're using real time since in-game time can differ a lot from wall-clock time. 

Btw everything you described isn't specific to ML-Agents - you can make an arbitrary MonoBehavior script and get the same  results without involving an Agent. How `FixedUpdate()` and `timeScale` work are determined by Unity engine and ML-Agents only provides options for you to configure those parameters. So I'd suggest that you could find better audience on the general Unity community forum instead of here for this type of questions.",hi tricky thing see description measuring per second relative time see likely get unexpected result real time since time differ lot time everything specific make arbitrary script get without agent work determined unity engine configure suggest could find better audience general unity community forum instead type,issue,negative,positive,neutral,neutral,positive,positive
821155448,"Please maintain TF2 compatibility! Not everyone prefers PyTorch, and that
should be OK.

On Thu, Apr 15, 2021 at 5:16 PM Ruo-Ping Dong ***@***.***>
wrote:

> Closed #4512 <https://github.com/Unity-Technologies/ml-agents/issues/4512>
> .
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/4512#event-4601631693>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABH7VG4DT54DWIIQ6NMK5N3TI5JR3ANCNFSM4R2ESFTQ>
> .
>


-- 
   interFusion Games LLC - because it's fun
",please maintain compatibility everyone dong wrote closed reply directly view interfusion fun,issue,positive,positive,neutral,neutral,positive,positive
820824812,"Thank you for your reply


Ruo-Ping Dong ***@***.***> 于 2021年4月16日周五 02:31写道：

> Hi,
>
> Since it is not a feature request or bug, I think you may find a better
> place for discussion on our official forums:
> https://forum.unity.com/forums/ml-agents.453//.
>
> That being said, by changing the number of agents in the field you're
> changing the task and the training might not behave the same as before.
> From your description I'm not sure if you have recorded new demos for new
> settings, changed the reward function, or made any other changes. The
> training will not work by just applying the single-agent demo files to
> multi-agent since the new environment is also depends on other agent's
> actions. Also if you have three agent but only one block then only one
> agent will get positive reward and thus the mean rewards are not comparable.
>
> I'll close this issue, but feel free to open a post on our forum for more
> discussions.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/5266#issuecomment-820606061>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AL4LEQEBMYY7RSWRTTZKCLTTI4PGHANCNFSM426VS57Q>
> .
>
",thank reply dong hi since feature request bug think may find better place discussion official said number field task training might behave description sure new demo new reward function made training work since new environment also also three agent one block one agent get positive reward thus mean comparable close issue feel free open post forum thread reply directly view,issue,positive,positive,positive,positive,positive,positive
820782443,"Hi,

From the error message, it seems like you're using imitation learning and the demonstrations you use have different observation size than the the setup in the scene.
Please make sure you're using demonstrations from the training scene with the exact same observations and actions setup.

Does that solve your problem?",hi error message like imitation learning use different observation size setup scene please make sure training scene exact setup solve problem,issue,negative,positive,positive,positive,positive,positive
820774598,"Sorry for the late reply on this thread and hope it helps. 
As I mentioned this connection problem can be caused by many possible reasons. This thread has diverted a lot from the original post and it's hard to further look into each response since each can be coming from different setup issue. Also there's no known connection issue/bug in our code and the original post seems to be a training ID error. Given all the reasons above I'm closing this issue, but feel free to reopen it if there's still problems around the original post.

For people who run into the same problem, these are some recommended steps to follow:
1. Make sure you have installed ml-agents successfully. Install C# package and python training from the same release is _strongly_ recommended.
2. Try with any of our example environment first. If you're unable to get the examples training, there might be something wrong with your installation or environment setup. 
3. If you're able to train the examples but not your own environment, make sure that the environment does not need user interaction to launch and the Agents' Behavior Parameters > Behavior Type is set to ""Default""
4. If you tried all the steps above and still couldn't get it working, please open a post on our [forum](https://forum.unity.com/forums/ml-agents.453/) to ask for help, or if you believe it's a bug please report a new github issue with the details of your machine setup so we have better idea of how to help.",sorry late reply thread hope connection problem many possible thread lot original post hard look response since coming different setup issue also known connection code original post training id error given issue feel free reopen still around original post people run problem follow make sure successfully install package python training release try example environment first unable get training might something wrong installation environment setup able train environment make sure environment need user interaction launch behavior behavior type set default tried still could get working please open post forum ask help believe bug please report new issue machine setup better idea help,issue,positive,positive,positive,positive,positive,positive
820773020,"To conclude all the questions in the thread:

""Couldn't connect to trainer on port 5004"" is a log message that tells you the editor is not connect to a python trainer and not always refers to an error, so:
1. If you're able to get the training running and you're seeing the message only at inference time (only running in the editor, no training): this message is totally normal. It's only a message saying it's not connect to trainer.

2. If you're unable to get the training started and you see the timeout error message: The error message in newer version offers more information about probable cause of this:
```
mlagents_envs.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that :
The environment does not need user interaction to launch
The Agents' Behavior Parameters > Behavior Type is set to ""Default""
The environment and the Python interface have compatible versions.
```
So make sure you're using compatible C# and python package and set Behavior Type to ""Default"". Generally it's recommended to use C# and python package from the same release. Some other machine setup like firewalls and port usage might affect as well. 

To answer specific questions in the thread:
* If you need to specify a specific port, specify `--base-port` in CLI. `mlagents-learn --help` gives you more details on all the available training arguments.
* In the original post, the reason why the training is failing is pointed out in the error message in screenshot: `mlagents.trainers.exception.UnityTrainerException: Previous data from this run ID was found. Either specify a new run ID, use --resume to resume this run, or use the --force parameter to overwrite existing data.` Please make sure you're using unique ID for each training session. This should have nothing to do with connection problem and it's failing because the training session could not start due to duplicated ID.",conclude thread could connect trainer port log message editor connect python trainer always error able get training running seeing message inference time running editor training message totally normal message saying connect trainer unable get training see error message error message version information probable cause unity environment took long respond make sure environment need user interaction launch behavior behavior type set default environment python interface compatible make sure compatible python package set behavior type default generally use python package release machine setup like port usage might affect well answer specific thread need specify specific port specify help available training original post reason training failing pointed error message previous data run id found either specify new run id use resume resume run use force parameter overwrite please make sure unique id training session nothing connection problem failing training session could start due id,issue,negative,positive,positive,positive,positive,positive
820763427,@andrewcoh I am currently running Sorter to make sure it works. Can I have your eyes on the implementation to see if it makes sense?,currently running sorter make sure work implementation see sense,issue,negative,positive,positive,positive,positive,positive
820761082,"Hi,
Is there a way to reproduce this issue with one of the example environments on the latest version of ML-Agents?
I am unable to reproduce this issue.",hi way reproduce issue one example latest version unable reproduce issue,issue,negative,neutral,neutral,neutral,neutral,neutral
820757335,"Hi,
The error message was re-written let us know if you are still having issues with the GymWrapper!
",hi error message let u know still,issue,negative,neutral,neutral,neutral,neutral,neutral
820752705,"Hi @migberbay 
Do you still have this issue? Do you still observe it in more recent versions of ML-Agents?",hi still issue still observe recent,issue,negative,neutral,neutral,neutral,neutral,neutral
820751987,"> Looks good, but for clarity, I would do :
> 
> ```
>  if( !Record){ 
>      return;
>   } 
> 
>  *rest of the code*
> ```

Done",good clarity would record return rest code done,issue,positive,positive,positive,positive,positive,positive
820749619,"Can we add a short blurb in the changelog? Since it may result in e.g. models not loading between R16 and R17, slight changes in behavior",add short blurb since may result loading slight behavior,issue,negative,negative,neutral,neutral,negative,negative
820728839,Closing this issue and please feel free to open up a post in forum if you have further question.,issue please feel free open post forum question,issue,positive,positive,positive,positive,positive,positive
820726649,"Closing the issue due to inactivity and no obvious bug. 
If your issue wasn't resolved, please feel free to re-open it.",issue due inactivity obvious bug issue resolved please feel free,issue,positive,positive,neutral,neutral,positive,positive
820724303,"Since the issue has been resolved, I'll close the issue.",since issue resolved close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
820720730,No longer needed as the critic is separate from the policy since Release 15.,longer critic separate policy since release,issue,negative,neutral,neutral,neutral,neutral,neutral
820709151,"Hey @fredleefarr this is fixed on the current main branch now.
The fix will go into the next release which is planned on next week.

Thanks again for reporting this!",hey fixed current main branch fix go next release next week thanks,issue,negative,positive,neutral,neutral,positive,positive
820618774,"Thanks, I'm able to reproduce it and it's indeed a bug in our code. I'll get a fix out soon.

Thanks for raising this!",thanks able reproduce indeed bug code get fix soon thanks raising,issue,positive,positive,positive,positive,positive,positive
820612493,"Hi, sorry, applies to both releases 15 & 16 (and probably earlier versions, but I haven't checked those)

ml-agents: 0.25.0,
ml-agents-envs: 0.25.0,
Communicator API: 1.5.0,
PyTorch: 1.7.1+cpu",hi sorry probably checked communicator,issue,negative,negative,negative,negative,negative,negative
820607947,"Hi,

Please follow our template for reporting a bug so that we are able to get all necessary information to look into the issue.

To your question, can you provide the ML-Agents version are you on? ",hi please follow template bug able get necessary information look issue question provide version,issue,negative,positive,positive,positive,positive,positive
820606061,"Hi,

Since it is not a feature request or bug, I think you may find a better place for discussion on our official forums: https://forum.unity.com/forums/ml-agents.453//.

That being said, by changing the number of agents in the field you're changing the task and the training might not behave the same as before. From your description I'm not sure if you have recorded new demos for new settings, changed the reward function, or made any other changes. The training will not work by just applying the single-agent demo files to multi-agent since the new environment is also depends on other agent's actions. Also if you have three agent but only one block then only one agent will get positive reward and thus the mean rewards are not comparable.

I'll close this issue, but feel free to open a post on our forum for more discussions.",hi since feature request bug think may find better place discussion official said number field task training might behave description sure new demo new reward function made training work since new environment also agent also three agent one block one agent get positive reward thus mean comparable close issue feel free open post forum,issue,positive,positive,positive,positive,positive,positive
818898968,"No log files are made, just 2 json files (timers and training_status). I am calling RequestDecision, the game is turn based and I call it when it is this agents turn.",log made calling game turn based call turn,issue,negative,negative,negative,negative,negative,negative
818729895,"Hi @zako42 

Just to clarify, you've seen these [colab examples?](https://github.com/Unity-Technologies/ml-agents/tree/main/docs#python-tutorial-with-google-colab) Can you elaborate on what more you'd like beyond what's contained in the colab?",hi clarify seen elaborate like beyond,issue,negative,positive,positive,positive,positive,positive
818726433,"Is there anything in the player logs located in `results/<run-id>/run_logs/Player-x.log`?

This is only happening on Reset? Do you know if the call to `OnEpisodeBegin` for the next game is occurring?

Also, can you clarify what you mean by ""manually being called"" in the initial post. Are you using `RequestDecision`?",anything player happening reset know call next game also clarify mean manually initial post,issue,negative,negative,negative,negative,negative,negative
818319879,"I just removed the lines that caused all of the warning and it still froze.
What exactly happens is that at the end of a game, and I have only seen it happen at the end of a game, is that Unity freezes and then python says that the environment has stopped. I the return to Unity and it starts to play using the heuristic and python is frozen.",removed warning still froze exactly end game seen happen end game unity python environment stopped return unity play heuristic python frozen,issue,negative,negative,negative,negative,negative,negative
818317518,I'd also add a test in `test_stats.py` (prob just check that the mock writer's method is called in `test_stat_reporter_add_summary_write`) and a blurb in the docs here: https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Plugins.md,also add test prob check mock writer method blurb,issue,negative,neutral,neutral,neutral,neutral,neutral
818277776,"Sorry for the confusion.  I mis-read the stuff about Doxygen -- that was for generating C# docs.  The python stuff is just using regular docstrings, so can just use python tool to generate docs for those if needed (not sure what I was thinking).  Closing the issue, sorry about that.",sorry confusion stuff generating python stuff regular use python tool generate sure thinking issue sorry,issue,negative,negative,negative,negative,negative,negative
818077121,I do know that I get a lot of warnings in Unity due to the collect observations not just being called when it is not that AI's turn to make a move.,know get lot unity due collect ai turn make move,issue,negative,negative,negative,negative,negative,negative
818076195,Unity temporarily stops but then goes back to playing using the heuristic.,unity temporarily go back heuristic,issue,negative,neutral,neutral,neutral,neutral,neutral
818074940,"Does it crash on the Unity side and then the python side times out (`UnityTimeoutException`) or does everything freeze? I'm assuming the editor console logs are empty too?

I'm not sure where the bug could be originating. It might help to remove some features (curiosity and self-play) to determine if it's something in the trainers or just C#.",crash unity side python side time everything freeze assuming editor console empty sure bug could might help remove curiosity determine something,issue,negative,positive,positive,positive,positive,positive
817971709,"Hey @nathan60107,
This is my fault.  I added a script that depends on the Newtonsoft json package to our project.  I can update that script to only compile if that package is present.  I'll make a pull request for it soon.  My apologies for the confusion.  You should be able to delete that script in your project in order to get things working for you.",hey fault added script package project update script compile package present make pull request soon confusion able delete script project order get working,issue,negative,positive,positive,positive,positive,positive
817943277,"Hi @holohuang 

The slot for the game object will not appear until your scripts have compiled, which can't happen since there are errors.  Without seeing more of your code, it looks like you might be missing the `using Unity.MLAgents;`, `using Unity.MLAgents.Sensors;`, or `Unity.MLAgents.Actuators;`directives or are not inheriting from the Agent class.",hi slot game object appear ca happen since without seeing code like might missing agent class,issue,negative,negative,negative,negative,negative,negative
817928649,Update: restarting my pc several times fixed it.,update several time fixed,issue,negative,positive,neutral,neutral,positive,positive
817925264,"Hi @George056 

This is not a bug, just a warning that we are using a torch function signature of `nonzero` that is deprecated.

However, crashing after a few minutes sounds like an issue. Can you share the error logs?",hi bug warning torch function signature nonzero however like issue share error,issue,negative,neutral,neutral,neutral,neutral,neutral
816079707,hi @xogur6889 - I just had someone from the team confirm that everything is alright.,hi someone team confirm everything alright,issue,negative,neutral,neutral,neutral,neutral,neutral
816013061,"Thanks, outstandingly fast feedback!!

I'll keep a watch on it, let's see If I can get this working and train something that makes sense :)",thanks outstandingly fast feedback keep watch let see get working train something sense,issue,negative,positive,positive,positive,positive,positive
816004536,"Hi @joslat,
I added this documentation and it is definitely misleading now that you've brought it up. If you have already opened up the `Project` Folder, the 3DBall example should be under `Assets/ML-Agents/Examples/3DBall`. 

The sample will show up when the package is actually installed from the package manager, not when you clone the git repo and open up the `Project` project.  I'll update the documentation and have you review it for me along with another team member.  Thank you for the important feedback.
Cheers,
Chris",hi added documentation definitely misleading brought already project folder example sample show package actually package manager clone git open project project update documentation review along another team member thank important feedback,issue,positive,positive,neutral,neutral,positive,positive
815553458,Do you know in which python file I could implement it for myself?,know python file could implement,issue,negative,neutral,neutral,neutral,neutral,neutral
815219690,"> @surfnerd Anything that needs to change in the input system actuators? I had a quick look and it didn't look like it.

I don't think so.  The input actuator just fills the ActionBuffers with whatever input state exists in the Heuristic method every frame.  So there was no need to do any sort of `clear` in that case.",anything need change input system quick look look like think input actuator whatever input state heuristic method every frame need sort clear case,issue,positive,positive,positive,positive,positive,positive
815142922,@surfnerd Anything that needs to change in the input system actuators? I had a quick look and it didn't look like it.,anything need change input system quick look look like,issue,negative,positive,positive,positive,positive,positive
815048688,"thank you for your help, I have solved the issue by running them same time-scale 💯  @dongruoping. ",thank help issue running,issue,positive,neutral,neutral,neutral,neutral,neutral
814591937,"Hey,
it sounds great 👍 😃

thank you for clarifying that, I will try it out.",hey great thank try,issue,positive,positive,positive,positive,positive,positive
814477182,Sorry for the delay on this. I fixed the failing tests and will merge this now. Thanks for your contribution!,sorry delay fixed failing merge thanks contribution,issue,negative,negative,neutral,neutral,negative,negative
814422464,"Hi,

Thanks for bringing up a good point. Using ELO as a lesson completion measure is more reasonable in this case.
We're considering adding more measures or even custom ones for more flexibility curriculum learning and we'll prioritize it when possible.

On the other hand, I think you have a bit misunderstanding of how ""progress"" works. The Steps/MaxSteps is referring to the current/max step in the trainer settings which you specified in trainer config, and not the agent's max step. It specifies what proportion of the whole training you want to spend on a lesson. For example you can to train on the first lesson for 20% of the steps, and the rest 80% on the second lesson.",hi thanks good point lesson completion measure reasonable case considering even custom flexibility curriculum learning possible hand think bit misunderstanding progress work step trainer trainer agent step proportion whole training want spend lesson example train first lesson rest second lesson,issue,positive,positive,positive,positive,positive,positive
813672924,"@mbaske - cool, let me know if you have any feedback on the PR: https://github.com/Unity-Technologies/ml-agents/pull/5223

I changed things slightly to take a context struct instead, so that we can add information if needed in the future without changing the signature.",cool let know feedback slightly take context instead add information future without signature,issue,negative,positive,neutral,neutral,positive,positive
813464785,"Experimental update: 

![image](https://user-images.githubusercontent.com/5085265/113593592-98a64080-9604-11eb-86ba-d49097f9ff7a.png)
 Hallway trains a little faster than on `main` (green line is `main`, 2 others are 2 runs of this PR). 
 
![image](https://user-images.githubusercontent.com/5085265/113593684-b1165b00-9604-11eb-992a-2c2c4c8c0ab3.png)
Value estimates are much cleaner. Value loss shows a similar difference. I suspect this is b/c of the lack of random-length zero-pad at the beginning of the sequence. 
",experimental update image hallway little faster main green line main image value much cleaner value loss similar difference suspect lack beginning sequence,issue,negative,positive,neutral,neutral,positive,positive
812846628,"Hi @chriselion - This looks like a good solution. In cases when I write my own decision requester, it's mostly because I need to switch it on and off easily. ",hi like good solution write decision requester mostly need switch easily,issue,positive,positive,positive,positive,positive,positive
812703145,"Thanks @vincentpierre !

Since the fix has been merged I'll close it.
",thanks since fix close,issue,negative,positive,positive,positive,positive,positive
812702363,"Hi, 

There are many potential reasons that make the training not working with larger grid size. Expanding the size might make the reward more sparse and thus harder to train, and you might also need to adjust your training parameters.

In this case I don't see obvious bugs in the toolkit that caused the training to fail and it's more likely a result of the training setup and your custom environment, so I'm closing this issue here. I'd suggest you make a post on our [forum](https://forum.unity.com/forums/ml-agents.453/) which is a better place for general discussions, and put more details like how you modified the environment, what have you tried, etc.",hi many potential make training working grid size expanding size might make reward sparse thus harder train might also need adjust training case see obvious training fail likely result training setup custom environment issue suggest make post forum better place general put like environment tried,issue,positive,positive,neutral,neutral,positive,positive
812542717,"To follow up on this, I am currently working on a project in which I want to train an agent to combat another, specifically in a sword fight duel. For now, I've used self-play, but I believe it would greatly improve the outcome if the agents could learn from imitating a person duelling in VR. So essentially, have one student sword fighter and one teacher sword fighter (person in VR) combatting each other. However, I dont see a way in which I can use the current imitation learning approach to do so, since meaningful demonstrations can only be done by the teacher if the student starts learning and fighting back during the demonstrations. 

I might be wrong to believe the deprecated online behavioral cloning would solve this, but when I look at older documentations for this, it seems like it could. Is there another way to achieve what I'm describing with the current release?

Thanks in advance!",follow currently working project want train agent combat another specifically sword fight duel used believe would greatly improve outcome could learn person essentially one student sword fighter one teacher sword fighter person however dont see way use current imitation learning approach since meaningful done teacher student learning fighting back might wrong believe behavioral would solve look older like could another way achieve current release thanks advance,issue,positive,positive,neutral,neutral,positive,positive
812049632,"Sure, please update if you're able to get it running or if we need to dig in further.

Also model size doesn't change with steps, it's affected by your network configurations like hidden_units, num_layers, memory_size, etc. Not sure how small it is but if you can import it in the editor successfully then it should be fine.",sure please update able get running need dig also model size change affected network like sure small import editor successfully fine,issue,positive,positive,positive,positive,positive,positive
811986715,"I think this warrants a doc change. In `Training-ML-Agents` under ""Loading an existing model"", we can say something like ""If the network architecture changes, you may still load an existing model, and ML-Agents will only load the parts of the model that haven't changed. For instance, if you add a new reward signal, the existing model will load but the new reward signal will be initialized from scratch. If you have a model with a visual encoder (CNN) but change the `hidden_units`, the CNN will be loaded but the body of the network will not be.""",think doc change loading model say something like network architecture may still load model load model instance add new reward signal model load new reward signal scratch model visual change loaded body network,issue,positive,positive,neutral,neutral,positive,positive
811631706,"> There's no barracuda inference issue that we're aware of now.
> So my guess is one possible cause is that if your game has physics that's dependent on time scale you might see issues running training and inference at different time scale.
> Have you tried running training and inference using the same time scale?

you have point, let me try that I will run both on same time-scale. but why the model size is so small, after so many steps ? is it fine ? ",barracuda inference issue aware guess one possible cause game physic dependent time scale might see running training inference different time scale tried running training inference time scale point let try run model size small many fine,issue,negative,positive,neutral,neutral,positive,positive
811347905,"There's no barracuda inference issue that we're aware of now. 
So my guess is one possible cause is that if your game has physics that's dependent on time scale you might see issues running training and inference at different time scale.
Have you tried running training and inference using the same time scale?",barracuda inference issue aware guess one possible cause game physic dependent time scale might see running training inference different time scale tried running training inference time scale,issue,negative,negative,neutral,neutral,negative,negative
811308118,"Thank you very much for your kind and quick reply! Absolutely solved my
problem.

Vincent-Pierre BERGES ***@***.***> ezt írta (időpont: 2021.
márc. 31., Sze, 20:15):

> Hi @nagybalint25 <https://github.com/nagybalint25>
> I was able to reproduce the issue. The problem is that your agent is done,
> do the environment must reset :
> The following code should do the trick :
>
> from mlagents_envs.environment import UnityEnvironment
> from gym_unity.envs import UnityToGymWrapper
> gym.logger.set_level(40)
>
> def main():
>     unity_env = UnityEnvironment(file_name=""3dballv2"")
>     env = UnityToGymWrapper(unity_env,  uint8_visual=True,allow_multiple_obs=True)
>     env.reset()
>     for _ in range(1000):
>         env.render()
>         o,r,d,_ = env.step(env.action_space.sample()) #random action
>         if d:
>             env.reset()
>     env.close()
>
>
>
> if __name__ == '__main__':
>     main()
>
> The error message is wrong and I will work to resolve this. Thank you for
> raising this issue.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/5204#issuecomment-811302884>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ASZG3QM26W56G2EUGOYKR5LTGNRD7ANCNFSM42E2OF7Q>
> .
>
",thank much kind quick reply absolutely problem hi able reproduce issue problem agent done environment must reset following code trick import import main range random action main error message wrong work resolve thank raising issue reply directly view,issue,negative,positive,positive,positive,positive,positive
811302884,"Hi @nagybalint25 
I was able to reproduce the issue. The problem is that your agent is done, do the environment must reset : 
The following code should do the trick : 

```
from mlagents_envs.environment import UnityEnvironment
from gym_unity.envs import UnityToGymWrapper
gym.logger.set_level(40)

def main():
    unity_env = UnityEnvironment(file_name=""3dballv2"")
    env = UnityToGymWrapper(unity_env,  uint8_visual=True,allow_multiple_obs=True)
    env.reset()
    for _ in range(1000):
        env.render()
        o,r,d,_ = env.step(env.action_space.sample()) #random action
        if d:
            env.reset()
    env.close()    
    


if __name__ == '__main__':
    main()
```

The error message is wrong and I will work to resolve this. Thank you for raising this issue.",hi able reproduce issue problem agent done environment must reset following code trick import import main range random action main error message wrong work resolve thank raising issue,issue,negative,negative,neutral,neutral,negative,negative
810784096,"> Hi, a few thing I want to clarify:
> 
> 1. Just to double check, did you drag and drop the trained models into your agents? Does that imported successfully without any errors (the field ""model"" showed the model name you assigned and no warnings or errors showed up)?
> 2. Did you modify anything between running training and inference? Like timescale or any other configurations?
> 3. By saying ""agent just stuck"", is your agent doing something and just not acting the same way as in training, or is it not working at all?

1. yes I have drag and drop the model without any error
2. no, my training command was `mlagents-learn race_config --run-id=""test1"" --time-scale=3` in unity timescale was 1 default 
3. it is not acting the same as training, in training I can see it is acting as expected the mean reward is getting higher but in game agent just stoped where it started, the thing I have noticed is that after 1 million steps the model size is only 93kb the size should increase right ? I guess the agent does not getting data from the model. ",hi thing want clarify double check drag drop trained successfully without field model model name assigned modify anything running training inference like saying agent stuck agent something acting way training working yes drag drop model without error training command test unity default acting training training see acting mean reward getting higher game agent thing million model size size increase right guess agent getting data model,issue,positive,positive,neutral,neutral,positive,positive
810757093,"Hi, a few thing I want to clarify: 
1. Just to double check, did you drag and drop the trained models into your agents? Does that imported successfully without any errors (the field ""model"" showed the model name you assigned and no warnings or errors showed up)?
2. Did you modify anything between running training and inference? Like timescale or any other configurations?
3. By saying ""agent just stuck"", is your agent doing something and just not acting the same way as in training, or is it not working at all?",hi thing want clarify double check drag drop trained successfully without field model model name assigned modify anything running training inference like saying agent stuck agent something acting way training working,issue,negative,positive,positive,positive,positive,positive
810657240,"Hi @mbaske,
Sorry this has been sitting around for a while. I'm looking at adding something for this but had a question about the proposed solution - do you actually need MakeRequests virtual, or would something like this be better?
```csharp
        void MakeRequests(int academyStepCount)
        {
            if (ShouldRequestDecision(academyStepCount))
            {
                m_Agent?.RequestDecision();
            }
            if (ShouldRequestAction(academyStepCount))
            {
                m_Agent?.RequestAction();
            }
        }

        protected virtual bool ShouldRequestDecision(int academyStepCount)
        {
            return academyStepCount % DecisionPeriod == 0;
        }

        protected virtual bool ShouldRequestAction(int academyStepCount)
        {
            return TakeActionsBetweenDecisions;
        }
```",hi sorry sitting around looking something question solution actually need virtual would something like better void virtual bool return virtual bool return,issue,positive,neutral,neutral,neutral,neutral,neutral
810492663,"Hey @ohernpaul, 
We have logged this issue internally as MLA-812 and will update this thread once work is complete on it.  Thank you for your very detailed feedback!",hey logged issue internally update thread work complete thank detailed feedback,issue,negative,positive,positive,positive,positive,positive
810456345,"Hmm, if we're to change the console I feel like the main motivator would be if it is easier to parse by script. I think this approach is actually pretty good, except when there's intruding messages (e.g. checkpoint save, no episode completed, etc.) Then it gets kind of janky.",change console feel like main would easier parse script think approach actually pretty good except intruding save episode kind,issue,positive,positive,positive,positive,positive,positive
810451403,"> Just wondering, what would it look like if there were two behavior names?

We can make it do anything we want! Right now it would do something like : 
```
2021-03-30 10:38:12 INFO [stats.py:197] 
2021-03-30 10:38:12 INFO [stats.py:198] Behavior Name |  Step | Time Elapsed | Mean Cumulative Reward | Std of Reward | Is Training | 
2021-03-30 10:38:12 INFO [stats.py:201]        3DBall | 50000 |       82.175 |                  1.207 |         0.772 |        True | 
2021-03-30 10:38:20 INFO [stats.py:201]    3DBallHard | 50000 |       90.035 |                  2.335 |         3.272 |        True | 
2021-03-30 10:39:30 INFO [stats.py:197] 
2021-03-30 10:39:30 INFO [stats.py:198] Behavior Name |   Step | Time Elapsed | Mean Cumulative Reward | Std of Reward | Is Training | 
2021-03-30 10:39:30 INFO [stats.py:201]        3DBall | 100000 |      160.409 |                  2.030 |         1.340 |        True | 
2021-03-30 10:39:45 INFO [stats.py:201]    3DBallHard | 100000 |      175.507 |                  2.343 |         3.920 |        True | 
2021-03-30 10:40:53 INFO [stats.py:201]        3DBall | 150000 |      243.202 |                  3.287 |         2.432 |        True | 
2021-03-30 10:41:14 INFO [stats.py:201]    3DBallHard | 150000 |      263.860 |                  2.305 |         5.221 |        True | 
```

There are some things that need to be flushed out, I am just wandering if it is something worth doing?",wondering would look like two behavior make anything want right would something like behavior name step time mean cumulative reward reward training true true behavior name step time mean cumulative reward reward training true true true true need wandering something worth,issue,positive,positive,positive,positive,positive,positive
810444940,"Just wondering, what would it look like if there were two behavior names?",wondering would look like two behavior,issue,negative,neutral,neutral,neutral,neutral,neutral
809929443,"I reproduced the issue using 3DBall if this will help. 

I also stepped through with a debugger and found that the env_manager.close() connects to a unity environment and brain before closing.

1. env_manager.close()
2. self.conn.send(EnvironmentRequest(EnvironmentCommand.CLOSE))
3. PipeConnection._send_bytes(self,buf)
4. ov, err = _winapi.WriteFile(...) <-- creates unity env and connects to brain before closing?

```
class PhaseLauncher():
    def __init__(self):
        
        ##########################################################
        self.wkspace_dir = 'path\\to\\wkspace\\'
        self.mlagents_dir = self.wkspace_dir + 'ml-agents-master\\'
        self.config_dir = self.mlagents_dir + 'config\\'
        self.builds_dir = self.mlagents_dir + 'builds\\'
        self.results_dir = self.mlagents_dir + 'results\\reproduce_test\\'
        
        self.results_dir_cpy = self.mlagents_dir + 'results\\reproduce_test_copy\\'
        ##########################################################
        self.phase = 0
        self.run_id = '3dball_ppo'
        
        self.quality = 1
        self.height = 300
        self.width = 300
        self.no_graphics = False
        self.use_init_from = False
        self.use_env = True
        self.nb_envs = 5
        self.do_inference = False
        self.init_from = ''
        self.loop_counter = 0
        self.seed = 0
        
        self.phase_config_dir = self.config_dir + 'debug_phases\\'
        
        self.runs_array = []
        
        ##########################################################

    def Start(self):
        
        #loop to represent phases
        for i in range(2):
            #call run_training and pass the run_options object
            print(""---Phase Start---"")
            if self.loop_counter == 0:
                self.seed = 1337
                self.run_id = self.run_id + '_' + str(self.seed)
            else:
                self.seed = 101
                self.run_id = ''.join([self.run_id.split('_')[0], '_' + str(self.seed)])
                
            self.GetRunOptions(self.phase_config_dir + '3DBall.yaml')
    
    ##########################################################
    def GetRunOptions(self, phase_config_path):
        """"""
        RUN OPTIONS SECTION
        
        (most pulled from settings.py in mlagents/trainers)
        """"""
        #==================================================
        print(""---Building Run Options---"")
        
        #Define Config Dict 
        configured_dict: Dict[str, Any] = {
            ""checkpoint_settings"": {},
            ""env_settings"": {},
            ""engine_settings"": {},
            ""torch_settings"": {},
        }
        
        #fill dict with params defined in yaml file
        configured_dict.update(load_config(phase_config_path))
        #==================================================
        
        #Fill what would be CLI args with values defined in script
        configured_dict[""checkpoint_settings""]['run_id'] = self.run_id
        configured_dict[""checkpoint_settings""]['results_dir'] = self.results_dir
        configured_dict[""checkpoint_settings""]['force'] = True
        configured_dict[""checkpoint_settings""]['inference'] = self.do_inference
        
        configured_dict[""engine_settings""]['width'] = self.width
        configured_dict[""engine_settings""]['height'] = self.height
        configured_dict[""engine_settings""]['quality_level'] = self.quality
        configured_dict[""engine_settings""]['no_graphics'] = self.no_graphics
        
        configured_dict[""env_settings""]['env_path'] = self.builds_dir + '3DBall'
        configured_dict[""env_settings""]['num_envs'] = self.nb_envs
        #==================================================

        final_runoptions = RunOptions.from_dict(configured_dict)
        
        self.RunTraining(final_runoptions)
        
    ##########################################################
    def RunTraining(self, run_options):   
        #run_training(self.seed, run_options)
        run_cli(run_options)
        
        shutil.copytree(self.results_dir + self.run_id, self.results_dir_cpy + self.run_id)
        
        time.sleep(2)

    ##########################################################

if __name__ == ""__main__"":
    pl = PhaseLauncher()
    pl.Start()
    
```
",issue help also stepped found unity environment brain self err unity brain class self false false true false start self loop represent phase range call pas object print start else self run section print run define fill defined file fill would defined script true self,issue,positive,negative,negative,negative,negative,negative
809811058,"> Might not be related to this PR, but should we add a warning in the docs about using hypernetworks for larger hidden_units values? We might even be able to auto-detect it in settings.py, e.g. if the resulting model will be bigger than 50mb print a warning

There is this line in the documentation:
```
If set to `hyper` (default) a [HyperNetwork](https://arxiv.org/pdf/1609.09106.pdf)
will be used to generate some of the
weights of the policy using the goal observations as input. Note that using a
HyperNetwork requires a lot of computations, it is recommended to use a smaller
number of hidden units in the policy to alleviate this.
```

I am hesitant to throw a warning if the model is going to be large because we never know what the user has in mind...",might related add warning might even able resulting model bigger print warning line documentation set hyper default used generate policy goal input note lot use smaller number hidden policy alleviate hesitant throw warning model going large never know user mind,issue,negative,positive,neutral,neutral,positive,positive
809799460,"Might not be related to this PR, but should we add a warning in the docs about using hypernetworks for larger `hidden_units` values? We might even be able to auto-detect it in `settings.py`, e.g. if the resulting model will be bigger than 50mb print a warning",might related add warning might even able resulting model bigger print warning,issue,negative,positive,positive,positive,positive,positive
809754445,"(sorry, can't comment inline for the file removal). `gridworld.png` is still being referenced:
```
$ git grep gridworld.png 
docs/Learning-Environment-Design-Agents.md:![Agent RenderTexture Debug](images/gridworld.png)
docs/Learning-Environment-Examples.md:![GridWorld](images/gridworld.png)
```
(and that should have failed the link checker)",sorry ca comment file removal still git agent link checker,issue,negative,negative,negative,negative,negative,negative
808213632,Thanks @dongruoping will look out for the setting in release 16. ,thanks look setting release,issue,negative,positive,positive,positive,positive,positive
807915652,"Update: the new interface `MultiAgentGroup` for multi-agent training is released our latest release (release 15)! You can find the API and examples of how to use it in our [documentation](https://github.com/Unity-Technologies/ml-agents/blob/release_15/docs/Learning-Environment-Design-Agents.md#groups-for-cooperative-scenarios). Welcome to checkout it out and leave us some feedback :)

Closing this issue since I think both your requests have been resolved (an interface to manage a group of agents, and collect stats after all agents taken their actions). If there's anything I'm missing, feel free to re-open it.",update new interface training latest release release find use documentation welcome leave u feedback issue since think resolved interface manage group collect taken anything missing feel free,issue,positive,positive,positive,positive,positive,positive
807912853,"Update: 

After looking into this more, the 4 sec blocking happens in the first message exchange in Grpc no matter the connection succeeded or failed. It's something inside the Grpc package that's out of our control and only happens with Windows.

What we can do here is that we recently added ML-Agents package settings which you can configure in the editor for your project. It has the option ""Connect to trainer"" that you can specify whether you want to connect to trainer or not. This should allows you to skip the connection time if you don't want to connect at all.

ML-Agents package settings is currently in main and will be included in the next release. More details please checkout our [doc](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Package-Settings.md).",update looking sec blocking first message exchange matter connection something inside package control recently added package configure editor project option connect trainer specify whether want connect trainer skip connection time want connect package currently main included next release please doc,issue,negative,positive,neutral,neutral,positive,positive
807698247,"Cool, thanks for clarifying.  I'm talking about this with the team.  We will get back to you soon.  ",cool thanks talking team get back soon,issue,positive,positive,positive,positive,positive,positive
807592007,"I've been watching training progress from phase to phase and it seems to be learning correctly and that the problem is probably related only to tensorboard (statswriter?)

Update: My temporary fix is copying the previous phase result directory to a new location before the next phase starts training and overwritting the new result values. ",watching training progress phase phase learning correctly problem probably related update temporary fix previous phase result directory new location next phase training new result,issue,negative,positive,neutral,neutral,positive,positive
807589819,"> Hi @ohernpaul,
> This is a lot to take in so let me make sure I understand you correctly before moving forward. From what I've read, you are launching sequential training steps in one process without shutting down the previous training process. You are doing this by creating RunOption instances with updated information you want to use for your next phase of training.
> 
> Does that sound accurate to you?

Chris, thanks for the quick response. Yes, sequential training sessions where each session uses initialize-from to start the new phase with knowledge from previous phase. I hacked together this automation pipeline using the RunOptions and run_training suggestions from my previous post about hyper parameters. My guess is that the session termination is not clearing correctly.

Outline: 
- folder of config yaml files that contain all the hyper params and environment (curriculum) params for each ""phase""
- folder containing 1 build (exe) of training environment
- python script (PhaseLauncher) that fills in the rest of the Checkpoint/Environment/Engine settings (as defined in trainers/settings.py) into the RunOptions object.
- Each config file is read in at the start of a phase and fills a new RunOptions Object, passes this to run_training (or run_cli)
- As run_training (or run_cli) finishes, logic handles clearing the last RunOptions object, loads the yaml config for the next phase, fills a new RunOptions object, and passes that to run_training again. 
- My guess is that run_training (or run_cli) is never fully terminating the same way it would if it was actually run in a terminal window.

I can provide more info if needed! 
",hi lot take let make sure understand correctly moving forward read sequential training one process without shutting previous training process information want use next phase training sound accurate thanks quick response yes sequential training session session start new phase knowledge previous phase hacked together pipeline previous post hyper guess session termination clearing correctly outline folder contain hyper environment curriculum phase folder build training environment python script rest defined object file read start phase new object logic clearing last object next phase new object guess never fully way would actually run terminal window provide,issue,positive,positive,positive,positive,positive,positive
807570758,"Hi @ohernpaul,
This is a lot to take in so let me make sure I understand you correctly before moving forward.  From what I've read, you are launching sequential training steps in one process without shutting down the previous training process.  You are doing this by creating RunOption instances with updated information you want to use for your next phase of training.

Does that sound accurate to you?",hi lot take let make sure understand correctly moving forward read sequential training one process without shutting previous training process information want use next phase training sound accurate,issue,negative,positive,positive,positive,positive,positive
806882446,"And please provide more information about your environment, like what sensors/actuators are you using? What does your agent code look like?",please provide information environment like agent code look like,issue,positive,neutral,neutral,neutral,neutral,neutral
806880992,"Hi, 
The GitHub issues page is for feature requests or bugs you have found and want to report.  Could you please post this question in the [forums](https://forum.unity.com/forums/ml-agents.453/) and I'll answer you there. ",hi page feature found want report could please post question answer,issue,negative,neutral,neutral,neutral,neutral,neutral
806731159,I'm using ML-Agents v1.7.2 (Dec. 23 preview) which has the same issue.. The ELO self play rating always seems to revert back to the default 1200 when resuming training.. I doubt this has any effect on the learning progress of the agent though because it always seems to recover quickly but it would be nice to have a consistent value when resuming training ,preview issue self play rating always revert back default training doubt effect learning progress agent though always recover quickly would nice consistent value training,issue,positive,positive,positive,positive,positive,positive
806585372,"emm... I know what happen ,my game is match3, and has many different celltypes.... the more types the more gc...",know happen game match many different,issue,negative,positive,neutral,neutral,positive,positive
805852438,"ah yes thats it, i knew i missed something out. Thanks.",ah yes thats knew something thanks,issue,positive,positive,positive,positive,positive,positive
805849907,I believe you have to call reset first. ,believe call reset first,issue,negative,positive,positive,positive,positive,positive
805808659,"When I build the default environment e.g. Gridworld, I encounter the same problem. 
",build default environment encounter problem,issue,negative,neutral,neutral,neutral,neutral,neutral
805237723,"Yes, I agree.  I also would not expect that.  I'll see if we can make that a bit better in the future. 
Cheers,
Chris",yes agree also would expect see make bit better future,issue,positive,positive,positive,positive,positive,positive
805188184,"Yes, I found them now, I expected the parameters to be directly on the behaviourParameters, not nested in another subclass.",yes found directly another subclass,issue,negative,positive,neutral,neutral,positive,positive
805180284,"I don't believe you looked quite hard enough.  The fields you want are on BrainParameters, which lives on BehaviorParameters.

```
var bp = gameObject.GetComponent<BehaviorParameters>();
bp.BrainParameters.VectorObservationSize = 5;
...
var agent = gameObject.AddComponent<YouAgent>();
agent.SetModel(""behavior"", pathToModel);",believe quite hard enough want agent behavior,issue,negative,negative,negative,negative,negative,negative
805179501,"Here is the [BehaviorParameters](https://github.com/Unity-Technologies/ml-agents/blob/release_15/com.unity.ml-agents/Runtime/Policies/BehaviorParameters.cs) class and the [BrainParameters](https://github.com/Unity-Technologies/ml-agents/blob/release_15/com.unity.ml-agents/Runtime/Policies/BrainParameters.cs) class.  The latter will allow you to set the expected VectorObservationSize and other properties from code, and it is a field of BehaviorParameters.
",class class latter allow set code field,issue,negative,neutral,neutral,neutral,neutral,neutral
805176950,"you can get the BehaviorParamters component before you add the agent component:
```
var bp = gameObject.GetComponent<BehaviorParameters>();
// Setup the brain parameters
var agent = gameObject.AddComponent<YouAgent>();
agent.SetModel(""behavior"", pathToModel);
```
",get component add agent component setup brain agent behavior,issue,negative,neutral,neutral,neutral,neutral,neutral
805175214,How can I set the observation vector size? The BehaviourParameters are automatically added by adding the agent.,set observation vector size automatically added agent,issue,negative,neutral,neutral,neutral,neutral,neutral
805173138,"Hi @white-cold,
It looks like the model you set on the agent doesn't match he expected vector observation size of the BehaviorParamters.  This is not supported.  The BehaviorParameters properties are set by the user, not by the current model it holds. ",hi like model set agent match vector observation size set user current model,issue,negative,neutral,neutral,neutral,neutral,neutral
805132638,The two agents are very different since I removed the code that controls the agent out of the original push block.  I did cringe at the duplication of GoalDetect.  ,two different since removed code agent original push block cringe duplication,issue,negative,positive,positive,positive,positive,positive
805056234,Is there enough duplicated code that this could changes to be a new scene in PushBlock? Although maybe the original PushBlock is going away soon (to be combined with WallJump) - @vincentpierre any thoughts?,enough code could new scene although maybe original going away soon combined,issue,negative,positive,positive,positive,positive,positive
805028307,"To clarify:
``` 
FileNotFoundError: [Errno 2] No such file or directory: 'none\\checkpoint.pt'
```
is the first error. We probably also need to make sure that the path for `training_status.json` exists before trying to save there (or just skip saving in this case), but that happens _after_ the exception caused by your bad configuration.",clarify file directory first error probably also need make sure path trying save skip saving case exception bad configuration,issue,negative,positive,neutral,neutral,positive,positive
805025388,"> `init_path: none`

I think you need `init_path: null` instead. From the logs:
```
FileNotFoundError: [Errno 2] No such file or directory: 'none\\checkpoint.pt'
```",none think need null instead file directory,issue,negative,neutral,neutral,neutral,neutral,neutral
804903669,"Hi,
You are using the ml-agents python package at version 0.23.0 and the POCA trainer was released at version 0.25.0.

Please read the release notes for release 15 (when POCA) was released and review the table of packages and versions required to use the latest features.  

Push block Collab is a new environment trained with POCA. ",hi python package version trainer version please read release release review table use latest push block new environment trained,issue,negative,positive,positive,positive,positive,positive
804667767,"I have tried but it seems it didnt understand what it get the rewards for.
(Combination of mobs he buys / lanes he attack)
ran for few days and it still doesnt get any good vs real players.
Note the Elo is going up and the rewards mean as well but still feels like
it doesnt link why it geta rewarded.

בתאריך יום ג׳, 23 במרץ 2021, 00:52, מאת Chris Goy ***@***.***
>:

> Hi @mamaorha <https://github.com/mamaorha>,
> I have logged this feature request internally as MLA-1877.
>
> Have you tried just setting the reward at the end of the round if the
> agent wins that round? Or are you trying to give intermediate rewards
> before the round is over?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/5163#issuecomment-804447859>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABS2XRI2YTZABEQFQ64X6TLTE7C4TANCNFSM4ZONJCEA>
> .
>
",tried didnt understand get combination attack ran day still doesnt get good real note going mean well still like doesnt link geta goy hi logged feature request internally tried setting reward end round agent round trying give intermediate round reply directly view,issue,negative,positive,neutral,neutral,positive,positive
804447859,"Hi @mamaorha,
I have logged this feature request internally as  MLA-1877.  

Have you tried just setting the reward at the end of the round if the agent wins that round?  Or are you trying to give intermediate rewards before the round is over?  ",hi logged feature request internally tried setting reward end round agent round trying give intermediate round,issue,positive,negative,negative,negative,negative,negative
804252484,Tested locally and works fine.,tested locally work fine,issue,negative,positive,positive,positive,positive,positive
803203511,@dongruoping I forgot to tag you before - does this look OK with the channel mapping?,forgot tag look channel,issue,negative,neutral,neutral,neutral,neutral,neutral
803199875,"Hi @white-cold,
I've logged this request internally as MLA-1876 and we will update this issue when work has been completed on it.
Cheers,
Chris",hi logged request internally update issue work,issue,negative,neutral,neutral,neutral,neutral,neutral
803176445,"@surfnerd I would like to request this as a feature, I am afraid I can't find how to add the request label",would like request feature afraid ca find add request label,issue,negative,negative,negative,negative,negative,negative
803163896,"If you'd like to request a feature, please do so.  Otherwise, I think questions like these are better suited to the [ML-Agents Forum.](https://forum.unity.com/forums/ml-agents.453/)",like request feature please otherwise think like better forum,issue,positive,positive,positive,positive,positive,positive
803163395,"Hi @white-cold,
You can do this with [barracuda manually](https://docs.unity3d.com/Packages/com.unity.barracuda@1.3/manual/ModelOutput.html), but we don't have an API at the ML-Agents level.",hi barracuda manually level,issue,negative,neutral,neutral,neutral,neutral,neutral
802954361,"another way to solve this is to request decisions with ""identifier"" for example
requestDecision(string identifier) -> in a turn base game that identifier can be the ""round number""
add rewards are summed based on that identifier - when a new one arrives (exmaple next round) accumulate the reward for all the actions on that round and start accumolating new reward for the current round",another way solve request identifier example string identifier turn base game identifier round number add summed based identifier new one next round accumulate reward round start new reward current round,issue,positive,negative,negative,negative,negative,negative
802631168,Thank you SO MUCH for your hard work!! @sven1977. You and RLLib are awesome.,thank much hard work awesome,issue,positive,positive,positive,positive,positive,positive
802527202,"Update: The latest ML-Agents [Release 15](https://github.com/Unity-Technologies/ml-agents/tree/release_15) has offered new API `MultiAgentGroup` to achieve this and our new trainer supports cooperative behaviors training.
Please refer to our [documentation](https://github.com/Unity-Technologies/ml-agents/blob/release_15_docs/docs/Learning-Environment-Design-Agents.md#groups-for-cooperative-scenarios) or checkout the new example environments: [Cooperative Push Block](https://github.com/Unity-Technologies/ml-agents/blob/release_15_docs/docs/Learning-Environment-Examples.md#cooperative-push-block) and [Dungeon Escape](https://github.com/Unity-Technologies/ml-agents/blob/release_15_docs/docs/Learning-Environment-Examples.md#dungeon-escape).

Closing this issue since it has been resolved.",update latest release new achieve new trainer training please refer documentation new example push block dungeon escape issue since resolved,issue,negative,positive,positive,positive,positive,positive
802321990,you may want to re-create this with the new v2-staging head,may want new head,issue,negative,positive,positive,positive,positive,positive
802247851,"Hey @StevePeters-US,
Thanks for your request.  I have logged it internally as  MLA-1870 and will update this thread when work has been completed on it.
Cheers,
Chris",hey thanks request logged internally update thread work,issue,negative,positive,neutral,neutral,positive,positive
802146322,"2018 tests are failing and are expected since I removed the 2019_OR_NEWER flags.
Do we plan to remove them?",failing since removed plan remove,issue,negative,neutral,neutral,neutral,neutral,neutral
802145616,"Tests added. I placed it under DevProject to have a place to create settings asset on the fly during tests. 
The reason is that I found it hard to write meaningful tests without actually interact with asset and I can’t use something like MockFileSystem since the asset APIs require things placed under `Asset/`. 
To reserve a place that I can create assets that won't conflict with existing files I put the tests in a separate folder in DevProject.

",added place create asset fly reason found hard write meaningful without actually interact asset use something like since asset require reserve place create asset wo conflict put separate folder,issue,positive,positive,positive,positive,positive,positive
801271124,"@surfnerd, thank you so much for responding to both of my posts - your comments have helped me ground my understanding of what exactly mlagents-learn is doing on the python end and how its interacting with the UnityEnvironment API. 

> mlagents-learn is _our_ implementation of training using the UnityEnvironment API. The UnityEnvironment API is not intended to be a way for you to invoke mlagents-learn through python.

I misunderstood this concept at first and thought UnityEnvironment was part of mlagents, but it is clear now!

> importing the `run_training()` method (`from mlagents.trainers.learn import run_training`), constructing a `RunOptions` object

I appreciate the suggestion and will use this in my hyper param search script. Thank you again for outlining all of this.",thank much ground understanding exactly python end implementation training intended way invoke python misunderstood concept first thought part clear method import object appreciate suggestion use hyper param search script thank,issue,positive,positive,positive,positive,positive,positive
800786749,"Hi @ohernpaul,
I heard back from a colleague on our research team.  What they said was there isn’t a supported way to do this, but you can do some of it by importing the `run_training()` method (`from mlagents.trainers.learn import run_training`), constructing a `RunOptions` object (defined in `settings.py`), and calling `run_training` with this object. That’ll let you configure hyperparameters. Configuring the `EnvironmentParameters` will require a deeper modification, however.

I have logged this request internally as  MLA-1867 and we will update you on this issue when we make progress on the request. 

Thanks for your feedback and for your interest in ML-Agents.",hi back colleague research team said way method import object defined calling object let configure require modification however logged request internally update issue make progress request thanks feedback interest,issue,positive,positive,neutral,neutral,positive,positive
800766388,the additional args like `--no-graphics` would go into the `additional_args` parameter.   Those are forwarded to the unity executable. ,additional like would go parameter unity executable,issue,negative,neutral,neutral,neutral,neutral,neutral
800765303,"Hi @ohernpaul,
I believe I responded to you in the forums.  Since your question is phrased slightly differently here, I'll respond slightly differently and post my response to you in the forums below.

The UnityEnvironment API is what mlagents-learn is built on top of.  It is intended for you to be able to build your own algorithms to train a unity environment.  mlagents-learn is _our_ implementation of training using the UnityEnvironment API.  The UnityEnvironment API is not intended to be a way for you to invoke mlagents-learn through python.  

I have reached out to the research team to chime in to see if they answer your questions more specifically.

**From the forums:**
The resolution and quality settings actually go into the [engine_config_channel](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Python-API.md#engineconfigurationchannel).

I don't think setting a config file is the intention of using the UnityEnvironment API. I believe that it is for more low-level interactions with a Unity binary, where you may want to use the API to design your own reinforcement learning algorithms, inspect the simulation from python, or other low-level tasks. Please read the [Interacting With A Unity Environment ](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Python-API.md#interacting-with-a-unity-environment) documentation for more information.",hi believe since question slightly differently respond slightly differently post response built top intended able build train unity environment implementation training intended way invoke python research team chime see answer specifically resolution quality actually go think setting file intention believe unity binary may want use design reinforcement learning inspect simulation python please read unity environment documentation information,issue,positive,positive,positive,positive,positive,positive
800527664,"> There is a release coming out this week, but it will not contain these changes. The changes will make it into the April release alongside the changes to make physics and physics2d optional dependencies as well.

Thank you! I'll report back if there are any problems.",release coming week contain make release alongside make physic optional well thank report back,issue,positive,neutral,neutral,neutral,neutral,neutral
800513393,"> @dongruoping yep just caught that, the fix works 👍

Can you also fix the `torch.BoolTensor` in the same file?",yep caught fix work also fix file,issue,negative,neutral,neutral,neutral,neutral,neutral
800486465,"> Do you think we should enable gpu tests in all python PRs? I think we've seen couple times this fails in nightly CI and would be nice to catch them earlier.

I agree. Spoke with @surfnerd about this; I've added it to the Release Checklist. I guess we have to weight the cost of spinning up a GPU machine vs. catching these earlier. Not sure what the tradeoff is. ",think enable python think seen couple time nightly would nice catch agree spoke added release guess weight cost spinning machine catching sure,issue,positive,positive,positive,positive,positive,positive
800485311,"@dongruoping yep just caught that, the fix works 👍 ",yep caught fix work,issue,negative,neutral,neutral,neutral,neutral,neutral
800479478,Do you think we should enable gpu tests in all python PRs? I think we've seen couple times this fails in nightly CI and would be nice to catch them earlier.,think enable python think seen couple time nightly would nice catch,issue,negative,positive,positive,positive,positive,positive
800478117,"There is a release coming out this week, but it will not contain these changes. The changes will make it into the April release alongside the changes to make physics and physics2d optional dependencies as well.",release coming week contain make release alongside make physic optional well,issue,negative,neutral,neutral,neutral,neutral,neutral
800466287,"One thing I noticed in networks.py:line 271
`attn_mask = only_first_obs_flat.isnan().type(torch.FloatTensor)`
this will move tensors on cpu. 

Since we're using default tensor type we should avoid calling `torch.FloatTensor` and use `.float()` instead.
I've fixed several once before so I figured I should explicitly call this out.
",one thing line move since default tensor type avoid calling use instead fixed several figured explicitly call,issue,negative,positive,neutral,neutral,positive,positive
800448388,"@mmattar what if we split it into two lines, one for RL and one for IL? e.g.

- Support for training single-agent, multi-agent cooperative, and multi-agent competitive scenarios via several deep reinforcement learning algorithms (PPO, SAC, MA-POCA).
- Support for learning from demonstrations through two imitation learning algorithms (BC and GAIL). 

I think the callout of the algorithms is more for researchers TBH, they would want to know which algorithms the toolkit support at a glance. We might not need to spell out the entire acronym for this audience (though POCA isn't a known/Googleable algorithm). ",split two one one support training competitive via several deep reinforcement learning sac support learning two imitation learning think would want know support glance might need spell entire acronym audience though algorithm,issue,positive,negative,neutral,neutral,negative,negative
800439539,"> Do you manage to train on GPU after this fix ?

It's still failing: (Yamato job here: https://yamato.cds.internal.unity3d.com/jobs/497-ml-agents/tree/develop-poca-gpu/.yamato%252Fpytest-gpu.yml%2523pytest_gpu/5821068/job) need to debug",manage train fix still failing job need,issue,negative,neutral,neutral,neutral,neutral,neutral
800407436,"BTW, if anyone else runs into pre-commit issues after this commit, you have to have `dotnet` version newer than 3.1. I had 2.1 and had to reinstall using the mac installer [here](https://dotnet.microsoft.com/download/dotnet/5.0).",anyone else commit version reinstall mac installer,issue,negative,neutral,neutral,neutral,neutral,neutral
799880911,"It’s only used for 2018.4 where apparently that check isn’t a warning.

On Mon, Mar 15, 2021 at 16:44 Chris Elion ***@***.***> wrote:

> I thought we had warnings as errors in the project - should this have
> failed CI?
>
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/pull/5117#issuecomment-799834862>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAKLGSG4JA2OEPMJKPBEC5TTD2LUNANCNFSM4ZHNDZ6A>
> .
>
-- 
*Chris Goy*
Senior Software Developer | ML Agents
San Francisco
<https://unity3d.com/?utm_source=unity3d&utm_medium=email&utm_campaign=company-information-2016-Global-Unity-Email-Sig>
",used apparently check warning mon mar wrote thought project state reply directly view goy senior developer san,issue,negative,positive,neutral,neutral,positive,positive
799841015,"> @JesseTG,
> which version of unity are you using, and which version of the package are you using? Optional dependencies will only work with Unity versions 2019.4 and later.

I'm using 2020.2.7, and I intend to upgrade to 2021.1.0 when it comes out.

> He @JesseTG,
> #5109 was merged into `main`. Please let us know if this fixes your issue.
> Thanks,
> Chris

With pleasure, thank you very much! When's the next release?",version unity version package optional work unity later intend upgrade come main please let u know issue thanks pleasure thank much next release,issue,positive,positive,neutral,neutral,positive,positive
799823180,"He @JesseTG,
#5109 was merged into `main`.  Please let us know if this fixes your issue.
Thanks,
Chris",main please let u know issue thanks,issue,positive,positive,positive,positive,positive,positive
799796052,"@JesseTG, 
which version of unity are you using, and which version of the package are you using?  Optional dependencies will only work with Unity versions 2019.4 and later.",version unity version package optional work unity later,issue,negative,neutral,neutral,neutral,neutral,neutral
797874695,"Hi @JesseTG,
We have logged this request as MLA-1848 internally and will update this issue when work has been completed on this request.  Thanks for your feedback.
Cheers,
Chris",hi logged request internally update issue work request thanks feedback,issue,negative,positive,neutral,neutral,positive,positive
797616251,"> Thanks for the info @JesseTG,
> I'm going to create a PR that makes the analytics module an optional dependency. This should address your problem and hopefully anyone else that is using an NDA platform.

Thank you! I appreciate it.",thanks going create analytics module optional dependency address problem hopefully anyone else platform thank appreciate,issue,positive,positive,positive,positive,positive,positive
797615299,"Thanks for the info @JesseTG,
I'm going to create a PR that makes the analytics module an optional dependency.  This should address your problem and hopefully anyone else that is using an NDA platform.",thanks going create analytics module optional dependency address problem hopefully anyone else platform,issue,positive,positive,positive,positive,positive,positive
797608008,"> 
> 
> Is the platform you are using under NDA? Or is there a define that the analytics code uses that we aren't aware of?

The platform itself is under NDA. Well, its development details. The platform itself is quite famous (and odds are good you have one in your living room -- or in your backpack).

No unusual `#define`s are in use, to my knowledge; the Analytics DLLs are just not included in the compiler's search path when my platform is active.",platform define analytics code aware platform well development platform quite famous odds good one living room unusual define use knowledge analytics included compiler search path platform active,issue,positive,positive,positive,positive,positive,positive
797590082,"Is the platform you are using under NDA?  Or is there a define that the analytics code uses that we aren't aware of?

",platform define analytics code aware,issue,negative,positive,positive,positive,positive,positive
797000073,"Hi @Dastyn 

The `network_settings` you see there is because a number of the reward signals (like GAIL and RND) use neural networks, and this allows those network to be modified. In this case, the extrinsic signal does not use them, so they can be safely ignored. 

Thanks for bringing up the multiple of 4 issue. We will add this too the backlog to have a better error message raised in the future.",hi see number reward like use neural network case extrinsic signal use safely thanks multiple issue add backlog better error message raised future,issue,positive,positive,positive,positive,positive,positive
796917446,"Hi @ParkHaeChan 

Unfortunately ML-Agents is not really designed for the use-case you have in mind of sending large amounts of data back and forth at every update of the engine. If it is possible, I would recommend trying to export your model as an onnx file, and running it natively within Unity using the Barracuda inference package. That will likely result in orders of magnitude improvement in processing time, as there will be no need for costly communication between the two processes. https://github.com/Unity-Technologies/barracuda-release",hi unfortunately really designed mind sending large data back forth every update engine possible would recommend trying export model file running natively within unity barracuda inference package likely result magnitude improvement time need costly communication two,issue,negative,positive,neutral,neutral,positive,positive
796875723,"Hi @RedTachyon,
are you experiencing this during training?  Don't forget that for every decision request during training, there is a batched call to the python process over gRPC to send observations and retrieve actions.  Could you provide more information about what your agent's action/observation space is? What sensors/actuators/onActionReceived/CollectObservations look like?

It's hard to diagnose issues without any of this information. 
Cheers,
Chris",hi training forget every decision request training call python process send retrieve could provide information agent space look like hard diagnose without information,issue,negative,negative,negative,negative,negative,negative
796387343,last commit only contains change for comment. skipping the test.,last commit change comment skipping test,issue,negative,neutral,neutral,neutral,neutral,neutral
796321009,"@Musubee , nope.  At least it looks like @chriselion got it.  ",nope least like got,issue,negative,negative,negative,negative,negative,negative
796320042,"Looks like this got lost in a [merge](https://github.com/Unity-Technologies/ml-agents/pull/5044/files#diff-fcf6d384bb8308546cecce54bb9678c715f95c119b402f9b61a78e0cf3d8fbc4) the other day, thanks for catching it.",like got lost merge day thanks catching,issue,negative,positive,positive,positive,positive,positive
796305885,"> Hi @JesseTG
> 
> Thanks for bringing this to our attention. This package should only have an impact on the Editor, and not the builds containing ML-Agents. Can you share what error you are running into, and at what point it is occuring? It is when importing the package, or when trying to build for your platform? It would also be helpful to know the specific platform which you are having issues targeting, as that will help us to reproduce the issue.

The platform I'm using does not support Unity Analytics; when I set it to be the active build platform, all Unity Analytics APIs are simply unavailable, resulting in compiler errors. This occurs in the editor, so I assume it would be the same when building.

I get pretty much the same error in multiple places, all in `TrainingAnalytics` and `InferenceAnalytics`. Here's one of the errors:

```
[CompilerError] The type name 'AnalyticsResult' could not be found in the namespace 'UnityEngine.Analytics'. This type has been forwarded to assembly 'UnityEngine.UnityAnalyticsModule, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null' Enable the built in package 'Unity Analytics' in the Package Manager window to fix this error.
Compiler Error at Library\PackageCache\com.unity.ml-agents@1.8.0-preview\Runtime\Analytics\TrainingAnalytics.cs:67 column 17
65:               {
66:   #if UNITY_EDITOR
-->67:                   AnalyticsResult result = EditorAnalytics.RegisterEventWithLimit(eventName, k_MaxEventsPerHour, k_MaxNumberOfElements, k_VendorKey);
68:   #else
69:                   AnalyticsResult result = AnalyticsResult.UnsupportedPlatform;

The type name 'AnalyticsResult' could not be found in the namespace 'UnityEngine.Analytics'. This type has been forwarded to assembly 'UnityEngine.UnityAnalyticsModule, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null' Enable the built in package 'Unity Analytics' in the Package Manager window to fix this error.
```

Unfortunately, I can't publicly say which platform I'm targeting as its details are confidential. However, there are other useful things I can say:

- You can easily reproduce this issue for any target platform by simply disabling `com.unity.modules.unityanalytics` in the package manager. The effect, errors, and fix should be the same.
- My target platform doesn't offer the `UnityEngine.Analytics` assemblies, even if `com.unity.modules.unityanalytics` is enabled.
- I believe this issue affects multiple NDA-protected platforms and that the same fix should work for all of them. Whichever guess you make will probably be the correct one.
",hi thanks attention package impact editor share error running point package trying build platform would also helpful know specific platform help u reproduce issue platform support unity analytics set active build platform unity analytics simply unavailable resulting compiler editor assume would building get pretty much error multiple one type name could found type assembly enable built package analytics package manager window fix error compiler error column result else result type name could found type assembly enable built package analytics package manager window fix error unfortunately ca publicly say platform confidential however useful say easily reproduce issue target platform simply package manager effect fix target platform offer even believe issue multiple fix work whichever guess make probably correct one,issue,positive,positive,positive,positive,positive,positive
796235381,"Whoops, that's my fault.  If you want to submit a PR please do and assign me to it.",whoop fault want submit please assign,issue,negative,neutral,neutral,neutral,neutral,neutral
795754198,"Do we need some safeguards around:
* using `coma2` trainer but agents have groupID 0
* using PPO or SAC trainer, but agents with non-0 groupID

? I think a warning would suffice, but without it, somebody is going to accidentally turn team training ""halfway"" on and get frustrated.

edit: not a blocker for this PR, but would like to see it in a followup before the release.",need around coma trainer sac trainer think warning would suffice without somebody going accidentally turn team training halfway get edit blocker would like see release,issue,negative,negative,neutral,neutral,negative,negative
794701418,Hi @RedTachyon - this was implemented in https://github.com/Unity-Technologies/ml-agents/pull/5036 and will be in the next release (tentatively next week).,hi next release tentatively next week,issue,negative,neutral,neutral,neutral,neutral,neutral
794539980,"Hi @JesseTG 

Thanks for bringing this to our attention. This package should only have an impact on the Editor, and not the builds containing ML-Agents. Can you share what error you are running into, and at what point it is occuring? It is when importing the package, or when trying to build for your platform? It would also be helpful to know the specific platform which you are having issues targeting, as that will help us to reproduce the issue.",hi thanks attention package impact editor share error running point package trying build platform would also helpful know specific platform help u reproduce issue,issue,positive,positive,neutral,neutral,positive,positive
794438968,"> Is this able to run on GPU? In _compare_two_policies we move the actors explicitly and I just saw that we also added `self.actor.to(default_device())` to TorchPolicy. I didn't see that in optimizers and I wonder if we need to do the same thing as well.

I think I need to add this too, but the different optimizers have different components. I'll try iterating through the `get_modules()` and moving each of those to the default device.

**edit** Never mind, this should work b/c the optimizer moves itself to the default device on initialization. So if it fails, it means the optimizer itself is broken. ",able run move explicitly saw also added see wonder need thing well think need add different different try moving default device edit never mind work default device broken,issue,negative,positive,neutral,neutral,positive,positive
794236321,"Hi @vincentpierre 
Thanks for your efforts!
Good news, after pulling the very last release, it works fine: BC and/or GAIL enabled in the yaml config file with Sorter demo files can now learn.

For the record, please find below the pull history:
```
$ git reflog --date=iso
d0b3e9966 (HEAD -> main, origin/main, origin/HEAD) HEAD@{2021-03-09 18:36:48 +0100}: pull: Fast-forward
5e87e2cf9 HEAD@{2021-03-07 10:34:16 +0100}: clone: from https://github.com/Unity-Technologies/ml-agents.git
```
Thanks again for your reactivity.
Dastyn",hi thanks good news last release work fine file sorter learn record please find pull history git reflog head main head pull head clone thanks reactivity,issue,positive,positive,positive,positive,positive,positive
794204263,"Hi @Dastyn 
Despite my efforts, I have not been able to reproduce the error. It is possible that something changed on master. Can you give me the hash of the commit you are on? Can you also try again after a `git pull`? I think the error comes from the embedding size of the attention layer. Yours says 130, but it should be `gail.encoding_size // 4 = 128`. My guess is that there is some extra embedding coming maybe from the `use_actions` setting of GAIL. ",hi despite able reproduce error possible something master give hash commit also try git pull think error come size attention layer guess extra coming maybe setting,issue,positive,positive,positive,positive,positive,positive
794130992,"Hi @Ladun 
Thank you for raising this issue. I think the confusion comes from the word step, which means different things in different contexts. Steps can be interpreted as a ""decision point"". In this case +0.1 per step for 3DBall is correct. 
I logged this issue as MLA-1838, and we will look into it.",hi thank raising issue think confusion come word step different different decision point case per step correct logged issue look,issue,negative,neutral,neutral,neutral,neutral,neutral
794123759,"Hi @ataerabi 
Please use the template for submitting issues, there are instructions on how to write an issue in a way that is reproducible to us. My guess on your issue is that you are using the wrong version of ML-Agents. You maybe got the package from the Unity Package manager (version 1.0.6) while using the examples from the master branch of github (version 1.8.1-preview). I would advise you to clone the whole github repo if you are trying to use the master branch or use the examples from version 1.0.6 on this branch : https://github.com/Unity-Technologies/ml-agents/tree/com.unity.ml-agents_1.0.6
If the error persists, please submit an issue with you complete setup so we can reproduce the error.",hi please use template write issue way reproducible u guess issue wrong version maybe got package unity package manager version master branch version would advise clone whole trying use master branch use version branch error please submit issue complete setup reproduce error,issue,negative,negative,neutral,neutral,negative,negative
793811647,"Ok, I can reproduce the error on the Ray RLlib side now (yes, it's a change in the ML-Agents API for `set_action_for_agent` calls). I'll provide a fix in RLlib.
@budbreaker @jinPrelude ",reproduce error ray side yes change provide fix,issue,negative,neutral,neutral,neutral,neutral,neutral
792951890,"@ervteng I don't think we should be picking encoders based on the string; we should add flags to explicitly control this (which is what we've been doing with e.g. dimension properties and ObservationType). This is intended to help with debugging, but I think it might make sense for the demonstration filtering too.

@vincentpierre Please see the linked issue - this was a request from a user of the LLAPI to help them debug their sensors+observations.

> Is there maybe a way to test it end to end ?

 I'll a check in the LLAPI test that the name is set to something non-empty



",think based string add explicitly control dimension intended help think might make sense demonstration filtering please see linked issue request user help maybe way test end end check test name set something,issue,positive,neutral,neutral,neutral,neutral,neutral
792247482,"Definitely difficult to investigate.
Then closing the issue.",definitely difficult investigate issue,issue,negative,negative,negative,negative,negative,negative
792087085,"Hi, you need to install the trainers from the branch you got the sorter environment from.  It looks like you got the **last** release of the trainers published to PyPi.  Use `pip install -e ` syntax to install ml-agents-envs and ml-agents from the brach you are working with.",hi need install branch got sorter environment like got last release use pip install syntax install brach working,issue,negative,neutral,neutral,neutral,neutral,neutral
791839455,"Hi guys. Thanks for the reply. i tried to download from the main branch, on win 10. Now I am trying to download on other win 10 machine. Thanks and a last question: what is the size of the zip file on disk ?.",hi thanks reply tried main branch win trying win machine thanks last question size zip file disk,issue,positive,positive,positive,positive,positive,positive
791715472,"I'm actually in support of this in the long run. In the future we can go as far as e.g. configuring different encoders for different sensors in the YAML. Also in demos we could use it to, for instance, record _all_ the sensor but only train using some subset of them, and save people from re-recording demos over and over. ",actually support long run future go far different different also demo could use instance record sensor train subset save people demo,issue,positive,positive,neutral,neutral,positive,positive
791682333,Fix made and merged (#5041). Thank you for raising this issue.,fix made thank raising issue,issue,negative,neutral,neutral,neutral,neutral,neutral
791019727,@awjuliani Here is the PR to modify the CI tests accordingly. Please review : https://github.com/Unity-Technologies/ml-agents-cloud-internal/pull/292,modify accordingly please review,issue,negative,neutral,neutral,neutral,neutral,neutral
790849838,"> https://github.com/Unity-Technologies/ml-agents/blob/develop-remove-some-scenes/docs/images/example-envs.png needs to be modified to remove the old environments.

Yes, I was wondering about that image. On one hand, I think it looks good for posterity (a little bit of nostalgia), on the other, these are outdated... I can try to make a new one and ask for feedback on slack.",need remove old yes wondering image one hand think good posterity little bit nostalgia outdated try make new one ask feedback slack,issue,positive,positive,neutral,neutral,positive,positive
790003657,"Shelving this for now, since research won't be able to make use of it for a while, and we're still not sure on the trajectory/experience handling.",shelving since research wo able make use still sure handling,issue,negative,positive,positive,positive,positive,positive
789966438,"Yamato tests are running, so that proves this does what it needs to do. Will cancel them to save resources, and merge.",running need cancel save merge,issue,negative,neutral,neutral,neutral,neutral,neutral
789946693,I have the same issue as @jinPrelude when trying to run 3DBall and Tennis ,issue trying run tennis,issue,negative,neutral,neutral,neutral,neutral,neutral
789629252,"Hi @surfnerd ,
Thank you for your quick respense , I will just add the package locally or create a repo for the package and add it via git url , both works for me . Cheers !
",hi thank quick add package locally create package add via git work,issue,positive,positive,positive,positive,positive,positive
789282022,"Sounds good. I'll need to discuss it with some folks, but I think it's pretty easy to add. I have this logged as MLA-1823 in our internal tracker.",good need discus think pretty easy add logged internal tracker,issue,positive,positive,positive,positive,positive,positive
789247751,"Yes, I actually came across the BehaviorSpecs a bit later, but I figured trying to match the dimensions is a level of insanity I'm not ready for. Anyways, yes, adding the name would certainly work!

",yes actually came across bit later figured trying match level insanity ready anyways yes name would certainly work,issue,positive,positive,positive,positive,positive,positive
789169841,"Hi @SlimGabsi,
One of my fellow teammates was patient enough to wait for the resolution which completed in about 5 minutes.  The package manager team also notified us that they are landing optimizations that will (hopefully) make it into the next release of 2019.4.  I'm going to close this issue for now.  Thank you for your feedback and interest in ML-Agents.
Cheers,
Chris",hi one fellow patient enough wait resolution package manager team also notified u landing hopefully make next release going close issue thank feedback interest,issue,positive,neutral,neutral,neutral,neutral,neutral
789168604,"Sorry for the delay on this.

In the latest release, BehaviorSpec changed to have a list of ObservationSpec NamedTuple on it ([source](https://github.com/Unity-Technologies/ml-agents/blob/dd4fd79a059fbedc9149566acf1c99a71d50a9cf/ml-agents-envs/mlagents_envs/base_env.py#L477-L504)); these correspond 1:1 with the observations at runtime. It sounds like if we add the name to this, you'll be able to do what you want, right?

> From experimentation, I figured out that the main sensor is the last entry in the list.

Sensors are sorted alphabetically by name on the C# side. The agent's VectorSensor will get the name `$""VectorSensor_size{observationSize}""` (where `observationSize` is the Agent's observation space size). So that's *probably* the last one in the list, but not necessarily :)",sorry delay latest release list source correspond like add name able want right experimentation figured main sensor last entry list sorted alphabetically name side agent get name agent observation space size probably last one list necessarily,issue,negative,positive,positive,positive,positive,positive
789123186,Are there any warnings/errors in the editor log around the time Unity freezes? Are you able to attach a debugger and see where it's stuck?,editor log around time unity able attach see stuck,issue,negative,positive,positive,positive,positive,positive
789068554,I am able to reproduce.  We recent switched our default branch to 'main' so maybe there is a bug in the package resolver.  I'll bring it up with their team and update this thread when I hear back.,able reproduce recent switched default branch maybe bug package resolver bring team update thread hear back,issue,negative,positive,positive,positive,positive,positive
789019835,"Hi @SlimGabsi,
Can you fill out the information in the issue template, please?  We need information like your version of Unity, OS, etc.
Cheers,
Chris",hi fill information issue template please need information like version unity o,issue,positive,neutral,neutral,neutral,neutral,neutral
788197907,"@chriselion  This is affected by #4939 and I think the release is safe.
The forum post seems to be a different issue and that should have already been resolved in the latest version.
",affected think release safe forum post different issue already resolved latest version,issue,negative,positive,positive,positive,positive,positive
788192548,"Does this affect release 13? If so, we should probably do a hotfix release...",affect release probably release,issue,negative,neutral,neutral,neutral,neutral,neutral
787535448,"`GameObject bullet = Instantiate(bulletPrefab, shootingPoint.position, shootingPoint.rotation);`

Doing this in fixed update could potentially mean that you are instantiating a whole lot of bullets.  I don't know if that's the source of the issue, but if your neural network learns to shoot and then gets close to the target to keep shooting I could see why your environment would freeze.

Also, `FindGameObejctWithTag`is very slow.  You would be better off caching the bullets in a list and destroying them in OnEpisodeBegin.

That being said, you may want to limit the number of bullets that are instantiated.  Calling Instantiate and Destroy are also very slow operations.  You may want to consider object pooling for this case.",bullet fixed update could potentially mean whole lot know source issue neural network shoot close target keep shooting could see environment would freeze also slow would better list said may want limit number calling destroy also slow may want consider object case,issue,negative,negative,neutral,neutral,negative,negative
786986799,"I actually updated this a few months ago on the release_2_verified branch (https://github.com/Unity-Technologies/ml-agents/pull/4535) but must not have updated the `release_2_verified_docs` tag. That's updated now.

Thanks!",actually ago branch must tag thanks,issue,negative,positive,neutral,neutral,positive,positive
786979920,"Which zip file are you trying to use, and what platform? As @surfnerd said, the zip files for releases or the `main` branch are generated by Github, not us. But I was able to download and unzip both `main` and `release_13` on mac and windows 10.",zip file trying use platform said zip main branch u able main mac,issue,negative,positive,positive,positive,positive,positive
786956331,"Yes correct. You'll need a script like an environment controller that tracks the FixedUpdates or whatever stepping frequency in your case, and call Academy.Instance.EnvironmentStep() accordingly. ",yes correct need script like environment controller whatever stepping frequency case call accordingly,issue,positive,neutral,neutral,neutral,neutral,neutral
786934775,"Hi @alexArayaM,
If you are on Mac this might happen.  This isn't an ML-Agents bug, but an issue you have with GitHub.",hi mac might happen bug issue,issue,negative,neutral,neutral,neutral,neutral,neutral
786458276,"This is my code. I left out the heuristic method so it is not so messy. There are probably some mistakes, i am still working on it.

I know i can resume the training. But the problem is, that not only Unity Freezes. 
1. Unity Freezes 2. At the same time my whole system gets slower, and most of the time i must restart it. But sometimes it is enough to restart Unity.

```
using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using Unity.MLAgents;
using Unity.MLAgents.Actuators;
using Unity.MLAgents.Sensors;

public class EnemyMovement : Agent {

    private float health;
    private bool isShooting;
    private float shotsTaken;

  

    //getting a reference to the dumbTarget
    public GameObject dumbTarget;
    DumbTarget dumbTargetScript;

    private void Start()
    {
      //Reference to dumbTarget
        dumbTargetScript = dumbTarget.GetComponent<DumbTarget>();
    }
    public override void OnEpisodeBegin() {
        //New Positions etc. for agentTarget
        transform.localPosition = new Vector2(Random.Range(-47.52f, 48.5f), Random.Range(48.1f, -46.42f));
        shotsTaken = 0f;
        health = 100f;


        //deleting all bullets
        GameObject[] bullets = GameObject.FindGameObjectsWithTag(""Bullet"");
        foreach (GameObject bullet in bullets)
            GameObject.Destroy(bullet);


        //dumbTarget
        dumbTargetScript.health = 100f;
    }

    public Material winMaterial;
    public Material loseMaterial;
    public SpriteRenderer wallMeshRenderer;

    public override void CollectObservations(VectorSensor sensor) {
        sensor.AddObservation(transform.rotation);
        sensor.AddObservation(transform.localPosition);

        //observations for dumbTarget
        sensor.AddObservation(dumbTarget.transform.localPosition);
    }

    //saves the last Rotation after the agents stopped moving
    Quaternion currentRotation;
    public float moveSpeed = 10f;

    public override void OnActionReceived(ActionBuffers actions) {
        //Getting input for playerMovement
        float xMovement = actions.ContinuousActions[0];
        float yMovement = actions.ContinuousActions[1];

        //Moving the player
        transform.GetComponent<Rigidbody2D>().velocity = new Vector2(xMovement * moveSpeed, yMovement * moveSpeed);

        //Rotating the player
        Vector3 movement = new Vector3(xMovement, yMovement, 0);
        Quaternion lookRotation = Quaternion.LookRotation(Vector3.forward, movement);
        Vector3 rotation = lookRotation.eulerAngles;
        transform.rotation = Quaternion.Euler(0f, 0f, rotation.z);
        

        //as long as the agent is moving, the current rotation gets saved
        if ((xMovement != 0f) || (yMovement != 0f)) {
            currentRotation = transform.rotation;
            Debug.Log(""Test"" + currentRotation);
        }

        //if the agent stops moving the last saved rotaion will be used => fixed the snapping-back bug
        if ((xMovement == 0f) && (yMovement == 0f)) {
            Debug.Log(""Stop"");;
            transform.rotation = currentRotation;
        }

        //Agent gets penalty for every step
        int firstStepCount = StepCount;
        if (firstStepCount < StepCount)
        {
            SetReward(-0.1f);
        }
        //Agent gets huge penalty if it does not reach the goal/kills target
        if (StepCount == MaxStep)
        {
            SetReward(-2f);
            EndEpisode();
        }

        //shooting
        float distanceBetween = Vector3.Distance(transform.position, dumbTarget.transform.localPosition);
        if (distanceBetween <= 30f)
        {
            SetReward(+4f);
            float shooting = actions.ContinuousActions[2];
            if (shooting > 0.4f)
            {
                SetReward(-0.5f);
            shotsTaken++;
            GameObject bullet = Instantiate(bulletPrefab, shootingPoint.position, shootingPoint.rotation);
            Rigidbody2D rbbullet = bullet.GetComponent<Rigidbody2D>();
            rbbullet.AddForce(shootingPoint.up * shootingForce, ForceMode2D.Impulse);
            }
        }


        //Agent gets big reward if it kills the target
        if (dumbTargetScript.health <= 0f)
        {
            wallMeshRenderer.material = winMaterial;
            SetReward(+7f);
            EndEpisode();
        }

        //If the dumbTargetCurrentHealth is lower that dumbTargetScript.health, then the dumbTarget was hit
        float dumbTargetCurrentHealth = dumbTargetScript.health;
        if (dumbTargetCurrentHealth > dumbTargetScript.health)
        {
            SetReward(+1f);
        }

        ////reduces the amount of shots the agent should take
        float firstShotsTakenCount = shotsTaken;
        if (firstShotsTakenCount < shotsTaken)
        {
            SetReward(-2f);
        }

    }


   
    private void OnTriggerEnter2D(Collider2D other) {
      ////when agent hits wall
        if (other.tag == ""Border"") {
            SetReward(-1f);
            wallMeshRenderer.material = loseMaterial;
            EndEpisode();
        }


        if (other.tag == ""Bullet"")
        {
            health -= 25f;
            Debug.Log(""Playerhealth "" + health);
            SetReward(-1f);

            if (health <= 0f)
            {
                Debug.Log(""Player is dead"");
                wallMeshRenderer.material = loseMaterial;
                SetReward(-4f);
                EndEpisode();
            }
        }
    }

     
    
}
```

",code left heuristic method messy probably still working know resume training problem unity unity time whole system time must restart sometimes enough restart unity public class agent private float health private bool private float reference public private void start public override void new vector health bullet bullet bullet public material public material public public override void sensor last rotation stopped moving quaternion public float public override void input float float player new vector player vector movement new vector quaternion movement vector rotation long agent moving current rotation saved test agent moving last saved used fixed bug stop penalty every step huge penalty reach target float float shooting shooting bullet big reward target lower hit float amount agent take float private void agent wall border bullet health health health player dead,issue,negative,positive,neutral,neutral,positive,positive
786318104,You can also resume training from where you left off by passing `--resume` to `mlagents-learn`,also resume training left passing resume,issue,negative,neutral,neutral,neutral,neutral,neutral
786237239,"Hi @preuschhi,
Without any indication of what your code is doing, it's really hard to figure out why this is happening.  Can you share your code with us to help us diagnose this?",hi without indication code really hard figure happening share code u help u diagnose,issue,negative,negative,negative,negative,negative,negative
786223594,The PR was merged to master but after the most recent release was cut and so it will be in the March release.  You should be able to try it on master though.,master recent release cut march release able try master though,issue,negative,positive,positive,positive,positive,positive
786190219,"Thanks, this will be helpful. But I'm not exactly sure, where would I call Academy.Instance.EnvironmentStep()? Do I need a separate script that keeps track of everything, including the FixedUpdates etc?",thanks helpful exactly sure would call need separate script track everything,issue,positive,positive,positive,positive,positive,positive
786155372,"Need to add the branch target for yamato triggers and github actions.

There are also some links in the pull request templates.

Try `git grep -i master` and see if anything else looks relevant",need add branch target also link pull request try git master see anything else relevant,issue,negative,positive,positive,positive,positive,positive
786021692,"Continuing this thread to avoid spam, because I see the PR has been merged. 

So I'm not sure if I'm missing something obvious, but it seems that in the newest release, the BufferSensorComponent is internal, which seems to mean that I can't actually access it from my script? I'll probably just copy-paste the implementation or something for now, but you might want to change it to public like the other components.",thread avoid see sure missing something obvious release internal mean ca actually access script probably implementation something might want change public like,issue,negative,negative,neutral,neutral,negative,negative
785515983,"The problem is a bit more obvious when the yaml is formatted mono-spaced. The `curiosity` section should be a key in the `reward_signals` dictionary, so you basically need to add 2 spaces in front of the last 5 lines of your file:
```
%YAML 1.2
--- 
behaviors: 
  EnemyMovement: 
    trainer_type: ppo
    hyperparameters: 
      batch_size: 2048
      buffer_size: 20480
      learning_rate: 0.0003
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings: 
      normalize: false
      hidden_units: 256
      num_layers: 2
      vis_encode_type: simple
    keep_checkpoints: 5
    checkpoint_interval: 500000
    max_steps: 50000000
    time_horizon: 128
    summary_freq: 10000
    threaded: true
    reward_signals: 
      extrinsic: 
        gamma: 0.99
        strength: 1.0
      curiosity: 
        strength: 0.02
        gamma: 0.99
        encoding_size: 256
        learning_rate: ""3.0e-4""
```",problem bit obvious curiosity section key dictionary basically need add front last file beta epsilon linear normalize false simple threaded true extrinsic gamma strength curiosity strength gamma,issue,positive,negative,neutral,neutral,negative,negative
785513647,Note: I updated your post to use triple-backticks (```) for the log and yaml. Will reply in a minute.,note post use log reply minute,issue,negative,neutral,neutral,neutral,neutral,neutral
785482375,"Thanks for the reply.

For coordinating agents respawning and controlling episode at a group level, the upcoming new interface `MultiAgentGroup` (not yet released) will be very helpful for that. It provides you a way to register/manage a collection of Agents to the group and handle episodes at the group level. Stay tuned and welcome to try it out and give us feedback once it's released!

The statistics collecting is a bit more tricky. Like you said doing it in either CollectObservations or OnActionReceived is kind of problematic. By default, the Academy steps during every FixedUpdate, but you can disable automatic stepping and manually call Academy.Instance.EnvironmentStep(). So the current best way to achieve what you're saying would be: disable the automatic Academy stepping and manually call Academy.Instance.EnvironmentStep(), collect the statistics, and then call the next EnvironmentStep(). In that case you can make sure the statistics is collected _after_ all agents executed the actions.

As to your summaries of the using of LateUpdate, if your scene and actions are very physics-based, you probably want the Agent stepping at a regular interval w.r.t. to the physics. The effect of that depends on your scene and it's probably ok in some cases. 

I see that the execution order might not be obvious to tell from the docs. I'll work on improving that.
",thanks reply episode group level upcoming new interface yet helpful way collection group handle group level stay tuned welcome try give u feedback statistic bit tricky like said either kind problematic default academy every disable automatic stepping manually call current best way achieve saying would disable automatic academy stepping manually call collect statistic call next case make sure statistic collected executed scene probably want agent stepping regular interval physic effect scene probably see execution order might obvious tell work improving,issue,positive,positive,positive,positive,positive,positive
785451496,"So my original use case for a Manager agent was coordinating agents respawning after an episode ends. Each agent has some sort of objective of its own, but only after they're all achieved, the episode can end for everyone. This is no longer the case in my code, but I might very well come back to this at some point.

The other is collecting statistics - I want to log, at each decision step, the mean distance of an agent from its destination, whether or not the agent is in a collision right now, stuff like that. For consistency, it has to be either before any agents act and perhaps change their (internal) state, or after all of them do. Currently I'm still fighting with Unity to make this work consistently in CollectObservations, without any double measurements.

Why consistency is important - at the moment, each agent has a Collision flag, which is set to 0 during OnActionReceived, and to 1 whenever a collision happens. If I log statistics during OnActionReceived of the Manager, some agents may be prematurely ""cleared"". If I log statistics during CollectObservations, there's a double message when the episode ends (terminal step) and a new one begins (decision step). I'm still figuring out how to get around that.



FWIW, I'd extremely appreciate some more detailed description of the execution order, something like this but for ML-Agents https://docs.unity3d.com/Manual/ExecutionOrder.html

So far through reading obscure docs and experimentation I established the following (which I hope is correct):
1. Everything happens around FixedUpdates; therefore, using Update and LateUpdate will cause big troubles
2. First all agents collect observations, then all agents choose actions. Then I think the physics engine makes a step? And then collisions etc. are processed.
3. The order in which agents collect observations etc. is invariant, but arbitrary.",original use case manager agent episode agent sort objective episode end everyone longer case code might well come back point statistic want log decision step mean distance agent destination whether agent collision right stuff like consistency either act perhaps change internal state currently still fighting unity make work consistently without double consistency important moment agent collision flag set whenever collision log statistic manager may prematurely log statistic double message episode terminal step new one decision step still get around extremely appreciate detailed description execution order something like far reading obscure experimentation established following hope correct everything around therefore update cause big first collect choose think physic engine step order collect invariant arbitrary,issue,positive,positive,neutral,neutral,positive,positive
785344327,"Hi,

We are currently working on more features around multi-agent training and some of them will hopefully come in the next few releases. That includes a new interface that allows you to put agents into a group and enable collaborative behaviors. This should allow you to make sure the episodes are synchronized and do the management in a better way without creating a ""fake agent"".  

However, we have not yet supported synchronizing agent's action within a step but it does sound to be useful. Can you share a bit more about your use case and how does that help with your problem? Like is there any other use besides collecting statistics? How taking actions in specific order help with your case or you just need something happen before/after all agents have acted? We're actively working on a better support for multi-agent training and we'll put this in our road map. 

Internally tracked as [MLA-1797](https://jira.unity3d.com/browse/MLA-1797).",hi currently working around training hopefully come next new interface put group enable collaborative allow make sure synchronized management better way without fake agent however yet agent action within step sound useful share bit use case help problem like use besides statistic taking specific order help case need something happen actively working better support training put road map internally tracked,issue,positive,positive,positive,positive,positive,positive
784671287,"Reacher is referenced in the general docs (https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Design-Agents.md#continuous-actions) - not saying we shouldn't remove it, just need to make sure you update that too.",reacher general saying remove need make sure update,issue,negative,positive,positive,positive,positive,positive
784476029,"I found it very exhausting to write a proper, simple yet modular ReplayBuffer for this Python API (coming from a gym background). I don't want to handle it with the amounts of dicts the Colab 02 Example did it(and a lot of coupling). 
And in the case `decision_steps: 12 terminal_steps: 1` I don't know whether the Terminal Step happened before or after the Decision Step for the agent. Logically it makes only sense that the terminal step has happened before the decision step, but do I truly know this? Did I mention that in the previously mentioned case, I get 1 Terminal Step and 1 Decision Step for the same agent_id.

Maybe I have some thinking mistakes here, but I found it a lot harder to work with Unity ml_agents, than the general gym API. Even the API from your old 0.4.0 (with Brains) was way easier for me. But maybe I'm missing some part of this :) 

Thank you so much for helping! I really appreciate this!",found exhausting write proper simple yet modular python coming gym background want handle example lot coupling case know whether terminal step decision step agent logically sense terminal step decision step truly know mention previously case get terminal step decision step maybe thinking found lot harder work unity general gym even old brain way easier maybe missing part thank much helping really appreciate,issue,positive,negative,neutral,neutral,negative,negative
784436972,"Thanks for raising this. The reason for this is that agents can terminate in between decision intervals (set in the `DecisionRequester` component) but receive decision steps on the decision interval. I do not believe it is a bug but perhaps we can clean this up a bit so that the behavior is more predictable.

Would you mind elaborating on what this is complicating for you?",thanks raising reason terminate decision set component receive decision decision interval believe bug perhaps clean bit behavior predictable would mind,issue,positive,positive,positive,positive,positive,positive
784430200,"I think the biggest change is using this bound for the initialization of the network
```
bound = math.sqrt(1 / (layer_size * self.input_size))
```",think biggest change bound network bound,issue,negative,neutral,neutral,neutral,neutral,neutral
784428138,"> Is LayerNorm() from the paper also, or just something you've found that works well?

Not from the paper, but the paper assumed that the activations of the intermediate layers were mean 0 and had a defined variance (at least for initialization). So I put a LayerNorm in there to make sure that was the case.",paper also something found work well paper paper assumed intermediate mean defined variance least put make sure case,issue,positive,negative,neutral,neutral,negative,negative
784415823,"Is `LayerNorm()` from the paper also, or just something you've found that works well?",paper also something found work well,issue,negative,neutral,neutral,neutral,neutral,neutral
784361469,"Ah, ok, that's easy enough to compensate. Thank you.

For reference, it also shows up here: https://github.com/Unity-Technologies/ml-agents/blob/b136878d4327c6bba8146f50ccb10862cad22570/ml-agents/mlagents/trainers/torch/agent_action.py#L36

Feel free to close this issue if it's desired behaviour or leave it open if you want to investigate. 
I for my part have the information I need.",ah easy enough compensate thank reference also feel free close issue desired behaviour leave open want investigate part information need,issue,positive,positive,positive,positive,positive,positive
783722931,"I **think** this is due to rescaling of the actions in the trainer.
When exporting the action, we actually clip between -3 and 3 and then divide by 3. (3 is very arbitrary)
I think this explains what you are seeing, with the actions having the same distribution but rescaled.
See here : https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/torch/action_model.py#L168",think due trainer action actually clip divide arbitrary think seeing distribution see,issue,negative,negative,neutral,neutral,negative,negative
783700720,"Brilliant, all set then :). Thank you very much for your time!",brilliant set thank much time,issue,positive,positive,positive,positive,positive,positive
783700146,"Fantastic, this is great. Thank you very much!",fantastic great thank much,issue,positive,positive,positive,positive,positive,positive
783699562,"Hi, 

I do not think we will be able to address this issue in the near future (This is what I meant on the closed PR). 
The issue of ever-rising entropy is real, but there is no easy solution to it.

The TanhGaussian distribution should **NOT** be using the underlying Gaussian distribution as entropy because they are not the same. (Our trainers currently do not use the entropy of the tanh gaussian distribution anywhere, so we did not implement the entropy method) : 
![Entropy as funtion of mu and sigma](https://user-images.githubusercontent.com/28320361/108773215-dea2bb80-7512-11eb-8e5f-75c8146bd227.png)


If you want to try to approximate the entropy of the tanh distribution, I **think** this is the exact equation : 
<img width=""1071"" alt=""Screen Shot 2021-02-22 at 13 57 47"" src=""https://user-images.githubusercontent.com/28320361/108775501-fcbdeb00-7515-11eb-9f63-db7c3cd1a64f.png"">



",hi think able address issue near future meant closed issue entropy real easy solution distribution underlying distribution entropy currently use entropy tanh distribution anywhere implement entropy method entropy mu sigma want try approximate entropy tanh distribution think exact equation screen shot,issue,positive,positive,positive,positive,positive,positive
783670810,"Thank you for having taken the time to investigate this. Not sure if your final sentence (""Therefore I will close this now and add this issue on our backlog"") refers just to that PR or the general issue of ever-rising entropies with sparse rewards.

In the (now closed) PR you argue that exact entropy-calculation for the tanh-distribution is not worth it. I agree - if the alternative is having some sort of approximation. tanh-squashing sounds promising enough to me that I would give it a shot.
But as far as I can tell the TanhGaussianDistInstance is just relying on the parent-class (GaussianDistInstance) for entropy calculation. Which brings me back to ever-rising entropies. Is that correct or am I missing something?

unmodified entropy-code of GaussianDist / TanhGaussianDist for reference:
```
class GaussianDistInstance(DistInstance):
    def __init__(self, mean, std):
        super().__init__()
        self.mean = mean
        self.std = std

    def sample(self):
        sample = self.mean + torch.randn_like(self.mean) * self.std
        return sample

    def log_prob(self, value):
        var = self.std ** 2
        log_scale = torch.log(self.std + EPSILON)
        return (
            -((value - self.mean) ** 2) / (2 * var + EPSILON)
            - log_scale
            - math.log(math.sqrt(2 * math.pi))
        )

    def pdf(self, value):
        log_prob = self.log_prob(value)
        return torch.exp(log_prob)

    def entropy(self):
        return torch.mean(
            0.5 * torch.log(2 * math.pi * math.e * self.std ** 2 + EPSILON),
            dim=1,
            keepdim=True,
        )  # Use equivalent behavior to TF

    def exported_model_output(self):
        return self.sample()


class TanhGaussianDistInstance(GaussianDistInstance):
    def __init__(self, mean, std):
        super().__init__(mean, std)
        self.transform = torch.distributions.transforms.TanhTransform(cache_size=1)

    def sample(self):
        unsquashed_sample = super().sample()
        squashed = self.transform(unsquashed_sample)
        return squashed

    def _inverse_tanh(self, value):
        capped_value = torch.clamp(value, -1 + EPSILON, 1 - EPSILON)
        return 0.5 * torch.log((1 + capped_value) / (1 - capped_value) + EPSILON)

    def log_prob(self, value):
        unsquashed = self.transform.inv(value)
        return super().log_prob(unsquashed) - self.transform.log_abs_det_jacobian(
            unsquashed, value
        )
```",thank taken time investigate sure final sentence therefore close add issue backlog general issue sparse closed argue exact worth agree alternative sort approximation promising enough would give shot far tell entropy calculation back correct missing something unmodified reference class self mean super mean sample self sample return sample self value epsilon return value epsilon self value value return entropy self return epsilon use equivalent behavior self return class self mean super mean sample self super return self value value epsilon epsilon return epsilon self value unsquashed value return super unsquashed unsquashed value,issue,positive,positive,neutral,neutral,positive,positive
783626353,Would you mind sharing the rest of the values in your behavior parameters script? E.g. the discrete action sizes/branches?,would mind rest behavior script discrete action,issue,negative,positive,neutral,neutral,positive,positive
783622231,"Actually, this is what we do in our Bouncer environment wherein we have _removed_ the decision requester component and call [RequestDecision manually.](https://github.com/Unity-Technologies/ml-agents/blob/master/Project/Assets/ML-Agents/Examples/Bouncer/Scripts/BouncerAgent.cs#L79) We call this 'On Demand decisions'. I am sorry that this isn't well documented. I will fix that.

The [relevant documentation](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Design-Agents.md#decisions)",actually bouncer environment wherein decision requester component call manually call demand sorry well fix relevant documentation,issue,negative,negative,neutral,neutral,negative,negative
783619085,"Thanks for the pull request! Linking to the Gridworld example is a great idea.

I will look through your code snippet.",thanks pull request linking example great idea look code snippet,issue,positive,positive,positive,positive,positive,positive
783615734,"i did not change any Python code.  I looket at this piece of the Error Output =>RuntimeError: cannot sample n_sample > prob_dist.size(-1) samples without replacement<= and increased the continuous actions size by ""1"" and then eveything was okey. My  intuition for increasing the continuous action size was rather try and test because I am still very new to this package.

",change python code piece error output sample without replacement continuous size intuition increasing continuous action size rather try test still new package,issue,negative,positive,positive,positive,positive,positive
783564881,"Here is a plot of the Entropy for the Gaussian and for the TanhGaussian 

![Entropy as funtion of mu and sigma](https://user-images.githubusercontent.com/28320361/108749973-53ff9380-74f5-11eb-9525-76037ce7666d.png)

The two are different, the Gaussian distribution's entropy is maximum when the standard deviation is infinite and is independent of the mean. On the other hand, the entropy of the TanhGaussian is maximum for a mean 0 and a finite standard deviation (which is to be expected)

The exact equation or even an approximating the entropy of the TanhGaussian is rather hard to compute and there does not seem to be a lot of benefits in being exact in the entropy calculation (a lot of RL algorithms use clipped gaussian which also has a different entropy than a regular gaussian). Therefore I will close this now and add this issue on our backlog.
",plot entropy entropy mu sigma two different distribution entropy maximum standard deviation infinite independent mean hand entropy maximum mean finite standard deviation exact equation even entropy rather hard compute seem lot exact entropy calculation lot use clipped also different entropy regular therefore close add issue backlog,issue,negative,negative,neutral,neutral,negative,negative
783469724,"Hi @preuschhi 

Did you modify any python code?  It's a bit concerning that the fix was to increase the continuous action size since it looks like the error occurred with the discrete distribution. Can you explain your intuition for increasing the continuous action size?",hi modify python code bit concerning fix increase continuous action size since like error discrete distribution explain intuition increasing continuous action size,issue,negative,positive,neutral,neutral,positive,positive
783427449,"Hi @yiorgosl 

Epsilon is controlled by the same schedule as the learning rate. So, if you set the `learning_rate_schedule` to constant, epsilon will not decay. If you need further fine grained control, you'll have to modify code in the ppo optimizer. 

Can you share your use case for this? Just for completeness, epsilon controls the size of the PPO trust region. Epsilon of 0.6 is a bit large and may lead to some instability.",hi epsilon schedule learning rate set constant epsilon decay need fine grained control modify code share use case completeness epsilon size trust region epsilon bit large may lead instability,issue,positive,positive,positive,positive,positive,positive
782849684,So I did some research and went through some code trying to understand what's going on under the hood. So to achieve what I described would it be a good idea to disable the `Fixed Update` in the `Academy.cs` internal class `AcademyFixedUpdateStepper` and use the `Academy.Instance.EnvironmentStep();` in my agent class after I manually requested a decision using `RequestDecision();` or will this be problematic on functionality I haven't yet considered in the `Academy.cs` code?,research went code trying understand going hood achieve would good idea disable fixed update internal class use agent class manually decision problematic functionality yet considered code,issue,negative,positive,positive,positive,positive,positive
782708436,"Ah okay, that's good to know. So the cumulated number of ""fixed update calls"" are the ones that the agent also compares against the Max Step count to decide wether to end the episode? In the example code, the agent basically moves on a grid, but with a smooth motion, so while the agent is moving, I don't want it to take any observations or try to request any actions. How would something like this be done? I understand that this might be quiet specific, but at the same time, this might be also helpful for others to understand and gain full control over when actions and observations are exactly happening and not flooding the ""brain"" with requests and observations in situations not needed. At this point obviously being ignorant here, in the context of my understanding of ML-Agents, this feels a bit like a black box and I really would like to crack it open :). This might be also interesting from a performance perspective. I hope what I'm trying to describe makes any sense. Thanks a lot in advance!",ah good know number fixed update agent also step count decide wether end episode example code agent basically grid smooth motion agent moving want take try request would something like done understand might quiet specific time might also helpful understand gain full control exactly happening flooding brain point obviously ignorant context understanding bit like black box really would like crack open might also interesting performance perspective hope trying describe sense thanks lot advance,issue,positive,positive,positive,positive,positive,positive
782706917,"Hi @andrewcoh, thank you so much for getting back to this! In my example should have included full code. In my code, a couple of lines down, I have implemented as you said already. But it felt quite tedious and sort of home-cooked.

Oh great, I didn't know it is implemented in the grid world example. I think adding the example link to the documentation would make a lot of sense! Thank you so much. I'll submit a pull request with a potential change, hoping to make your life easier as well.

In case this is useful, my full sandbox code trying to get my head around action masking:

```css
using System.Collections;
using UnityEngine;
using Unity.MLAgents;
using Unity.MLAgents.Sensors;
using Unity.MLAgents.Actuators;

public class MyAgent : Agent
{
    public Transform target;
    int branch;
    int[] actionIndices;

    Unity.MLAgents.Policies.BehaviorType behaviorType;
    //Default = 0
    //HeuristicOnly = 1
    //InferenceOnly = 2

    public override void Initialize()
    {
        
    }

    // Start is called before the first frame update
    void Start()
    {
        // cache policy of agent
        this.behaviorType = GetComponent<Unity.MLAgents.Policies.BehaviorParameters>().BehaviorType;

        this.branch = 0;
        this.actionIndices = new int[0] {};
    }

    private void Update()
    {
        if (Input.anyKeyDown && this.behaviorType == Unity.MLAgents.Policies.BehaviorType.HeuristicOnly)
        {
            // if heuristic react to key presses
            Debug.Log(""some button is pressed"");
            RequestDecision();
        }
    }

    public override void Heuristic(in ActionBuffers actionsOut)
    {
        Debug.Log(""Call Heuristic"");

        ActionSegment<int> discreteActionsOut = actionsOut.DiscreteActions;

        if(Input.GetKey(KeyCode.W))
        {
            discreteActionsOut[0] = 0;
        }
        else if(Input.GetKey(KeyCode.A))
        {
            discreteActionsOut[0] = 1;
        }
        else if (Input.GetKey(KeyCode.S))
        {
            discreteActionsOut[0] = 2;
        }
        else if (Input.GetKey(KeyCode.D))
        {
            discreteActionsOut[0] = 3;
        }
        else
        {
            discreteActionsOut[0] = 4;
        }
    }

    public override void OnActionReceived(ActionBuffers actions)
    {
        Debug.Log(""Call OnActionReceived"");
        Debug.Log(""Action number: "" + this.StepCount);

        int movement = actions.DiscreteActions[0];
        Debug.Log(""Action: "" + movement);

        if (movement == 0)
        {
            StartCoroutine(SmoothMove(Vector3.forward,1f, 1f, 0.1f));
        }
        else if(movement == 1)
        {
            StartCoroutine(SmoothMove(Vector3.left, 1f, 1f, 0.1f));
        }
        else if (movement == 2)
        {
            StartCoroutine(SmoothMove(Vector3.back, 1f, 1f, 0.1f));
        }
        else if (movement == 3)
        {
            StartCoroutine(SmoothMove(Vector3.right, 1f, 1f, 0.1f));
        }

        // Reached target?
        float distanceToTarget = Vector3.Distance(this.transform.localPosition, this.target.localPosition);
        if (distanceToTarget < 0.01f)
        {
            // if yes, reward agent and reset environment
            SetReward(1.0f);
            EndEpisode();
        }

        // Check if agent fell off platform
        if (this.transform.localPosition.y < 0)
        {
            // if agent fell, reset environment
            EndEpisode();
        }
    }

    public override void CollectObservations(VectorSensor sensor)
    {
        Debug.Log(""Call CollectObservations"");

        // Target and Agent positions
        sensor.AddObservation(this.target.localPosition);
        sensor.AddObservation(this.transform.localPosition);

        // if 
        this.actionIndices = new int[4] { 0, 1, 2, 3 }; // just allowed to do nothing
    }


    public override void OnEpisodeBegin()
    {
        Debug.Log(""Call OnEpisodeBegin"");

        // If the Agent fell of the plane, reset physics
        if (this.transform.localPosition.y < 0)
        {
            this.transform.localPosition = new Vector3(0, 0.5f, 0);
        }

        // Move target to new spot
        this.target.localPosition = new Vector3(Random.Range(-4.5f, 4.5f),
                                                0.5f,
                                                Random.Range(-4.5f, 4.5f));

        Debug.Log(""Call OnEpisodeBegin: RequestDecision"");
        RequestDecision();
    }

    IEnumerator SmoothMove(Vector3 dir, float distance, float totalTime, float timeStep)
    {
        // trigger Mask to not allow any movement
        int stepCount = (int)(totalTime / timeStep);
        float stepSize = distance / stepCount;

        this.actionIndices = new int[4] { 0,1,2,3 }; // just allowed to do nothing

        for (int i = 0; i < stepCount; i++)
        {
            this.transform.position += dir * stepSize;
            yield return new WaitForSeconds(timeStep);
        }

        this.actionIndices = new int[0] {}; // allow everything

        // if not heuristic request decision after coroutine has finished
        if (this.behaviorType != Unity.MLAgents.Policies.BehaviorType.HeuristicOnly)
        {
            Debug.Log(""Requesting decision after coroutine"");
            RequestDecision();
        }
    }

    
    public override void WriteDiscreteActionMask(IDiscreteActionMask actionMask)
    {
        /*
        string array = """";
        for(int i = 0; i < this.actionIndices.Length; i++)
        {
            array += this.actionIndices[i].ToString();
            if(i < this.actionIndices.Length - 1)
            {
                array += "","";
            }
        }

        Debug.Log(""writing mask: "" + array);
        */
        
        // branch index = 0, int[] { 0, 1, 2, 3 }
        actionMask.WriteMask(this.branch, this.actionIndices);
    }
}

```",hi thank much getting back example included full code code couple said already felt quite tedious sort oh great know grid world example think example link documentation would make lot sense thank much submit pull request potential change make life easier well case useful full sandbox code trying get head around action public class agent public transform target branch public override void initialize start first frame update void start cache policy agent new private void update heuristic react key button public override void heuristic call heuristic else else else else public override void call action number movement action movement movement else movement else movement else movement target float yes reward agent reset environment check agent fell platform agent fell reset environment public override void sensor call target agent new nothing public override void call agent fell plane reset physic new vector move target new spot new vector call vector float distance float float trigger mask allow movement float distance new nothing yield return new new allow everything heuristic request decision finished decision public override void string array array array writing mask array branch index,issue,positive,positive,positive,positive,positive,positive
782706821,Adding a hint to an example project using discrete action masks,hint example project discrete action,issue,negative,positive,neutral,neutral,positive,positive
782691030,"Hi @philippds 

I see why this is confusing. The step counter on the agent is counting fixed update calls which is not necessarily 1 to 1 with actions. An action will be requested every `n` fixed updates where n corresponds to the `decision interval` specified in your decision requester component.

Let me know if I can clarify anything.",hi see step counter agent counting fixed update necessarily action every fixed decision interval decision requester component let know clarify anything,issue,negative,positive,positive,positive,positive,positive
782689215,"Hi @philippds 

I'm sorry that you are finding our documentation confusing. In order to implement masking, we recommend you override the `Agent.WriteDiscreteActionMask()` method.

I just want to be sure you're referring to this [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Design-Agents.md#masking-discrete-actions). Additionally, our [Gridworld ](https://github.com/Unity-Technologies/ml-agents/blob/master/Project/Assets/ML-Agents/Examples/GridWorld/Scripts/GridAgent.cs#L39) example environment uses action masking. Would it be helpful to link to this example too? ",hi sorry finding documentation order implement recommend override method want sure additionally example environment action would helpful link example,issue,positive,positive,neutral,neutral,positive,positive
782394844,"It seems using the truncated gaussian distribution is not good enough. We need a clipped Gaussian distribution entropy, but I cannot find an easy way to implement its calculation. Same goes with TanhGaussian, the entropy calculation has not been done and probably does not have a non-integral solution.",truncated distribution good enough need clipped distribution entropy find easy way implement calculation go entropy calculation done probably solution,issue,positive,positive,positive,positive,positive,positive
782313915,"cc @hvpeteet, @sini - this will introduce a (backwards-compatible) change for the YAMLs - just checking to make sure it won't cause any issues. ",introduce change make sure wo cause,issue,negative,positive,positive,positive,positive,positive
782220114,"Maybe add a small readme that this is just a trimmed-down version of the ""main"" PushBlock example that uses the input system? And link to it from the docs in the extensions package?",maybe add small version main example input system link package,issue,negative,negative,neutral,neutral,negative,negative
782219499,"I didn't look over all the files, but quick feedback based on the list:

We probably don't need these - maybe ProjectSettingsOverrides.cs
* ML-Agents-Input-Example/Assets/ML-Agents/Editor/Tests/StandaloneBuildTest.cs (73)
* ML-Agents-Input-Example/Assets/ML-Agents/Examples/SharedAssets/Scripts/ModelOverrider.cs (322)
* ML-Agents-Input-Example/Assets/ML-Agents/Examples/SharedAssets/Scripts/ProjectSettingsOverrides.cs (69)

Did you re-record this? If not, probably OK to remove.
* ML-Agents-Input-Example/Assets/ML-Agents/Examples/PushBlock/Demos/ExpertPush.demo (0)

Don't think we need anything for visual here.
* ML-Agents-Input-Example/Assets/ML-Agents/Examples/PushBlock/Prefabs/PushBlockVisualArea.prefab (1397)
* ML-Agents-Input-Example/Assets/ML-Agents/Examples/PushBlock/Scenes/VisualPushBlock.unity (983)


Pick one :)
* ML-Agents-Input-Example/Assets/ML-Agents/Examples/PushBlock/TFModels/PushBlock.nn (0)
* ML-Agents-Input-Example/Assets/ML-Agents/Examples/PushBlock/TFModels/PushBlock.onnx (0)

In general, anything we can prune down in here? I don't think the demo needs to look as pretty as the ""main"" example.
* ML-Agents-Input-Example/Assets/ML-Agents/Examples/SharedAssets.meta (8)

Don't think we need these
* ML-Agents-Input-Example/Assets/ML-Agents/Examples/SharedAssets/Meshes/symbol_circle.fbx (0)
* ML-Agents-Input-Example/Assets/ML-Agents/Examples/SharedAssets/Meshes/symbol_star.fbx (0)
* ML-Agents-Input-Example/Assets/ML-Agents/Examples/SharedAssets/Meshes/symbol_tri.fbx (0)
* ML-Agents-Input-Example/Assets/ML-Agents/Examples/SharedAssets/Meshes/symbol_x.fbx (0)

",look quick feedback based list probably need maybe probably remove think need anything visual pick one general anything prune think need look pretty main example think need,issue,negative,positive,positive,positive,positive,positive
781844382,"Failing test (before the compile error was fixed): https://yamato.cds.internal.unity3d.com/jobs/497-ml-agents/tree/yamato-mobile-compile/.yamato%252Fstandalone-build-webgl-test.yml%2523test_webgl_standalone_2020.2/5387793
Passing test (after it was fixed in another PR): https://yamato.cds.internal.unity3d.com/jobs/497-ml-agents/tree/yamato-mobile-compile/.yamato%252Fstandalone-build-webgl-test.yml%2523test_webgl_standalone_2020.2/5415416",failing test compile error fixed passing test fixed another,issue,negative,positive,neutral,neutral,positive,positive
781766241,"Manual tested that I can do a WebGL build locally.

I have a PR to start doing a WebGL build in yamato to guard against similar compiler errors in the future: https://github.com/Unity-Technologies/ml-agents/pull/4966 (currently failing because it lacks this fix)",manual tested build locally start build guard similar compiler future currently failing fix,issue,negative,neutral,neutral,neutral,neutral,neutral
780693554,"Thank you, I was able to reproduce the issue. We are investigating this but like you said, the computation of entropy for clipped gaussian or tanhgaussian might be very hard to compute. This should not happen if there are enough rewards in the environment. An option could be clipping the standard deviation of the gaussian to avoid it to spread out too much, but this would probably cause other issues.",thank able reproduce issue investigating like said computation entropy clipped might hard compute happen enough environment option could clipping standard deviation avoid spread much would probably cause,issue,negative,positive,neutral,neutral,positive,positive
780646566,"We could

On Tue, Feb 16, 2021 at 15:54 Chris Elion <notifications@github.com> wrote:

> Should we cherry-pick this to the release branch too? cc @surfnerd
> <https://github.com/surfnerd>
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/pull/4949#issuecomment-780194234>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAKLGSENS6K5XT56W37YWW3S7MAUDANCNFSM4XXFZ4HQ>
> .
>
-- 
*Chris Goy*
Senior Software Developer | ML Agents
San Francisco
<https://unity3d.com/?utm_source=unity3d&utm_medium=email&utm_campaign=company-information-2016-Global-Unity-Email-Sig>
",could tue wrote release branch reply directly view goy senior developer san,issue,negative,positive,neutral,neutral,positive,positive
780544058,I fixed it by increasing continuous actions size,fixed increasing continuous size,issue,negative,positive,neutral,neutral,positive,positive
780244055,"Training event example:
```json5
{
    ""TrainingSessionGuid"": ""d04b7dd1-7ea5-4d55-8687-aced0e523b92"",
    ""BehaviorName"": ""3f738d62904cac5f135005b006651ad6"",
    ""ObservationSpecs"": [
        {
            ""SensorName"": ""Match3 Sensor"",
            ""CompressionType"": ""PNG"",
            ""BuiltInSensorType"": 9,
            ""DimensionInfos"": [
                {
                    ""Size"": 9,
                    ""Flags"": 0
                },
                {
                    ""Size"": 8,
                    ""Flags"": 0
                },
                {
                    ""Size"": 9,
                    ""Flags"": 0
                }
            ]
        }
    ],
    ""ActionSpec"": {
        ""NumContinuousActions"": 0,
        ""NumDiscreteActions"": 1,
        ""BranchSizes"": [
            127
        ]
    },
// --begin new field--
    ""ActuatorInfos"": [
        {
            ""BuiltInActuatorType"": 3,
            ""NumContinuousActions"": 0,
            ""NumDiscreteActions"": 1
        },
        {
            ""BuiltInActuatorType"": 1,
            ""NumContinuousActions"": 0,
            ""NumDiscreteActions"": 0
        }
    ],
// --end new field--
    ""MLAgentsEnvsVersion"": ""0.24.0.dev0"",
    ""TrainerCommunicationVersion"": ""1.4.0""
}
```",training event example match sensor size size size begin new field end new field dev,issue,negative,positive,positive,positive,positive,positive
780116041,"Just to try to give more info, I set up an example environment that does close to nothing.

In every decision it gives a single observation. It's always 0.
There is no reward.

For every 10000 steps it collects all received actions, bins them into a histogram and writes the resulting data to a csv-file. 

I like to think of this environment of a scenario with a very sparse reward.
And I'd expect the algorithm to try to maximize entropy as to explore more of the action-space. 

However, even after a very low amout of steps (100000 steps is enough), the agent will only ever try -1 and 1.
![grafik](https://user-images.githubusercontent.com/17814024/108120067-fd332d80-70a0-11eb-929a-158eeabe3eb7.png)


```
using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using Unity.MLAgents;
using Unity.MLAgents.Sensors;
using Unity.MLAgents.Actuators;

public class EntropyTestAgent : Agent
{


    int[] sampledActions = new int[401]; //actions will binned from -2 to 2 in increments of 0.01

    long myStepCount;
    

    // Start is called before the first frame update
    void Start()
    {
        myStepCount = 0;
        clearEntries();
    }


    void clearEntries()
    {
        //set all items to 0
        System.Array.Clear(sampledActions, 0, sampledActions.Length);
    }



    void saveSampledActionsToDisk()
    {
        //first best code I could find to write csv-files
               
        Debug.Log(""writing to file "" + Application.persistentDataPath + ""\\sampledActions_"" + myStepCount.ToString() + "".csv"");

        System.IO.StreamWriter writer = new System.IO.StreamWriter(Application.persistentDataPath + ""\\sampledActions_"" + myStepCount.ToString() + "".csv"");

        for (int i = 0; i < sampledActions.Length; i++)
        {

            writer.WriteLine(sampledActions[i]);
        }

        writer.Flush();
        writer.Close();

    }

    public override void CollectObservations(VectorSensor sensor)
    {
        sensor.AddObservation(0); //observation will never change
    }

    public override void OnActionReceived(ActionBuffers actions)
    {
        myStepCount++;
       

        //add action to sampledActions-array

        //get action
        float action;
        action = actions.ContinuousActions[0];

        //find index
        int indexToIncrease;
        indexToIncrease = (int)((action + 2)/0.01f);

        sampledActions[indexToIncrease]++;

        if ((myStepCount % 1000)==0)
        {

            EndEpisode();

            Debug.Log(base.CompletedEpisodes);

        }


        if ((myStepCount % 10000)==0)
        {
            saveSampledActionsToDisk();

            //clear all entries to get ready for next 10k actions
            clearEntries();
        }
    }

}
```



This is my configuration, which I took directly from the [Making a new Environment Page](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Create-New.md#training-the-environment)

```
behaviors:
  EntropyTest:
    trainer_type: ppo
    hyperparameters:
      batch_size: 10
      buffer_size: 100
      learning_rate: 3.0e-4
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000
```

this is how I call it: 
`mlagents-learn config\EntropyTest_config.yaml --run-id=EntropyTest_006 --time-scale=100`

this is the entropy-output in tensorboard (shows 240k steps):
![grafik](https://user-images.githubusercontent.com/17814024/108120824-2acca680-70a2-11eb-9414-4fee83ec4a6e.png)

",try give set example environment close nothing every decision single observation always reward every received histogram resulting data like think environment scenario sparse reward expect algorithm try maximize entropy explore however even low enough agent ever try public class agent new long start first frame update void start void void best code could find write writing file writer new public override void sensor never change public override void action action float action action index action get ready next configuration took directly making new environment page beta epsilon linear normalize false extrinsic gamma strength call,issue,positive,positive,neutral,neutral,positive,positive
780043221,"Hello @awjuliani! 
Sorry for my English. I have this problems too (Only with SAC trainer_type. If I restart learning, session will work during 5-7k steps)

**Behavior Parameters:**
![image](https://user-images.githubusercontent.com/61820944/108106412-f4d9f300-709e-11eb-8c53-0778f8e51d78.png)

**GridSensor:**
![image](https://user-images.githubusercontent.com/61820944/108106491-1044fe00-709f-11eb-8b37-f48bd83f0144.png)

**Tensoboard enviroment**
![image](https://user-images.githubusercontent.com/61820944/108107483-5c447280-70a0-11eb-8887-0ddc44070031.png)


**Tensoboard policy**
![image](https://user-images.githubusercontent.com/61820944/108107437-48007580-70a0-11eb-976c-a46e2839c82e.png)


**Trainer config**
![image](https://user-images.githubusercontent.com/61820944/108106883-95c8ae00-709f-11eb-9afb-61bbdfe56c29.png)



",hello sorry sac restart learning session work behavior image image image policy image trainer image,issue,negative,negative,negative,negative,negative,negative
780009199,"Hi, 

The images in the link looks fine to me.
Please submit issues using our issue template instead of a single link so we could help.

Closing this since there's no clear issue pointed out.",hi link fine please submit issue template instead single link could help since clear issue pointed,issue,positive,positive,positive,positive,positive,positive
778930738,The python versions should change to the next `.dev0` version too.,python change next version,issue,negative,neutral,neutral,neutral,neutral,neutral
778591843,"hi @unityjeffrey
I listened to your opinion and edited it as a new README file. Is everything all right?",hi opinion new file everything right,issue,negative,positive,positive,positive,positive,positive
777964983,"> Hi,
> 
> Can you provide the version (ML-Agent, OS, Unity) you're using?
> I wasn't able to reproduce the same performance drop on the latest branch.
> 
> Also please follow the issue template to submit an issue, thank you.

Thanks for the reply. I was using ML-Agents Release 7. Looks like the issue was fixed between then and now.",hi provide version o unity able reproduce performance drop latest branch also please follow issue template submit issue thank thanks reply release like issue fixed,issue,positive,positive,positive,positive,positive,positive
777949488,"Hi, 

Can you provide the version (ML-Agent, OS, Unity) you're using? 
I wasn't able to reproduce the same performance drop on the latest branch.

Also please follow the issue template to submit an issue, thank you.",hi provide version o unity able reproduce performance drop latest branch also please follow issue template submit issue thank,issue,negative,positive,positive,positive,positive,positive
777642593,"hi @xogur6889 - someone from the team did a review and made some suggested changes.  I've attached his review in this comment.  Can you make the edits and I will approve?

[README.md.zip](https://github.com/Unity-Technologies/ml-agents/files/5967098/README.md.zip)

Thank you for your contribution!

",hi someone team review made attached review comment make approve thank contribution,issue,negative,neutral,neutral,neutral,neutral,neutral
777053878,"Do you have an estimate of how long it could take until it's merged? I'm not in an extreme hurry, but that also depends on what's the expected timeline.

My particular use case, partially implemented in my code, is as follows: I create a new ISensor attached to an agent. The sensor at each step of the simulation finds all colliders within a certain radius (Physics.OverlapSphere), filters them to only get objects of a certain type, and then returns the positions of all those objects. At the moment padding/truncating the vector to a fixed size, ideally outputting a vector of size 2*n where n is the number of nearby interesting objects.",estimate long could take extreme hurry also particular use case partially code create new attached agent sensor step simulation within certain radius get certain type moment vector fixed size ideally vector size number nearby interesting,issue,positive,positive,positive,positive,positive,positive
776974148,"You may want to hide whitespace changes when reviewing this 
![image](https://user-images.githubusercontent.com/6877802/107564091-be7c0f80-6b96-11eb-927e-66c881214a84.png)
",may want hide image,issue,negative,neutral,neutral,neutral,neutral,neutral
776921499,"Hi @RedTachyon 

This feature has been written but unfortunately merging to master is blocked due to a dependency on a Barracuda version that is giving us problems with the export. The pull request is here https://github.com/Unity-Technologies/ml-agents/pull/4909.

If you can share some more details about what you'd like to do with this feature, I may be able to give you some advice on using this branch with the caveat that this is currently a 'use at your own risk' feature.",hi feature written unfortunately master blocked due dependency barracuda version giving u export pull request share like feature may able give advice branch caveat currently risk feature,issue,negative,negative,neutral,neutral,negative,negative
776805796,"> @unityjeffrey - we will ask local translator for some help.

I already told the person who worked on the translation(This is the ""local translator"" you're talking about, right?). But if you ask him again, I'll wait.",ask local translator help already told person worked translation local translator talking right ask wait,issue,negative,positive,neutral,neutral,positive,positive
776319137,"Hi @nolan-dev 

Thank you very much for raising this. Your solution of `pass` -> `continue` works if the learning team doesn't change but I do not believe it will in the case of swapping the learning team.

I think with a few more flags though we can address this properly and get the speed up you are reporting. I'll follow-up on this thread with a fix. ",hi thank much raising solution pas continue work learning team change believe case swapping learning team think though address properly get speed thread fix,issue,positive,positive,neutral,neutral,positive,positive
776245517,@xogur6889 - we will ask local translator for some help.,ask local translator help,issue,negative,neutral,neutral,neutral,neutral,neutral
776114009,I manually cancelled a subset of the jobs - we still get coverage on multiple editor versions and variants of the tests.,manually subset still get coverage multiple editor,issue,negative,neutral,neutral,neutral,neutral,neutral
775494845,"> Looks fine. Is this going to break the colab notebooks when we release the next version?

It will not ""break"" them since the version of ML-Agents used on the colab is fixed. But when we upgrade the colabs to use the next release version, I will need to modify them. JIRA for tracking: [MLA-1764](https://jira.unity3d.com/browse/MLA-1764)",fine going break release next version break since version used fixed upgrade use next release version need modify,issue,negative,positive,positive,positive,positive,positive
775482318,"Hi @pt2302 

I've just merged a fix for this issue into master. https://github.com/Unity-Technologies/ml-agents/pull/4921. It should make it into the next release. I will close the issue for now. Thanks again for bringing it to our attention. ",hi fix issue master make next release close issue thanks attention,issue,negative,positive,neutral,neutral,positive,positive
774371011,"@awjuliani, thanks for the response!

Yes, exactly. Currently, the (uncaught) exception triggers a `Unity.MLAgents.Academy.Dispose()` call, which then leads to `UnityEngine.Application.Internal_ApplicationQuit()`, interrupting the training process.",thanks response yes exactly currently uncaught exception call interrupting training process,issue,positive,positive,neutral,neutral,positive,positive
774351974,"Hi @pt2302 

Just to clarify, you'd like the code to also catch the `UnauthorizedAccessException` exception and log the same warning. ",hi clarify like code also catch exception log warning,issue,negative,neutral,neutral,neutral,neutral,neutral
774348310,Can you also share what your observation space is? As there may be some input there that the network is not able to handle gracefully.,also share observation space may input network able handle gracefully,issue,positive,positive,positive,positive,positive,positive
774348027,"Hi @netcob64 

Can you share your tensorboard output? I am curious about the entropy chart in particular.  ",hi share output curious entropy chart particular,issue,positive,positive,neutral,neutral,positive,positive
774306555,There was already an example in the docs of how 3DBall could be modified to use [Observable]. I updated it with the actual code from 3DBallHard,already example could use observable actual code,issue,negative,neutral,neutral,neutral,neutral,neutral
773583629,"Hi @fredleefarr,

Thanks for letting us know, and glad the temporary solution works for you.
Sorry that we haven't gotten to fix it yet but we'll try to prioritize it.",hi thanks u know glad temporary solution work sorry gotten fix yet try,issue,positive,positive,neutral,neutral,positive,positive
773314239,"Was also experiencing this issue which I reported in the forum https://forum.unity.com/threads/editor-ml-agents-enterplaymode-time.1051160/

Have applied the suggested change to RpcCommunicator.cs and play mode time is greatly reduced.",also issue forum applied change play mode time greatly reduced,issue,negative,positive,positive,positive,positive,positive
772864347,That is exactly right. The variance in the network is state-independent. ,exactly right variance network,issue,negative,positive,positive,positive,positive,positive
772810204,"Got it. So during test time this weight doesn't change right? Meaning variance is independent of state? Meaning it's not really a measure of uncertainty of the model, but rather just an exploration parameter?",got test time weight change right meaning variance independent state meaning really measure uncertainty model rather exploration parameter,issue,negative,positive,positive,positive,positive,positive
772790698,"Hi @edhyah 

This is a trainable weight vector which gets updated during the learning process. We don't reset it during curriculum learning, as this would likely result in catastrophic forgetting. Instead, we use an entropy regularization term which is constantly pushing the variance to be larger. In cases where the task distribution has changed, like in curriculum learning, this entropy bonus should naturally allow the variance to increase again to explore more again.",hi trainable weight vector learning process reset curriculum learning would likely result catastrophic forgetting instead use entropy regularization term constantly pushing variance task distribution like curriculum learning entropy bonus naturally allow variance increase explore,issue,positive,positive,neutral,neutral,positive,positive
772706423,"Hi @aurelien78 

Thanks for pointing this out. I agree that such a modified reward would likely make the agent movement smoother. Our initial goal with this environment was to provide as simple an example as possible, and as such did not add an energy penalty. If you would like to make a PR with this change, we can take a look at it. ",hi thanks pointing agree reward would likely make agent movement smoother initial goal environment provide simple example possible add energy penalty would like make change take look,issue,positive,positive,neutral,neutral,positive,positive
772704634,"Hi @Remi-Niel 

Because it is not a feature request or bug, I think you may find a better audience for your question on the official forums: https://forum.unity.com/forums/ml-agents.453//. 

That being said, depending on the specifics of your environment, you should find that having more cores on your machine will allow you to run more instances of the environment, and thus collect more observations per second. One potential bottleneck however may be rendering. Especially if you are using visual observations, there may be less than ideal scaling with the number of cores. That being said, concurrent environments are run in parallel on the python side, so there are no inherent bottlenecks to the kind of scaling you are interested in. ",hi feature request bug think may find better audience question official said depending environment find machine allow run environment thus collect per second one potential bottleneck however may rendering especially visual may le ideal scaling number said concurrent run parallel python side inherent kind scaling interested,issue,positive,positive,positive,positive,positive,positive
771926825,"It's the log of the variance that is initialized to 0, so the variance is actually initialized to 1. ",log variance variance actually,issue,negative,neutral,neutral,neutral,neutral,neutral
771223511,Hi @ali6ml it looks like you may have created this by accident as there is no description of what the bug is.  I'm going to close this issue.  Please open another issue with the template filled out if you have any issues.,hi like may accident description bug going close issue please open another issue template filled,issue,negative,positive,positive,positive,positive,positive
771183597,"> Can you make it so that user stats can use this too? I think it's pretty small, you'd just need to
> 
> * Add a new StatsAggregationMethod enum values (say, HISTOGRAM) on python and C#
> * Make sure `Environment/Cumulative Reward` uses the new value
> * Check StatsSummary.aggregation_method in TensorBoard writer instead of hard-coding the key

Made these changes. ",make user use think pretty small need add new say histogram python make sure reward new value check writer instead key made,issue,positive,positive,positive,positive,positive,positive
771110703,"Can you make it so that user stats can use this too? I think it's pretty small, you'd just need to
* Add a new StatsAggregationMethod enum values (say, HISTOGRAM) on python and C#
* Make sure `Environment/Cumulative Reward` uses the new value
* Check StatsSummary.aggregation_method in TensorBoard writer instead of hard-coding the key",make user use think pretty small need add new say histogram python make sure reward new value check writer instead key,issue,positive,positive,positive,positive,positive,positive
770085472,"Hi,

I think the issue is resolved on the current master. I ran this code :
```
from mlagents_envs.environment import UnityEnvironment

env = UnityEnvironment()

name= list(env.behavior_specs.keys())[0]

for i in range(40):
    env.reset()
    behavior_name = list(env.behavior_specs)[0]
    decision_steps, terminal_steps = env.get_steps(behavior_name)
    print(list(terminal_steps),list(decision_steps))
```

And got the following result : 

```
[] [0]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
[] [0, 1, 2, 3, 4, 5, 6, 7, 8]
```

Please let me know if you are still facing this issue or if there is a problem with the way I am trying to reproduce the error.
Thank you.",hi think issue resolved current master ran code import list range list print list list got following result please let know still facing issue problem way trying reproduce error thank,issue,negative,neutral,neutral,neutral,neutral,neutral
769445380,"I mean that the network configuration might change as we refactor things in the code in the future, and make the tests more difficult to pass then.",mean network configuration might change code future make difficult pas,issue,negative,negative,negative,negative,negative,negative
769423462,":sadtrombone:

>     Error: A Unity package must not have an author field. Please remove the field. Read more about this error and potential solutions at https://docs.unity3d.com/Packages/com.unity.package-validation-suite@latest/index.html?preview=1&subfolder=/manual/manifest_validation_error.html%23a_unity_package_must_not_have_an_author_field",error unity package must author field please remove field read error potential,issue,negative,neutral,neutral,neutral,neutral,neutral
769343397,"Hi @sterlingcrispin,
If you are still having issues with this could you post it to the [Barracuda Issues Github](https://github.com/Unity-Technologies/barracuda-release/issues)?  Thank you for your feedback.  

Ping @mantasp.",hi still could post barracuda thank feedback ping,issue,negative,neutral,neutral,neutral,neutral,neutral
769337545,"Hi @GProulx,
Sorry for the late response.  I hope you were able to resolve this issue in the version you were using.  Since this bug was fixed in a later release of ML-Agents, I am closing this issue.  Thank you for your feedback. ",hi sorry late response hope able resolve issue version since bug fixed later release issue thank feedback,issue,positive,negative,neutral,neutral,negative,negative
769336089,"Just some bookkeeping - the change to remove `action_probs` from the model was merged a while ago. Since the discussion on value estimate is tracked elsewhere, I'm going to close this issue.",bookkeeping change remove model ago since discussion value estimate tracked elsewhere going close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
769335322,"Hi @rsta80,
If you look at the documentation, [`Input.GetButtonDown`](https://docs.unity3d.com/ScriptReference/Input.GetButtonDown.html) is only `true` during the frame where the button was first pressed.  Since ml-agents runs in the FixedUpdate loop, you may never get that event.  Instead, I recommend that you use [`Input.GetButton`](https://docs.unity3d.com/ScriptReference/Input.GetButton.html).  This method returns `true` if the button is currently being held down and `false` otherwise.  

I will close this for now.  In the future, please post questions like these to the [ML-Agents Forum](https://forum.unity.com/forums/ml-agents.453/)
Cheers,
Chris",hi look documentation true frame button first since loop may never get event instead recommend use method true button currently false otherwise close future please post like forum,issue,positive,positive,positive,positive,positive,positive
769329680,"Hi @dlindmark,
I'm going to close this issue since it seems like it was resolved.  Thank you!  Please reopen this if you have further issues.",hi going close issue since like resolved thank please reopen,issue,positive,neutral,neutral,neutral,neutral,neutral
769239043,"I just merged a change to the `master` branch that allows setting `torch.device` from the command line, and also displays the torch device being used for easier debugging. That'll be in the next release, or you can try it out on `master` now.",change master branch setting command line also torch device used easier next release try master,issue,negative,neutral,neutral,neutral,neutral,neutral
768775798,"Don't have a strong opinion here, but I don't think it's immediately obvious to a newcomer what the difference between a Verified Package and a normal release is. It _may_ be more intuitive if we have a separate, dedicated table for verified packages.",strong opinion think immediately obvious newcomer difference package normal release intuitive separate table,issue,positive,positive,positive,positive,positive,positive
768644542,"Hi @chriselion,

Thanks for your response. What do u mean by network configurations here? I can try optimizing the test using different seeds, but I am not sure if there is a mechanism to try out different network configurations?

Would something like this be a more reasonable way to optimize the tests and avoid overfitting?",hi thanks response mean network try test different sure mechanism try different network would something like reasonable way optimize avoid,issue,positive,positive,neutral,neutral,positive,positive
768059257,"The SideChannel allocations are addressed in https://github.com/Unity-Technologies/ml-agents/pull/4886
I added an overload for VectorSensor.AddObservation in https://github.com/Unity-Technologies/ml-agents/pull/4887 (and also cleaned up a lot more garbage during inference).",added overload also lot garbage inference,issue,negative,neutral,neutral,neutral,neutral,neutral
767944039,"Thanks for trying to improve the test times. I talked it over with some folks for the team, and we're worried that this might end up ""overfitting"" to the particular seed and network configuration, and make it more brittle to changes in the future. So I think we'll leave the tests as they are for now.",thanks trying improve test time team worried might end particular seed network configuration make brittle future think leave,issue,positive,positive,positive,positive,positive,positive
767831166,"Just curious, how do you get this profiler windows? That looks pretty helpful in checking each function call.",curious get profiler pretty helpful function call,issue,positive,positive,neutral,neutral,positive,positive
767789123,"No problem, thanks for pointing me in the right direction. I have MLA-1724 created for this.",problem thanks pointing right direction,issue,negative,positive,positive,positive,positive,positive
767373394,"Hi Chris, great to hear that it wasn't a false alarm. Thank you for the super quick responses!",hi great hear false alarm thank super quick,issue,positive,positive,positive,positive,positive,positive
767357832,"Thanks a lot! I fully understand it's unnecessary for ML training to have such large image size. But in case this will be handled later, grpc streaming service might be an update for your rpc service of output : )",thanks lot fully understand unnecessary training large image size case handled later streaming service might update service output,issue,negative,positive,neutral,neutral,positive,positive
767299385,"Thanks, I was able to confirm that by modifying one of our examples:

![image](https://user-images.githubusercontent.com/6877802/105802954-302a4b80-5f51-11eb-988f-ffa99175e0fc.png)

I think we can add an `AddObservation(IList<float> observation)` overload to VectorSensor, which should help get rid of this allocation.",thanks able confirm one image think add float observation overload help get rid allocation,issue,negative,positive,positive,positive,positive,positive
767253625,"@chriselion Ok I see. That explains why the environment didn't change at the beginning of each episode when I add a reset function to OnEnvironmentReset().

I would take your recommendations by destroying the agent and triggering the scene change there (e.g., from OnDisabled()?). Hope that can solve this recursive issue.",see environment change beginning episode add reset function would take agent scene change hope solve recursive issue,issue,positive,neutral,neutral,neutral,neutral,neutral
767239341,"I think I see what you're saying. This is happening in your Agent's`CollectObservations(VectorSensor sensor)` method, right? If you use `sensor.AddObservation(_observationLadarInfoOffRoad);` it would treat the List as an IEnumerable:
https://github.com/Unity-Technologies/ml-agents/blob/30bcb018bcac6dd73ed64d34016131d03cd6514d/com.unity.ml-agents/Runtime/Sensors/VectorSensor.cs#L167-L173
which _might_ allocate memory in the `foreach` loop; I need to look into it further. Your approach is equivalent but definitely doesn't need to allocate anything.",think see saying happening agent sensor method right use would treat list allocate memory loop need look approach equivalent definitely need allocate anything,issue,positive,positive,positive,positive,positive,positive
767134559,"Hi Chris,

Thank you for the quick response. It is probably best if I leave the modifications for you. :) 

Btw - and maybe that should be a separate issue - but I saw a strange memory allocation too in another part of ml_agents, but that I managed to fix on my side. So if I did the following, that caused memory allocations:

`sensor.AddObservation(_observationLadarInfoOffRoad);`

where  `private List<float> _observationLadarInfoOffRoad;` 

When I added the observations in a for loop myself, the memory allocation disappeared:

`
for (int i = 0, length = _observationLadarInfoOffRoad.Count; i < length; i++) {
    sensor.AddObservation(_observationLadarInfoOffRoad[i]);
}
`


",hi thank quick response probably best leave maybe separate issue saw strange memory allocation another part fix side following memory private list float added loop memory allocation length length,issue,positive,positive,positive,positive,positive,positive
767128383,"We've got this logged in our internal tracker as MLA-1720. I'll probably get to it this week, but it should be pretty straightforward if you want to submit a PR for it instead.",got logged internal tracker probably get week pretty straightforward want submit instead,issue,positive,positive,positive,positive,positive,positive
767123754,"Hi,
I think you're right that we're allocating a BinaryWriter and MemoryStream on each step, even if there are no messages to process:
https://github.com/Unity-Technologies/ml-agents/blob/30bcb018bcac6dd73ed64d34016131d03cd6514d/com.unity.ml-agents/Runtime/SideChannels/SideChannelManager.cs#L121-L126

We can definitely early out in that case (or maybe reuse the objects). Is that the main allocation you're seeing?",hi think right step even process definitely early case maybe reuse main allocation seeing,issue,negative,positive,positive,positive,positive,positive
767062170,"OnEnvironmentReset() is only at the the very beginning of training and when the trainer issues a full reset, which doesn't happen under normal circumstances (only when using curriculum).

Instead of triggering the scene change from an Agent's OnEpisodeBegin, I'd recommend you do something like destroying the Agent when it's done, triggering the scene change, and then instantiating a new Agent. That should handle the problem you described about switching recursively.",beginning training trainer full reset happen normal curriculum instead scene change agent recommend something like agent done scene change new agent handle problem switching,issue,negative,positive,positive,positive,positive,positive
767055071,"Hi @a0s,
As the error messages indicate, `--use-feature=2020-resolver` doesn't do anything in the latest version of pip, so you don't need to try that anymore.

It looks like the main problem is the `h5py` package. We don't currently support python 3.9; can you try downgrading to 3.8 and see if that helps?

For the numpy version, I would recommend sticking with `numpy==numpy-1.18.5`",hi error indicate anything latest version pip need try like main problem package currently support python try see version would recommend sticking,issue,negative,positive,positive,positive,positive,positive
767044675,"Hi @ZijianHan-VolvoCars,
That's correct, we can't support images that take up more than 4MB for training. By default, we use GIF compression so that might help with some of the image size, but that also might not help much depending on your scene.

I tried this locally by changing the GridWorld example to use uncompressed observations at 750 x 750 (3 channels and 4 bytes/pixel results in ~6.75MB). This results in a ""Resource Exhausted"" GRPC exception, but we silently ignore the content of the exception and exit Play Mode without any error message, which is something we should handle better.",hi correct ca support take training default use gif compression might help image size also might help much depending scene tried locally example use uncompressed resource exhausted exception silently ignore content exception exit play mode without error message something handle better,issue,positive,positive,neutral,neutral,positive,positive
766950450,"Hello, I noticed the way how ml agent create the grpc service to send out the observations from Unity. I know the limit for each message in grpc is around 4MB, and it means that the observation of image size cannot be larger than that right?
Actually I've been having some issue broadcasting large image with grpc service in Unity ( not exactly using your ml agent), but I saw how you publish image data so just might wanna ask here",hello way agent create service send unity know limit message around observation image size right actually issue large image service unity exactly agent saw publish image data might wan na ask,issue,negative,positive,positive,positive,positive,positive
766248737,"@chriselion I tried to switch scenes from OnEpisodeBegin and it works fine. The only problem is that, OnEpisodeBegin will be called at the start of each episode to initialize the agent, therefore I need to avoid switching scenes recursively.

As you mentioned, the Academy is not a singleton therefore there is not need to create an instance. For Academy's `OnEnvironmentReset` delegate, does it mean that this action will be called at the begining of each episode, regardless of single scene or multiple scenes? I think I can also add an action to switch scenes to `OnEnvironmentReset`.",tried switch work fine problem start episode initialize agent therefore need avoid switching academy singleton therefore need create instance academy delegate mean action episode regardless single scene multiple think also add action switch,issue,negative,positive,neutral,neutral,positive,positive
766233132,"Hm, the problem is still here (on macos 10.15.7)

```
> pip install mlagents --use-feature=2020-resolver
WARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.
Collecting mlagents
  Using cached mlagents-0.23.0-py3-none-any.whl (199 kB)
Requirement already satisfied: protobuf>=3.6 in ./lib/python3.9/site-packages (from mlagents) (3.14.0)
Requirement already satisfied: grpcio>=1.11.0 in ./lib/python3.9/site-packages (from mlagents) (1.35.0)
Requirement already satisfied: tensorboard>=1.15 in ./lib/python3.9/site-packages (from mlagents) (2.4.1)
Requirement already satisfied: mlagents-envs==0.23.0 in ./lib/python3.9/site-packages (from mlagents) (0.23.0)
Requirement already satisfied: attrs>=19.3.0 in ./lib/python3.9/site-packages (from mlagents) (20.3.0)
Requirement already satisfied: numpy<2.0,>=1.13.3 in ./lib/python3.9/site-packages (from mlagents) (1.19.5)
Collecting cattrs<1.1.0,>=1.0.0
  Using cached cattrs-1.0.0-py2.py3-none-any.whl (14 kB)
Requirement already satisfied: Pillow>=4.2.1 in ./lib/python3.9/site-packages (from mlagents) (8.1.0)
Requirement already satisfied: h5py>=2.9.0 in ./lib/python3.9/site-packages (from mlagents) (3.1.0)
Requirement already satisfied: pyyaml>=3.1.0 in ./lib/python3.9/site-packages (from mlagents) (5.4.1)
Requirement already satisfied: torch<1.8.0,>=1.6.0 in ./lib/python3.9/site-packages (from mlagents) (1.7.1)
Requirement already satisfied: cloudpickle in ./lib/python3.9/site-packages (from mlagents-envs==0.23.0->mlagents) (1.6.0)
Collecting numpy<2.0,>=1.13.3
  Using cached numpy-1.18.5-cp39-cp39-macosx_10_15_x86_64.whl
Requirement already satisfied: six>=1.5.2 in ./lib/python3.9/site-packages (from grpcio>=1.11.0->mlagents) (1.15.0)
Collecting h5py>=2.9.0
  Using cached h5py-3.1.0-cp39-cp39-macosx_10_9_x86_64.whl (2.9 MB)
  Using cached h5py-3.0.0-cp39-cp39-macosx_10_9_x86_64.whl (2.9 MB)
  Using cached h5py-2.10.0.tar.gz (301 kB)
Requirement already satisfied: setuptools>=41.0.0 in ./lib/python3.9/site-packages (from tensorboard>=1.15->mlagents) (49.2.1)
Requirement already satisfied: werkzeug>=0.11.15 in ./lib/python3.9/site-packages (from tensorboard>=1.15->mlagents) (1.0.1)
Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./lib/python3.9/site-packages (from tensorboard>=1.15->mlagents) (1.8.0)
Requirement already satisfied: requests<3,>=2.21.0 in ./lib/python3.9/site-packages (from tensorboard>=1.15->mlagents) (2.25.1)
Requirement already satisfied: absl-py>=0.4 in ./lib/python3.9/site-packages (from tensorboard>=1.15->mlagents) (0.11.0)
Requirement already satisfied: markdown>=2.6.8 in ./lib/python3.9/site-packages (from tensorboard>=1.15->mlagents) (3.3.3)
Requirement already satisfied: wheel>=0.26 in ./lib/python3.9/site-packages (from tensorboard>=1.15->mlagents) (0.36.2)
Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./lib/python3.9/site-packages (from tensorboard>=1.15->mlagents) (0.4.2)
Requirement already satisfied: google-auth<2,>=1.6.3 in ./lib/python3.9/site-packages (from tensorboard>=1.15->mlagents) (1.24.0)
Requirement already satisfied: pyasn1-modules>=0.2.1 in ./lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->mlagents) (0.2.8)
Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->mlagents) (4.2.0)
Requirement already satisfied: rsa<5,>=3.1.4 in ./lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->mlagents) (4.7)
Requirement already satisfied: requests-oauthlib>=0.7.0 in ./lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->mlagents) (1.3.0)
Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->mlagents) (0.4.8)
Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard>=1.15->mlagents) (2020.12.5)
Requirement already satisfied: chardet<5,>=3.0.2 in ./lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard>=1.15->mlagents) (4.0.0)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard>=1.15->mlagents) (1.26.2)
Requirement already satisfied: idna<3,>=2.5 in ./lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard>=1.15->mlagents) (2.10)
Requirement already satisfied: oauthlib>=3.0.0 in ./lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->mlagents) (3.1.0)
Requirement already satisfied: typing-extensions in ./lib/python3.9/site-packages (from torch<1.8.0,>=1.6.0->mlagents) (3.7.4.3)
Building wheels for collected packages: h5py
  Building wheel for h5py (setup.py) ... error
  ERROR: Command errored out with exit status 1:
   command: /Users/a0s/jupiter-notebook/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/m6/xclqx4bj1f709jdbq5g5z5_m0000gp/T/pip-install-hyq9hq5r/h5py_b5e46cc6c9984745be5e75f0c7a09732/setup.py'""'""'; __file__='""'""'/private/var/folders/m6/xclqx4bj1f709jdbq5g5z5_m0000gp/T/pip-install-hyq9hq5r/h5py_b5e46cc6c9984745be5e75f0c7a09732/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/m6/xclqx4bj1f709jdbq5g5z5_m0000gp/T/pip-wheel-06au4if2
       cwd: /private/var/folders/m6/xclqx4bj1f709jdbq5g5z5_m0000gp/T/pip-install-hyq9hq5r/h5py_b5e46cc6c9984745be5e75f0c7a09732/
  Complete output (64 lines):
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.macosx-10.15-x86_64-3.9
  creating build/lib.macosx-10.15-x86_64-3.9/h5py
  copying h5py/h5py_warnings.py -> build/lib.macosx-10.15-x86_64-3.9/h5py
  copying h5py/version.py -> build/lib.macosx-10.15-x86_64-3.9/h5py
  copying h5py/highlevel.py -> build/lib.macosx-10.15-x86_64-3.9/h5py
  copying h5py/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/h5py
  copying h5py/ipy_completer.py -> build/lib.macosx-10.15-x86_64-3.9/h5py
  creating build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
  copying h5py/_hl/files.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
  copying h5py/_hl/compat.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
  copying h5py/_hl/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
  copying h5py/_hl/selections.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
  copying h5py/_hl/dataset.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
  copying h5py/_hl/vds.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
  copying h5py/_hl/selections2.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
  copying h5py/_hl/group.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
  copying h5py/_hl/datatype.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
  copying h5py/_hl/attrs.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
  copying h5py/_hl/dims.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
  copying h5py/_hl/base.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
  copying h5py/_hl/filters.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
  creating build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_deprecation.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_dimension_scales.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_attribute_create.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_file_image.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_h5d_direct_chunk.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_h5f.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_dataset_getitem.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_group.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_dataset_swmr.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_slicing.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_h5pl.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_attrs.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_attrs_data.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_h5t.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_h5p.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_dims_dimensionproxy.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_datatype.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/common.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_dataset.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_file.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_selections.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_threads.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_dtype.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_h5.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_file2.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_completions.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_filters.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_base.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  copying h5py/tests/test_objects.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
  creating build/lib.macosx-10.15-x86_64-3.9/h5py/tests/test_vds
  copying h5py/tests/test_vds/test_highlevel_vds.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests/test_vds
  copying h5py/tests/test_vds/test_virtual_source.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests/test_vds
  copying h5py/tests/test_vds/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests/test_vds
  copying h5py/tests/test_vds/test_lowlevel_vds.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests/test_vds
  running build_ext
  Loading library to get version: libhdf5.dylib
  error: dlopen(libhdf5.dylib, 6): image not found
  ----------------------------------------
  ERROR: Failed building wheel for h5py
  Running setup.py clean for h5py
Failed to build h5py
Installing collected packages: numpy, h5py, cattrs, mlagents
  Attempting uninstall: numpy
    Found existing installation: numpy 1.19.5
    Uninstalling numpy-1.19.5:
      Successfully uninstalled numpy-1.19.5
  Attempting uninstall: h5py
    Found existing installation: h5py 3.1.0
    Uninstalling h5py-3.1.0:
      Successfully uninstalled h5py-3.1.0
    Running setup.py install for h5py ... error
    ERROR: Command errored out with exit status 1:
     command: /Users/a0s/jupiter-notebook/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/m6/xclqx4bj1f709jdbq5g5z5_m0000gp/T/pip-install-hyq9hq5r/h5py_b5e46cc6c9984745be5e75f0c7a09732/setup.py'""'""'; __file__='""'""'/private/var/folders/m6/xclqx4bj1f709jdbq5g5z5_m0000gp/T/pip-install-hyq9hq5r/h5py_b5e46cc6c9984745be5e75f0c7a09732/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/m6/xclqx4bj1f709jdbq5g5z5_m0000gp/T/pip-record-ou_6b9gs/install-record.txt --single-version-externally-managed --compile --install-headers /Users/a0s/jupiter-notebook/include/site/python3.9/h5py
         cwd: /private/var/folders/m6/xclqx4bj1f709jdbq5g5z5_m0000gp/T/pip-install-hyq9hq5r/h5py_b5e46cc6c9984745be5e75f0c7a09732/
    Complete output (64 lines):
    running install
    running build
    running build_py
    creating build
    creating build/lib.macosx-10.15-x86_64-3.9
    creating build/lib.macosx-10.15-x86_64-3.9/h5py
    copying h5py/h5py_warnings.py -> build/lib.macosx-10.15-x86_64-3.9/h5py
    copying h5py/version.py -> build/lib.macosx-10.15-x86_64-3.9/h5py
    copying h5py/highlevel.py -> build/lib.macosx-10.15-x86_64-3.9/h5py
    copying h5py/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/h5py
    copying h5py/ipy_completer.py -> build/lib.macosx-10.15-x86_64-3.9/h5py
    creating build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
    copying h5py/_hl/files.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
    copying h5py/_hl/compat.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
    copying h5py/_hl/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
    copying h5py/_hl/selections.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
    copying h5py/_hl/dataset.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
    copying h5py/_hl/vds.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
    copying h5py/_hl/selections2.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
    copying h5py/_hl/group.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
    copying h5py/_hl/datatype.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
    copying h5py/_hl/attrs.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
    copying h5py/_hl/dims.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
    copying h5py/_hl/base.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
    copying h5py/_hl/filters.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/_hl
    creating build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_deprecation.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_dimension_scales.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_attribute_create.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_file_image.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_h5d_direct_chunk.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_h5f.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_dataset_getitem.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_group.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_dataset_swmr.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_slicing.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_h5pl.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_attrs.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_attrs_data.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_h5t.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_h5p.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_dims_dimensionproxy.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_datatype.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/common.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_dataset.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_file.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_selections.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_threads.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_dtype.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_h5.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_file2.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_completions.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_filters.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_base.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    copying h5py/tests/test_objects.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests
    creating build/lib.macosx-10.15-x86_64-3.9/h5py/tests/test_vds
    copying h5py/tests/test_vds/test_highlevel_vds.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests/test_vds
    copying h5py/tests/test_vds/test_virtual_source.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests/test_vds
    copying h5py/tests/test_vds/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests/test_vds
    copying h5py/tests/test_vds/test_lowlevel_vds.py -> build/lib.macosx-10.15-x86_64-3.9/h5py/tests/test_vds
    running build_ext
    Loading library to get version: libhdf5.dylib
    error: dlopen(libhdf5.dylib, 6): image not found
    ----------------------------------------
  Rolling back uninstall of h5py
  Moving to /Users/a0s/jupiter-notebook/lib/python3.9/site-packages/h5py-3.1.0.dist-info/
   from /Users/a0s/jupiter-notebook/lib/python3.9/site-packages/~5py-3.1.0.dist-info
  Moving to /Users/a0s/jupiter-notebook/lib/python3.9/site-packages/h5py/
   from /Users/a0s/jupiter-notebook/lib/python3.9/site-packages/~5py
ERROR: Command errored out with exit status 1: /Users/a0s/jupiter-notebook/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/m6/xclqx4bj1f709jdbq5g5z5_m0000gp/T/pip-install-hyq9hq5r/h5py_b5e46cc6c9984745be5e75f0c7a09732/setup.py'""'""'; __file__='""'""'/private/var/folders/m6/xclqx4bj1f709jdbq5g5z5_m0000gp/T/pip-install-hyq9hq5r/h5py_b5e46cc6c9984745be5e75f0c7a09732/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/m6/xclqx4bj1f709jdbq5g5z5_m0000gp/T/pip-record-ou_6b9gs/install-record.txt --single-version-externally-managed --compile --install-headers /Users/a0s/jupiter-notebook/include/site/python3.9/h5py Check the logs for full command output.




> pip uninstall numpy
Found existing installation: numpy 1.18.5
Uninstalling numpy-1.18.5:
  Would remove:
    /Users/a0s/jupiter-notebook/bin/f2py
    /Users/a0s/jupiter-notebook/bin/f2py3
    /Users/a0s/jupiter-notebook/bin/f2py3.9
    /Users/a0s/jupiter-notebook/lib/python3.9/site-packages/numpy-1.18.5.dist-info/*
    /Users/a0s/jupiter-notebook/lib/python3.9/site-packages/numpy/*
Proceed (y/n)? Y
  Successfully uninstalled numpy-1.18.5




> pip install h5py --use-feature=2020-resolver
WARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.
Requirement already satisfied: h5py in ./lib/python3.9/site-packages (3.1.0)
Collecting numpy>=1.19.3
  Using cached numpy-1.19.5-cp39-cp39-macosx_10_9_x86_64.whl (15.6 MB)
Installing collected packages: numpy
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
mlagents-envs 0.23.0 requires numpy<1.19.0,>=1.14.1, but you have numpy 1.19.5 which is incompatible.
Successfully installed numpy-1.19.5
```",problem still pip install warning longer effect since default dependency resolver pip become error pip requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied pillow requirement already satisfied requirement already satisfied requirement already satisfied torch requirement already satisfied requirement already satisfied six requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied markdown requirement already satisfied wheel requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied torch building collected building wheel error error command exit status command open compile code complete output running running build running build running loading library get version error image found error building wheel running clean build collected found installation successfully uninstalled found installation successfully uninstalled running install error error command exit status command open compile code install record compile complete output running install running build running build running loading library get version error image found rolling back moving moving error command exit status open compile code install record compile check full command output pip found installation would remove proceed successfully uninstalled pip install warning longer effect since default dependency resolver pip become error pip requirement already satisfied collected error pip dependency resolver currently take account behaviour source following dependency incompatible successfully,issue,positive,positive,positive,positive,positive,positive
766163752,I am able to train the model but after the training gets finished I see the warning is shown in the Unity Editor. Can someone please help with this issue? TIA  ,able train model training finished see warning shown unity editor someone please help issue,issue,negative,positive,positive,positive,positive,positive
765724430,"> If I understand correctly, you build a startup scene where you have this script attached. You pass scene name from command line and this script will load the scene with the Awake function call.

Yeah, we add all the example scenes to the build, along with [this scene](https://github.com/Unity-Technologies/ml-agents/blob/release_12/Project/Assets/ML-Agents/Examples/Startup/Startup.unity) as the first scene.

> I have a question here about switching between scenes in the build: if I want to switch between scenes during training, say at the new episode. Does it mean that I need to call LoadScene from each scene OnEpisodeBegin() so that a new scene can be loaded?

I'm not that experienced with Unity's scene management. I think what you described is fine, as long as it doesn't destroy the Agent as a side effect (since that will end the episode for the Agent again). Regardless, I don't think there's anything ML-Agents-specific about scene management.

---

Historical note: in old versions of ML-Agents, the Academy was an abstract class that you had to have an instance of in the scene, and _that_ was problematic for multiple scenes. But the Academy has been a singleton since early 2020.",understand correctly build scene script attached pas scene name command line script load scene awake function call yeah add example build along scene first scene question switching build want switch training say new episode mean need call scene new scene loaded experienced unity scene management think fine long destroy agent side effect since end episode agent regardless think anything scene management historical note old academy abstract class instance scene problematic multiple academy singleton since early,issue,negative,positive,positive,positive,positive,positive
765695992,"> I use a laptop with two graphics cards.
> The standalone graphics card is GTX1060.
> Core display is Intel UHD Graphics 630.
> Is it possible to use GPU on ordinary desktop computers?

We currently always use device 0. You should be able to override this by setting the CUDA_VISIBLE_DEVICES environment variable before running `mlagents-learn` - on Mac/Linux, you can do this with `CUDA_VISIBLE_DEVICES=1 mlagents-learn ...` but I'm not sure if this syntax works in PowerShell. I'm going to add a commandline option to control this better, and add some better logging to make it clear whether torch is using CPU or GPU.

> Can I save some training time if I use a GPU?

Unfortunately, I don't have a better answer than ""it depends"". In general, we haven't seen that reinforcement learning gets much speedup on GPU, but this might change if you use a larger batch size or have visual observations.

> Do you have any case related to industrial control?

I don't think anybody has used it for that yet. If you'd like to discuss it more, could you email our team at ml-agents@unity3d.com and we can talk more about it?",use two graphic graphic card core display graphic possible use ordinary currently always use device able override setting environment variable running sure syntax work going add option control better add better logging make clear whether torch save training time use unfortunately better answer general seen reinforcement learning much might change use batch size visual case related industrial control think anybody used yet like discus could team talk,issue,positive,positive,positive,positive,positive,positive
765638173,Will add unit test and changelog this afternoon.,add unit test afternoon,issue,negative,neutral,neutral,neutral,neutral,neutral
765047067,"Hi @chriselion ! Thanks for your response!

I see what you mean here. If I understand correctly, you build a startup scene where you have [this script](https://github.com/Unity-Technologies/ml-agents/blob/release_12/Project/Assets/ML-Agents/Examples/Startup/Scripts/Startup.cs) attached. You pass scene name from command line and this script will load the scene with the `Awake` function call.

I have a question here about switching between scenes in the build: if I want to switch between scenes during training, say at the new episode. Does it mean that I need to call `LoadScene` from each scene `OnEpisodeBegin()` so that a new scene can be loaded?   

",hi thanks response see mean understand correctly build scene script attached pas scene name command line script load scene awake function call question switching build want switch training say new episode mean need call scene new scene loaded,issue,negative,negative,neutral,neutral,negative,negative
765026412,"> Hi @torsilver,
> The ""Inference Device"" controls how the Agent performs inference, not training.
> 
> Can you run this from the commandline?
> 
> ```
> python -c 'import torch; print(torch.cuda.is_available())'
> ```
> 
> If that prints `True`, ML-Agents should use the GPU for training. If it prints `False`, there's something wrong with your `torch` setup, but it's not specific to ML-Agents, so you'll need to look elsewhere to get help debugging it.

Hello chriselion
PS C:\Windows\system32> python -c 'import torch; print(torch.cuda.is_available())'
True
I use a laptop with two graphics cards.
The standalone graphics card is GTX1060.
Core display is Intel UHD Graphics 630.
Is it possible to use GPU on ordinary desktop computers?
At present, I want to apply ML-Agents to industrial control of complex systems.
Can I save some training time if I use a GPU?
Do you have any case related to industrial control?",hi inference device agent inference training run python torch print true use training false something wrong torch setup specific need look elsewhere get help hello python torch print true use two graphic graphic card core display graphic possible use ordinary present want apply industrial control complex save training time use case related industrial control,issue,positive,negative,neutral,neutral,negative,negative
765023419,"Hi @KevinWu57,
There's nothing in ML-Agents that prevents you from using multiple scenes. For some of our internal tests, we build with all of the example scenes, then use command-line args to switch to the desired scene using [this script](https://github.com/Unity-Technologies/ml-agents/blob/release_12/Project/Assets/ML-Agents/Examples/Startup/Scripts/Startup.cs).",hi nothing multiple internal build example use switch desired scene script,issue,negative,neutral,neutral,neutral,neutral,neutral
764984185,"Hi @torsilver,
The ""Inference Device"" controls how the Agent performs inference, not training.

Can you run this from the commandline?
```
python -c 'import torch; print(torch.cuda.is_available())'
```
If that prints `True`, ML-Agents should use the GPU for training. If it prints `False`, there's something wrong with your `torch` setup, but it's not specific to ML-Agents, so you'll need to look elsewhere to get help debugging it.
",hi inference device agent inference training run python torch print true use training false something wrong torch setup specific need look elsewhere get help,issue,negative,negative,negative,negative,negative,negative
764916688,"I think for a test - you could do a test just for `training_behaviors`, something very similar to the `test_reset_collects_results_from_all_envs` in `test_subprocess_env_manager.py`. It just tests that it calls the EnvironmentCommand for all workers. We have unit tests that test that `training_behaviors` is called, so we just need to test that calling it does the right thing. ",think test could test something similar unit test need test calling right thing,issue,negative,positive,positive,positive,positive,positive
764788933,I did a CI run and saw nothing strange happening.,run saw nothing strange happening,issue,negative,negative,neutral,neutral,negative,negative
763990358,"Hi,
You'll need something like `xvfb` to do visual observations on a remote machine. There's an example of setting this up in the collab notebook [here](https://colab.research.google.com/drive/1Qg6E5kmf9n4G8rc5lXHIM_cQzMUFGH-g#forceEdit=true&sandboxMode=true&scrollTo=htb-p1hSNX7D) (under the ""setup"" section).

We don't currently support multiple GPUs for training. We have a request for this logged already as MLA-662 in our internal tracker, but there's no timeline for implementing it.

Torch will use a single GPU as long as its set up correctly. You’ll need to install a torch version that is compatible with your CUDA version, though - otherwise it won’t detect your GPU devices and will run on CPU instead.",hi need something like visual remote machine example setting notebook setup section currently support multiple training request logged already internal tracker torch use single long set correctly need install torch version compatible version though otherwise detect run instead,issue,positive,negative,neutral,neutral,negative,negative
763971325,"If I build the environment without headless mode, it will always output `UnityTimeOutException` on the server.
![image](https://user-images.githubusercontent.com/46989361/105245116-4b96f000-5b26-11eb-9e8c-65fcb85d1b37.png)
",build environment without headless mode always output server image,issue,negative,neutral,neutral,neutral,neutral,neutral
763851274,"Hi all,
We still have this feature in our backlog of things we want to do, and since we are a small team we haven't gotten to it yet.  I will post any updates here as they come.
Cheers,
Chris",hi still feature backlog want since small team gotten yet post come,issue,negative,negative,negative,negative,negative,negative
763065144,Thank you for raising up this issue. We will be investigating. This is logged as MLA-1702 for internal tracking.,thank raising issue investigating logged internal,issue,negative,neutral,neutral,neutral,neutral,neutral
763019364,"> I wonder if we have an environment which would benefit from the higher timescale. It would likely have to have neither physics nor rendering, so perhaps Match3?

AFAIK we don't override the defaults for training in any scene.

I'm not sure Match3 would benefit since it doesn't have any physics or animation; moves are requested on every step.",wonder environment would benefit higher would likely neither physic rendering perhaps match override training scene sure match would benefit since physic animation every step,issue,positive,positive,positive,positive,positive,positive
763007665,"I wonder if we have an environment which would benefit from the higher timescale. It would likely have to have neither physics nor rendering, so perhaps Match3? ",wonder environment would benefit higher would likely neither physic rendering perhaps match,issue,negative,positive,positive,positive,positive,positive
761618251,"Hi @vincentpierre - Thanks for your quick reply!

The use case I had, was that I wanted to evaluate a single agent in multiple training areas in parallel. Once all these areas had finished, Python would reset the environment to evaluate the next agent in the population.

Your design decision is clear though, so I will close this issue.

Regarding your explanation, I do see `get_steps` return values where `decision_steps` is empty and `terminal_steps` is not, which seems to contradict part of your statement. How can this be? ",hi thanks quick reply use case evaluate single agent multiple training parallel finished python would reset environment evaluate next agent population design decision clear though close issue regarding explanation see return empty contradict part statement,issue,negative,positive,neutral,neutral,positive,positive
761607107,"This is resolve my case as found on Where to find the win32api module for Python?

**pip install pypiwin32**",resolve case found find module python pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
761269962,"Actually, looks like we already have that in the ""base"" event.",actually like already base event,issue,negative,negative,negative,negative,negative,negative
761245088,"@dongruoping good call, we can add [`sys.platform`](https://docs.python.org/3/library/sys.html#sys.platform) if the equivalent isn't already there. Let me check what gets automatically added.",good call add equivalent already let check automatically added,issue,negative,positive,positive,positive,positive,positive
761239155,"Hi, The short answer is that it is by design. When an Agent is terminated (in this case destroyed) it will send a message to a buffer to be sent later. The buffer is then sent to Python as soon as another Agent (Or the same Agent after respawn) needs to request a decision. This is to avoid sending messages that only contain information about the death of Agents and do not really need an input from Python.

In RL, the environment can be reset at any time by Python and in ML-Agents it is not necessary to manually reset the environment, it is assumed that when the game reached a terminal state (like there are no Agents in the scene for example) the game will reset itself and spawn new Agents.

In short, there should be no moment where the game no longer has any Agents taking decisions and is waiting on Python to reset. It is not assumed that Python will send a reset signal when no Agents are alive in the scene.

I hope this helps. What is your use case for this ?",hi short answer design agent case send message buffer sent later buffer sent python soon another agent agent need request decision avoid sending contain information death really need input python environment reset time python necessary manually reset environment assumed game terminal state like scene example game reset spawn new short moment game longer taking waiting python reset assumed python send reset signal alive scene hope use case,issue,negative,negative,neutral,neutral,negative,negative
761105363,"> > Should we link to the CodeMonkey videos too?
> 
> That makes sense to me. @unityjeffrey - any reason not to?
> 
> @chriselion - can you slack me the links?

Added",link sense reason slack link added,issue,negative,neutral,neutral,neutral,neutral,neutral
760580445,"> Should we link to the CodeMonkey videos too?

That makes sense to me. @unityjeffrey - any reason not to?

@chriselion - can you slack me the links?",link sense reason slack link,issue,negative,neutral,neutral,neutral,neutral,neutral
760343731,"I think this can work, but I don't see how it simplifies the code, `create_input_processors` now returns 4 elements, I preferred `get_masks` on the EntityEmbeddings, because it meant EntityEmbeddings was responsible for the whole preprocessing of the variable length inputs. I also did prefer having layer norm inside either the EntityEmbeddings or the ResidualSelfAttention module rather than inside the network body.",think work see code preferred meant responsible whole variable length also prefer layer norm inside either module rather inside network body,issue,negative,positive,positive,positive,positive,positive
759758033,"Thank you very much.
Perhaps I didn't scrutinize the code.
I'm sorry.
",thank much perhaps scrutinize code sorry,issue,negative,negative,negative,negative,negative,negative
759755092,"Also, you specify `public override void Initialize()` outside of the definition of RollerAgent (you closed that bracket too early)",also specify public override void initialize outside definition closed bracket early,issue,negative,neutral,neutral,neutral,neutral,neutral
759741757,"It looks like the Agent class does not have a method called `Initialize` which should be the case if you are using the latest ML-Agents. What version of ML-Agents are you using and are you using the documentation adapted to that release ?
Also, please use the bug template form when submitting an issue.",like agent class method initialize case latest version documentation release also please use bug template form issue,issue,positive,positive,positive,positive,positive,positive
759724342,"As you can see from the code, I'm a unity beginner.
Please support.",see code unity beginner please support,issue,positive,neutral,neutral,neutral,neutral,neutral
759705017,"@vincentpierre I thought that might be the case, but was worried that they might say ""you need to post this on ml-agents"" so I figured I would open a thread here as well. 

If I learn anything there, I will add a link to it here.",thought might case worried might say need post figured would open thread well learn anything add link,issue,negative,neutral,neutral,neutral,neutral,neutral
759701351,"I think you need to post this issue on the RLlib repo, this error happens when the base port specified to open the Unity environment is already in use. You will need to manually specify another base port, but I am not sure how RLlib allows you to do this.",think need post issue error base port open unity environment already use need manually specify another base port sure,issue,negative,negative,negative,negative,negative,negative
758703783,You can use xvfb with ml-agents. See this [forum post](https://forum.unity.com/threads/how-do-i-get-visual-observations-in-headless-environment.927530/) and [github issue](https://github.com/Unity-Technologies/ml-agents/issues/1786#issuecomment-551280552) ,use see forum post issue,issue,negative,neutral,neutral,neutral,neutral,neutral
758604793,"Seems like it was because of the version of the python API, I needed v0.23.0 to go with release 12 (for anyone else that has this issue)",like version python go release anyone else issue,issue,negative,neutral,neutral,neutral,neutral,neutral
758408297,Thank you for the info. Seems we need e.g. Xvfb for no_graphics=False. Are there any options that we can get visual observation as e.g. an array without rendering to a game-window? We would like to run on docker images and can't set a window system.,thank need get visual observation array without rendering would like run docker ca set window system,issue,negative,neutral,neutral,neutral,neutral,neutral
758289870,"only change between the last commit and the previous one was adding a comment, so going to merge without tests finishing.",change last commit previous one comment going merge without finishing,issue,negative,negative,neutral,neutral,negative,negative
758286594,"hi @yoheigon hope you are doing well.  Apologies for the late reply.  Thank you for writing this up, this would be beneficial for our users who are wanting to use ML-Agents with Sagemaker.  I was wondering if this document would be better suited to be hosted on the Amazon site (and we can link from here).  We have gotten away from providing documentation and support to run ml-agents on cloud providers as there is a support burden and knowledge gap on our end.

if you think it makes sense to include it in the core ml-agents documentation, then I would suggest the following changes:

1.  In the title, please include a note that this is only tested for a specific version of ML-Agents.  This will prevent users from trying to follow these instructions on a later version of ML-Agents.
2.  Please also include a note to indicate that this isn't generally supported by the ML-Agents team, but that it's included in case it may be helpful
3.  If folks have any questions or additional followups, is there an Amazon POC or support email address that can be added?  We end up getting questions in the forums or issues and would be good to be able to route them to someone at Amazon.

For 1-2, the Azure doc is a good example.  https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Microsoft-Azure.md

Let me know on my note above, I'll also make some comments in the documentation itself.

Thanks!

Jeff
",hi hope well late reply thank writing would beneficial wanting use wondering document would better site link gotten away providing documentation support run cloud support burden knowledge gap end think sense include core documentation would suggest following title please include note tested specific version prevent trying follow later version please also include note indicate generally team included case may helpful additional support address added end getting would good able route someone azure doc good example let know note also make documentation thanks jeff,issue,positive,positive,positive,positive,positive,positive
758286561,"Thanks for nudging me.

Here is my fork: 
Phong13/ml-agents

The branch is  release_7_branch_with_value_estimate

There are a few other non value estimate changes in there. The main change is model.py to add a TF identifier for the value estimate. Then some changes to export the model.

On the C# side some changes so that the value estimate can be retrieved. I modified BehaviourParameters so that it has three models because most of the time I only need either action or valueEstimate and it is very expensive querying both.",thanks fork branch non value estimate main change add identifier value estimate export model side value estimate three time need either action expensive querying,issue,positive,negative,neutral,neutral,negative,negative
758284746,"Hi @yoheigon 

@Ihsees is correct in pointing out that no_graphics=True will prevent visual observations from being sent correctly.  Please set this flag to False.",hi correct pointing prevent visual sent correctly please set flag false,issue,negative,negative,negative,negative,negative,negative
758280565,Thank you very much for pointing this out. I will update the tutorial.,thank much pointing update tutorial,issue,negative,positive,positive,positive,positive,positive
758265105,"Without the fix, the added test fails:
```python
    def test_pickle():
        # Make sure RunOptions is pickle-able.
        run_options = RunOptions()
        p = pickle.dumps(run_options)
>       pickle.loads(p)

ml-agents/mlagents/trainers/tests/test_settings.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = DefaultTrainerDict(<class 'mlagents.trainers.settings.TrainerSettings'>, {})
args = (<class 'mlagents.trainers.settings.TrainerSettings'>,)

    def __init__(self, *args):
>       super().__init__(TrainerSettings, *args)
E       TypeError: 'type' object is not iterable

ml-agents/mlagents/trainers/settings.py:684: TypeError
```",without fix added test python make sure self class class self super object iterable,issue,positive,positive,positive,positive,positive,positive
758209975,"> Hi,
> What version of the `com.unity.ml-agents` package are you using? I believe it's 1.0.6 (or another 1.0.x version), but the IActuator interface was added in the [`1.4.0-preview`](https://github.com/Unity-Technologies/ml-agents/releases/tag/release_7) package.
> 
> If that's the case, you should either:
> 
> * Update the package to a 1.4.0-preview or later (depending on your Unity version, you may need to change some package manager settings to be able to see preview packages).
> * Get the examples from a tag that corresponds to the 1.0.x package, for example [`com.unity.ml-agents_1.0.6`](https://github.com/Unity-Technologies/ml-agents/tree/com.unity.ml-agents_1.0.6)

If thats the case, would be amazing to update the tutorials that are using Unity.MLAgents.Actuators; like the second one on the documentation",hi version package believe another version interface added package case either update package later depending unity version may need change package manager able see preview get tag package example thats case would amazing update like second one documentation,issue,positive,positive,positive,positive,positive,positive
758180777,"Hi @brccabral,
Thanks for the clarification.  If that's the case, I'd recommend keeping these changes in your fork.  At the moment, we are not accepting new environments.  We appreciate your support and contributions to the project.  For now, I will close this pull request.  Please feel free to reply with any questions or concerns.
Cheers,
Chris",hi thanks clarification case recommend keeping fork moment new appreciate support project close pull request please feel free reply,issue,positive,positive,positive,positive,positive,positive
757452465,"Hello yoheigon,

if I remember correctly visual observations are only supported when a (game-)window is rendered.

By using ""no_graphics = true"" you are explicitly telling unity NOT to render a game-window.

Can you try with ""no_graphics=false"" and see if that fixes your issue?",hello remember correctly visual window true explicitly telling unity render try see issue,issue,negative,positive,positive,positive,positive,positive
757186975,"THANK YOU! It took me hours to finally made the connection:
I also had an old trained model assigned with my agent. After increasing the number of observations I got the error ""mismatch between vector observation size (163) and number of observations written (166)"". And when I increased the observation size by 3 to match 166, then the number of written ones also increased by 3!
It turned out that the old model could not work with these different circumstances. Of course it doesn't and I never intended it to be able to do that, but I just couldn't link the error message with the old model. Assigning ""none"" fixed the issue. Thank you for providing this hint.

I guess ML-Agent's Error Feedback could be a little better :)",thank took finally made connection also old trained model assigned agent increasing number got error mismatch vector observation size number written observation size match number written also turned old model could work different course never intended able could link error message old model none fixed issue thank providing hint guess error feedback could little better,issue,negative,positive,positive,positive,positive,positive
757071388,"Sorry that this has sat open for so long. Agent.OnEnable() and OnDisable() has been virtual since https://github.com/Unity-Technologies/ml-agents/pull/3590.

Academy was changed to a singleton, so it no longer has a specific FixedUpdate method and can't be inherited from.",sorry sat open long virtual since academy singleton longer specific method ca,issue,negative,negative,negative,negative,negative,negative
756912256,"Thanks, I made a few small cleanups and reverts. Will merge this when tests pass.",thanks made small merge pas,issue,negative,negative,neutral,neutral,negative,negative
756894725,"Sorry, we need to discuss this internally more, and some of the relevant people are on vacation this week.",sorry need discus internally relevant people vacation week,issue,negative,negative,neutral,neutral,negative,negative
756501885,"On https://github.com/Unity-Technologies/ml-agents/pull/4833 and https://github.com/Unity-Technologies/ml-agents/pull/4834, you can see the
```
remove yaml condition (shouldn't run test)    cd399bd
```
doesn't triggers the test, but the subsequent commit that modifies the generated file does trigger them.",see remove condition run test test subsequent commit file trigger,issue,negative,neutral,neutral,neutral,neutral,neutral
756493423,"Thanks, I think it's almost there! I left a few final comments but otherwise it looks pretty good.

Sorry to keep harping on the newlines, but I don't think a bug in pycharm's display is a worthwhile reason to change. I'm OK with them in the files that you have other changes in, but files where those are the only changes should be reverted (I can do this for you in git, as long as you have the setting on the PR to allow repo owners to push changes).",thanks think almost left final otherwise pretty good sorry keep think bug display reason change git long setting allow push,issue,positive,positive,neutral,neutral,positive,positive
755937069,"Hi @chriselion ,
I did the changes you requested, except the docstring, someone already have a tracking issue in jetbrains
https://youtrack.jetbrains.com/issue/PY-26281

This is what happens in my case:
![image](https://user-images.githubusercontent.com/130455/103863477-c61b3680-5075-11eb-8ff7-ee2dcf4c3b4b.png)
",hi except someone already issue case image,issue,negative,neutral,neutral,neutral,neutral,neutral
755784587,">  I would have to rewrite all the writers classes (ConsoleWriter, GaugeWriter, TensorboardWriter) as they all have their own write_stats() method.

If you add the property I suggested, you wouldn't need to add any extra logic in the writers. You could even call the property `stats_value`.

> Also, if the aggregation is SUM, there is no need to store AVG data in stats_dict[].

Maybe, but I'd rather not make the logic any more complicated.

> And, in the future, I could use other aggregations, MAX/MIN/PERCENTILE. Would the class StatsSummary have more and more fields?

We can add them as we need them.",would rewrite class method add property would need add extra logic could even call property also aggregation sum need store data maybe rather make logic complicated future could use would class add need,issue,negative,negative,negative,negative,negative,negative
755565938,"Yes, I can reproduce it in any environment. I use Python 3.7.9 and have everything properly installed. No errors or warnings what so ever. When I set the BehaviourType to Default and train the AI it does train how it should and sped up. But when I set the time scale to 1 (--time-scale 1) the Time.timeScale in unity is in fact set to 1, but the step count increases faster thus the game is faster than heuristic or inference and not at all time scale 1 but more like 2 or 3. Using a little script I wrote I evaluated how many steps happen in a second. When doing heuristic or inference, it is at 50 which should be time scale 1. When training at time scale 1, it is at about 150. Here is the script I wrote:
```
using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using Unity.MLAgents;

public class GameSpeed : MonoBehaviour
{
    public Agent agent;

    void Start()
    {
        StartCoroutine(OneSec());
    }

    private IEnumerator OneSec()
    {
        int stepCountStart = agent.StepCount;
        yield return new WaitForSecondsRealtime(1);
        Debug.Log(agent.StepCount - stepCountStart);
        StartCoroutine(OneSec());
    }
}
```",yes reproduce environment use python everything properly ever set default train ai train sped set time scale unity fact set step count faster thus game faster heuristic inference time scale like little script wrote many happen second heuristic inference time scale training time scale script wrote public class public agent agent void start private yield return new,issue,positive,positive,neutral,neutral,positive,positive
755484229,"> General feedback:
> 
> * Should this be called ObservationType instead of SensorType?
> * Is there a less generic term that we can use than ""(Sensor/Observation)Type""? That would also get around using `type` as a member variable on the python side, which is OK but discouraged since it's a builtin function.

Thanks for the code review, Chris. These are both good questions. The issue with `ObservationType` is that one of those types is ""Observation,"" so we'd need to change whatever that is called to disambiguate it. Vince actually renamed the `ObservationSpec` to a `SensorSpec`, so this naming for the ""type"" lines us.

I agree though that ""type"" is too generic, and potentially problematic wrt python naming. Definitely open to other more appropriate names for it.",general feedback instead le generic term use type would also get around type member variable python side since function thanks code review good issue one observation need change whatever actually naming type u agree though type generic potentially problematic python naming definitely open appropriate,issue,positive,positive,positive,positive,positive,positive
755459411,"General feedback:
* Should this be called ObservationType instead of SensorType?
* Is there a less generic term that we can use than ""(Sensor/Observation)Type""? That would also get around using `type` as a member variable on the python side, which is OK but discouraged since it's a builtin function.",general feedback instead le generic term use type would also get around type member variable python side since function,issue,negative,positive,neutral,neutral,positive,positive
755063245,"Hi @chriselion 
In my commit ""b7328ba"" I did just as you mentioned in Option 1.

```
class StatsSummary(NamedTuple):
    mean: float
    std: float
    sum: float
    num: int
    aggregation: StatsAggregationMethod

    @staticmethod
    def empty() -> ""StatsSummary"":
        return StatsSummary(0.0, 0.0, 0.0, 0, StatsAggregationMethod.AVERAGE)
```

but I would have to rewrite all the writers classes (```ConsoleWriter```, ```GaugeWriter```, ```TensorboardWriter```) as they all have their own ```write_stats()``` method.
By renaming it to ""stats_value"", all three classes get the implementation without major changes.
Also, if the aggregation is SUM, there is no need to store AVG data in ```stats_dict[]```.
And, in the future, I could use other aggregations, MAX/MIN/PERCENTILE.
Would the class StatsSummary have more and more fields?

Option 2 would not give much flexibility to add other aggregations, we are writing our own implementations, but numpy already have them implemented.",hi commit option class mean float float sum float aggregation empty return would rewrite class method three class get implementation without major also aggregation sum need store data future could use would class option would give much flexibility add writing already,issue,negative,negative,neutral,neutral,negative,negative
754984444,"Hi @stickseloni,
Thanks for pointing this out (as well as the other PR). 

On the `master` branch, we no longer support TensorFlow, so this section is no longer necessary. We missed removing it when we made the other changes, but I made a [PR](https://github.com/Unity-Technologies/ml-agents/pull/4826) to remove it.

If it's OK with you, I'll make a PR on the `release_12_branch` that encapsulates this change and the other one. Otherwise, you'll need to redo them by branching from there and making a PR with that as the target branch.",hi thanks pointing well master branch longer support section longer necessary removing made made remove make change one otherwise need redo branching making target branch,issue,positive,positive,neutral,neutral,positive,positive
754742174,Hey @22ms - this seems worth looking into. Have you been able to reproduce with one of the example environments? ,hey worth looking able reproduce one example,issue,negative,positive,positive,positive,positive,positive
754740163,Just a heads-up - we currently don't support Python 3.9 as some of our dependencies (e.g. PyTorch 1.7.0) don't support it yet. But feel free to try!,currently support python support yet feel free try,issue,positive,positive,positive,positive,positive,positive
754135046,"""you may only define one of `paths` and `paths-ignore` for a single event"" from https://github.com/Unity-Technologies/ml-agents/actions/runs/461597735

So guess we won't be doing this...",may define one single event guess wo,issue,negative,negative,neutral,neutral,negative,negative
753512486,"I was able to train the agent to go to the Goal side, but the agent always selects the goal on the right, it doesn't care if the target is X or O. I added the Walls to avoid falling

![image](https://user-images.githubusercontent.com/130455/103463963-1f394200-4ce5-11eb-9386-afd6f9c70f80.png)
",able train agent go goal side agent always goal right care target added avoid falling image,issue,negative,positive,positive,positive,positive,positive
753475183,"I think the issue was related to the python version. Using 3.8 and 3.7 seems to work fine:
![image](https://user-images.githubusercontent.com/11029629/103458466-db583400-4d08-11eb-8f1a-0d2e1907bd7e.png)
",think issue related python version work fine image,issue,negative,positive,positive,positive,positive,positive
752893901,"`shammis-MacBook-Air:mlagents shammiseth$ mlagents-learn config/ppo/3DBall.yaml --run-id=first3DBallRun --force


                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓

        
 Version information:
  ml-agents: 0.23.0,
  ml-agents-envs: 0.23.0,
  Communicator API: 1.3.0,
  PyTorch: 1.7.1
2020-12-31 08:50:55 INFO [learn.py:275] run_seed set to 4927
2020-12-31 08:50:55 INFO [environment.py:205] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
2020-12-31 08:51:55 INFO [subprocess_env_manager.py:186] UnityEnvironment worker 0: environment stopping.
2020-12-31 08:51:55 INFO [trainer_controller.py:85] Saved Model
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.7/bin/mlagents-learn"", line 11, in <module>
    load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')()
  File ""/Users/shammiseth/Projects/MLagents/mlagents/ml-agents/mlagents/trainers/learn.py"", line 280, in main
    run_cli(parse_command_line())
  File ""/Users/shammiseth/Projects/MLagents/mlagents/ml-agents/mlagents/trainers/learn.py"", line 276, in run_cli
    run_training(run_seed, options)
  File ""/Users/shammiseth/Projects/MLagents/mlagents/ml-agents/mlagents/trainers/learn.py"", line 153, in run_training
    tc.start_learning(env_manager)
  File ""/Users/shammiseth/Projects/MLagents/mlagents/ml-agents-envs/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/Users/shammiseth/Projects/MLagents/mlagents/ml-agents/mlagents/trainers/trainer_controller.py"", line 174, in start_learning
    self._reset_env(env_manager)
  File ""/Users/shammiseth/Projects/MLagents/mlagents/ml-agents-envs/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/Users/shammiseth/Projects/MLagents/mlagents/ml-agents/mlagents/trainers/trainer_controller.py"", line 109, in _reset_env
    env_manager.reset(config=new_config)
  File ""/Users/shammiseth/Projects/MLagents/mlagents/ml-agents/mlagents/trainers/env_manager.py"", line 67, in reset
    self.first_step_infos = self._reset_env(config)
  File ""/Users/shammiseth/Projects/MLagents/mlagents/ml-agents/mlagents/trainers/subprocess_env_manager.py"", line 299, in _reset_env
    ew.previous_step = EnvironmentStep(ew.recv().payload, ew.worker_id, {}, {})
  File ""/Users/shammiseth/Projects/MLagents/mlagents/ml-agents/mlagents/trainers/subprocess_env_manager.py"", line 95, in recv
    raise env_exception
mlagents_envs.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that :
	 The environment does not need user interaction to launch
	 The Agents' Behavior Parameters > Behavior Type is set to ""Default""
	 The environment and the Python interface have compatible versions.
shammis-MacBook-Air:mlagents shammiseth$ `

HI I am not able to resolve this issue can someone tell me how to change the port number because I think unity is not able to communicate with post no 5004 on Mac. ",force version information communicator set listening port start training pressing play button unity editor worker environment stopping saved model recent call last file line module file line main file line file line file line wrapped return file line file line wrapped return file line file line reset file line file line raise unity environment took long respond make sure environment need user interaction launch behavior behavior type set default environment python interface compatible hi able resolve issue someone tell change port number think unity able communicate post mac,issue,positive,positive,positive,positive,positive,positive
752890054,"HI I get the same error on Mac with release build 12 and unity version 2020.. I also get popup in unity when I press play "" libgrpc_csharp_ext.x64.bundle” cannot be opened because the developer cannot be verified. with delete and cancel button. But still not able to connect to post 5004 and my firewall is disabled. ",hi get error mac release build unity version also get unity press play developer delete cancel button still able connect post disabled,issue,negative,positive,positive,positive,positive,positive
752875763,"Hello I too can't find config/trainer_config.yaml with release 12 under the config folder I have pop, imitation, sac folder. I am trying to follow the tutorial from this page https://github.com/Unity-Technologies/ml-agents/blob/release_2_verified_docs/docs/Getting-Started.md",hello ca find release folder pop imitation sac folder trying follow tutorial page,issue,negative,negative,negative,negative,negative,negative
752693481,Hi @Graph-IKE - you shouldn't have to import the ML-Agents folder into your project anymore. Have you tried installing it as a package using the Unity Package Manager? (under Window -> Package Manager),hi import folder project tried package unity package manager window package manager,issue,negative,neutral,neutral,neutral,neutral,neutral
752133152,"thanks @clankill3r for your note.  we def recognize that installing python, especially for Unity developers who do not actively use python, can be challenging.

We've made a note of your request and we are working on an internal prototype that could potentially wrap the entire installation process without having to install Python.",thanks note recognize python especially unity actively use python made note request working internal prototype could potentially wrap entire installation process without install python,issue,positive,positive,neutral,neutral,positive,positive
752079439,"Ok, now I'm at the step **Install the mlagents Python package**

https://github.com/Unity-Technologies/ml-agents/blob/release_12_docs/docs/Installation.md#install-the-mlagents-python-package

Which states:

> On Windows, you'll have to install the PyTorch package separately prior to installing ML-Agents.

What is meant by ML-Agents is unclear, if it is the unity project then it is odd that we have to install the packages from within unity in one of the steps above.

I also installed the latest Visual C++ Redistributable.

Ok, I got it working now. I think the rest will also work. If not I will post here. I hope this post is helpfull to someone, and also shines light on how complicated this is to get working.
I really hope this repository ships someday with python and it's packages  included.
Just like blender, inkscape and more programs just include python within their program to avoid installation issues.

This whole thread is not really a issue with this repository itself, I think python is not in a healthy state in general.
It would however be a huge benefit if those problems could be encapsulated.",step install python package install package separately prior meant unclear unity project odd install within unity one also latest visual got working think rest also work post hope post someone also light complicated get working really hope repository someday python included like blender include python within program avoid installation whole thread really issue repository think python healthy state general would however huge benefit could,issue,positive,positive,positive,positive,positive,positive
752065133,"Ok I tried the `--user` like this: 

> pip install --upgrade --user pip

Which gives me:

> ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.

Ok, now I did run Powershell as an administrator and did the command: `pip install --upgrade pip`.
Which resulted in 

> Requirement already satisfied: pip in c:\python-envs\ml-agents\lib\site-packages (20.3.3)

So that seems to be ok.



",tried user like pip install upgrade user pip error perform user install user visible run administrator command pip install upgrade pip requirement already satisfied pip,issue,negative,positive,positive,positive,positive,positive
752063847,"At this point the `pip3` command works.

I am now ready for the  **Install the mlagents Python package** step.
https://github.com/Unity-Technologies/ml-agents/blob/release_12_docs/docs/Installation.md#install-the-mlagents-python-package

Which detours me to the **guide on Virtual Environments** https://github.com/Unity-Technologies/ml-agents/blob/release_12_docs/docs/Using-Virtual-Environment.md

Here I am asked again to install pip. 

https://github.com/Unity-Technologies/ml-agents/blob/release_12_docs/docs/Using-Virtual-Environment.md#installing-pip-required

This time using `curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py` but since I already did a step to get pip working, I won't follow this step.
Note, at this point I don't have a `python3 ` command! But `python` just points at the python 3.7 version. This is, as told above after a fresh install of python 3.7 following the link provided.

I created a python-env using `python -m venv python-envs\ml-agents`.
I activated the env, not that this issue that I had before is gone at this point:
https://github.com/Unity-Technologies/ml-agents/issues/4787

Ok now i'm at step 4.

> 4. Upgrade to the latest pip version using `pip install --upgrade pip`

This goes wrong:

> 
> (ml-agents) PS C:\> pip install --upgrade pip
> Collecting pip
>   Using cached pip-20.3.3-py2.py3-none-any.whl (1.5 MB)
> Installing collected packages: pip
>   Attempting uninstall: pip
>     Found existing installation: pip 20.1.1
>     Uninstalling pip-20.1.1:
>       Successfully uninstalled pip-20.1.1
> ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'C:\\Users\\clank\\AppData\\Local\\Temp\\pip-uninstall-aq_460x2\\pip.exe'
> Consider using the `--user` option or check the permissions.

I continue in the next post.



",point pip command work ready install python package step guide virtual install pip time curl since already step get pip working wo follow step note point python command python python version told fresh install python following link provided python issue gone point step upgrade latest pip version pip install upgrade pip go wrong pip install upgrade pip pip collected pip pip found installation pip successfully uninstalled error could install due access consider user option check continue next post,issue,negative,positive,positive,positive,positive,positive
752053607,"Since `pip3` is not included I went to the link with instructions.

![pip3_instructions](https://user-images.githubusercontent.com/738650/103281717-decf8080-49d3-11eb-93a3-d4e675b3c5a3.png)

The link points to a page only usefull if you are using **linux**.

![installing_pip](https://user-images.githubusercontent.com/738650/103282598-96659200-49d6-11eb-8ba1-60b92e224839.png)

But if I go two headers higher in the table of contents I end up here:
https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/

Using this command:

`py -m pip --version`

I get:

> pip 20.1.1 from C:\Users\clank\AppData\Local\Programs\Python\Python37\lib\site-packages\pip (python 3.7)

So that seems to be good.
I also made sure pip is up to date using: `py -m pip install --upgrade pip`

",since pip included went link link page go two higher table content end command pip version get pip python good also made sure pip date pip install upgrade pip,issue,positive,positive,positive,positive,positive,positive
752046998,"Ok, I removed all python versions from my machine and I will list al the crap I get into here.

I'm at the step:

> Install Python 3.6.1 or Higher

https://github.com/Unity-Technologies/ml-agents/blob/release_12_docs/docs/Installation.md#install-python-361-or-higher

I followed the link and installed 3.7, I also marked the option to add it to the path.
In case someone is wondering, it got installed here:
C:\Users\clank\AppData\Local\Programs\Python\Python37

`pip3` is not included.

",removed python machine list al crap get step install python higher link also marked option add path case someone wondering got pip included,issue,negative,negative,negative,negative,negative,negative
751813642,We removed the requirement for recording demonstrations from the editor a while ago in this PR: https://github.com/Unity-Technologies/ml-agents/pull/3488/files,removed requirement recording editor ago,issue,negative,neutral,neutral,neutral,neutral,neutral
751813598,Sorry you never got a response on this before. We removed the requirement for recording demonstrations from the editor a while ago in this PR: https://github.com/Unity-Technologies/ml-agents/pull/3488/files,sorry never got response removed requirement recording editor ago,issue,negative,negative,negative,negative,negative,negative
751708208,"@lkuich Hi, Loren thanks for your advice to solve the problem!
I tried to download your modified unity package, but it seems something goes wrong with your upload host.
I'm in a rush now and honestly don't know anything about building DLL files out of source, would you do me a favor and give me a download link to your modified DLL file or send it to my email?! 
my email:hajiarab1988@gmail.com
thank you",hi thanks advice solve problem tried unity package something go wrong host rush honestly know anything building source would favor give link file send thank,issue,positive,positive,neutral,neutral,positive,positive
751699013,"After trying to also run this locally I'm suspecting that the enlighten threads message is not the cause. I think it appears just before closing the process, maybe only in server builds. Since when I'm running a server build locally on windows the process freezes and as soon as I stop it with ctrl+C I get the Enlighen threads message.
I still have no idea why the training was crashing but it's probably something different. I think from our part we can drop this issue for now. I will let you know if I find anything new.",trying also run locally enlighten message cause think process maybe server since running server build locally process soon stop get message still idea training probably something different think part drop issue let know find anything new,issue,negative,positive,neutral,neutral,positive,positive
751615811,"Same problem When I run  [unity3d_env_local.py](https://github.com/ray-project/ray/blob/master/rllib/examples/unity3d_env_local.py)(rllib example for unity3d environment).

**Describe the bug**
If I run the [unity3d_env_local.py](https://github.com/ray-project/ray/blob/master/rllib/examples/unity3d_env_local.py)(rllib example for unity3d environment) it returns the error like below:
```
Failure # 1 (occurred at 2020-12-28_15-57-15)
Traceback (most recent call last):
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/tune/trial_runner.py"", line 519, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py"", line 497, in fetch_result
    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/_private/client_mode_hook.py"", line 47, in wrapper
    return func(*args, **kwargs)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/worker.py"", line 1391, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(AttributeError): [36mray::PPO.train()[39m (pid=24483, ip=192.168.0.176)
  File ""python/ray/_raylet.pyx"", line 479, in ray._raylet.execute_task
  File ""python/ray/_raylet.pyx"", line 431, in ray._raylet.execute_task.function_executor
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/agents/trainer.py"", line 523, in train
    raise e
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/agents/trainer.py"", line 509, in train
    result = Trainable.train(self)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/tune/trainable.py"", line 183, in train
    result = self.step()
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py"", line 148, in step
    res = next(self.train_exec_impl)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 756, in __next__
    return next(self.built_iterator)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 783, in apply_foreach
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 783, in apply_foreach
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 843, in apply_filter
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 843, in apply_filter
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 783, in apply_foreach
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 783, in apply_foreach
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 783, in apply_foreach
    for item in it:
  [Previous line repeated 1 more time]
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 876, in apply_flatten
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 828, in add_wait_hooks
    item = next(it)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 783, in apply_foreach
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 783, in apply_foreach
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 783, in apply_foreach
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/execution/rollout_ops.py"", line 69, in sampler
    yield workers.local_worker().sample()
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 645, in sample
    batches = [self.input_reader.next()]
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py"", line 94, in next
    batches = [self.get_data()]
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py"", line 216, in get_data
    item = next(self.rollout_provider)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py"", line 663, in _env_runner
    base_env.send_actions(actions_to_send)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/env/base_env.py"", line 399, in send_actions
    obs, rewards, dones, infos = env.step(agent_dict)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/env/unity3d_env.py"", line 129, in step
    action_dict[key])
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/mlagents_envs/environment.py"", line 356, in set_action_for_agent
    action = action_spec._validate_action(action, None, behavior_name)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/mlagents_envs/base_env.py"", line 404, in _validate_action
    if actions.continuous.shape != _expected_shape:
AttributeError: 'numpy.ndarray' object has no attribute 'continuous'
```
**To Reproduce**
I followed the steps in the comments in [unity3d_env_local.py](https://github.com/ray-project/ray/blob/master/rllib/examples/unity3d_env_local.py). I run the script using torch framework but tf also returns the same error.
1) Install Unity3D and `pip install mlagents`.
2) Open the Unity3D Editor and load an example scene from the following
   ml-agents pip package location:
   `.../ml-agents/Project/Assets/ML-Agents/Examples/`
3) change default framework from tf to torch
4) run the script(3DBall)

**Console logs / stack traces**
```
cd /home/jinprelude/Documents/rllib ; /usr/bin/env /home/jinprelude/anaconda3/envs/rllib/bin/python /home/jinprelude/.vscode/extensions/ms-python.python-2020.12.424452561/pythonFiles/lib/python/debugpy/launcher 44291 -- /home/jinprelude/Documents/rllib/run_unity3d.py 
WARNING:tensorflow:From /home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2020-12-28 15:56:50,417 INFO services.py:1171 -- View the Ray dashboard at http://127.0.0.1:8265
== Status ==
Memory usage on this node: 8.3/62.8 GiB
Using FIFO scheduling algorithm.
Resources requested: 1/8 CPUs, 0/1 GPUs, 0.0/33.59 GiB heap, 0.0/11.57 GiB objects (0/1.0 accelerator_type:GTX)
Result logdir: /home/jinprelude/ray_results/PPO
Number of trials: 1/1 (1 RUNNING)


(pid=24483) WARNING:tensorflow:From /home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
(pid=24483) Instructions for updating:
(pid=24483) non-resource variables are not supported in the long term
(pid=24483) 2020-12-28 15:56:54,184     INFO trainer.py:633 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
(pid=24483) No game binary provided, will use a running Unity editor instead.
(pid=24483) Make sure you are pressing the Play (|>) button in your editor to start.
(pid=24483) 2020-12-28 15:57:14,621     INFO trainable.py:102 -- Trainable.setup took 20.438 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
(pid=24483) 2020-12-28 15:57:14,621     WARNING util.py:43 -- Install gputil for GPU system monitoring.
(pid=24483) Created UnityEnvironment for port 5004
(pid=24483) 2020-12-28 15:57:14,710     WARNING deprecation.py:30 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!
2020-12-28 15:57:15,231 ERROR trial_runner.py:607 -- Trial PPO_unity3d_dd8d1_00000: Error processing event.
Traceback (most recent call last):
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/tune/trial_runner.py"", line 519, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py"", line 497, in fetch_result
    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/_private/client_mode_hook.py"", line 47, in wrapper
    return func(*args, **kwargs)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/worker.py"", line 1391, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(AttributeError): ray::PPO.train() (pid=24483, ip=192.168.0.176)
  File ""python/ray/_raylet.pyx"", line 479, in ray._raylet.execute_task
  File ""python/ray/_raylet.pyx"", line 431, in ray._raylet.execute_task.function_executor
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/agents/trainer.py"", line 523, in train
    raise e
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/agents/trainer.py"", line 509, in train
    result = Trainable.train(self)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/tune/trainable.py"", line 183, in train
    result = self.step()
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py"", line 148, in step
    res = next(self.train_exec_impl)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 756, in __next__
    return next(self.built_iterator)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 783, in apply_foreach
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 783, in apply_foreach
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 843, in apply_filter
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 843, in apply_filter
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 783, in apply_foreach
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 783, in apply_foreach
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 783, in apply_foreach
    for item in it:
  [Previous line repeated 1 more time]
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 876, in apply_flatten
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 828, in add_wait_hooks
    item = next(it)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 783, in apply_foreach
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 783, in apply_foreach
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/util/iter.py"", line 783, in apply_foreach
    for item in it:
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/execution/rollout_ops.py"", line 69, in sampler
    yield workers.local_worker().sample()
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 645, in sample
    batches = [self.input_reader.next()]
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py"", line 94, in next
    batches = [self.get_data()]
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py"", line 216, in get_data
    item = next(self.rollout_provider)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py"", line 663, in _env_runner
    base_env.send_actions(actions_to_send)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/env/base_env.py"", line 399, in send_actions
    obs, rewards, dones, infos = env.step(agent_dict)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/ray/rllib/env/unity3d_env.py"", line 129, in step
    action_dict[key])
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/mlagents_envs/environment.py"", line 356, in set_action_for_agent
    action = action_spec._validate_action(action, None, behavior_name)
  File ""/home/jinprelude/anaconda3/envs/rllib/lib/python3.7/site-packages/mlagents_envs/base_env.py"", line 404, in _validate_action
    if actions.continuous.shape != _expected_shape:
AttributeError: 'numpy.ndarray' object has no attribute 'continuous'
== Status ==
Memory usage on this node: 8.5/62.8 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/33.59 GiB heap, 0.0/11.57 GiB objects (0/1.0 accelerator_type:GTX)
Result logdir: /home/jinprelude/ray_results/PPO
Number of trials: 1/1 (1 ERROR)
Number of errored trials: 1
+-------------------------+--------------+------------------------------------------------------------------------------------------+
| Trial name              |   # failures | error file                                                                               |
|-------------------------+--------------+------------------------------------------------------------------------------------------|
| PPO_unity3d_dd8d1_00000 |            1 | /home/jinprelude/ray_results/PPO/PPO_unity3d_dd8d1_00000_0_2020-12-28_15-56-52/error.txt |
+-------------------------+--------------+------------------------------------------------------------------------------------------+

== Status ==
Memory usage on this node: 8.5/62.8 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/33.59 GiB heap, 0.0/11.57 GiB objects (0/1.0 accelerator_type:GTX)
Result logdir: /home/jinprelude/ray_results/PPO
Number of trials: 1/1 (1 ERROR)
Number of errored trials: 1
+-------------------------+--------------+------------------------------------------------------------------------------------------+
| Trial name              |   # failures | error file                                                                               |
|-------------------------+--------------+------------------------------------------------------------------------------------------|
| PPO_unity3d_dd8d1_00000 |            1 | /home/jinprelude/ray_results/PPO/PPO_unity3d_dd8d1_00000_0_2020-12-28_15-56-52/error.txt |
+-------------------------+--------------+------------------------------------------------------------------------------------------+
```
**Screenshots**
If applicable, add screenshots to help explain your problem.

**Environment (please complete the following information):**
- Unity Version: Unity 2020.2.1f1
- OS + version: Ubuntu 18.04
- _ML-Agents version_: Release 12(ML-Agents : 1.7.2, ML-Agents extensions : 0.0.1, mlagents : 0.23.0, mlagents-envs : 0.23.0)
- _Torch version_: 1.7.1
- _ray version_: 1.2.0.dev0
- _Environment_: 3DBall unity example environment.

Thanks for your hard work!",problem run example environment describe bug run example environment error like failure recent call last file line result trial file line result file line wrapper return file line get raise file line file line file line train raise file line train result self file line train result file line step next file line return next file line item file line item file line item file line item file line item file line item file line item previous line repeated time file line item file line item next file line item file line item file line item file line sampler yield file line sample file line next file line item next file line file line file line step key file line action action none file line object attribute reproduce run script torch framework also install pip install open editor load example scene following pip package location change default framework torch run script console stack warning removed future version long term view ray dashboard status memory usage node gib fifo algorithm gib heap gib result number running warning removed future version long term current warn information set use game binary provided use running unity editor instead make sure pressing play button editor start took trainable slow initialize consider setting reduce actor creation warning install system port warning use instead raise error future error trial error event recent call last file line result trial file line result file line wrapper return file line get raise ray file line file line file line train raise file line train result self file line train result file line step next file line return next file line item file line item file line item file line item file line item file line item file line item previous line repeated time file line item file line item next file line item file line item file line item file line sampler yield file line sample file line next file line item next file line file line file line step key file line action action none file line object attribute status memory usage node gib fifo algorithm gib heap gib result number error number trial name error file status memory usage node gib fifo algorithm gib heap gib result number error number trial name error file applicable add help explain problem environment please complete following information unity version unity o version release dev unity example environment thanks hard work,issue,negative,negative,neutral,neutral,negative,negative
750710310,"@andrewcoh OK , I check the source code and figure it out. Thank you for you information. 
@18goldr Hi, based on my codes above, I try the following things to fix it. 

it should be 

    from mlagents_envs.base_env import ActionTuple
    if i < 10:

        action = ActionTuple(np.array([[1.0,0.0]], dtype=np.float32))

        env.set_actions(single, action)

    if i == 15:

        action = ActionTuple(np.array([[0.0,1.0]], dtype=np.float32))

        env.set_actions(single, action)

    env.step()


",check source code figure thank information hi based try following fix import action single action action single action,issue,negative,positive,neutral,neutral,positive,positive
750707412,"@andrewcoh Hi, Thank you for your advice. I have encountered the same problem. I look it up in the API description following your advice. I do find the description, which is:

""Set Actions :env.set_actions(behavior_name: str, action: ActionTuple) Sets the actions for a whole agent group. action is an ActionTuple, which is made up of a 2D np.array of dtype=np.int32 for discrete actions, and dtype=np.float32 for continuous actions. The first dimension of np.array in the tuple is the number of agents that requested a decision since the last call to env.step(). The second dimension is the number of discrete or continuous actions for the corresponding array.""

However there is no more information.
Could you reveal some examples to define the tuple. I can not manage it.

I have some codes, which can work at previous version of ml tool kit. 
    if i < 10:
        action = np.array([[1.0,0.0]], dtype=np.float32)
        env.set_actions(single, action)
    if i == 15:
        action = np.array([[0.0,1.0]], dtype=np.float32)
        env.set_actions(single, action)
    env.step()

 Could help me to modify it to work?
    ",hi thank advice problem look description following advice find description set action whole agent group action made discrete continuous first dimension number decision since last call second dimension number discrete continuous corresponding array however information could reveal define manage work previous version tool kit action single action action single action could help modify work,issue,negative,positive,neutral,neutral,positive,positive
750618622,"> Please add a unit test that would have caught this.

This is an issue caused by tensor's underlying memory allocation and so far I can only reproduce this error on a GPU machine. 
We already have tests for this but the CI doesn't run on GPUs. Not sure if it's possible to do that.",please add unit test would caught issue tensor underlying memory allocation far reproduce error machine already run sure possible,issue,negative,positive,positive,positive,positive,positive
750613646,Please add a unit test that would have caught this.,please add unit test would caught,issue,negative,neutral,neutral,neutral,neutral,neutral
750494447,Glad that fixed the issue! We'll get it merged in to the next hotfix. ,glad fixed issue get next,issue,negative,positive,positive,positive,positive,positive
750490138,Just tried out the change from @dongruoping and it fixed the error for me. I no longer get the error on Hallway nor the other project I was originally trying to use it on. Nice!,tried change fixed error longer get error hallway project originally trying use nice,issue,negative,positive,positive,positive,positive,positive
750485362,"I'm not using ""--cpu"", so I think that means I'm using GPU training right? Yes Windows 10. I am able to replicate with the Hallway example.
`mlagents-learn config\ppo\Hallway.yaml --run-id hallway`",think training right yes able replicate hallway example hallway,issue,negative,positive,positive,positive,positive,positive
750462889,"Hi @18goldr 

Are you using a custom trainer, policy architecture or env_manager? The LLAPI expects a base_env.ActionTuple as an input to set_action/set_action_for_agent which is new to this release.",hi custom trainer policy architecture input new release,issue,negative,positive,positive,positive,positive,positive
750458926,"It seems to be a re-emergence of this bug, which happened with Torch 1.4.0: https://github.com/Unity-Technologies/ml-agents/issues/4555 
I haven't been able to replicate on Mac with Torch 1.7.0 and the Hallway environment that uses discrete-LSTM - so there must be something different about my setup 🤔 ",bug torch able replicate mac torch hallway environment must something different setup,issue,negative,positive,positive,positive,positive,positive
750457228,Are you by any chance using GPU training? Windows? Just trying to narrow the bug down. ,chance training trying narrow bug,issue,negative,negative,negative,negative,negative,negative
750419073,Thank for the info @akTwelve.  This may help narrow the bug down.,thank may help narrow bug,issue,positive,negative,negative,negative,negative,negative
750408354,"I got the same error on a separate project as soon as I added ""memory"" to my yaml file. Same Torch version and ML-Agents release.",got error separate project soon added memory file torch version release,issue,negative,neutral,neutral,neutral,neutral,neutral
749850438,"Hi @surfnerd,
I don't have anything planned for the moment. 
I was curious to see how the Render feature works with ML, not only about having multiple goals, but what if I added collectibles like coins or power ups. 
Just wanted to understand how the Render works and ended up creating the Multi Goals scene. 
Thanks, 
Bruno ",hi anything moment curious see render feature work multiple added like power understand render work ended scene thanks,issue,positive,positive,neutral,neutral,positive,positive
749839272,"Hi @brccabral,
Thanks for your contribution!  We are curious if you have a larger feature that you were working towards or if you just wanted to tinker and try things out.  
Cheers,
Chris",hi thanks contribution curious feature working towards tinker try,issue,positive,positive,neutral,neutral,positive,positive
749690215,"Hi @Faulo,
Thanks for finding this.  We will fix it ASAP",hi thanks finding fix,issue,negative,positive,positive,positive,positive,positive
749594645,Thanks - the new doc package makes is easier to see certain warnings. This project is just for our internal use.,thanks new doc package easier see certain project internal use,issue,positive,positive,positive,positive,positive,positive
749228506,"Hi @clankill3r,
You should be able to just type those commands into the cmd prompt on windows (with the correct paths, of course).  Let me know if you run in to other issues.  

I've tracked this internally as MLA-1665 and will update this issue once it's resolved.
Cheers,
Chris",hi able type prompt correct course let know run tracked internally update issue resolved,issue,negative,positive,positive,positive,positive,positive
749124382,"Hi @clankill3r,
I agree, that is my fault as I created that folder.  I have filed this as MLA-1663 internally and we will update this issue once we have resolved the issue.
Cheers,
Chris
",hi agree fault folder internally update issue resolved issue,issue,negative,neutral,neutral,neutral,neutral,neutral
749123000,"Hi @Sh3yn3,
We have logged this internally as  MLA-1662 and will update this ticket once we have a resolution for it.  Thank you for your feedback.",hi logged internally update ticket resolution thank feedback,issue,negative,neutral,neutral,neutral,neutral,neutral
749084681,"I think this is an issue with Enlighten threads and not with ML-Agents. It would help me if you tried to run 30 environments **without ML-Agents** for 10-20 hours. Depending on wether the error happens or not, we will know if it is a ML-Agents issue or not. 
",think issue enlighten would help tried run without depending wether error know issue,issue,positive,neutral,neutral,neutral,neutral,neutral
748456567,"I'm having the same issue it seems, usually happens after say 10-20 hours of training with 30 environments.
It's strange because we're using 2D only. We are using 2D lights but that has nothing to do with enlighten as far as I understand.
I will try to change our lighting settings and see if it helps. Will post here when I know more.",issue usually say training strange nothing enlighten far understand try change lighting see post know,issue,negative,negative,neutral,neutral,negative,negative
747884801,"awww :( . The thing is I think if ML-Agents can support the inference outside unity engine. This will make implementing reinforcement learning on real-world things ( robotics ) so so much easier @chriselion and powerful. This is my feedback for the Unity ML-Agents :)
   ",thing think support inference outside unity engine make reinforcement learning much easier powerful feedback unity,issue,positive,positive,positive,positive,positive,positive
747776693,"Sorry this never got answered before. Using torch means that all models are output in .onnx format. You're welcome to try it, but we don't provide and support for doing inference outside of the Unity engine.",sorry never got torch output format welcome try provide support inference outside unity engine,issue,positive,positive,positive,positive,positive,positive
747557054,"Nope @nullbyte91, But I think I would try again soon because ML-Agents now supports PyTorch. ",nope think would try soon,issue,negative,neutral,neutral,neutral,neutral,neutral
747547665,@Shubhamai Did you figure it out the deployment flow from the trained model?,figure deployment flow trained model,issue,negative,neutral,neutral,neutral,neutral,neutral
745303621,"@Hylus I got the project to work with the above instrucions on 2019.4.9f1.

I downloaded the project from drive, removed the .meta files with 
```
cd /Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle
find . -name '*.meta' -delete
```

 and ran xattr in `./Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX`",got project work project drive removed find ran,issue,negative,neutral,neutral,neutral,neutral,neutral
744761972,Remaining yamato tests aren't needed since none of the python changes affect runtime.,since none python affect,issue,negative,neutral,neutral,neutral,neutral,neutral
744754173,"> Not sure if there are any others of the Brain Parameters...

Did a quick scan through all images and updated all that I found",sure brain quick scan found,issue,negative,positive,positive,positive,positive,positive
744694425,"Hi @brccabral 

Thanks for making this PR. It looks like you also change the default number of obstacles to 2 in the inspector here. Could you change that back to 1? Otherwise, I think our users will appreciate this additional flexibility.",hi thanks making like also change default number inspector could change back otherwise think appreciate additional flexibility,issue,positive,positive,neutral,neutral,positive,positive
744675006,"Seems like there is a fair amount of unity-file duplication here, for prefabs and materials. I think we can likely get to the point of re-using almost all assets for this new task variant. ",like fair amount duplication think likely get point almost asset new task variant,issue,positive,positive,positive,positive,positive,positive
744674163,I wonder if it is more straightforward to modify the variable speed walker than to introduce this new variant? ,wonder straightforward modify variable speed walker introduce new variant,issue,negative,positive,positive,positive,positive,positive
744024037,"Hi! Out of curiousity, how do you even handle your variable observation count on the neural network side of things? The only way I'm aware of is some kind of recurrent sequence encoder. I suppose you have implemented a custom neural network layout in python?",hi even handle variable observation count neural network side way aware kind recurrent sequence suppose custom neural network layout python,issue,positive,positive,positive,positive,positive,positive
742854952,"oh man, nvm... its late... i restarted WinSCP and now the file is shown correctly... so THank you very much, this fixes literly all my problems. Thank you!",oh man late file shown correctly thank much thank,issue,positive,negative,neutral,neutral,negative,negative
742849932,"i just interrupted it, quick interuupts will write the file, but if i let it run for 10 minutes it states aborted and the file is not written
",interrupted quick write file let run aborted file written,issue,negative,positive,positive,positive,positive,positive
742823936,"im force overwriting the same test set, so  i rely on the last modified time provided by winscp. i was just bugged by the Aborted (core dumped) messageoccuring with that not written .onnx. maybe not saving it to that .onnx was just a onetime problem, i'm in the process of traing again and i now let it run uninterrupted. i'll see tomorrow if saving worked or if its reproducable. 
Thank you for your help!",force test set rely last time provided aborted core written maybe saving onetime problem process let run uninterrupted see tomorrow saving worked thank help,issue,negative,neutral,neutral,neutral,neutral,neutral
742821688,"According to that log, it looks like the ONNX file was indeed saved. Does it not appear in the listed location?",according log like file indeed saved appear listed location,issue,positive,neutral,neutral,neutral,neutral,neutral
742793791,"But it seemed to encounter a diffrent problem: the model is not exported as the last modified suggests. is that aborted message on the bottom related to a specifc failure?

![grafik](https://user-images.githubusercontent.com/34577718/101828907-7d359800-3b32-11eb-8b7c-7d072294894d.png)
",encounter problem model last aborted message bottom related failure,issue,negative,negative,negative,negative,negative,negative
742773657,"ah i think i got it now!

more data means more time to train but better representation of whats happening. If thats too big and we train longer, this will always scale its difficulty from here!
picking a smaller data amount to train, makes it less ""representing"" of the scene informations, but pytorch can train quick enough from it, that the scaling in upcoming training doesnt matter so much",ah think got data time train better representation whats happening thats big train longer always scale difficulty smaller data amount train le scene train quick enough scaling upcoming training doesnt matter much,issue,negative,positive,positive,positive,positive,positive
742770217,"ok i still dont get the concept. The buffer gets filled with our observations data. once the buffer is full it will get send to python and that trains with the data. wouldn't smaller buffer sizes trigger the sending process to start just much quicker? if i have husge buffers i can collect the data and the network trains as quickly as it can and i feed it liek a factory with data all the time. why is it faster to train if that buffer is much smaller?

But thats defintly it! Thank you so much!
![grafik](https://user-images.githubusercontent.com/34577718/101824454-815eb700-3b2c-11eb-8e2a-4b28d95824bb.png)

any downsides when i pick a buffer too small?",still dont get concept buffer filled data buffer full get send python data would smaller buffer size trigger sending process start much collect data network quickly feed factory data time faster train buffer much smaller thats thank much pick buffer small,issue,negative,positive,positive,positive,positive,positive
742757899,"In PPO the updating of the neural network happens when the buffer is filled up, and then the buffer is emptied. In SAC, once the buffer is filled (enough), then updating the network happens more or less every episode step, leading to a greater slow-down in terms of number of episodes. ",neural network buffer filled buffer sac buffer filled enough network le every episode step leading greater number,issue,negative,positive,positive,positive,positive,positive
742752774,"i have choosen the largest ""typical range"" provided in the wiki, as my robot shall clean the entire room with strategy. so i thoughed i need him to collect alot of experience to find a good startegy for a longterm activity.

I'll try siginifantly smaller batch and puffer and call results back, 
the non CNN training seems also to get steadily slower, just at a smaller haste, so that would make sense to be the same reason.
![grafik](https://user-images.githubusercontent.com/34577718/101821768-b406b080-3b28-11eb-867e-16581b4b2411.png)


But what exactly you mena by the network sampling? pytorch can't keep up with training on so much data which i send all the time? or does unity stop sending data if its too much?",typical range provided robot shall clean entire room strategy need collect experience find good activity try smaller batch puffer call back non training also get steadily smaller haste would make sense reason exactly network sampling ca keep training much data send time unity stop sending data much,issue,positive,positive,positive,positive,positive,positive
742747506,"Hi @simon-winter 

Thank you for providing all of this additional information. Due to how SAC and PPO work you will see different behaviors. In general, SAC will be slower, as it performs more training per episode step of the environment. What may be happening is that as the buffer gets larger, it eventually passes the minimum size to start sampling from it for network updates (10000 items). I would experiment with much smaller values for the buffer and batch and share the results. Are the actions discrete or continuous? If they are discrete, you can likely get away with an order of magnitude smaller values for these buffer parameters.",hi thank providing additional information due sac work see different general sac training per episode step environment may happening buffer eventually minimum size start sampling network would experiment much smaller buffer batch share discrete continuous discrete likely get away order magnitude smaller buffer,issue,positive,negative,neutral,neutral,negative,negative
742738074,"First, thank you for your help!

I inspected the environment closer and can describe as follows:

a few simple bool, Vector 3 observations, about 10. the output is just like tank controls l/r wheel forward/stop/back, so super simple.
Additionally though i want to provide an map for the agent of its surroundings. a very simple pixelraster (30x30) where 3 diffrent colors stand for obstacle/floor/not set. A data based minimap so to speak. This raster i want to put through the CNN by providing the pxielraster as visual observation, as that seems much more reasonable then putting the pixelraster in as 900 individual int observations. 
Using RenderTexture or camera over texture just seems so much overhead, so i use the provided ((https://github.com/mbaske/grid-sensor/tree/master/GridSensor)) custom grid observation, which does visual observations on my 2d int-array-pixelgrid.
Also i use the SAC trainer.

When starting the training i checked 1k step summarys and had a go: 
- the first 10k steps go really quick (10 seconds between sumamries). This 10k steps corrospond to my setting of `buffer_init_steps: 10000`
- suddendly it takes 3 minutes for 1k steps
- next it takes 20 minutes to train 1ksteps

so a rapid descent in progress.
![grafik](https://user-images.githubusercontent.com/34577718/101817827-4310ca00-3b23-11eb-9b2a-19b1028b38ba.png)

so is it possible that after the `buffer_init_steps` the pixelgrid data starts accumulating in the experience buffer and that just keeps bloating up the amount of data? (Which unit is the `buffer_size: 50000` in? 50K experiences so it scales with map? or is it fixed like byte? is the experience buffer variable in size and not prefilled with my pixelgrid-fake data? If so it shouldn't grow difficulter over time, right?

Which process in pytorch takes longer over time and why? shouldn't training difficultie stay constant?

Also i did check my envirnment for memory leaks ect!
 - taskmanager shows cosntant memory usage (~60%)
- cpu also sits at 60% (why can't i get 100% consistently while training? server never went above 60% with 8 cores x3.6ghz)
- i also checked that i free unity specific memory things by calling Resources.UnloadUnusedAssets(); every episode begin + not allocating unity objects/components like `var bla = new Texture2D` in the first place. (only allocating basic c# types with new)
-same goes for arrays and lists , i dont reallocate even.
the scene only uses very simple wheel physic collider and a little basic vector math. nothing taxing in update loops scene runs at several 100fps
the only not very basic operation i do is raycasting a bit, but thats literly the only thing that actually happens besides minor physics simulation (its a cleaning robot simulation and i want to train the cleaning robot). alos its only a handfull of raycasts.

Note: i am currently training wihtout CNN and use the 1000 basic int observations. its quite slow, but it stays reasonable quick for atleast 50k steps and is still going. (i'll update here incase it also gets stuck/decreases in speed but i don't think that will happen)
![grafik](https://user-images.githubusercontent.com/34577718/101818955-e6161380-3b24-11eb-871e-3b139c7bbd47.png)
",first thank help environment closer describe simple bool vector output like tank wheel super simple additionally though want provide map agent surroundings simple color stand set data based speak raster want put providing visual observation much reasonable individual camera texture much overhead use provided custom grid observation visual also use sac trainer starting training checked step go first go really quick setting next train rapid descent progress possible data experience buffer bloating amount data unit scale map fixed like experience buffer variable size data grow time right process longer time training stay constant also check memory memory usage also ca get consistently training server never went also checked free unity specific memory calling every episode begin unity like new first place basic new go dont reallocate even scene simple wheel physic little basic vector math nothing taxing update scene several basic operation bit thats thing actually besides minor physic simulation cleaning robot simulation want train cleaning robot note currently training use basic quite slow stay reasonable quick still going update incase also speed think happen,issue,positive,positive,neutral,neutral,positive,positive
742664092,"Can you share some about the environment you are training? What kinds of observations are there? The slow-down is likely due to the environment, as you say. ",share environment training likely due environment say,issue,negative,negative,neutral,neutral,negative,negative
742545098,"The new server does not cause my application to ""kill"" itself, atleast fo the time being. But i noticed that the training steps become subsequently slower, as in the first 10k steps pass in minutes while the next 10k need hours. This also happens on my local machine build, so i think the problem lys within the training or the environment itself and the former server just handled a possible memleak or similar differently. 
 this is the timings file of said behavior:

```
{
    ""name"": ""root"",
    ""metadata"": {
        ""timer_format_version"": ""0.1.0"",
        ""start_time_seconds"": ""1607595274"",
        ""python_version"": ""3.6.9 (default, Oct  8 2020, 12:12:24) \n[GCC 8.4.0]"",
        ""command_line_arguments"": ""/usr/local/bin/mlagents-learn /root/explorerSAC_config.yaml --env=/root/BuildLinux/lin --run-id=sac --no-graphics --force"",
        ""mlagents_version"": ""0.22.0"",
        ""mlagents_envs_version"": ""0.22.0"",
        ""communication_protocol_version"": ""1.2.0"",
        ""pytorch_version"": ""1.7.0+cu92"",
        ""numpy_version"": ""1.19.4"",
        ""end_time_seconds"": ""1607595286""
    },
    ""total"": 12.59061484100016,
    ""count"": 1,
    ""self"": 0.033602250000285494,
    ""children"": {
        ""run_training.setup"": {
            ""total"": 0.003998755999873538,
            ""count"": 1,
            ""self"": 0.003998755999873538
        },
        ""TrainerController.start_learning"": {
            ""total"": 12.553013835000002,
            ""count"": 1,
            ""self"": 0.03308288199968956,
            ""children"": {
                ""TrainerController._reset_env"": {
                    ""total"": 0.7129038279999804,
                    ""count"": 1,
                    ""self"": 0.7129038279999804
                },
                ""TrainerController.advance"": {
                    ""total"": 11.646286337000447,
                    ""count"": 203,
                    ""self"": 0.002890762003062264,
                    ""children"": {
                        ""env_step"": {
                            ""total"": 11.643395574997385,
                            ""count"": 203,
                            ""self"": 8.072210629995652,
                            ""children"": {
                                ""SubprocessEnvManager._take_step"": {
                                    ""total"": 3.567691051001475,
                                    ""count"": 203,
                                    ""self"": 0.020044783000457755,
                                    ""children"": {
                                        ""TorchPolicy.evaluate"": {
                                            ""total"": 3.5476462680010172,
                                            ""count"": 203,
                                            ""self"": 0.06425901000329759,
                                            ""children"": {
                                                ""TorchPolicy.sample_actions"": {
                                                    ""total"": 3.4833872579977196,
                                                    ""count"": 203,
                                                    ""self"": 3.4833872579977196
                                                }
                                            }
                                        }
                                    }
                                },
                                ""workers"": {
                                    ""total"": 0.003493894000257569,
                                    ""count"": 202,
                                    ""self"": 0.0,
                                    ""children"": {
                                        ""worker_root"": {
                                            ""total"": 12.320292559997824,
                                            ""count"": 202,
                                            ""is_parallel"": true,
                                            ""self"": 4.885522642997785,
                                            ""children"": {
                                                ""run_training.setup"": {
                                                    ""total"": 0.0,
                                                    ""count"": 0,
                                                    ""is_parallel"": true,
                                                    ""self"": 0.0,
                                                    ""children"": {
                                                        ""steps_from_proto"": {
                                                            ""total"": 0.011005758000010246,
                                                            ""count"": 1,
                                                            ""is_parallel"": true,
                                                            ""self"": 0.0003166559999954188,
                                                            ""children"": {
                                                                ""_process_visual_observation"": {
                                                                    ""total"": 0.010426326000015251,
                                                                    ""count"": 2,
                                                                    ""is_parallel"": true,
                                                                    ""self"": 0.00026279399958184513,
                                                                    ""children"": {
                                                                        ""observation_to_np_array"": {
                                                                            ""total"": 0.010163532000433406,
                                                                            ""count"": 6,
                                                                            ""is_parallel"": true,
                                                                            ""self"": 0.00013330100000530365,
                                                                            ""children"": {
                                                                                ""process_pixels"": {
                                                                                    ""total"": 0.010030231000428103,
                                                                                    ""count"": 6,
                                                                                    ""is_parallel"": true,
                                                                                    ""self"": 0.0007816070005901565,
                                                                                    ""children"": {
                                                                                        ""image_decompress"": {
                                                                                            ""total"": 0.009248623999837946,
                                                                                            ""count"": 6,
                                                                                            ""is_parallel"": true,
                                                                                            ""self"": 0.009248623999837946
                                                                                        }
                                                                                    }
                                                                                }
                                                                            }
                                                                        }
                                                                    }
                                                                },
                                                                ""_process_vector_observation"": {
                                                                    ""total"": 0.000262775999999576,
                                                                    ""count"": 2,
                                                                    ""is_parallel"": true,
                                                                    ""self"": 0.000262775999999576
                                                                }
                                                            }
                                                        },
                                                        ""UnityEnvironment.step"": {
                                                            ""total"": 0.08557706200008397,
                                                            ""count"": 1,
                                                            ""is_parallel"": true,
                                                            ""self"": 0.0006010500003412744,
                                                            ""children"": {
                                                                ""UnityEnvironment._generate_step_input"": {
                                                                    ""total"": 0.00016605299992988876,
                                                                    ""count"": 1,
                                                                    ""is_parallel"": true,
                                                                    ""self"": 0.00016605299992988876
                                                                },
                                                                ""communicator.exchange"": {
                                                                    ""total"": 0.08177106199991613,
                                                                    ""count"": 1,
                                                                    ""is_parallel"": true,
                                                                    ""self"": 0.08177106199991613
                                                                },
                                                                ""steps_from_proto"": {
                                                                    ""total"": 0.0030388969998966786,
                                                                    ""count"": 1,
                                                                    ""is_parallel"": true,
                                                                    ""self"": 0.0002606250000098953,
                                                                    ""children"": {
                                                                        ""_process_visual_observation"": {
                                                                            ""total"": 0.002563538000003973,
                                                                            ""count"": 2,
                                                                            ""is_parallel"": true,
                                                                            ""self"": 0.00036613600013879477,
                                                                            ""children"": {
                                                                                ""observation_to_np_array"": {
                                                                                    ""total"": 0.0021974019998651784,
                                                                                    ""count"": 6,
                                                                                    ""is_parallel"": true,
                                                                                    ""self"": 0.00013559099988924572,
                                                                                    ""children"": {
                                                                                        ""process_pixels"": {
                                                                                            ""total"": 0.0020618109999759326,
                                                                                            ""count"": 6,
                                                                                            ""is_parallel"": true,
                                                                                            ""self"": 0.0009174289998554741,
                                                                                            ""children"": {
                                                                                                ""image_decompress"": {
                                                                                                    ""total"": 0.0011443820001204585,
                                                                                                    ""count"": 6,
                                                                                                    ""is_parallel"": true,
                                                                                                    ""self"": 0.0011443820001204585
                                                                                                }
                                                                                            }
                                                                                        }
                                                                                    }
                                                                                }
                                                                            }
                                                                        },
                                                                        ""_process_vector_observation"": {
                                                                            ""total"": 0.00021473399988281017,
                                                                            ""count"": 2,
                                                                            ""is_parallel"": true,
                                                                            ""self"": 0.00021473399988281017
                                                                        }
                                                                    }
                                                                }
                                                            }
                                                        }
                                                    }
                                                },
                                                ""UnityEnvironment.step"": {
                                                    ""total"": 7.43476991700004,
                                                    ""count"": 201,
                                                    ""is_parallel"": true,
                                                    ""self"": 0.09699524199936604,
                                                    ""children"": {
                                                        ""UnityEnvironment._generate_step_input"": {
                                                            ""total"": 0.03602335800133005,
                                                            ""count"": 201,
                                                            ""is_parallel"": true,
                                                            ""self"": 0.03602335800133005
                                                        },
                                                        ""communicator.exchange"": {
                                                            ""total"": 6.89298212099834,
                                                            ""count"": 201,
                                                            ""is_parallel"": true,
                                                            ""self"": 6.89298212099834
                                                        },
                                                        ""steps_from_proto"": {
                                                            ""total"": 0.40876919600100337,
                                                            ""count"": 201,
                                                            ""is_parallel"": true,
                                                            ""self"": 0.03866307800353752,
                                                            ""children"": {
                                                                ""_process_visual_observation"": {
                                                                    ""total"": 0.3270108069980324,
                                                                    ""count"": 402,
                                                                    ""is_parallel"": true,
                                                                    ""self"": 0.027272977997427006,
                                                                    ""children"": {
                                                                        ""observation_to_np_array"": {
                                                                            ""total"": 0.2997378290006054,
                                                                            ""count"": 1206,
                                                                            ""is_parallel"": true,
                                                                            ""self"": 0.02095674199631503,
                                                                            ""children"": {
                                                                                ""process_pixels"": {
                                                                                    ""total"": 0.2787810870042904,
                                                                                    ""count"": 1206,
                                                                                    ""is_parallel"": true,
                                                                                    ""self"": 0.104499624999562,
                                                                                    ""children"": {
                                                                                        ""image_decompress"": {
                                                                                            ""total"": 0.17428146200472838,
                                                                                            ""count"": 1206,
                                                                                            ""is_parallel"": true,
                                                                                            ""self"": 0.17428146200472838
                                                                                        }
                                                                                    }
                                                                                }
                                                                            }
                                                                        }
                                                                    }
                                                                },
                                                                ""_process_vector_observation"": {
                                                                    ""total"": 0.04309531099943342,
                                                                    ""count"": 402,
                                                                    ""is_parallel"": true,
                                                                    ""self"": 0.04309531099943342
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                ""trainer_threads"": {
                    ""total"": 7.650099996681092e-05,
                    ""count"": 1,
                    ""self"": 7.650099996681092e-05,
                    ""children"": {
                        ""thread_root"": {
                            ""total"": 0.0,
                            ""count"": 0,
                            ""is_parallel"": true,
                            ""self"": 0.0,
                            ""children"": {
                                ""trainer_advance"": {
                                    ""total"": 11.527443817012227,
                                    ""count"": 19114,
                                    ""is_parallel"": true,
                                    ""self"": 0.2529891150329604,
                                    ""children"": {
                                        ""process_trajectory"": {
                                            ""total"": 11.274454701979266,
                                            ""count"": 19114,
                                            ""is_parallel"": true,
                                            ""self"": 11.274454701979266
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                ""TrainerController._save_models"": {
                    ""total"": 0.16066428699991775,
                    ""count"": 1,
                    ""self"": 0.0039965840001059405,
                    ""children"": {
                        ""RLTrainer._checkpoint"": {
                            ""total"": 0.1566677029998118,
                            ""count"": 1,
                            ""self"": 0.1566677029998118
                        }
                    }
                }
            }
        }
    }
}
```
",new server cause application kill time training become subsequently first pas next need also local machine build think problem lys within training environment former server handled possible similar differently file said behavior name root default force total count self total count self total count self total count self total count self total count self total count self total count self total count self total count self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count self total count true self total count true self total count true self total count self total count self,issue,positive,positive,positive,positive,positive,positive
742434212,"i setup the training on a diffrent server hoster. same ubuntu and jsut installed the packages. i trained always with ""--no-graphics and a headless build, nonetheless i get this run log:

![grafik](https://user-images.githubusercontent.com/34577718/101760656-70398a00-3adb-11eb-9f8d-25bb8121be92.png)

i will update this threat in a few hours if the current instance trains or crashes aswell",setup training server hoster trained always headless build nonetheless get run log update threat current instance aswell,issue,negative,neutral,neutral,neutral,neutral,neutral
742145693,"Barracuda errors:
```
		Library/PackageCache/com.unity.barracuda@1.1.2-preview/Barracuda/Runtime/Core/Internals/TestSetLoader.cs(242,38): error CS0103: The name 'JsonUtility' does not exist in the current context
		Library/PackageCache/com.unity.barracuda@1.1.2-preview/Barracuda/Runtime/Core/Internals/TestSetLoader.cs(266,9): error CS0103: The name 'ImageConversion' does not exist in the current context
```",barracuda error name exist current context error name exist current context,issue,negative,neutral,neutral,neutral,neutral,neutral
742068873,"Hi @simon-winter 

We currently do not actively support training on Azure. You are free to run mlagents on any linux cloud hosting service, as long as you have the correct permissions to install the necessary packages. Ubuntu should work fine. ",hi currently actively support training azure free run cloud hosting service long correct install necessary work fine,issue,positive,positive,positive,positive,positive,positive
742068148,hi @simon-winter please also sign CLA as well.  thanks!,hi please also sign well thanks,issue,positive,positive,positive,positive,positive,positive
741959805,"Hi @RedTachyon 

Thanks for the request. This is an interesting use-case that you bring up. As you mention, we currently check for NaN and Inf because these are typically useless values that will ruin a training run. While I understand the thinking behind the requested change, there are likely better ways to flag observations as being ignorable than sending NaNs. Having a pre-set value to correspond to ignorable values is one. Another would be to send a separate observation vector of masking values in addition to the observations themselves. ",hi thanks request interesting bring mention currently check nan typically useless ruin training run understand thinking behind change likely better way flag ignorable sending value correspond ignorable one another would send separate observation vector addition,issue,negative,positive,neutral,neutral,positive,positive
741070830,"Glad you were able to get things fixed. Having multiple numpy versions on the same machine can sometimes conflict with one another, which was my hunch.",glad able get fixed multiple machine sometimes conflict one another hunch,issue,negative,positive,positive,positive,positive,positive
741068889,"Hi @awjuliani 

I had numpy previously installed outside the virtual environment with version 1.18.1. After your message I checked the installed numpy version inside the virtual environment and for some reason 1.19.4 was installed, which must have been done by either pip install pytorch or mlagents, because I didn't do anything else in the virtual environment. Even while 1.19.4 was installed pip install mlagents --use-feature=2020-resolver didn't throw any errors and also didn't reinstall numpy with a different verion. Maybe because of the installed numpy outside the virtual environment?

running: pip install numpy==1.18.5 manually solved the issue now and I can correctly see the output from the command ""mlagents-learn --help"". Thank you for pointing me in the right direction.",hi previously outside virtual environment version message checked version inside virtual environment reason must done either pip install anything else virtual environment even pip install throw also reinstall different maybe outside virtual environment running pip install manually issue correctly see output command help thank pointing right direction,issue,positive,positive,neutral,neutral,positive,positive
741050134,"Hi @Krephix 

Can you verify that numpy was correctly installed, and that you only have one version installed on the machine? ",hi verify correctly one version machine,issue,negative,neutral,neutral,neutral,neutral,neutral
741042771,"Also to add something: there were no errors while installing pytorch or mlagents with pip. Also, I can execute the line ""from torch._C import *"" (last line indicated by the error traceback) without any errors when I run python normally in the console.",also add something pip also execute line import last line error without run python normally console,issue,negative,positive,neutral,neutral,positive,positive
740966661,"Gail tests passed.
The two failed tests are simple_rl tests that didn't reached the goal reward, which we should fix in staging.",two goal reward fix staging,issue,positive,neutral,neutral,neutral,neutral,neutral
740825887,This change can go into hybrid staging or directly into master if staging is too huge,change go hybrid staging directly master staging huge,issue,negative,positive,positive,positive,positive,positive
740242938,"After investigating options, we won't be adding a separate opt-out for analytics; users will be able to use the existing options to opt out from all events. This will be clarified in a README soon.",investigating wo separate analytics able use opt soon,issue,negative,positive,positive,positive,positive,positive
740062825,"Hey, I think you branched off an experimental branch from @Hunter-Unity 
You probably want to cherry-pick the last commit of this PR (754e3cb).
",hey think branched experimental branch probably want last commit,issue,negative,positive,neutral,neutral,positive,positive
739424170,"Thank you! I didnt expand the preview section and took the latest stable which was 1.0.6, the not matching one.
Taking the most recent preview version and the error is gone.",thank didnt expand preview section took latest stable matching one taking recent preview version error gone,issue,negative,positive,positive,positive,positive,positive
739420918,"I seem to be able import the `explorer.onnx` file just fine on both Unity 2018.4 and 2020.1 when using ML-Agents release 10.  I notice you are on Barracuda version 1.0.4 from the error messages you have. There is also a warning message at the start of training that says that you have different versions of ML-Agents in Unity and C#. I think you are using `ML-Agents package 1.0.6 in Unity` which is not compatible with release 10 of the trainers (`trainers version 0.22.0`). It says on the [documentation or package 1.0.6](https://docs.unity3d.com/Packages/com.unity.ml-agents@1.0/manual/index.html) of the package that it is ""strongly recommended to use `trainers version 0.16.0` but trainers up to `trainer version 0.20.0` are supported"". 

To resolve compatibility, you need to either upgrade the MLAgents Unity package to a more recent (but preview) version (such as 1.6.0) in the Unity package manager OR downgrade the Python trainers to at least version 0.20.0 (`pip install mlagents==0.20.0`).

The versions of ML-Agents can be confusing, there is a doc [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Versioning.md) to clarify some things",seem able import file fine unity release notice barracuda version error also warning message start training different unity think package unity compatible release version documentation package package strongly use version trainer version resolve compatibility need either upgrade unity package recent preview version unity package manager downgrade python least version pip install doc clarify,issue,negative,positive,positive,positive,positive,positive
738966273,"Followup from offline - this only currently enables LLAPI use of 2D observations. Use with mlagents-learn will still result in an exception, e.g. here
https://github.com/Unity-Technologies/ml-agents/blob/75f8a2bcc72f677bd8960dc0c2eef4113f61c174/ml-agents/mlagents/trainers/torch/utils.py#L169-L184",currently use use still result exception,issue,negative,neutral,neutral,neutral,neutral,neutral
738924537,"We should remove the constraint files and their usage
https://github.com/Unity-Technologies/ml-agents/blob/3c07161e298ad70372b8f982c521d21e44103dad/.github/workflows/pytest.yml#L23
which are used to install various versions of tensorflow during CI.",remove constraint usage used install various,issue,negative,neutral,neutral,neutral,neutral,neutral
738922354,"We can remove the upper limit on numpy
https://github.com/Unity-Technologies/ml-agents/blob/3c07161e298ad70372b8f982c521d21e44103dad/ml-agents-envs/setup.py#L51
Which was added to keep TF happy in https://github.com/Unity-Technologies/ml-agents/pull/4274.",remove upper limit added keep happy,issue,positive,positive,positive,positive,positive,positive
738406212,"@ykeuter I pushed an update to the changelog on your branch.

It looks the CLA check didn't trigger on the initial PR; you need to sign that before I can merge the change.",update branch check trigger initial need sign merge change,issue,negative,neutral,neutral,neutral,neutral,neutral
738377377,"Hi, I was actually really hoping this would exist in ML-Agents. My use case is for multiagent scenarios - I want an agent to observe whatever other agents are nearby, which can vary over time. 

My current workaround is just getting all the nearby agents and selecting N closest ones, or padding with NaNs if there's fewer neighbors than maximum, to make sure I don't accidentally use any placeholder values. It's... not ideal, but guess that's what I'm stuck with for now.",hi actually really would exist use case want agent observe whatever nearby vary time current getting nearby padding maximum make sure accidentally use ideal guess stuck,issue,negative,positive,positive,positive,positive,positive
738116000,Thank you. I did as you said. I am using it now. I hadn't noticed that the extension wasn't going downloaded.,thank said extension going,issue,negative,neutral,neutral,neutral,neutral,neutral
737858022,Nevermind. Turns out the -default- yaml script was executed due to a typo in behavior name. This went without warnings. Maybe nice to throw an exception for this scenario in the future?,turn script executed due typo behavior name went without maybe nice throw exception scenario future,issue,negative,positive,positive,positive,positive,positive
737569273,"> > The changes look good to me except for the python syntax errors around referencing VectorActionsDeprecated before they are assigned.
> 
> Can you point out where the syntax errors are? I didn't see python syntax issues around VectorActionsDeprecated

```
During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/venv/bin/mlagents-learn"", line 33, in <module>
    sys.exit(load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')())
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/ml-agents/mlagents/trainers/learn.py"", line 280, in main
    run_cli(parse_command_line())
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/ml-agents/mlagents/trainers/learn.py"", line 276, in run_cli
    run_training(run_seed, options)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/ml-agents/mlagents/trainers/learn.py"", line 153, in run_training
    tc.start_learning(env_manager)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/ml-agents-envs/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 201, in start_learning
    self._save_models()
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/ml-agents-envs/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 84, in _save_models
    self.trainers[brain_name].save_model()
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/ml-agents/mlagents/trainers/trainer/rl_trainer.py"", line 215, in save_model
    model_checkpoint = self._checkpoint()
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/ml-agents-envs/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/ml-agents/mlagents/trainers/trainer/rl_trainer.py"", line 189, in _checkpoint
    checkpoint_path = self.model_saver.save_checkpoint(self.brain_name, self.step)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/ml-agents/mlagents/trainers/model_saver/torch_model_saver.py"", line 57, in save_checkpoint
    self.export(checkpoint_path, behavior_name)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/ml-agents/mlagents/trainers/model_saver/torch_model_saver.py"", line 62, in export
    self.exporter.export_policy_model(output_filepath)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/ml-agents/mlagents/trainers/torch/model_serialization.py"", line 111, in export_policy_model
    dynamic_axes=self.dynamic_axes,
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/venv/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 230, in export
    custom_opsets, enable_onnx_checker, use_external_data_format)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/venv/lib/python3.7/site-packages/torch/onnx/utils.py"", line 91, in export
    use_external_data_format=use_external_data_format)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/venv/lib/python3.7/site-packages/torch/onnx/utils.py"", line 639, in _export
    dynamic_axes=dynamic_axes)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/venv/lib/python3.7/site-packages/torch/onnx/utils.py"", line 411, in _model_to_graph
    use_new_jit_passes)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/venv/lib/python3.7/site-packages/torch/onnx/utils.py"", line 379, in _create_jit_graph
    graph, torch_out = _trace_and_get_graph_from_model(model, args)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/venv/lib/python3.7/site-packages/torch/onnx/utils.py"", line 342, in _trace_and_get_graph_from_model
    torch.jit._get_trace_graph(model, args, strict=False, _force_outplace=False, _return_inputs_states=True)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/venv/lib/python3.7/site-packages/torch/jit/_trace.py"", line 1148, in _get_trace_graph
    outs = ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/venv/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/venv/lib/python3.7/site-packages/torch/jit/_trace.py"", line 130, in forward
    self._force_outplace,
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/venv/lib/python3.7/site-packages/torch/jit/_trace.py"", line 116, in wrapper
    outs.append(self.inner(*trace_inputs))
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/venv/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 725, in _call_impl
    result = self._slow_forward(*input, **kwargs)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/venv/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 709, in _slow_forward
    result = self.forward(*input, **kwargs)
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/ml-agents/mlagents/trainers/torch/networks.py"", line 294, in forward
    encoding, masks
  File ""/Users/bokken/build/output/Unity-Technologies/ml-agents/ml-agents/mlagents/trainers/torch/action_model.py"", line 173, in get_action_out
    return continuous_out, discrete_out, action_out_deprecated
UnboundLocalError: local variable 'action_out_deprecated' referenced before assignment
```
 

from here: https://yamato.cds.internal.unity3d.com/jobs/497-ml-agents/tree/develop-hybrid-actionproto/.yamato%252Ftraining-int-tests.yml%2523test_mac_training_int_2018.4/4450760/job/(log:Execution)

around line 586 in the execution log.",look good except python syntax around assigned point syntax see python syntax around handling exception another exception recent call last file line module file line main file line file line file line wrapped return file line file line wrapped return file line file line file line wrapped return file line file line file line export file line file line export file line export file line file line file line graph model file line model file line strict file line result input file line forward file line wrapper file line result input file line result input file line forward file line return local variable assignment around line execution log,issue,negative,positive,positive,positive,positive,positive
737555940,"> The changes look good to me except for the python syntax errors around referencing VectorActionsDeprecated before they are assigned.

Can you point out where the syntax errors are? I didn't see python syntax issues around VectorActionsDeprecated",look good except python syntax around assigned point syntax see python syntax around,issue,negative,positive,positive,positive,positive,positive
737472309,"it is fixed i deleted all the other python install and now it does work
",fixed python install work,issue,negative,positive,neutral,neutral,positive,positive
737441150,"Hi, can you please provide the information required in the bug template? 


**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Console logs / stack traces**
Please wrap in [triple backticks (```)](https://help.github.com/en/articles/creating-and-highlighting-code-blocks) to make it easier to read.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Environment (please complete the following information):**
- Unity Version: [e.g. Unity 2020.1f1]
- OS + version: [e.g. Windows 10]
- _ML-Agents version_: (e.g. ML-Agents v0.8, or latest `develop` branch from source)
- _Torch version_: (you can run `pip3 show torch` to get this)
- _Environment_: (which example environment you used to reproduce the error)

It be useful for me to know what your Python and pip versions are. It is extremely strange that there is a triple = sign in the error you posted in your first message, there are suppose to be only two...",hi please provide information bug template reproduce reproduce behavior go click scroll see error console stack please wrap triple make easier read applicable add help explain problem environment please complete following information unity version unity o version latest develop branch source run pip show torch get example environment used reproduce error useful know python pip extremely strange triple sign error posted first message suppose two,issue,negative,positive,positive,positive,positive,positive
737355703,"i am using python 3.7 and i activated my venv. when i try to download torch it does not work i do not no why and how to fix this problem. i am following this course https://www.youtube.com/watch?v=zPFU30tbyKs&t=132s 
the command that i use is provided by the github",python try torch work fix problem following course command use provided,issue,negative,neutral,neutral,neutral,neutral,neutral
737172656,"> @ralph367 What version of python are you using? Tensorflow doesn't yet support python3.8 (see [tensorflow/tensorflow#33374](https://github.com/tensorflow/tensorflow/issues/33374)) so if you're using 3.8, you'll need to either downgrade to python 3.7, or wait for them to build new packages.

it worked!!thanks for suggestion
",ralph version python yet support python see need either downgrade python wait build new worked thanks suggestion,issue,positive,positive,positive,positive,positive,positive
736934585,This discussion has gotten off topic from the original one. Please open a thread on the [Unity ML-Agents Forum](https://forum.unity.com/forums/ml-agents.453/) if you have further questions.,discussion gotten topic original one please open thread unity forum,issue,positive,positive,positive,positive,positive,positive
736744210,"@nauutilus Just to follow up, the barracuda team will be updating those instructions in their next release.",follow barracuda team next release,issue,negative,neutral,neutral,neutral,neutral,neutral
736735881,"Great, we applied those fixes for the latest release (Release 10), so you don't need to keep using the branch.

I'll close this issue.",great applied latest release release need keep branch close issue,issue,positive,positive,positive,positive,positive,positive
736725212,"> Hi,
> I was able to reproduce the error you're seeing. There are a couple of ways to work around it:
> 
> * Run `pip install numpy==1.18.5` - this is the newest version before 19.0
> * When you run `pip install mlagents`, add `--use-feature=2020-resolver`
> * Update pip again so that you get version 20.3 (which was released shortly after you posted this).

Hi Chriselion,
thanks a lot for your help,
that totally fixed my issue and I'm able to run the mlagents-learn.
I really appreciate 🙏",hi able reproduce error seeing couple way work around run pip install version run pip install add update pip get version shortly posted hi thanks lot help totally fixed issue able run really appreciate,issue,positive,positive,positive,positive,positive,positive
736715477,"Hi there :) 
It seems to be fine right now on your branch MLA-1503-int-overflow-nan. 
My new training now is on 2180000000 step and it's continue. Thank you for help.",hi fine right branch new training step continue thank help,issue,positive,positive,positive,positive,positive,positive
736437667,"Main problem is that current version of ""ML-Agents package"" in Unity doesn't include ""ML-Agents Extensions"" which is used in example project
To fix this:
First instal Package from Unity Manager
Then you have to download current release version from git
After it you got 2 ways to install needed package:
1. Instal using link to github
    In the Package Manager hit the ""+"" button, and select ""Add package from git URL""  (<- This one wont solve problem at the moment cause git owner has to accept your request)
    Place there:
                 git+https://github.com/Unity-Technologies/ml-agents.git?path=com.unity.ml-agents.extensions
    Or in the your manifest.json file located in Project -> Packages add next line below ""com.unity.ml-agents"":
                 ""com.unity.ml-agents.extensions"": ""git+https://github.com/Unity-Technologies/ml-agents.git?path=com.unity.ml-agents.extensions"",
2. Local Instalation
    In the Package Manager hit the ""+"" button, and select ""Add package from disk""
    Find ""com.unity.ml-agents"" in downloaded repo
    Select ""package.json""
    Then same steps but with ""com.unity.ml-agents.extensions"" folder
    (After it you also may spot ""UnityEditor.AsyncHTTPClient:Done(State, Int32)"" error, it easily fixes by disabeling Edit -> Preferences -> Show Asset Store Search Hits )
",main problem current version package unity include used example project fix first package unity manager current release version git got way install package link package manager hit button select add package git one wont solve problem moment cause git owner accept request place file project add next line local package manager hit button select add package disk find select folder also may spot done state error easily edit show asset store search,issue,negative,positive,positive,positive,positive,positive
736167868,"I will try to get a fork up in the next few days. I am using Release 7. I
plan to upgrade to Release 11 when it comes out in a week or so. There will
be fewer changes needed then as my branch includes some workarounds for a
few bugs.


On Sun, Nov 29, 2020 at 11:19 AM TIP <notifications@github.com> wrote:

> Hi Phong13
>
> Thank you for replying and yes I would be really interested if you could
> forward a fork for modified ML Agents to provide the GetValueEstimate.
>
> I fully support the importance of having the GetValueEstimate()
> incorporated.
> Has there been any further development on this @sini
> <https://github.com/sini> ?
> It would certainly be a game changer.
>
> Many thanks!
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/4492#issuecomment-735441309>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACWVKENCGLJ7OLLDL5MOAOLSSKNFDANCNFSM4RPTAYOQ>
> .
>


-- 
Ian Deane
DigitalOpus <http://digitalopus.ca>
Twitter: @DigitalOpus <https://twitter.com/DigitalOpus>
",try get fork next day release plan upgrade release come week branch sun tip wrote hi thank yes would really interested could forward fork provide fully support importance incorporated development would certainly game changer many thanks thread reply directly view twitter,issue,positive,positive,positive,positive,positive,positive
736135937,"I was unable to reproduce this with the WallJump example scene, which uses SetModel, on Unity 2020.1.5f1.

Can you please include the full callstack of the error, and some of the code around where you're calling SetModel from? Also, you probably need to check that the NNModel is non-null; I would imagine this this isn't handled as gracefully in IL2CPP builds as it normally.",unable reproduce example scene unity please include full error code around calling also probably need check would imagine handled gracefully normally,issue,negative,negative,neutral,neutral,negative,negative
735963733,"Hi,
I was able to reproduce the error you're seeing. There are a couple of ways to work around it:
* Run `pip install numpy==1.18.5` - this is the newest version before 19.0
* When you run `pip install mlagents`, add `--use-feature=2020-resolver`
* Update pip again so that you get version 20.3 (which was released shortly after you posted this).",hi able reproduce error seeing couple way work around run pip install version run pip install add update pip get version shortly posted,issue,negative,positive,positive,positive,positive,positive
735441309,"Hi Phong13

Thank you for replying and yes I would be really interested if you could
forward a fork for modified ML Agents to provide the GetValueEstimate.

I fully support the importance of having the GetValueEstimate() incorporated.
Has there been any further development on this @sini  ?
It would certainly be a game changer.

Many thanks!",hi thank yes would really interested could forward fork provide fully support importance incorporated development would certainly game changer many thanks,issue,positive,positive,positive,positive,positive,positive
735269516,"Hi Tectonic IP,

It might help to add a comment to the issue on Github, the more support
there is for this, the more likely it will get added.

If you are interested. I have modified ML Agents to provide the
GetValueEstimate. I could create a fork on Github that has this.



On Sat, Nov 28, 2020 at 9:14 AM Tectonic IP <notifications@github.com>
wrote:

> I spoke with the author responsible for this original change and he
> explained that the reason we removed GetValueEstimate is because it's
> unavailable when using Heuristic / Player control. It would also require a
> hook into mlagents_envs that is not necessarily useful when using this API
> directly.
>
> Basically, value estimates are not part of the typical RL loop.
>
> I know we closed your last request on this issue. I'll bring it up for
> more discussion in our next team meeting.
>
> Very interested to here of the GetValueEstimate also. Many thanks!
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/4492#issuecomment-735257916>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACWVKEPIUA7MJVLV5UG64LTSSEVXLANCNFSM4RPTAYOQ>
> .
>


-- 
Ian Deane
DigitalOpus <http://digitalopus.ca>
Twitter: @DigitalOpus <https://twitter.com/DigitalOpus>
",hi tectonic might help add comment issue support likely get added interested provide could create fork sat tectonic wrote spoke author responsible original change reason removed unavailable heuristic player control would also require hook necessarily useful directly basically value part typical loop know closed last request issue bring discussion next team meeting interested also many thanks thread reply directly view twitter,issue,positive,positive,positive,positive,positive,positive
735019689,"> Hi,
> What version of the `com.unity.ml-agents` package are you using? I believe it's 1.0.6 (or another 1.0.x version), but the IActuator interface was added in the [`1.4.0-preview`](https://github.com/Unity-Technologies/ml-agents/releases/tag/release_7) package.
> 
> If that's the case, you should either:
> 
> * Update the package to a 1.4.0-preview or later (depending on your Unity version, you may need to change some package manager settings to be able to see preview packages).
> * Get the examples from a tag that corresponds to the 1.0.x package, for example [`com.unity.ml-agents_1.0.6`](https://github.com/Unity-Technologies/ml-agents/tree/com.unity.ml-agents_1.0.6)

Hi Chriselion,

Greeting. 
Thanks for the quick response. Following your answer, it works now. 

For anyone who might see the same problem, my solution is:
1. In package manager->advanced, choose show preview packages. Then install ML Agents 1.4.0.
2. Download the corresponding 1.4.0 package from https://github.com/Unity-Technologies/ml-agents/releases/tag/release_7",hi version package believe another version interface added package case either update package later depending unity version may need change package manager able see preview get tag package example hi greeting thanks quick response following answer work anyone might see problem solution package advanced choose show preview install corresponding package,issue,positive,positive,positive,positive,positive,positive
734929613,"Hi,
What version of the `com.unity.ml-agents` package are you using? I believe it's 1.0.6 (or another 1.0.x version), but the IActuator interface was added in the [`1.4.0-preview`](https://github.com/Unity-Technologies/ml-agents/releases/tag/release_7) package.

If that's the case, you should either:
* Update the package to a 1.4.0-preview or later (depending on your Unity version, you may need to change some package manager settings to be able to see preview packages).
* Get the examples from a tag that corresponds to the 1.0.x package, for example [`com.unity.ml-agents_1.0.6`](https://github.com/Unity-Technologies/ml-agents/tree/com.unity.ml-agents_1.0.6)",hi version package believe another version interface added package case either update package later depending unity version may need change package manager able see preview get tag package example,issue,negative,positive,positive,positive,positive,positive
733557230,"Hello @andrewcoh,

I'll try this (once more!).

Indeed, so far, I tried with e.g.:
`mlagents-learn config/GridWorld.yaml --env gw.exe --run-id gw013 --force --no-graphics` 
in order to speed up the training.
But, in in-editor mode, it might be even much longer, AFAIK.

=> Then do you mean that it could be more efficient to do the learning through in-editor mode (i.e. visual obs could depend on ""graphics"" duly displayed on the screen)? 
Or is it just a way to visually check on my side that learning is evolving correctly through steps?

Thanks for your help.
",hello try indeed far tried force order speed training mode might even much longer mean could efficient learning mode visual could depend graphic duly displayed screen way visually check side learning correctly thanks help,issue,positive,positive,neutral,neutral,positive,positive
733174521,Can you try just running `mlagents-learn config/ppo/GridWorld.yaml --force` and train in-editor?,try running force train,issue,negative,neutral,neutral,neutral,neutral,neutral
732297843,"Hi @andrewcoh,

Yes, the distributed GridWorld yaml was used.
I tried, for hours, several times then with different seeds (using a GPU GTX 1080).

Could you be so kind to give me more details on the number of steps it required on your side,
and how many parallel environments (num-envs) were specified? Of course, it was certainly a different workstation, but it will help me figure out what can be, and cannot be, done on my side in the hope to obtain +/- the same distributed .nn!!

Thanks in advance for that.
",hi yes distributed used tried several time different could kind give number side many parallel course certainly different help figure done side hope obtain distributed thanks advance,issue,positive,positive,positive,positive,positive,positive
732224932,"Sure. I am running a computer vision experiment, an object recognition task (not strightly RL). At each iteration I am passing Y frames of X objects. The network is a RNN that learns to recognize these objects. I want the network to be trained on a variable number of frames per object. 

I have done some research in the code today and it doesn't seem possible to implement this features by expanding the classes in ml-agents. A perfect point to check whether to pass or not a sensor is in `Agents.SendInfoToBrain`. My idea was to inherit from `Agent` and reimplement that function with a check on the sensor object (keeping everything the same -literally copy and pasting everything - but check the sensor in `RequestDecision`). But that functions is full of private attributes, so I can't just copy it in my inherited agents. I can't even `Base.Call()` it because otherwise it will send the decision. So my only option is to change `Agent` itself, which is.. bad. :(   ",sure running computer vision experiment object recognition task iteration passing network recognize want network trained variable number per object done research code today seem possible implement expanding class perfect point check whether pas sensor idea inherit agent function check sensor object keeping everything copy pasting everything check sensor full private ca copy ca even otherwise send decision option change agent bad,issue,positive,positive,positive,positive,positive,positive
732202461,"Hi @ValerioB88 

This is a feature that we are planning to add ourselves in the next month or two but we haven't yet decided on the best way to implement it. 

Thank you for showing your interest in this feature. Can you share your use case?",hi feature add next month two yet decided best way implement thank showing interest feature share use case,issue,positive,positive,positive,positive,positive,positive
732199410,"Hi @Dastyn 

I'm not having any trouble retraining our GridWorld environment on the master branch. It is possible that you just got unlucky with a random seed. Are you running with the default `.yaml' file for GridWorld?",hi trouble environment master branch possible got unlucky random seed running default file,issue,negative,negative,negative,negative,negative,negative
732179549,Thank you for pointing this out. I will add links to the documentation here https://github.com/Unity-Technologies/ml-agents/blob/release_10_docs/docs/Training-ML-Agents.md#environment-parameters where appropriate.,thank pointing add link documentation appropriate,issue,negative,positive,positive,positive,positive,positive
732177728,"Thanks for the thread @CubeMD @Ihsees .

Just to be sure we are all on the same page, 1 step in python corresponds to 1 decision period for one agent in C# and the decision period is the number of fixed updates between decisions.

As to tracking these things manually, you can add your own stats to Tensorboard from C# https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Using-Tensorboard.md#custom-metrics-from-unity (unless that's what you're already doing).

I don't think there's a way to optimize this in general since it will be highly dependent on each individual problem.

As far as alternative stats to measure step throughput, I can share some of your ideas with the team.",thanks thread sure page step python decision period one agent decision period number fixed manually add unless already think way optimize general since highly dependent individual problem far alternative measure step throughput share team,issue,positive,positive,positive,positive,positive,positive
731897164,can somebody address this issue ? why was this closed ?,somebody address issue closed,issue,negative,negative,neutral,neutral,negative,negative
731885736,"@ihsees Hi, thank you for bothering with a bump. I think that decisions per second is more descriptive than my steps or any of other metrics you mentioned.",hi thank bump think per second descriptive metric,issue,negative,neutral,neutral,neutral,neutral,neutral
731734442,"I agree. 

I have thought about this feature as well and realized that there are many different metrics to track which all correlate to each other, but are dependent on ""--num-envs"", the number of agents in one scene, and even the ""Decision Period"".

From the top of my head I can think of
FixedUpdateSteps / s in Unity
AcademySteps / s  in Unity (same as FixedUpdateSteps?)
AgentSteps / s in Unity (which is different than ""AcademySteps"", depending on the number of Agents per Scene, and the Decision Period)
Decisions / s in Python (which is higher than ""AgentSteps"", depending on ""--num-envs"")
Observations / s in Python (this should be the same as ""Decisions / s"")

Having said that, if there was a way to track Python-""Decisions / s"" in Tensorboard, this would really help me as well (I'm currently sitting here tracking these manually)
",agree thought feature well many different metric track correlate dependent number one scene even decision period top head think unity unity unity different depending number per scene decision period python higher depending python said way track would really help well currently sitting manually,issue,positive,positive,positive,positive,positive,positive
731418328,Thank you for letting me know. I saw that the issue was fixed in release 10. I will do more runs this weekend.,thank know saw issue fixed release weekend,issue,negative,positive,neutral,neutral,positive,positive
730552753,"Previous PR attempt was getting some weird merge conflicts. 

Manually kicked off a `[Yamato] com.unity.ml-agents test trunk on win ` test run, which is the one that fails on nightly runs.",previous attempt getting weird merge manually test trunk win test run one nightly,issue,negative,positive,neutral,neutral,positive,positive
729925155,Closing this PR and will fix the issue with a better solution.,fix issue better solution,issue,positive,positive,positive,positive,positive,positive
729923778,"A quick fix for editor would be replacing this line in `com.unity.ml-agents/Runtime/Communicator/RpcCommunicator.cs`:
```       
            var result = m_Client.Exchange(WrapMessage(unityOutput, 200));
```

with

```
#if UNITY_EDITOR
            var result = m_Client.Exchange(WrapMessage(unityOutput, 200), deadline: DateTime.Now.AddSeconds(1));
#else
            var result = m_Client.Exchange(WrapMessage(unityOutput, 200));
#endif
```

We're working on fixing this with a larger change, but this is an easy fix if it really annoys you.",quick fix editor would line result result deadline else result working fixing change easy fix really,issue,negative,positive,positive,positive,positive,positive
729399283,"@dongruoping Thanks, I will work with our Unity Developers to look into this aspect. Highly appreciate.",thanks work unity look aspect highly appreciate,issue,positive,positive,positive,positive,positive,positive
729398884,"Since I need this for my own purpose, I've added my own wrapper (based on Unity's wrapper) which can be found here https://github.com/laszukdawid/ai-traineree/blob/master/ai_traineree/tasks.py#L101 (or with associated commit https://github.com/laszukdawid/ai-traineree/commit/39dcf3188d0b14853508c48f63416a2df7a94a7e).

I'd appreciate any reply from Unity's team. I'm planning on adding more support for Multi Agent use cases and wouldn't mind contributing a bit.",since need purpose added wrapper based unity wrapper found associated commit appreciate reply unity team support agent use would mind bit,issue,positive,neutral,neutral,neutral,neutral,neutral
729271682,"Hi, 

We're still looking into this and see if we can do this with our environment, but theoretically it should work as is.
You just need to put your game on a port and specify that port in python.

One thing you should be careful about is the initialization of the Academy in your environment. You should make sure your game is actively listening on the port when you launch python.
For example our examples environments will try to connect as soon as they start, then fail to connect and start running inference. In that case the python thread has no way to connect to the game.

Besides that we'll be looking more into this.",hi still looking see environment theoretically work need put game port specify port python one thing careful academy environment make sure game actively listening port launch python example try connect soon start fail connect start running inference case python thread way connect game besides looking,issue,negative,negative,negative,negative,negative,negative
729247762,"Hi @18goldr,
The Unity coding style guide (which I can't find a public reference to, so you'll just have to trust me) says:

> Spell words using correct US-English spelling. Note that there are a few legacy exceptions that use GB-English that we must preserve, but do not add new ones.

In particular, `MonoBehaviour` is one of those legacy exceptions. We should be using ""Behavior"" everywhere else in this repo.

Are there any other ""Behaviour""s in the repo that you've found? The only ones I found were some comments in a test:
https://github.com/Unity-Technologies/ml-agents/blob/d4e0daeaea58f9610ac4a20dfaec157093a274d8/gym-unity/gym_unity/tests/test_gym.py#L143
(and another one in the same file). I have a PR to fix those in https://github.com/Unity-Technologies/ml-agents/pull/4663

Edit: here's a [copy of the style guide](https://github.com/needle-mirror/com.unity.polybrush/blob/master/CodeStyleGuide.cs~) that someone scraped from a package.",hi unity style guide ca find public reference trust spell correct spelling note legacy use must preserve add new particular one legacy behavior everywhere else behaviour found found test another one file fix edit copy style guide someone scraped package,issue,positive,positive,positive,positive,positive,positive
729237532,"> Looks this breaks the connection on all the yamato jobs...

I'm looking into this... it's breaking training with executable.
It's able to launch the environment but somehow not able to connect to it, and seems like the environment is running inference.
",connection looking breaking training executable able launch environment somehow able connect like environment running inference,issue,negative,positive,positive,positive,positive,positive
729208477,"Tested with match3 with checkpoint_interval=100. Without the fix It usually fail within 1000 steps without the fix.
By adding the lock It is able to run without error for more than 20000 steps so I assume it's working. ",tested match without fix usually fail within without fix lock able run without error assume working,issue,positive,neutral,neutral,neutral,neutral,neutral
729205397,"> Since it is a static variable, it should be created only once and be shared on all threads no?

I was thinking about the same thing and I just tried it now. It seems to work.",since static variable thinking thing tried work,issue,negative,positive,positive,positive,positive,positive
729204638," > exporting_to_onnx currently gets created locally, so you'll need to refactor some things so that the class gets shared (otherwise each thread will use its own lock, so nothing different actually happens)

I was thinking a static lock on the class. Something like :

```
class exporting_to_onnx:
    _local_data = threading.local()
    _lock = threading.Lock()
    _local_data._is_exporting = False

    def __enter__(self):
        self._lock.__enter__()
        self._local_data._is_exporting = True

    def __exit__(self, *args):
        self._lock.__exit__(*args)
        self._local_data._is_exporting = False

    @staticmethod
    def is_exporting():
        if not hasattr(exporting_to_onnx._local_data, ""_is_exporting""):
            return False
        return exporting_to_onnx._local_data._is_exporting
```
Since it is a static variable, it should be created only once and be shared on all threads no?",currently locally need class otherwise thread use lock nothing different actually thinking static lock class something like class false self true self false return false return since static variable,issue,positive,positive,neutral,neutral,positive,positive
729193020,"Yes I agree that would be something useful for multi-agent training, thanks for raising this.
This is something on our roadmap and I'll bring this up with the team.",yes agree would something useful training thanks raising something bring team,issue,positive,positive,positive,positive,positive,positive
729073203,">  I would prefer the lock be part of the `exporting_to_onnx`

`exporting_to_onnx` currently gets created locally, so you'll need to refactor some things so that the class gets shared (otherwise each thread will use its own lock, so nothing different actually happens)",would prefer lock part currently locally need class otherwise thread use lock nothing different actually,issue,negative,neutral,neutral,neutral,neutral,neutral
728651587,"I think this works, but I would prefer the lock be part of the `exporting_to_onnx` object if possible.",think work would prefer lock part object possible,issue,negative,neutral,neutral,neutral,neutral,neutral
728364726,"Using the same parameters in the PR (but constant learning rate), I hit the same exception after around 1.2M steps. Before that, both visual and vector were averaging about 39, which is already an improvement.

I'll dig into the exception some more; it's probably a bug in the torch trainers that we should get fixed ASAP.",constant learning rate hit exception around visual vector already improvement dig exception probably bug torch get fixed,issue,negative,positive,neutral,neutral,positive,positive
728363010,"Thanks for looking into this. Yes, this does help with my case. My main worry was the big lag spike when switching from the ""lobby"" scene to the first scene with agents in it. If it doesn't happen with builds that is fine.

It is still annoying in the editor though. It is valuable to be able to enter playmode and back very quickly to speed up debugging/development workflow. Thanks for looking at this.
",thanks looking yes help case main worry big lag spike switching lobby scene first scene happen fine still annoying editor though valuable able enter back quickly speed thanks looking,issue,positive,positive,positive,positive,positive,positive
728353514,"So this delay is expected in the editor since we create the communicator in case we want to run in-editor training. 
With binary executable this issue doesn't exist. Does that help with your case?

Also I agree there's something we can make it better with the editor and we're looking into it.",delay editor since create communicator case want run training binary executable issue exist help case also agree something make better editor looking,issue,positive,positive,positive,positive,positive,positive
728345176,"Hi,

Could you post this on the [barracuda github repo](https://github.com/Unity-Technologies/barracuda-release/issues)? This repo is specifically for ML-Agents, thank you.",hi could post barracuda specifically thank,issue,negative,neutral,neutral,neutral,neutral,neutral
728343268,"Running in the editor. I have not tested in a binary build.

Mot using the mlagents-learn command. All the agents in the scene have been configured to ""Inference Only"". The models were previously trained using training scenes and this is a game scene that is using the trained agents.
",running editor tested binary build mot command scene inference previously trained training game scene trained,issue,negative,negative,negative,negative,negative,negative
728254033,"A few more questions I'd like to know about, when you observe this delay:
1. Are you running in editor or using a binary build?
2. Are you running this with python `mlagents-learn` command? ",like know observe delay running editor binary build running python command,issue,negative,neutral,neutral,neutral,neutral,neutral
728141422,"I tried to train a few more and never had any problems with original hyperparameters. I couldn't find any significant difference between sensors in terms of how often this issue comes up. Also, I never reached the performance of my original post, so might have just been lucky.",tried train never original could find significant difference often issue come also never performance original post might lucky,issue,negative,positive,positive,positive,positive,positive
727617489,In a similar question to the original poster: is there any reason why the multi agent isn't supported? Code changes to make the gym-like API support isn't difficult so I'm trying figure out whether I'm missing something or this is purely conceptual difficulty.,similar question original poster reason agent code make support difficult trying figure whether missing something purely conceptual difficulty,issue,negative,negative,neutral,neutral,negative,negative
727467579,"I tried to train an uncompressed visual with these parameters but got this error every time around 2 millions. 

```
Traceback (most recent call last):
  File ""C:\Users\CubeMD\.conda\envs\ml-agents-repo\Scripts\mlagents-learn-script.py"", line 33, in <module>
    sys.exit(load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')())
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents\mlagents\trainers\learn.py"", line 280, in main
    run_cli(parse_command_line())
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents\mlagents\trainers\learn.py"", line 276, in run_cli
    run_training(run_seed, options)
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents\mlagents\trainers\learn.py"", line 153, in run_training
    tc.start_learning(env_manager)
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents-envs\mlagents_envs\timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents\mlagents\trainers\trainer_controller.py"", line 176, in start_learning
    n_steps = self.advance(env_manager)
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents-envs\mlagents_envs\timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents\mlagents\trainers\trainer_controller.py"", line 234, in advance
    new_step_infos = env_manager.get_steps()
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents\mlagents\trainers\env_manager.py"", line 112, in get_steps
    new_step_infos = self._step()
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents\mlagents\trainers\subprocess_env_manager.py"", line 255, in _step
    self._queue_steps()
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents\mlagents\trainers\subprocess_env_manager.py"", line 248, in _queue_steps
    env_action_info = self._take_step(env_worker.previous_step)
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents-envs\mlagents_envs\timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents\mlagents\trainers\subprocess_env_manager.py"", line 348, in _take_step
    all_action_info[brain_name] = self.policies[brain_name].get_action(
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents\mlagents\trainers\policy\torch_policy.py"", line 234, in get_action
    run_out = self.evaluate(
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents-envs\mlagents_envs\timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents\mlagents\trainers\policy\torch_policy.py"", line 203, in evaluate
    action, log_probs, entropy, memories = self.sample_actions(
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents-envs\mlagents_envs\timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents\mlagents\trainers\policy\torch_policy.py"", line 146, in sample_actions
    action_list = self.actor_critic.sample_action(dists)
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents\mlagents\trainers\torch\networks.py"", line 306, in sample_action
    action = action_dist.sample()
  File ""c:\users\cubemd\desktop\ml-agents-repo\ml-agents\mlagents\trainers\torch\distributions.py"", line 100, in sample
    return torch.multinomial(self.probs, 1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
```

Could it be related to low buffer size with new sensor? I might be wrong but isn't policy clipping in #4649 related to it? Should I make a new issue for it? I'll train with original hyperparameters to see if I get this error again.

I allowed edits to this branch for maintainers, if that's what you mean by permissions. I'm a bit new to this. 
",tried train uncompressed visual got error every time around million recent call last file line module file line main file line file line file line wrapped return file line file line wrapped return file line advance file line file line file line file line wrapped return file line file line file line wrapped return file line evaluate action entropy file line wrapped return file line file line action file line sample return probability tensor either nan element could related low buffer size new sensor might wrong policy clipping related make new issue train original see get error branch mean bit new,issue,negative,positive,neutral,neutral,positive,positive
727000333,"Thanks for the explanation, that does sound like a fair use case.
I'll bring it up with the team and see how we can incorporate this feature.",thanks explanation sound like fair use case bring team see incorporate feature,issue,positive,positive,positive,positive,positive,positive
726983311,"This looks great! I hadn't gotten a chance to play around with different hyperparameters after my initial attempt.

Personally I prefer constant learning rate, because if you see the reward flattens out, you can reduce the training steps to that point.

I'm going to retrain both vector and visual with these parameters over the weekend and will update the models on this branch (assuming I have push permissions).",great gotten chance play around different initial attempt personally prefer constant learning rate see reward reduce training point going retrain vector visual weekend update branch assuming push,issue,positive,positive,positive,positive,positive,positive
726974716,"Use case 1: I am working on a remote server where the training code is in notebooks, accessed by serverhost.com/training.ipynb. I launch a large game from an ssh session to remote server in the background with & and now I can continue interacting with that game from the python notebook, whenever I want to start from beginning, I can call env.reset()

Use case 2: The game is a huge simulation that takes several minutes to init/start but only seconds to reset(). So now if I am experimenting and fixing bugs in code, every time I restart my notebook kernel, I have to recreate the game as the env would launch the game. However if the game is running separately, then I can keep resetting the unity env and keep resetting my jupyter notebook kernel, without shutting down the large simulation and restarting it.",use case working remote server training code launch large game session remote server background continue game python notebook whenever want start beginning call use case game huge simulation several reset fixing code every time restart notebook kernel recreate game would launch game however game running separately keep unity keep notebook kernel without shutting large simulation,issue,negative,negative,negative,negative,negative,negative
726953802,"@ervteng, thank you for encouragement and suggestion. I am working on adapting match3 sensor to my game right now. If it works out I would like to share it.",thank encouragement suggestion working match sensor game right work would like share,issue,positive,negative,neutral,neutral,negative,negative
726953273,"Yes you're right, our current settings is that you have to specify `--env=PATH_TO_FILE` in order to train with binary build, otherwise it will connect to the editor.

Could you tell us a bit more about why that doesn't work for your case? Is there some specific reason you have to keep the binary running and then launch the training instead of letting the training session launch the game?",yes right current specify order train binary build otherwise connect editor could tell u bit work case specific reason keep binary running launch training instead training session launch game,issue,negative,negative,neutral,neutral,negative,negative
726916948,"hi @kirstenrichardson - would you be interested in being an alpha user for our cloud training service (specifically for the 2nd bullet on forums around visual encoder using resnet)? If so, I can provide some more details. can you shoot me an emai to jeffrey@unity3d.com?
",hi would interested alpha user cloud training service specifically bullet around visual provide shoot,issue,negative,positive,positive,positive,positive,positive
726910809,"hi @w3eydi - thank you again for your contribution.  apologies for the late reply, I was able to have a turkish speaker on the team finalize this.  

the checks are failing because you have trailing white spaces.  if you remove those, this should be good to go.  thank you again!",hi thank contribution late reply able speaker team finalize failing trailing white remove good go thank,issue,positive,positive,positive,positive,positive,positive
726427194,"Thanks for raising this, this is definitely one of the things that's on our list of features to add.
As you mentioned, the fastest temporary solution would be increasing the ""keep_checkpoints"" in config and track the best one with the training logs. 
I'll bring this up with the team and prioritize it when possible.

Similar request has been logged as MLA-553 internally.",thanks raising definitely one list add temporary solution would increasing track best one training bring team possible similar request logged internally,issue,positive,positive,positive,positive,positive,positive
726422151,"Hi,

Thanks for the information. After some testing, this is issue is only on Windows and the same thing typically only takes ~0.9s on Mac.
I also got ~4s delay with windows machine, and a delay of a few seconds is indeed an inconvenience for running a game.

We'll look more into this and see if we can get a fix out soon. Thanks for reporting this.",hi thanks information testing issue thing typically mac also got delay machine delay indeed inconvenience running game look see get fix soon thanks,issue,negative,negative,neutral,neutral,negative,negative
726236031,"> If you want these changes in release_10, I think it is fine. Do you want to modify the changelog or you don't think it is needed?

no, i dont think we need to modify change log.  i will merge.  thx!",want think fine want modify think dont think need modify change log merge,issue,negative,positive,positive,positive,positive,positive
726229159,"If you want these changes in release_10, I think it is fine. Do you want to modify the changelog or you don't think it is needed?
",want think fine want modify think,issue,negative,positive,positive,positive,positive,positive
726219125,@vincentpierre are you okay if I merge this into the release_10 branch?  made @ervteng requested changes but not in master yet.,merge branch made master yet,issue,negative,neutral,neutral,neutral,neutral,neutral
726041113,"Any Update about it ? 
error CS0234: The type or namespace name 'Extensions' does not exist in the namespace 'Unity.MLAgents' (are you missing an assembly reference?)
I'm taking also like that but eveerything is clear in to the code ",update error type name exist missing assembly reference taking also like clear code,issue,negative,negative,neutral,neutral,negative,negative
725643414,"Hello,

I measured the delay by using the System.Diagnostics.Stopwatch in Academy.InitializeEnvirontment

```
        void InitializeEnvironment()
        {
            ....
                System.Diagnostics.Stopwatch sw = new System.Diagnostics.Stopwatch();
                sw.Start();
                try
                {
                    var unityRlInitParameters = Communicator.Initialize(
                        new CommunicatorInitParameters
                        {
                            unityCommunicationVersion = k_ApiVersion,
                            unityPackageVersion = k_PackageVersion,
                            name = ""AcademySingleton"",
                            CSharpCapabilities = new UnityRLCapabilities()
                        });
                    UnityEngine.Random.InitState(unityRlInitParameters.seed);
                    // We might have inference-only Agents, so set the seed for them too.
                    m_InferenceSeed = unityRlInitParameters.seed;
                    TrainerCapabilities = unityRlInitParameters.TrainerCapabilities;
                    TrainerCapabilities.WarnOnPythonMissingBaseRLCapabilities();
                }
                catch
                {
                    Debug.Log($"""" +
                        $""Couldn't connect to trainer on port {port} using API version {k_ApiVersion}. "" +
                        ""Will perform inference instead.""
                    );
                    Communicator = null;
                }
                sw.Stop();
                Debug.Log(""Time to attempt communications: "" + sw.Elapsed);
            ....
        }
```

Note that the time does not seem to be consistent. I just tried again and it is now reporting four seconds. Yesterday it was reporting 10 seconds:

```
Time to attempt communications: 00:00:04.1327095
```
Just tested with the ""Basic"" scene and it too takes approximately four seconds

```
Time to init academy 00:00:04.1557059
```




",hello measured delay void new try new name new might set seed catch could connect trainer port port version perform inference instead communicator null time attempt note time seem consistent tried four yesterday time attempt tested basic scene approximately four time academy,issue,negative,positive,neutral,neutral,positive,positive
725093521,"HI @CubeMD, that's great to hear! 12 hours is actually not bad for such a large observation space. Definitely give the Match 3 sensors a go - they've been merged into `master`, and should speed learning up quite a bit. 

Encoders are created in Python here:  https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/torch/utils.py#L144

The type of encoder is currently determined simply by its dimension - a 3D sensor is given a CNN, 1d is given a fully-connected network. You can imagine adding new types of encoders for 2D observations as well. We're looking to change this in the _very_ near future with more sophisticated type detection, so stay tuned!",hi great hear actually bad large observation space definitely give match go master speed learning quite bit python type currently determined simply dimension sensor given given network imagine new well looking change near future sophisticated type detection stay tuned,issue,positive,positive,positive,positive,positive,positive
725075380,"Hi,

I'd like to clarify a bit more about this issue:

1. Can you provide more details on how you get the 10 second delay (like how you measure this 10 sec, within editor or with binary build, etc)?
2. Can you reproduce this issue with any of the example scene?

From what I observed, if I load any one of the example scene and press the play button (no python running, all agents set to default or inference-only), it starts running right away without noticeable delay. 
More details on how you observed this long delay would be helpful.",hi like clarify bit issue provide get second delay like measure sec within editor binary build reproduce issue example scene load one example scene press play button python running set default running right away without noticeable delay long delay would helpful,issue,positive,positive,neutral,neutral,positive,positive
724957629,"ahh yes. Does exactly what I need... 
I don't know how I missed that.

Thanks!",yes exactly need know thanks,issue,positive,positive,positive,positive,positive,positive
724340026,"> Looks good.
> 
> Is there a reason that there's both
> Project/Assets/ML-Agents/Examples/Match3/Prefabs/FBX/Match.fbx
> Project/Assets/ML-Agents/Examples/Match3/Prefabs/FBX/3.fbx
> ? Did ""Match 3"" get turned into two files?

i brought them in as 3 separate meshes so i could independently scale the 3 so this was intended",good reason match get turned two brought separate could independently scale intended,issue,negative,positive,positive,positive,positive,positive
724171159,"Hi @mehrdad995gta 
The code on github is a later version on ML-Agents compared to the version on the Unity Package Manager. To make it work, you will need to get the `Project` folder that corresponds to the [version of the package](https://github.com/Unity-Technologies/ml-agents/tree/com.unity.ml-agents_1.5.0). The corresponding github tag is `com.unity.ml-agents_1.5.0`",hi code later version version unity package manager make work need get project folder version package corresponding tag,issue,negative,neutral,neutral,neutral,neutral,neutral
724152444,"hi @gkhj0516 - unfortunately, we do not debug models generated outside of ML-Agents.  Please refer your questions to the Barracuda issues page [here](https://github.com/Unity-Technologies/barracuda-release) or the barracuda [forum](https://forum.unity.com/forums/barracuda.500/)",hi unfortunately outside please refer barracuda page barracuda forum,issue,negative,neutral,neutral,neutral,neutral,neutral
724136939,"hi @18goldr - thank you for raising this and certainly understand the annoyance.  this was primarily due to the international backgrounds and diversity across our community and core team members.

i have a ticket internally to see if we can resolve this in a straightforward and clean way.",hi thank raising certainly understand annoyance primarily due international diversity across community core team ticket internally see resolve straightforward clean way,issue,positive,positive,positive,positive,positive,positive
723704329,"Hello @ervteng, currently I am making a game that is somewhat similar to chess, however, there is quite a bit more stuff on the board and there are effects that can be applied to the pieces. 

From the previous projects, I know that observation size and structure are playing a crucial role in training efficiency. In my case, there are ~15 attributes related to each piece (compared to ~5 per tile), so I decided to separate pieces observations from tiles as most of them are empty at any given time. With such an approach, I was able to reduce the observation space size from 300 to 125.

Long story short I was able to train a decent AI in ~12 hours with 8 total environments running in parallel. I might have achieved better results if I continued training, however, after a more careful inspection of AI's playstyle I concluded that my approach might have over complicated things or was not sufficient. Also at a time, there was a bug due to which I was unable to resume training anyway (thank you for the fix).

I came around the grid and match3 sensors conveniently right as I started thinking about using convolutions for solving my problem and currently testing this approach to see if it produces better results.

In your reply, you mentioned that an appropriate encoder needs to be created for each sensor type. I am interested in the internal workings of ML-Agents and studying both C# and Python sides of things. Could you point me to any place where it is done for any sensor? 

I would be extremely happy to contribute to ML-Agents. I have been working with it since 0.4 and have made several projects during the time.",hello currently making game somewhat similar chess however quite bit stuff board effect applied previous know observation size structure crucial role training efficiency case related piece per tile decided separate empty given time approach able reduce observation space size long story short able train decent ai total running parallel might better continued training however careful inspection ai approach might complicated sufficient also time bug due unable resume training anyway thank fix came around grid match conveniently right thinking problem currently testing approach see better reply appropriate need sensor type interested internal python side could point place done sensor would extremely happy contribute working since made several time,issue,positive,positive,neutral,neutral,positive,positive
723420014,"> hi @mehrdad995gta - when you imported ml-agents from the assets folder, which version in ML-A are you using?

hi @unityjeffrey I was using ml-agents 1.5.0 preview, installed from the package manager.",hi asset folder version hi preview package manager,issue,negative,neutral,neutral,neutral,neutral,neutral
723222131,"@kirstenrichardson got it!  you are def free to use any of the colors, agents, and reference ML-Agents, the ArXiv paper and Unity.  

The only small change is the remove the Unity logo on the CubeTrack image (Unity prohibits use of the logo outside of company uses).  

I also read through your article (looks great!).  In the article, its okay to keep the image with the logos on the smaller examples (since that came from our repo and is licensed as such where you can use it).  Same comment as above, any of the images you generate with CubeTrack, if you can remove the logo.

Do let us know when it goes up on Towards Data Science.",got free use color reference paper unity small change remove unity image unity use outside company also read article great article keep image logo smaller since came licensed use comment generate remove let u know go towards data science,issue,positive,positive,positive,positive,positive,positive
723217383,"@unityjeffrey licensing - purely the use of the colours, headband etc. - can change if would prefer. Thanks",purely use headband change would prefer thanks,issue,negative,positive,positive,positive,positive,positive
723216096,"hi @kirstenrichardson - just to confirm, are you asking in terms of licensing / branding? or something more specific techincally?",hi confirm something specific,issue,negative,neutral,neutral,neutral,neutral,neutral
723213476,"@unityjeffrey just wanted to check the use of the AgentCube_Blue and AgentCube_Purple prefab in this project https://github.com/kirstenrichardson/CubeTrack is ok before the associated article goes up on Towards Data Science? 
",check use prefab project associated article go towards data science,issue,negative,neutral,neutral,neutral,neutral,neutral
723213005,"hi @mehrdad995gta - when you imported ml-agents from the assets folder, which version in ML-A are you using?",hi asset folder version,issue,negative,neutral,neutral,neutral,neutral,neutral
722938566,"Hi, @Oceania2018 , Is there any progress on the integration with YOLOv3 and ML-agent? I am looking forward to this :)",hi progress integration looking forward,issue,negative,neutral,neutral,neutral,neutral,neutral
722610990,"Some tests are failing because since this PR is not self-contained, and it needs some changes on python. Leave those to python PR will check when merge.",failing since need python leave python check merge,issue,negative,neutral,neutral,neutral,neutral,neutral
721891526,Is there any update regarding the capability documentation?,update regarding capability documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
721597537,"Hi @ervteng, thanks for your reply.

_Wrt NaN values in observations:_
I've already verified that no error-prone values were generated by behaviors, observations, or rewards.
All state observations are properly Mathf.Clamp'ed to ensure that they are between [-1f, 1f] (however, by design, they already belong to the correct interval).

_About the turning point of NaN:_
Impossible to say at this point: NaN appearing between two (1000-large) checkpoints for the two measurements.

I'll take your advice and turn Curiosity off in the next tries.
Thanks again.",hi thanks reply nan already state properly ensure however design already belong correct interval turning point nan impossible say point nan two two take advice turn curiosity next thanks,issue,positive,negative,neutral,neutral,negative,negative
721353042,"Agreed, this would be a useful feature. Hard part would be to make a simple entry in the configuration YAML that defines all of the layer configurations, without making it harder for beginner users. I'll log this request and we'll prioritize accordingly. 

Right now the fastest way to do it would be to edit the LinearEncoder in [layers.py](https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/torch/layers.py) and put in the new network architecture. It shouldn't have any issues exporting to ONNX, either. ",agreed would useful feature hard part would make simple entry configuration layer without making harder beginner log request accordingly right way would edit put new network architecture either,issue,positive,positive,neutral,neutral,positive,positive
721350936,"This seems like a Unity editor issue. Perhaps try posting in the Unity Editor forum (https://forum.unity.com/forums/editor-general-support.10/) or contacting Unity support. 

I'll update this issue if I find any additional information, as well. ",like unity editor issue perhaps try posting unity editor forum unity support update issue find additional information well,issue,positive,neutral,neutral,neutral,neutral,neutral
721350037,"Hi @ShirelJosef, it's a bit harder with the Gym interface because gym is much more limited in what you can pass. 
If you're using the Python-API, there is a stats_side_channel that you can instantiate on the Python side and call to get stats. 

Then on the C# side you can add stats following this here: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Using-Tensorboard.md#custom-metrics-from-unity

You can also make your own side channels: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Custom-SideChannels.md",hi bit harder gym interface gym much limited pas python side call get side add following also make side,issue,negative,negative,neutral,neutral,negative,negative
721347221,"Hey @Dastyn, ugh NaN's can be pretty crappy to debug. First thing I'd check is if there's any logs on the C# side saying that there are NaN observations. And check to see that the observations and rewards are all of reasonable size (around -1 to 1, and no huge positive and negative values). 

Also, from your plots is Curiosity or GAIL reward hitting NaN first? I suspect it might be coming from one of those modules. Since Curiosity doesn't work too well with SAC anyways, turning that off might help. ",hey ugh nan pretty first thing check side saying nan check see reasonable size around huge positive negative also curiosity reward nan first suspect might coming one since curiosity work well sac anyways turning might help,issue,positive,positive,positive,positive,positive,positive
721342306,"Hi @CubeMD, this is actually a great point and adding new sensor types is something we're actively working on. One of the tricks is that for each sensor type, the model side has to be able to create the appropriate encoder. With certain sensor dimensions, this is a real challenge. If you have a sensor that you've implemented, we'd be super happy to help merge it into the repo. 

If I may ask, what is your current use-case for the sensor you're looking to implement?",hi actually great point new sensor something actively working one sensor type model side able create appropriate certain sensor real challenge sensor super happy help merge may ask current sensor looking implement,issue,positive,positive,positive,positive,positive,positive
720982817,"mine problem solved writing here hopefully it will help someone
first go to this folder ""ml-agents-envs"" then call it

# %cd /content/ml-agents-release_8/ml-agents-envs
from mlagents_envs.environment import UnityEnvironment
",mine problem writing hopefully help someone first go folder call import,issue,negative,positive,positive,positive,positive,positive
720209330,"Hello. I tried to do the same thing, and found the reason. The thing is, in ""mlagents/torch_utils/torch.py"" torch means ""mlagents/trainers/torch"", you need to refactor rename ""mlagents/trainers/torch"" folder. Hope it helps :)",hello tried thing found reason thing torch need rename folder hope,issue,negative,neutral,neutral,neutral,neutral,neutral
720152344,"After some thoughts and discovering match3 branch, I decided to add a couple of things. I might have been too harsh with the wording of sensors being hard to reuse. What I meant is that having a board sensor is a good idea, as this functionality is common for many games, however, there might be ""leftover"" observations which might not fit in (eg turns left). I guess if a leftover observation is complex enough to require dimensionality it would deserve a sensor of its own anyway.
So far I have only used Vector Sensor for my projects, and I was afraid of missing out on a new feature. After taking a look at sensor implementation for match3 I think that writing own sensors might be a wiser decision.",match branch decided add couple might harsh wording hard reuse meant board sensor good idea functionality common many however might leftover might fit turn left guess leftover observation complex enough require dimensionality would deserve sensor anyway far used vector sensor afraid missing new feature taking look sensor implementation match think writing might wiser decision,issue,negative,negative,neutral,neutral,negative,negative
720079379,"I'm getting a similar error.

I have tried two versions of Python (3.6.7 and 3.7.9). But they show the same result.

  Windows10 64bit.

 Version information:
  ml-agents: 0.21.0,
  ml-agents-envs: 0.21.0,
  Communicator API: 1.2.0,
  TensorFlow: 2.3.1",getting similar error tried two python show result bit version information communicator,issue,negative,neutral,neutral,neutral,neutral,neutral
720065102,"Hi @chriselion,

I reduced the batch_size and the buffersize: it works now with SAC (PPO does not yield good results at first glance, on my case study).
As you mentioned, reducing the memory from compressed observations would be welcome.

Thank you for your help,
Dastyn

P.S. : W.r.t. VisualPyramids, I finally succeeded in launching DemoRecorder on the game (I don't know what went wrong before!). I will spend a bit of computation time to see if it makes crash the memory.",hi reduced work sac yield good first glance case study reducing memory compressed would welcome thank help finally game know went wrong spend bit computation time see crash memory,issue,positive,positive,positive,positive,positive,positive
719841057,"Thanks Chris! :)

Looking forward to the next release.
",thanks looking forward next release,issue,negative,positive,neutral,neutral,positive,positive
719833423,"Hi @Phong13,
I updated the model export so that action_probs will no longer appear as an output. I also added your comments to the internal issue we have tracking your value estimate requests.",hi model export longer appear output also added internal issue value estimate,issue,negative,neutral,neutral,neutral,neutral,neutral
718267466,I added MLA-1518 to our internal tracker to explore how to reduce the memory from compressed observations in the buffer.,added internal tracker explore reduce memory compressed buffer,issue,negative,neutral,neutral,neutral,neutral,neutral
718253630,"Hi folks,
Sorry for the delay; there's a lot going on here.

> I did not succeed in recording demos of VisualPyramids example (which should share many important features with my own case study), due to some crashes as soon as 'play' button is pressed into the Editor. 

Do you have a stack trace for this? Pressing play in VisualPyramids works for me (although the agent falls back to keyboard control since there's no model), and I was able to add a DemonstrationRecorder and save demonstrations.

---

As for memory: the buffer that stores the observations is going to dominate the memory usage. My back of the envelope math says that we need
`(buffer size) * (height * width * channels floats) * (4 bytes per float)`
bytes to store the visual observations.

@Dastyn In your case, that's 512000 * 50 * 50 * 12 * 4 = 61.44 gigabytes. I'm guessing the reason the virtual memory levels off around 250K steps is that it starts spilling to swap.

@sebastianulm You didn't say what trainer you're using. For SAC, with the buffer size in the GridWorld config file, that's 50000 * 480 * 255 * 3 * 4 = 73.44 gigabytes.

If you're passing float observations (which it sounds like @Dastyn is), I don't think quantizing them to save space would be a good choice. For observations that arrive as PNGs, we currently store them as uncompressed numpy float32 arrays, which is inefficient (we should either keep the compressed or as uint8 arrays). I'll log a feature request to handle that better.

In the meantime, if you're not able to reduce the size of your observations, I think you'll either need to shrink the buffers and/or use PPO instead of SAC.",hi sorry delay lot going succeed recording demo example share many important case study due soon button editor stack trace pressing play work although agent back keyboard control since model able add save memory buffer going dominate memory usage back envelope math need buffer size height width per float store visual case guessing reason virtual memory around swap say trainer sac buffer size file passing float like think save space would good choice arrive currently store uncompressed float inefficient either keep compressed log feature request handle better able reduce size think either need shrink use instead sac,issue,positive,positive,positive,positive,positive,positive
718238991,"I tried `--initialize-from` and it didn't help me. When steps from origin training + steps from new ""initialized-from"" becomes more than int32.Max - the same exception thrown.
It feels like some int var stored inside origin brain.

Well, ok, thanks anyway. Maybe i'll try to start new training with your changes if my new config wouldn't help me... Will try to say results here maybe in a month...",tried help origin training new becomes exception thrown like inside origin brain well thanks anyway maybe try start new training new would help try say maybe month,issue,positive,positive,positive,positive,positive,positive
718224162,"I was afraid of that. I don't know if there's any good way to reuse your checkpoints with the changes.

If you don't want to try out the patch, you can hopefully recover your existing data using the `--initialize-from` option and pass the previous run-id (I think `393_Patient_Pose_Train` based on your logs). This will start training over again from step 0 but using the existing weights. However, if there are NaNs in your latest checkpoint, you may need to delete it (or move it) in order to load from a previous checkpoint.

Edit: you can also modify the `results/<run_id>/<behavior_name>/checkpoint` file to control which checkpoint is loaded.",afraid know good way reuse want try patch hopefully recover data option pas previous think based start training step however latest may need delete move order load previous edit also modify file control loaded,issue,positive,positive,neutral,neutral,positive,positive
718139386,"I have the same Probem, I guess? 

Currently just starting out with MLAgents I keep running into memory leaks when doing ANYTHING with visual observation. So I checked out master, opened the GridWorld example and increased the Render Texture (and camera sensor) size to 480x255. 
After about a Minute the mlagents-learn process consumes 12+ Gigabytes of ram and still increases by about 200+ mb every few seconds. 

Yes, thats a ""bit larger"" image, but my current solution (homebrew) just eats it fine. So I cannot imagine that this should not be possible. Same as above guy, reducing this is not an option.
 
Using Unity 2019.4.4f1 on 5.9.1-2-MANJARO (Linux) but its likely the same on Windows.",guess currently starting keep running memory anything visual observation checked master example render texture camera sensor size minute process ram still every yes thats bit image current solution eats fine imagine possible guy reducing option unity likely,issue,positive,positive,neutral,neutral,positive,positive
718038237,"Cool, glad that fixed it. The bug was only introduced late last week, so you just had bad timing :)",cool glad fixed bug late last week bad timing,issue,negative,negative,neutral,neutral,negative,negative
717629414,"Thank you for your help.  I was pretty sure I pulled the latest from master this morning, but I was uncertain enough that I went ahead and pulled again.

What fortuitous timing that this bug would be fixed the very day I encounter it!  After training a new model with the latest code, it imports properly.

Once again, thanks for your prompt attention.

",thank help pretty sure latest master morning uncertain enough went ahead fortuitous timing bug would fixed day encounter training new model latest code properly thanks prompt attention,issue,positive,positive,positive,positive,positive,positive
717628255,"I've got exceptions when tried to resume training from checkpoint.

I suppose i can't just resume it with ur changes.

```
2020-10-28 03:51:08 INFO [tf_policy.py:165] Loading model for brain PatientPose?team=0 from results/393_Patient_Pose_Train/PatientPose.
2020-10-28 03:51:08.636221: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: tensor_name = global_step; expected dtype int64 does not equal original dtype int32
tensor_name = normalization_steps; expected dtype int64 does not equal original dtype int32
2020-10-28 03:51:08 INFO [trainer_controller.py:108] Saved Model
2020-10-28 03:51:09 INFO [environment.py:418] Environment shut down with return code 0.
2020-10-28 03:51:09 INFO [environment.py:418] Environment shut down with return code 0.
2020-10-28 03:51:10 INFO [environment.py:418] Environment shut down with return code 0.
2020-10-28 03:51:10 INFO [environment.py:418] Environment shut down with return code 0.
2020-10-28 03:51:11 INFO [environment.py:418] Environment shut down with return code 0.
2020-10-28 03:51:11 INFO [environment.py:418] Environment shut down with return code 0.
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call
    return fn(*args)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn
    target_list, run_metadata)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: tensor_name = global_step; expected dtype int64 does not equal original dtype int32
tensor_name = normalization_steps; expected dtype int64 does not equal original dtype int32
	 [[{{node save_1/RestoreV2}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 1290, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 956, in run
    run_metadata_ptr)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run
    run_metadata)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: tensor_name = global_step; expected dtype int64 does not equal original dtype int32
tensor_name = normalization_steps; expected dtype int64 does not equal original dtype int32
	 [[node save_1/RestoreV2 (defined at /lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'save_1/RestoreV2':
  File ""/bin/mlagents-learn"", line 8, in <module>
    sys.exit(main())
  File ""/lib/python3.7/site-packages/mlagents/trainers/learn.py"", line 322, in main
    run_cli(parse_command_line())
  File ""/lib/python3.7/site-packages/mlagents/trainers/learn.py"", line 318, in run_cli
    run_training(run_seed, options)
  File ""/lib/python3.7/site-packages/mlagents/trainers/learn.py"", line 163, in run_training
    tc.start_learning(env_manager)
  File ""/lib/python3.7/site-packages/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py"", line 218, in start_learning
    self._create_trainers_and_managers(env_manager, new_behavior_ids)
  File ""/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py"", line 204, in _create_trainers_and_managers
    self._create_trainer_and_manager(env_manager, behavior_id)
  File ""/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py"", line 180, in _create_trainer_and_manager
    trainer.add_policy(parsed_behavior_id, policy)
  File ""/lib/python3.7/site-packages/mlagents/trainers/ppo/trainer.py"", line 232, in add_policy
    self.optimizer = PPOOptimizer(self.policy, self.trainer_settings)
  File ""/lib/python3.7/site-packages/mlagents/trainers/ppo/optimizer.py"", line 99, in __init__
    self.policy.initialize_or_load()
  File ""/lib/python3.7/site-packages/mlagents/trainers/policy/tf_policy.py"", line 207, in initialize_or_load
    self._load_graph(self.model_path, reset_global_steps=reset_steps)
  File ""/lib/python3.7/site-packages/mlagents/trainers/policy/tf_policy.py"", line 162, in _load_graph
    self.saver = tf.train.Saver(max_to_keep=self.keep_checkpoints)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 828, in __init__
    self.build()
  File ""/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 878, in _build
    build_restore=build_restore)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 508, in _build_internal
    restore_sequentially, reshape)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 328, in _AddRestoreOps
    restore_sequentially)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py"", line 1696, in restore_v2
    name=name)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.7/bin/mlagents-learn"", line 8, in <module>
    sys.exit(main())
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/learn.py"", line 322, in main
    run_cli(parse_command_line())
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/learn.py"", line 318, in run_cli
    run_training(run_seed, options)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/learn.py"", line 163, in run_training
    tc.start_learning(env_manager)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py"", line 218, in start_learning
    self._create_trainers_and_managers(env_manager, new_behavior_ids)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py"", line 204, in _create_trainers_and_managers
    self._create_trainer_and_manager(env_manager, behavior_id)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py"", line 180, in _create_trainer_and_manager
    trainer.add_policy(parsed_behavior_id, policy)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/ppo/trainer.py"", line 232, in add_policy
    self.optimizer = PPOOptimizer(self.policy, self.trainer_settings)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/ppo/optimizer.py"", line 99, in __init__
    self.policy.initialize_or_load()
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/policy/tf_policy.py"", line 207, in initialize_or_load
    self._load_graph(self.model_path, reset_global_steps=reset_steps)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/policy/tf_policy.py"", line 177, in _load_graph
    self.saver.restore(self.sess, ckpt.model_checkpoint_path)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 1326, in restore
    err, ""a mismatch between the current graph and the graph"")
tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

tensor_name = global_step; expected dtype int64 does not equal original dtype int32
tensor_name = normalization_steps; expected dtype int64 does not equal original dtype int32
	 [[node save_1/RestoreV2 (defined at /lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'save_1/RestoreV2':
  File ""/bin/mlagents-learn"", line 8, in <module>
    sys.exit(main())
  File ""/lib/python3.7/site-packages/mlagents/trainers/learn.py"", line 322, in main
    run_cli(parse_command_line())
  File ""/lib/python3.7/site-packages/mlagents/trainers/learn.py"", line 318, in run_cli
    run_training(run_seed, options)
  File ""/lib/python3.7/site-packages/mlagents/trainers/learn.py"", line 163, in run_training
    tc.start_learning(env_manager)
  File ""/lib/python3.7/site-packages/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py"", line 218, in start_learning
    self._create_trainers_and_managers(env_manager, new_behavior_ids)
  File ""/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py"", line 204, in _create_trainers_and_managers
    self._create_trainer_and_manager(env_manager, behavior_id)
  File ""/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py"", line 180, in _create_trainer_and_manager
    trainer.add_policy(parsed_behavior_id, policy)
  File ""/lib/python3.7/site-packages/mlagents/trainers/ppo/trainer.py"", line 232, in add_policy
    self.optimizer = PPOOptimizer(self.policy, self.trainer_settings)
  File ""/lib/python3.7/site-packages/mlagents/trainers/ppo/optimizer.py"", line 99, in __init__
    self.policy.initialize_or_load()
  File ""/lib/python3.7/site-packages/mlagents/trainers/policy/tf_policy.py"", line 207, in initialize_or_load
    self._load_graph(self.model_path, reset_global_steps=reset_steps)
  File ""/lib/python3.7/site-packages/mlagents/trainers/policy/tf_policy.py"", line 162, in _load_graph
    self.saver = tf.train.Saver(max_to_keep=self.keep_checkpoints)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 828, in __init__
    self.build()
  File ""/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 878, in _build
    build_restore=build_restore)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 508, in _build_internal
    restore_sequentially, reshape)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 328, in _AddRestoreOps
    restore_sequentially)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py"", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py"", line 1696, in restore_v2
    name=name)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()
```",got tried resume training suppose ca resume ur loading model brain invalid argument equal original equal original saved model environment shut return code environment shut return code environment shut return code environment shut return code environment shut return code environment shut return code recent call last file line return file line file line equal original equal original node handling exception another exception recent call last file line restore file line run file line file line file line raise type message equal original equal original node defined original stack trace file line module main file line main file line file line file line wrapped return file line file line file line policy file line file line file line file line file line file line build file line file line reshape file line file line return file line file line file line return file line file line file line handling exception another exception recent call last file line module main file line main file line file line file line wrapped return file line file line file line policy file line file line file line file line file line restore err mismatch current graph graph likely due mismatch current graph graph please ensure graph based original error equal original equal original node defined original stack trace file line module main file line main file line file line file line wrapped return file line file line file line policy file line file line file line file line file line file line build file line file line reshape file line file line return file line file line file line return file line file line file line,issue,positive,positive,positive,positive,positive,positive
717597642,"Hi @grimli333 - I think we just fixed this last night. Can you make sure you have up to commit 72492272ef6e749dd43f0c23a7b4176bb23d2af4 (PR https://github.com/Unity-Technologies/ml-agents/pull/4608)?

(I'm assuming ""ML-Agents version: latest from github"" means latest `master` branch, not latest release)",hi think fixed last night make sure commit assuming version latest latest master branch latest release,issue,positive,positive,positive,positive,positive,positive
716917587,"My dream solution for this would be to configure what outputs are generated. In my case I want two .nn files.

- one would be action only
- one would be value_estimate only

The BehaviourParameters component could have a list of slots for .nn files. I would assign these files to the first and second slots. When I am requesting a decision for an agent I could specifiy which .nn models I want executed. This way I could query for the value_estimate only or the policy output or both at the same time if there was a .nn file configured to include both.",dream solution would configure case want two one would action one would component could list would assign first second decision agent could want executed way could query policy output time file include,issue,positive,positive,positive,positive,positive,positive
716914673,"Thanks for looking at this. I did hack the model_serialization.py POSSIBLE_OUTPUT_NODES as you described to produce some .nn files that had different combos of models in them. Here is what they show in the inspector. The value estimate is not normally exported because it doesn't have an identifier in the tensorflow graph (I hacked the trainer to add it back in). Also the value estimate name has changed to ""optimizer/value_estimate"". 

You can see in the inspector that there is more going on in the .nn file graph when there are additional outputs. (44 Layers vs. 36 Layers vs. 27 Layers). I presume this corresponds directly to the effort needed to process these graphs. 

If you want to generate some models of your own it is easy to do by hacking the list and in model_serialization.py. The walking biped example scene has similar dimensions to my models (512x3 layers, not as many inputs).

![image](https://user-images.githubusercontent.com/11359505/97244444-fdd44b00-17b5-11eb-82a2-4417adfe93b7.png)
",thanks looking hack produce different show inspector value estimate normally identifier graph hacked trainer add back also value estimate name see inspector going file graph additional presume directly effort process want generate easy hacking list walking biped example scene similar many image,issue,positive,positive,positive,positive,positive,positive
716907288,"Hi. Very appreciate for your quick response.

Sorry for missing version. Here is it:
 Unity: 2020.1.9f1
 ML Agents: 1.0.5
ml-agents-envs: 0.17.0

It's excluded - about NaN values in observations. I tested it few times with loading from checkpoint. The same behavior every time. 

As much i remember - i can't just update my ml-agents package and continue training. So i can see your branch changes tomorrow, will apply them on my version and then i'll say the results.",hi appreciate quick response sorry missing version unity nan tested time loading behavior every time much remember ca update package continue training see branch tomorrow apply version say,issue,negative,negative,neutral,neutral,negative,negative
716900443,"Hi @Phong13 - sorry for the delay on this.

You're right that it doesn't look like we're using ""action_probs"" or ""value_estimate"" at inference time. You could possibly hack the list of output nodes here
https://github.com/Unity-Technologies/ml-agents/blob/672b6081e6d36b85aa76b7014e12341b8ceb0597/ml-agents/mlagents/trainers/tf/model_serialization.py#L43-L45
(although it sounds like that might be what you're already doing). We can look into removing things from this list or making it more configurable in the future.

Would it be possible for you to provide a model file that demonstrates this, so that I can run it by the Barracuda team and make sure there's not a missed optimization opportunity for them? The actual weights don't matter so you can just run training for one step. The easiest format to work with would be the `frozen_graph_def.pb` file that's generated as part of training. Alternatively, if there's one of our example scenes that's similar to your setup and can be easily modified (either in the training configuration or Agent parameters) that would work too.

Thanks!",hi sorry delay right look like inference time could possibly hack list output although like might already look removing list making future would possible provide model file run barracuda team make sure optimization opportunity actual matter run training one step easiest format work would file part training alternatively one example similar setup easily either training configuration agent would work thanks,issue,positive,positive,neutral,neutral,positive,positive
716888227,"I haven't been able to reproduce the problem here yet. 

The error message indicate that there are NaN values in the observations(`RuntimeError: The observations provided had NaN values.`). I still believe it's _probably_ the int32 overflow resulting in NaN values in the actions, and then stepping Unity with NaN actions results in NaN observations. But it's also possible that the NaNs are coming directly from your observation calculations.

I made some changes based off of the `release_8` branch (that's the latest one; you didn't say which version you're running) that will:
* use int64 for the step values
* raise an error sooner if the trainer produces NaNs

Could you grab the [`MLA-1503-int-overflow-nan`](https://github.com/Unity-Technologies/ml-agents/tree/MLA-1503-int-overflow-nan) branch and see if that helps? I think either:
1) Everything will work fine
2) You'll see the ""The observations provided had NaN values"" exception again, in which case the error is in your environment
3) You'll see a ""NaN action detected"" exception, in which case it's still something in the trainer that isn't fixed with the int64 steps.",able reproduce problem yet error message indicate nan provided nan still believe overflow resulting nan stepping unity nan nan also possible coming directly observation made based branch latest one say version running use step raise error sooner trainer could grab branch see think either everything work fine see provided nan exception case error environment see nan action exception case still something trainer fixed,issue,negative,positive,positive,positive,positive,positive
716840904,"Thanks for the report, this definitely looks like a bug. At first glance, we need to change these
https://github.com/Unity-Technologies/ml-agents/blob/a14730fb4aa16820fe4b4a295a49e9c2c56d5b03/ml-agents/mlagents/trainers/tf/models.py#L44-L49
https://github.com/Unity-Technologies/ml-agents/blob/a14730fb4aa16820fe4b4a295a49e9c2c56d5b03/ml-agents/mlagents/trainers/tf/models.py#L194-L200
to int64s.

I'm going to try to reproduce this (by hacking some initial values, not running for 2B steps) and confirm that changing the type fixes the issue, but if you're blocked, you can try it out locally too.",thanks report definitely like bug first glance need change going try reproduce hacking initial running confirm type issue blocked try locally,issue,positive,positive,neutral,neutral,positive,positive
716683071,hi @RedTachyon - we provide a lower level Python API for any custom training (https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Python-API.md),hi provide lower level python custom training,issue,negative,neutral,neutral,neutral,neutral,neutral
716635520,Hi thanks for the interest. If you are still looking for advice on CL I would recommend the forums https://forum.unity.com/forums/ml-agents.453/ where other users can help you out with their experience as well.,hi thanks interest still looking advice would recommend help experience well,issue,positive,positive,positive,positive,positive,positive
715901814,"Thank you @chriselion for your help.
These are some complements and results.

First, this is below the config.yaml file used for the tests:
```
behaviors:
  AgentBehaviour:
    trainer_type: sac
    hyperparameters:
      learning_rate: 0.005
      learning_rate_schedule: constant
      batch_size: 128 
      buffer_size: 512000
      buffer_init_steps: 1000
      tau: 0.01
      steps_per_update: 100
      save_replay_buffer: false
      init_entcoef: 0.1
      reward_signal_steps_per_update: 10.0
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 3
      vis_encode_type: simple
    behavioral_cloning:
      demo_path: agent.demo
      strength: 0.5
      steps: 50000
      num_epoch: 10
      batch_size: 128
    reward_signals:
      curiosity:
       strength: 0.2
       gamma: 0.99
       encoding_size: 256
       learning_rate: 1e-5 
      extrinsic:
        gamma: 0.995
        strength: 0.9
      gail:
        gamma: 0.99
        strength: 0.1
        encoding_size: 256
        learning_rate: 0.0003
        use_actions: true
        use_vail: false
        demo_path: agent.demo
    keep_checkpoints: 5
    max_steps: 1000000
    time_horizon: 128
    summary_freq: 1000
    threaded: true
```

My case study uses visual observations, thanks to an ISensor (called by a SensorComponent) writing visual observations with shape = (50,50,12), for ('simple') CNN encoding.

Then, this is the screenshot of available virtual memory, as provided by psutil, on my case study, just before the same crash as described above:
![Capture](https://user-images.githubusercontent.com/36076511/97080086-22cf8f00-15f9-11eb-9d91-b16f694b637d.PNG)
Available virtual memory is captured in sac / trainer.py, in functions: _process_trajectory(), update_reward_signal(), and update_sac_policy().

Last, I did not succeed in recording demos of VisualPyramids example (which should share many important features with my own case study), due to some crashes as soon as 'play' button is pressed into the Editor. I did not investigate this point so far.
Note that the demo file provided with the Examples does not correspond to (not the same observation shape, or not for the Visual version of the example indeed).
This said, the executable VisualPyramids version does not raise the same issue.
But as there was no demo file available, the yaml config file used here did not contain BC and GAIL sections.
Then, not so representative ...

I will go on testing, but any thoughts in the meantime about this issue will be very welcome!
Thanks in advance.

EDIT: for information, memory consumption for the environment (Unity executable) is steady and reasonable (measured thanks to System.GC.GetTotalMemory(true)).
",thank help first file used sac constant tau false normalize false simple strength curiosity strength gamma extrinsic gamma strength gamma strength true false threaded true case study visual thanks writing visual shape available virtual memory provided case study crash capture available virtual memory sac last succeed recording demo example share many important case study due soon button editor investigate point far note file provided correspond observation shape visual version example indeed said executable version raise issue file available file used contain representative go testing issue welcome thanks advance edit information memory consumption environment unity executable steady reasonable measured thanks true,issue,positive,positive,positive,positive,positive,positive
715606578,"I reworded that section of the readme to clear up the ambiguity: https://github.com/Unity-Technologies/ml-agents/pull/4603
Thanks for pointing it out!",section clear ambiguity thanks pointing,issue,positive,positive,positive,positive,positive,positive
715584166,"Hi, sorry, I didn't have a chance to respond to your forum post until now. That's a better venue for this sort of discussion, so I'm going to close this issue.",hi sorry chance respond forum post better venue sort discussion going close issue,issue,positive,neutral,neutral,neutral,neutral,neutral
715507922,"For each ray, either
* it hits an object with one of the tags in `m_DetectableTags`
* it hits an object with another tag that isn't in `m_DetectableTags`
* it doesn't hit anything.

So there are `m_DetectableTags.Count + 2` possible results.",ray either object one object another tag hit anything possible,issue,negative,neutral,neutral,neutral,neutral,neutral
715502245,"Hi @Dastyn 
Reducing the `buffer_size` might help too.

14.6 MiB isn't _that_ big, so I'm assuming something else is eating up all your memory. We've had leaks in python before, but there are no known problems at the moment. It could also be coming from your Unity environment.

If you want to add some tracking with psutil, a good way to do this would be to add it as a stat so that it appears in tensorboard; that way we can see how it changes over time. A simple place you could add this is bu using a `stats_reporter` around here:
https://github.com/Unity-Technologies/ml-agents/blob/b7d2b808de1072dc563b6da70cbf2b8fa6a365bc/ml-agents/mlagents/trainers/trainer_controller.py#L245

I've heard that https://mg.pov.lt/objgraph/ is good for debugging these sorts of python leaks, but I've never used it myself.

Can you also attach the config file that you're using? If you can reproduce the behavior using that config and one of example environments, that would help us debug it more easily.

Thanks!

 ",hi reducing might help mib big assuming something else eating memory python known moment could also coming unity environment want add good way would add way see time simple place could add bu around good python never used also attach file reproduce behavior one example would help u easily thanks,issue,positive,positive,positive,positive,positive,positive
715475387,"@unityjeffrey yea absolutely, that's all I was checking, it was so close in design to existing environments I didn't want to not offer ",yea absolutely close design want offer,issue,negative,positive,positive,positive,positive,positive
715468755,"thanks @kirstenrichardson - this is pretty neat!  unfortunately, we currently are not accepting environment contributions at this time.  However, you are correct in that we currently do not have an environment that tracks a moving target.  Would you be interested to put a personal repo and sharing with the community [forum](https://forum.unity.com/forums/ml-agents.453/)?",thanks pretty neat unfortunately currently environment time however correct currently environment moving target would interested put personal community forum,issue,positive,positive,positive,positive,positive,positive
715374744,"I think that because need_increment in ml-agents/mlagents/trainers/settings.py cannot be changed by user and the user can't supply his own custom need_increment there should be a general custom value ""lesson_objective_completed"" between [0,1] which is calculated over the episode.
In each step the user provide a value ""increment_custom_lesson_objective"" between [0,1] which like reward is accumulated over the episode, to a maximum of 1.
Then like reward it can be averaged over several episodes and you can decide the threshold of lesson ""pass"" between [0,1] which is needed to pass the lesson.

From my understanding the only way is to add another field in 
ml-agents-envs/mlagents_envs/communicator_objects/agent_info_pb2.py
for ""increment_custom_lesson_objective""

I also want to add why this is needed:
Lets assume we have 2 spaceships, and the objective is to shoot the other spaceship. (+1 for kill, -1 for getting killed)
However, the spaceship can hit obstacles and crash and then it gets -1 reward and opponent +1.
The lessons should not increment when opponent crashes but only when your spaceship correctly shoot opponent and therefore the reward criteria is somewhat misleading.",think user user ca supply custom general custom value calculated episode step user provide value like reward episode maximum like reward several decide threshold lesson pas pas lesson understanding way add another field also want add assume objective shoot spaceship kill getting however spaceship hit crash reward opponent increment opponent spaceship correctly shoot opponent therefore reward criterion somewhat misleading,issue,positive,positive,neutral,neutral,positive,positive
714834659,"Thanks @CubeMD, we'll make sure the fix handles both cases.
",thanks make sure fix,issue,positive,positive,positive,positive,positive,positive
714827364,"Hi,
Both resume and initialize-from parameters result in the same issue in release 7 when using self-play. ",hi resume result issue release,issue,negative,neutral,neutral,neutral,neutral,neutral
714694248,"Note that it is useful to be able to export the models to separate independent .nn files. In my project I sometimes use the ""value_estimate"" and sometimes use the ""action"" output. These are not used at the same time in my project. It is most performant for me to have two models that can be queried separately. ",note useful able export separate independent project sometimes use sometimes use action output used time project performant two separately,issue,negative,positive,positive,positive,positive,positive
714661043,"Can you either attach your model file, or reproduce this behavior with a model from one of the example scenes? The actual weights of the model shouldn't matter, so you can just run training for a few steps.

Can you also confirm that you're using ""[Release 8](https://github.com/Unity-Technologies/ml-agents/releases/tag/release_8)"" (Unity package 1.5.0-preview, python package 0.21.0)? Since you're using `com.unity.barracuda@1.1.1-preview` I assume it's the one you're using - [v0.8](https://github.com/Unity-Technologies/ml-agents/releases/tag/0.8.0) is actually much older.",either attach model file reproduce behavior model one example actual model matter run training also confirm release unity package python package since assume one actually much older,issue,negative,positive,neutral,neutral,positive,positive
714656124,There are no implementations of this in ml-agents. The intent of the wording there was that you could use the [low-level python interface](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Python-API.md) to implement it yourself (but I don't like the wording personally).,intent wording could use python interface implement like wording personally,issue,negative,neutral,neutral,neutral,neutral,neutral
714603095,"Some more detail about this request:

Please implement it so that the ""policy"" and ""value_estimate"" models can be exported to separate .nn files:

- A combined .nn file that contains both ""policy"" and ""value_estimate""
- Or to a ""policy"" only .nn file
- Or to a ""value_estimate"" only .nn file

This is important for performance. Executing a model that contains the ""value_estimate"" takes almost twice as long as executing a model that contains only the ""policy"". Most of the time I want to query either the ""value_estimate"" or the ""Policy"" but rarely both together. It is a huge performance optimization to be able to separate these.
",detail request please implement policy separate combined file policy policy file file important performance model almost twice long model policy time want query either policy rarely together huge performance optimization able separate,issue,positive,positive,positive,positive,positive,positive
714021317,We're working on a fix in https://github.com/Unity-Technologies/ml-agents/pull/4593. We'll probably do a patch release with this fix sometime next week (depending on whether any other issues show up).,working fix probably patch release fix sometime next week depending whether show,issue,negative,neutral,neutral,neutral,neutral,neutral
713871159,"Hi,
I can reproduce the error on the latest release (release_8). I'll look in this and get back to you soon on a fix.

Also, I made a minor edit to your comment to use ``` to make the logs clearer. Hope you don't mind.",hi reproduce error latest release look get back soon fix also made minor edit comment use make clearer hope mind,issue,negative,positive,positive,positive,positive,positive
713850212,You need to accept the Contributor License Agreement before I can merge this.,need accept contributor license agreement merge,issue,positive,neutral,neutral,neutral,neutral,neutral
713789015,"Since the issue was resolved and has been inactive for some time, I'm closing it. @MoMe36 if you'd like to submit a PR for your changes, they're definitely welcome!",since issue resolved inactive time mome like submit definitely welcome,issue,positive,positive,positive,positive,positive,positive
713758371,"> @ervteng is there a separate PR to update the CI configs to use `--tensorflow`?

Yep - https://github.com/Unity-Technologies/ml-agents-cloud-internal/pull/223. It only affects the old CI, as the new CI doesn't use `--torch` but instead overrides the YAML/RunOptions and won't require changes. ",separate update use yep old new use torch instead wo require,issue,negative,positive,positive,positive,positive,positive
713725031,"@andrewcoh  I seem to have found a solution. First, the build needs to be in the project folder where I created the scene in unity editor. Secondly, I have to build the executable in debugging mode only. Third, when the environment opens after I call UnityEnvironment with the file name, I have to click ""wait"" when it crashes instead of ""force close"". 

Thanks for looking into it! Seems a little buggy but glad I got a solution so I can start using the awesome API.",seem found solution first build need project folder scene unity editor secondly build executable mode third environment call file name click wait instead force close thanks looking little buggy glad got solution start awesome,issue,positive,positive,positive,positive,positive,positive
713177631,"> Since PyTorch is default now, we should also update all .nn model files to .onnx files for the example scenes (or in separate PR?)

@dongruoping - I have that logged as a followup in https://jira.unity3d.com/browse/MLA-1435",since default also update model example separate logged,issue,negative,neutral,neutral,neutral,neutral,neutral
713058138,"Since PyTorch is default now, we should also update all .nn model files to .onnx files for the example scenes (or in separate PR?)",since default also update model example separate,issue,negative,neutral,neutral,neutral,neutral,neutral
713006290,"Hi, I logged this request as MLA-1485 and we will prioritize appropriately. 
The current succcess_criteria we use are reward and progress and these are easy to access in Python. 
How do you think we should implement this feature? We would need to communicate custom data between C# and Python and we would probably need to create a new API to do so. Any suggestions?",hi logged request appropriately current use reward progress easy access python think implement feature would need communicate custom data python would probably need create new,issue,positive,positive,positive,positive,positive,positive
712346990,"Thanks for the super fast response :).

",thanks super fast response,issue,positive,positive,positive,positive,positive,positive
712312164,"Hi,

We logged this request as MLA-1478 and we will prioritize when appropriate. I think this would be a super cool feature, but it would require changing the API quite a bit since currently, there is no room for user code (one the main thread) between the data being sent to barracuda and the actions being received by the Agents. I think that would require a more flexible API than the one we currently have (and that we will be working on). I also see this as being useful during training since Unity is blocked between sending observations and receiving actions as well.
",hi logged request appropriate think would super cool feature would require quite bit since currently room user code one main thread data sent barracuda received think would require flexible one currently working also see useful training since unity blocked sending well,issue,positive,positive,positive,positive,positive,positive
710417259,"This was fixed in #4346 a few months ago (not specifying a behavior will now raise an exception), and is available in the 0.20.0 (and later) version of the mlagents package.",fixed ago behavior raise exception available later version package,issue,negative,positive,positive,positive,positive,positive
710412826,"For reference, this was logged in our internal tracker as MLA-750. I'm closing this issue because we're not supporting the video recorder any further and wont' fix the bug.",reference logged internal tracker issue supporting video recorder wont fix bug,issue,negative,positive,positive,positive,positive,positive
710401453,"There is no support for converting `.nn` files to any other format. As @Shubhamai mentioned, if `tf2onnx` is installed, training will also produce an `.onnx` file and this will be importable into the editor.",support converting format training also produce file importable editor,issue,negative,neutral,neutral,neutral,neutral,neutral
710307807,"I think it could be related with the `python -m mlagents.trainers.upgrade_config -h`. How can I undo those changes?

Another topic, is the Versioning.md updated in each release? because in Release 1, 4, 5... branches have the same versions that file.
",think could related python undo another topic release release file,issue,negative,neutral,neutral,neutral,neutral,neutral
710270169,"Hi,

There was a refactor of the curriculum learning feature and in the current release, using the `--resume` argument will reload the curriculum in the lesson it was in when training was interrupted. ",hi curriculum learning feature current release resume argument reload curriculum lesson training interrupted,issue,negative,neutral,neutral,neutral,neutral,neutral
709039071,This issue has been clarified to me in the [forum](https://forum.unity.com/threads/regarding-env-get_steps-behavior_name.988298/#post-6418421). I do not think this is a bug and the issue should be updated.,issue forum think bug issue,issue,negative,neutral,neutral,neutral,neutral,neutral
708712154,"For posterity - this appears to be due to 
* a bug in pylint with py3.9: https://github.com/PyCQA/pylint/issues/3882
* the version of mypy we're on not supporting 3.9
* the github action not pinning to a python version",posterity due bug version supporting action pinning python version,issue,negative,positive,neutral,neutral,positive,positive
708564485,"Hi,

SAC can be computationally more expensive than PPO because it does back-propagation more frequently (since it can use old data, the buffer is not emptied and many more updates are performed). 
You can alleviate this to some extent by increasing the [steps_per_update](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Configuration-File.md#sac-specific-configurations) hyperparameter. ([See this configuration for example](https://github.com/Unity-Technologies/ml-agents/blob/master/config/sac/3DBall.yaml#L11))",hi sac expensive frequently since use old data buffer many alleviate extent increasing see configuration example,issue,negative,positive,neutral,neutral,positive,positive
708478435,"Yes, but the NullReferences that I brought here are inside com.unity.ml-agents, about Agent... as you can see in the logs. Our environment and the agents, academy... worked in other versions of ML-Agents. I followed the migration instruction each time.

Today I tried between different Python versions (0.16.0, 0.18.0...) and the camera doesn't work when I downgrade 🤷 .",yes brought inside agent see environment academy worked migration instruction time today tried different python camera work downgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
708362071,"Hi, any updates regarding this?
I am experiencing the same issue discussed above when using the 3DBall sample environment. The length of `decision_steps` plus the length of `terminal_steps` is not always the same as the number of agents in the scene. I understand that the logic behind requesting decisions has changed, but now I do not know how are you supposed to step the environment in order to collect all data.

If you do this while stepping the environment:
```
decision_steps, terminal_steps = env.get_steps(behavior_name)
print(len(decision_steps), len(terminal_steps))
```
You get an output looking like this:
```
>> 12, 0
>> 12, 0
...
>> 0, 1
```
The scene has 12 agents, but at some point one of them terminates and you get only one entry inside `terminal_steps`. 
How do I get the data from the remaining agents? Calling `get_steps()` once again doesn't seem to work. Do I need to set a new action for that particular agent and call `step() `again?

Thanks for the help
",hi regarding issue sample environment length plus length always number scene understand logic behind know supposed step environment order collect data stepping environment print get output looking like scene point one get one entry inside get data calling seem work need set new action particular agent call step thanks help,issue,positive,positive,neutral,neutral,positive,positive
708032080,"Don't think it needs to go in the changelog.

If you want this to run on the upcoming release, you should get it on that branch today; the plan is to release tomorrow morning.",think need go want run upcoming release get branch today plan release tomorrow morning,issue,negative,neutral,neutral,neutral,neutral,neutral
707994338,For @chriselion -- does this belong in the CHANGELOG? I don't believe we consider the Dockerfile officially supported any longer.,belong believe consider officially longer,issue,negative,neutral,neutral,neutral,neutral,neutral
707947112,"Yes, you have that correct and it's not obvious to me just yet that you're doing anything wrong. I'll do some troubleshooting and get back to you. ",yes correct obvious yet anything wrong get back,issue,negative,negative,negative,negative,negative,negative
707945524,"@andrewcoh  thanks for taking that into account. I did originally post on the forum because I thought it might be something I was doing wrong. 

I am able to get it training using the mlagents-learn command. The problem is that I want to use my own training algorithm instead of the built in ones. I was under the impression that we can't do that with mlagents-learn, we need to use the mlagents-envs. Is that correct?

",thanks taking account originally post forum thought might something wrong able get training command problem want use training algorithm instead built impression ca need use correct,issue,negative,positive,positive,positive,positive,positive
707942512,"Yep, I'm aware you're using Ubuntu.

I should've asked this earlier (but I think I saw your forum post mentioning this): are you able to get this to run using `mlagents-learn` and the --env command?",yep aware think saw forum post able get run command,issue,negative,positive,positive,positive,positive,positive
707935544,"Looks like inference is hanging on 2020.2. Debugging that, so hold off on reviews.",like inference hanging hold,issue,negative,neutral,neutral,neutral,neutral,neutral
707883472,"@andrewcoh  I have it set to windowed. Here is a screenshot of my player settings. I just want to be sure you know I am working with Ubuntu here. Could it be a Ubuntu issue? I can't get it to work with any scene. 
![Screenshot from 2020-10-13 13-03-26](https://user-images.githubusercontent.com/36868866/95892657-d365a500-0d54-11eb-9cf7-e0db463525d7.png)
",set player want sure know working could issue ca get work scene,issue,negative,positive,positive,positive,positive,positive
707860084,"Ahh, my apologies, thanks for clarifying. 

BTW I love how responsive you guys are and the transparency of this project. Feels like it is easy to communicate with you guys.",thanks love responsive transparency project like easy communicate,issue,positive,positive,positive,positive,positive,positive
707760933,"Ah, I see the issue occurs when actually trying to load the executable i.e. the 'is not responding' error. It could be a resolution mismatch.

Can you try setting Fullscreen Mode to 'Windowed' in the player settings when you create your build?",ah see issue actually trying load executable error could resolution mismatch try setting mode player create build,issue,negative,neutral,neutral,neutral,neutral,neutral
707753610,"Ah, so you are seeing NullReference issues in other scenes that have nothing to do with ML-Agents when you upgrade Unity?",ah seeing nothing upgrade unity,issue,negative,neutral,neutral,neutral,neutral,neutral
707736583,"> Hi Andrew,
> 
> I think the task you created ""Clean up EndEpisode demo code #4563"" misses the point.
> 
> IMO the demos are using EndEpisode correctly. There is nothing wrong with how the demos use the code. The problem is with the API itself.
> 
> Look at the code in Agent.cs. EndEpisode immediately fires ""OnEpisodeBegin"" without doing any checks about status of the agent (is the Agent already done). There is a mismatch between the number of times ""OnEpisodeBegin"" gets called and the true number of episodes. Many of these ""OnEpisodeBegin"" calls share an episode. If a developer is doing any bookkeeping inside ""OnEpisdoeBegin"" or keep statistics on training progress then they could get very bad data.

This wasn't intended to be the fix and I fully understand the issue you're raising. I was just going back through our EndEpisode documentation to see if it could be improved and found these small issues.",hi think task clean code point demo correctly nothing wrong demo use code problem look code immediately without status agent agent already done mismatch number time true number many share episode developer bookkeeping inside keep statistic training progress could get bad data intended fix fully understand issue raising going back documentation see could found small,issue,positive,negative,neutral,neutral,negative,negative
707441015,"Hi Andrew, IMO the problem here is that there is no integrity checking to enforce the structure of episodes and episode steps.

For PPO and SAC to work correctly, an episode should consist of a an ordered collection of steps. Each step should have:

Observations
Actions
Reward

The Academy should have some integrity checking to ensure that EndEpisode is being called in a valid context:

1. _The agent must be inside an episode_
2. There must be at least one step
3. The last step must be complete (has Observations, Actions, Reward)

If EndEpisode is called and this is not the case then errors should be generated. This bug is failing to enforce the first integrity condition. The Academy is executing ""EndEpisode"" code when there is no episode.
",hi problem integrity enforce structure episode sac work correctly episode consist ordered collection step reward academy integrity ensure valid context agent must inside must least one step last step must complete reward case bug failing enforce first integrity condition academy code episode,issue,positive,positive,neutral,neutral,positive,positive
707437243,"Hi Andrew,

I think the task you created ""Clean up EndEpisode demo code #4563"" misses the point. 

IMO the demos are using EndEpisode correctly. There is nothing wrong with how the demos use the code. The problem is with the API itself. 

Look at the code in Agent.cs.  EndEpisode immediately fires ""OnEpisodeBegin"" without doing any checks about status of the agent (is the Agent already done). There is a mismatch between the number of times ""OnEpisodeBegin"" gets called and the true number of episodes. Many of these ""OnEpisodeBegin"" calls share an episode. If a developer is doing any bookkeeping inside ""OnEpisdoeBegin"" or keep statistics on training progress then they could get very bad data.


",hi think task clean code point demo correctly nothing wrong demo use code problem look code immediately without status agent agent already done mismatch number time true number many share episode developer bookkeeping inside keep statistic training progress could get bad data,issue,positive,positive,neutral,neutral,positive,positive
707434232,"Hi Andrew,

IMO the dangerous part of this bug is that a single episode has multiple beginnings (OnEpisodeBegin). There should be a one-to-one relationship between an episode and OnEpisodeBegin. It is dangerous that this can get fired multiple times for a single episode. I can think of all sorts of things that can go wrong because of this.
",hi dangerous part bug single episode multiple relationship episode dangerous get fired multiple time single episode think go wrong,issue,negative,negative,negative,negative,negative,negative
707187248,"> Some of the checks are fairly complex. The systems shouldn't need to be aware of each other and check with each other if someone else has called EndEpisode.

This is a very fair point. It's not hard to imagine how this logic could (1) be complex and (2) live in different parts of a large codebase. I agree, the systems shouldn't need to be aware of each other.  I'll pass this on to the team to find a good solution.",fairly complex need aware check someone else fair point hard imagine logic could complex live different large agree need aware pas team find good solution,issue,positive,positive,positive,positive,positive,positive
707186478,"Thanks, I put a bit more thought into this overnight regarding the desired behavior.

For PPO and SAC to work correctly, an episode should consist of a an ordered collection of steps. Each step should have:

- Observations
- Actions
- Reward

==================
The Academy should have some integrity checking to ensure that EndEpisode is being called in a valid context:

- The agent must be inside an episode
- There must be at least one step
- The last step must be complete (has Observations, Actions, Reward)

If EndEpisode is called and this is not the case then errors should be generated.
",thanks put bit thought overnight regarding desired behavior sac work correctly episode consist ordered collection step reward academy integrity ensure valid context agent must inside episode must least one step last step must complete reward case,issue,positive,positive,neutral,neutral,positive,positive
707182244,"Thanks for looking at this. The code in the bug is a silly unrealistic example that shows the bug in its simplest form. 

Thanks for the ""if else"" suggestion. In my case the situation is not so simple. I am using multiple system that check different scenarios for the end of the episode.  Some of the checks are fairly complex. The systems shouldn't need to be aware of each other and check with each other if someone else has called EndEpisode. I can deal with it by having a single flag variable on the agent that is set if the episode has ended and check that....   but this is silly. Isn't there already an isDone variable? Yes... except it isn't exposed in the API. 



",thanks looking code bug silly unrealistic example bug form thanks else suggestion case situation simple multiple system check different end episode fairly complex need aware check someone else deal single flag variable agent set episode ended check silly already variable yes except exposed,issue,positive,negative,negative,negative,negative,negative
707173899,"Hi @Phong13 

I'm passing this on to the team to decide the best way to address this issue since we've seen similar things like this i.e. this older issue https://github.com/Unity-Technologies/ml-agents/issues/4220 and fix https://github.com/Unity-Technologies/ml-agents/pull/4227. Thank you for pointing this out.",hi passing team decide best way address issue since seen similar like older issue fix thank pointing,issue,positive,positive,positive,positive,positive,positive
707170849,"Hi @Phong13 

Is there a good reason to not use an if/else block to handle multiple conditions for ending an episode? Would this resolve your issue (at least functionally, I agree we can make the API safer)? Thank you for pointing this out.",hi good reason use block handle multiple ending episode would resolve issue least functionally agree make thank pointing,issue,positive,positive,positive,positive,positive,positive
706675733,"I updated PyTorch to v1.6 (CPU only). The problem does not occur using that version.
I didn't try it with CUDA yet (originally I tested with v1.4 and CUDA 10.1), because of the necessity of upgrading CUDA as well.",problem occur version try yet originally tested necessity well,issue,negative,positive,positive,positive,positive,positive
706529599,"Hello,
I am the author of https://github.com/QuantScientist/TorchRayLib and a very fervent PyTorch advocate. I didn't even know about the ml-agents project. 
I am mostly interested in combining Libtorch C++ into Unity though I have not experimented with that yet. 

I would love to help, 
",hello author fervent advocate even know project mostly interested combining unity though experimented yet would love help,issue,positive,positive,positive,positive,positive,positive
706522175,"Hi, Its an bug for unity alpha.",hi bug unity alpha,issue,negative,neutral,neutral,neutral,neutral,neutral
706295081,"> Why does it break GridCollector? Just because the observation size changes?

Yes right.",break observation size yes right,issue,negative,positive,positive,positive,positive,positive
706222110,"Hi @andrewcoh, thanks for looking into it. env.reset() did not resolve the error. Bellow is the results. 
![Screenshot from 2020-10-09 10-39-41](https://user-images.githubusercontent.com/36868866/95596996-16094380-0a1c-11eb-905f-9489040df50b.png)
",hi thanks looking resolve error bellow,issue,negative,positive,positive,positive,positive,positive
706207807,"Hi @justkittenaround 

Can you try calling `env.reset()` and then checking this list? Please see my screenshot:

<img width=""929"" alt=""Screen Shot 2020-10-09 at 10 13 18 AM"" src=""https://user-images.githubusercontent.com/54679309/95593938-4949d380-0a18-11eb-92a1-cdd3cb0594a1.png"">
",hi try calling list please see screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
706196760,"Hi @MarcoMeter 

I notice you are using PyTorch v1.4. Can you try upgrading to >=1.5?  Did you modify the trainer/environment code in any way or is this happening on a clean install?

I am unable to reproduce with release 7 or current master i.e everything seems to run/train with no issues. If upgrading your version of torch doesn't help, please let me know so I can look into it further.",hi notice try modify code way happening clean install unable reproduce release current master everything version torch help please let know look,issue,positive,negative,neutral,neutral,negative,negative
705891510,"> You are spot on. This is a bug and the test is failing because of it. Any suggestions on how to fix it ?

To make them the same, initialize the compressed buffer with PNGs that equals to all zeros after decompression. That means we have to create a texture equals to all zeros, which I'm not sure how to do it. Currently it's using a ""empty"" texture and somehow that's not zeros.",spot bug test failing fix make initialize compressed buffer decompression create texture sure currently empty texture somehow,issue,negative,positive,positive,positive,positive,positive
705888395,"> Might not be the focus of this PR but this reveals an issue to me that the stacking sensor initializes the compressed obs buffer with empty png and the uncompressed buffer with zeros, and empty png is not exactly the same as zero array. So if you do env.step() less than number of stacks it could probably fail.

You are spot on. This is a bug and the test is failing because of it. Any suggestions on how to fix it ?
",might focus issue sensor compressed buffer empty uncompressed buffer empty exactly zero array le number could probably fail spot bug test failing fix,issue,negative,negative,negative,negative,negative,negative
705864845,">  I can revert that no problem if you think it is unnecessary.

No strong feelings",revert problem think unnecessary strong,issue,negative,positive,neutral,neutral,positive,positive
705856515,"> Why do we need to move all the SharedAsset files?

It bothered me that it was part of Examples. I can revert that no problem if you think it is unnecessary.",need move part revert problem think unnecessary,issue,negative,negative,negative,negative,negative,negative
705850640,"(Sorry, can't comment on the file moves)
Why do we need to move all the SharedAsset files?

The texture you're using is symmetric on the horizontal axis, so it wouldn't detect some of the bugs we're trying to catch. Can you pick something that is asymmetric (and also non-square)?",sorry ca comment file need move texture symmetric horizontal axis would detect trying catch pick something asymmetric also,issue,negative,negative,negative,negative,negative,negative
705605566,"I for one very happily see the switch to PyTorch - it will make any future models and prototypes this much easier.

On this note, is it possible to (relatively easily) use custom models/training routines with the rest of the ML-A framework? Because I would see this as the main benefit of moving to PyTorch.",one happily see switch make future much easier note possible relatively easily use custom rest framework would see main benefit moving,issue,positive,positive,positive,positive,positive,positive
705604362,"I installed ml-agents on Windows just a couple days ago without many problems and everything works well, so I think it's alright? I believe pip install + adding the Unity package inside of Unity works perfectly.",couple day ago without many everything work well think alright believe pip install unity package inside unity work perfectly,issue,positive,positive,positive,positive,positive,positive
705425185,"Hi @andrewcoh ,

We have been upgrading but skipping some versions. We could make it run, but we had issues sometimes. We also upgraded Unity, and that brought more problems.

When we upgraded to release 7 from release 1, yes, we found failures, but we are not sure from where they came from. We came back soon to release 1. I can't say if those failures went away because we where merging scenes from different projects and we had errors.

Now, with the same code but in Unity3D 2019 instead of Unity3D 2018, we are also having NullReferenceExceptions where we haven't before in other scenes independent of ML-Agents. I don't know if this could bring any clue.",hi skipping could make run sometimes also unity brought release release yes found sure came came back soon release ca say went away different code instead also independent know could bring clue,issue,positive,positive,positive,positive,positive,positive
705250260,"> > Does this line make sense any more https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/ppo/optimizer_torch.py#L180?
> 
> What do you mean? This is the entropy regularization, I think we need it. Are you referring to the use of `masked_mean` ?

To the use of masked_mean.",line make sense mean entropy regularization think need use use,issue,negative,negative,negative,negative,negative,negative
705083416,"Hi @AMGMNPLK 

Just to make sure I understand what's going on, you have no issues with release 0.3 to release 1 but failure when you upgrade to release 7? 

Also, if you downgrade ML-Agents in the same env, these issues go away?",hi make sure understand going release release failure upgrade release also downgrade go away,issue,negative,positive,neutral,neutral,positive,positive
705073750,"Thanks for the follow up, I am asking around internally but while I do that have you tried setting `export TF_FORCE_GPU_ALLOW_GROWTH=true` in your terminal before running? There are similar issues that have been discussed on the TF forums[[1](https://github.com/tensorflow/tensorflow/issues/24828), [2](https://github.com/tensorflow/tensorflow/issues/28326)]",thanks follow around internally tried setting export terminal running similar,issue,negative,positive,neutral,neutral,positive,positive
704869419,"Tensorflow doesn't have any issues. I've been training agents without the camera properly. Here's the output of the commands you suggested:

```Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2020-10-07 14:20:45.990011: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
>>> with tf.Graph().as_default():
...     a = tf.constant(1)
...     b = tf.constant(2)
...     with tf.compat.v1.Session() as sess:
...         sess.run(a+b)
...
2020-10-07 14:20:55.017588: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-10-07 14:20:55.035223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1
coreClock: 1.7845GHz coreCount: 10 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.99GiB/s
2020-10-07 14:20:55.035389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-10-07 14:20:55.078774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-10-07 14:20:55.104331: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-10-07 14:20:55.119620: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-10-07 14:20:55.160498: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-10-07 14:20:55.187117: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-10-07 14:20:55.260227: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-10-07 14:20:55.260657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-10-07 14:20:55.264893: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-10-07 14:20:55.284949: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2527b6ff0e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-07 14:20:55.285070: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-07 14:20:55.286755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1
coreClock: 1.7845GHz coreCount: 10 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.99GiB/s
2020-10-07 14:20:55.288157: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-10-07 14:20:55.288872: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-10-07 14:20:55.289275: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-10-07 14:20:55.289747: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-10-07 14:20:55.290162: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-10-07 14:20:55.290551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-10-07 14:20:55.291063: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-10-07 14:20:55.291518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-10-07 14:20:56.646646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-07 14:20:56.646846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0
2020-10-07 14:20:56.647866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N
2020-10-07 14:20:56.649667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4831 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-10-07 14:20:56.654352: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x25220178680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-10-07 14:20:56.654473: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1060 6GB, Compute Capability 6.1
3
```",training without camera properly output python bit win type help copyright license information import successfully dynamic library sess successfully dynamic library found device name successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library visible binary use service platform host guarantee used device host default version found device name successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library visible device interconnect strength edge matrix device memory physical device name bus id compute capability service platform guarantee used device compute capability,issue,positive,positive,neutral,neutral,positive,positive
704630928,"> Can you also put a link to the jira issue in the PR summary?

Updated in the summary",also put link issue summary summary,issue,negative,neutral,neutral,neutral,neutral,neutral
704601471,"Thanks for taking interest in the project, there have been other posts as well that took an interest in runtime-learning. While I think we all agree that this would be awesome it is not currently a priority due to the large scope of the problem. For the purposes of long-term interest tracking I am including our internal tracking info, but this is not something that we have any plans to work on in the foreseeable future.

runtime-learning: MLA-1445",thanks taking interest project well took interest think agree would awesome currently priority due large scope problem interest internal something work foreseeable future,issue,positive,positive,positive,positive,positive,positive
704600594,"Thanks for reaching out, do you have any specific install issues (where our instructions don't work for windows)?

While it would be amazing to have these features (runtime learning and removing python install pain) they are not feasible at this time. I am including internal tracking numbers for both below so that we can track interest long-term.

runtime-learning:  MLA-1445
simplifying ml-agents install: MLA-1446
",thanks reaching specific install work would amazing learning removing python install pain feasible time internal track interest install,issue,positive,positive,positive,positive,positive,positive
704597642,"> Does this line make sense any more https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/ppo/optimizer_torch.py#L180?

The masked_mean was actually _not_ being used properly before (it was just a single value). It is now, as the entropy is BxN. ",line make sense actually used properly single value entropy,issue,negative,negative,neutral,neutral,negative,negative
704436733,"> Looks, good to me. I still think the useFrozenFlag should not be an option and should always be on. I tried the model and it worked. I did not try to train it.

Removed useFrozenFlag as suggested and verified the change does not affect the model.",good still think option always tried model worked try train removed change affect model,issue,negative,positive,positive,positive,positive,positive
704433905,"> @bjg2 , in this [link](http://tiny.cc/764wbz) you can find a singularity image that works with xvbf

@maystroh link is broken, could you update it?",link find singularity image work link broken could update,issue,negative,negative,negative,negative,negative,negative
704433295,"From what I see in the Udacity repository it's using 0.4.0 which is different than your settings.
This is a deprecated version of ML-Agents, and we don't guarantee it still run.
If it still doesn't work please refer to that repository of how to set up your environment since we no longer support it, thank you.",see repository different version guarantee still run still work please refer repository set environment since longer support thank,issue,positive,neutral,neutral,neutral,neutral,neutral
704383121,Hi is this related to ml-agents or is this just a Unity bug? I see https://issuetracker.unity3d.com/issues/linux-editor-empty-webgl-project-fails-building?_ga=2.56422444.601106834.1601925801-1907819386.1597705242 for this alpha version.,hi related unity bug see alpha version,issue,negative,neutral,neutral,neutral,neutral,neutral
704380944,"Thanks, can you run the following in the same python environment? This should help determine if this is an issue with ml-agents or with TF. 

```
import tensorflow as tf
with tf.Graph().as_default():
    a = tf.constant(1)
    b = tf.constant(2)
    with tf.compat.v1.Session() as sess:
        sess.run(a+b)
```

",thanks run following python environment help determine issue import sess,issue,positive,positive,neutral,neutral,positive,positive
703782266,"> Does this line make sense any more https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/ppo/optimizer_torch.py#L180?

What do you mean? This is the entropy regularization, I think we need it. Are you referring to the use of `masked_mean` ?",line make sense mean entropy regularization think need use,issue,negative,negative,negative,negative,negative,negative
703050256,"I have now made a new virtual environment with the following versions :
mlagents == 0.18.0
mlagents_envs == 0.18.0
tensorflow == 2.2.0
unity = 2020.1.4f1

still when I run them, I get the same error. 
how do you suggest i process so that i can pass this error?",made new virtual environment following unity still run get error suggest process pas error,issue,negative,positive,neutral,neutral,positive,positive
703018180,"I have a working nvidia driver. nvidia-smi gives this:
```
Sat Oct 03 03:58:18 2020
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 452.06       Driver Version: 452.06       CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 106... WDDM  | 00000000:01:00.0  On |                  N/A |
| 24%   55C    P0    32W / 120W |   1424MiB /  6144MiB |     14%      Default |
+-------------------------------+----------------------+----------------------+
```
",working driver sat driver version version name volatile fan temp compute mib mib default,issue,negative,neutral,neutral,neutral,neutral,neutral
702997072,"Thanks, this is likely an issue with your Tensorflow / NVIDIA drivers. Can you run `nvidia-smi` to see if you have a working install? ",thanks likely issue run see working install,issue,negative,positive,neutral,neutral,positive,positive
702837381,"Thanks for the report, we don't plan to support running learn.py directly going forward (running python -m mlagents.trainers.learn will be the preferred method if you need to run it directly). If you encounter any documentation that still refers to running it directly please let us know and we will be happy to change it so that others don't run into this going forward.",thanks report plan support running directly going forward running python preferred method need run directly encounter documentation still running directly please let u know happy change run going forward,issue,positive,positive,positive,positive,positive,positive
702674712,"Hi @chriselion - Guess, everything should be fine by now. :)
Thanks for instructions.",hi guess everything fine thanks,issue,positive,positive,positive,positive,positive,positive
702483415,"Added the trained model and updated the docs, so I think it'd be better to do re-review",added trained model think better,issue,negative,positive,positive,positive,positive,positive
702464892,It is easy for me to work around. I just comment out the that generates the error in the torch.py file. After that learn.py is working great. Feels like version 7 is a bit faster than previous versions. Reported it just so you are aware of the issue.,easy work around comment error file working great like version bit faster previous aware issue,issue,positive,positive,positive,positive,positive,positive
702426117,"Please update the migration guide, since anyone who's calling these in their code will need to update it.",please update migration guide since anyone calling code need update,issue,negative,neutral,neutral,neutral,neutral,neutral
702322576,"Thanks for reporting this, it looks like python import resolutions shot us in the foot in the specific case where you run learn.py directly. Does using `python -m mlagents.trainers.learn` work for your use case? IIUC it should be a suitable replacement. ",thanks like python import shot u foot specific case run directly python work use case suitable replacement,issue,positive,positive,positive,positive,positive,positive
702284114,"Hi @SergeyMatrosov - the `pre-commit` step is failing because of trailing whitespaces in the .md files. If you don't want to update them manually, the directions to install and run `pre-commit` are [here](https://github.com/Unity-Technologies/ml-agents/blob/master/com.unity.ml-agents/CONTRIBUTING.md#continuous-integration-ci), and that should fix the lines for you. Let me know if you have any problems getting it set up.",hi step failing trailing want update manually install run fix let know getting set,issue,negative,neutral,neutral,neutral,neutral,neutral
702275657,The last state (July) that I know of is that the engine team is still working on CPU rendering. It is not clear when this will be available.,last state know engine team still working rendering clear available,issue,negative,positive,positive,positive,positive,positive
702270833,"Hi, thanks for reaching out. Since this is more of a question rather than a feature request or bug would you mind moving it to the [forums](https://forum.unity.com/forums/ml-agents.453/)? This way if someone else has the same question it will continue to show up and be searchable in the future. 

Thanks so much for your interest",hi thanks reaching since question rather feature request bug would mind moving way someone else question continue show searchable future thanks much interest,issue,positive,positive,positive,positive,positive,positive
701729250,"~Leaving this here for posterity: This will not raise an error correctly on windows when installing via pip. That is because the cmdclass will NOT work when installing from whl~

Edit : Removed cmdclass, the user will see an error when trying to use mlagents-learn without having torch installed. cannot directly install torch on windows via pip.",posterity raise error correctly via pip work edit removed user see error trying use without torch directly install torch via pip,issue,negative,positive,neutral,neutral,positive,positive
701631950,"The AbstractBoard, Match3Sensor, and Match3Actuator are reusable. They should be generic enough to integrate into a game, although we may need to add some more features like ""special"" pieces.",generic enough integrate game although may need add like special,issue,positive,negative,neutral,neutral,negative,negative
700971842,"Our trainer code-base was written in TF v1.  We were planning to move to TF v2 and away from using v1.compat in order to leverage the TF v2 upgrades and syntax.  

With that, we decided to go with PyTorch instead of TF v2.  Since we would have to refactor and move off of TF v1, we decided on PyTorch for a host of reasons (improve our internal velocity, adoption of PyTorch). ",trainer written move away order leverage syntax decided go instead since would move decided host improve internal velocity adoption,issue,negative,neutral,neutral,neutral,neutral,neutral
700858970,"> > Ok I'll run more times to get a better number. This worked when I tried last time.
> 
> Does the scene use stacking by default or do I need to change something?

Oh so this one is still waiting for the stacking PR to merge (thus I made this one as draft) and will add default stacking after that.
Sorry I didn't made it clear.",run time get better number worked tried last time scene use default need change something oh one still waiting merge thus made one draft add default sorry made clear,issue,positive,positive,neutral,neutral,positive,positive
700857475,"> Ok I'll run more times to get a better number. This worked when I tried last time.

Does the scene use stacking by default or do I need to change something?
",run time get better number worked tried last time scene use default need change something,issue,negative,positive,positive,positive,positive,positive
700377936,"Thanks for more detailed explanation, and I was able to reproduce this following your steps. Thanks for reporting this.
This is being tracked internally as MLA-1410. We will try to get a fix out soon. ",thanks detailed explanation able reproduce following thanks tracked internally try get fix soon,issue,positive,positive,positive,positive,positive,positive
700359085,"> Also, I noticed that the number of steps in the config.yaml file is not enough to get the training through, please increase the number.

Ok I'll run more times to get a better number. This worked when I tried last time.


Also planning to add a RenderTextureAgent similar to GridWorld to show agent view.",also number file enough get training please increase number run time get better number worked tried last time also add similar show agent view,issue,positive,positive,positive,positive,positive,positive
700351816,"> Should I use either blue or purple? I did that just to make a difference between the original 3DBall and the Visual one.

@Hunter-Unity will know more about this. I think maybe a shade between blue and purple will do. 
Also, I noticed that the number of steps in the config.yaml file is not enough to get the training through, please increase the number.",use either blue purple make difference original visual one know think maybe shade blue purple also number file enough get training please increase number,issue,positive,positive,neutral,neutral,positive,positive
700345527,"> I think he colors of the agents should not be green. Green is reward in our informal color coding, blue and purple are agent color.
> Also, make sure you give a model of the agent.

Should I use either blue or purple? I did that just to make a difference between the original 3DBall and the Visual one.",think color green green reward informal color blue purple agent color also make sure give model agent use either blue purple make difference original visual one,issue,positive,positive,neutral,neutral,positive,positive
700323544,"Hi dongruoping,

The problem is not ""multiple Academy instances"". There is actually only one Academy instance. I am not attempting to ""Create my own instance"". The posted code is there to show that ""accessing Academy.Instance when not in a playmode leaks multiple GameObjects with AcadmeyFixedUpdateStepper"" on them.

The problem is that the training scene accumulates multiple, hidden GameObjects with the ""AcadmeyFixedUpdateStepper"" component on them (when Academy.Instance is accessed not-in-playmode). These hidden gameobjects are saved when the scene is saved. When the scene is trained, the environment is stepped multiple times per environment-step. The scene becomes untrainable, corrupt. It is fairly easy to repair... if you know that these extra, hidden AcadmeyFixedUpdateStepper's have been serialized in the scene. 

This bug is very serious because the training scene appears to be working correctly. There are no error messages in the Unity console or the Python console. Builds can be created. But the TensorBoard results are terrible and never improve no mater what you change. 

IMO accessing (when not in playmode):

_Academy.Instance_

Should not leak GameObjects with the ""AcadmeyFixedUpdateStepper"". There were 11 AcadmeyFixedUpdateStepper's in my scene.

If you want to create your own simple test case do the following.

- Open any of the example training scenes.
- In Academy.cs comment out the line: _m_StepperObject.hideFlags = HideFlags.HideInHierarchy;_
- Create a ""ContextMenu"" method that access the academy: _Academy.Instance_
- Fire your method when not in playmode: watch the ""AcademyFixedUpdateStepper"" GameObjects accumulate
- Save the scene and reopen it. See that these GameObjects are still there. Fire your method and make some more.
- Try to train the scene and observe that the training never gets anywhere
",hi problem multiple academy actually one academy instance create instance posted code show multiple problem training scene multiple hidden component hidden saved scene saved scene trained environment stepped multiple time per scene becomes untrainable corrupt fairly easy repair know extra hidden scene bug serious training scene working correctly error unity console python console terrible never improve mater change leak scene want create simple test case following open example training comment line create method access academy fire method watch accumulate save scene reopen see still fire method make try train scene observe training never anywhere,issue,negative,negative,negative,negative,negative,negative
700221754,"In your code, a new Academy instance is instantiate every time the function is called by saying `Academy myAcademy = Academy.Instance;`, and that's why you ended up having multiple Academy instances. 

The Academy is initialized when the first time being accessed, so you should be able to call it directly instead of first creating your own `myAcademy`. Please refer to [this doc](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Design.md) for example of accessing Academy.",code new academy instance every time function saying academy ended multiple academy academy first time able call directly instead first please refer doc example academy,issue,negative,positive,positive,positive,positive,positive
700206665,"Yes I think it would be good to be include in CI, so that we can capture it if we change the way combining vector and visual observations. It could take quite some time though.
FYI: Using the current config, the reward bounce between -1~1 in the first 1.5M steps, get to ~20 at around 2M steps and ~50 around 3M steps.",yes think would good include capture change way combining vector visual could take quite time though current reward bounce first get around around,issue,positive,positive,positive,positive,positive,positive
700198901,"As the error message suggested, this is likely either a wrong setting in the executable or the version of the executable and python communicator is incompatible.

From your description it looks like you downloaded a pre-built environment executable file but installed the latest ML-Agents package, which have different versions. I would suggest to first make sure you're using ML-Agent environment and python package from the same version.",error message likely either wrong setting executable version executable python communicator incompatible description like environment executable file latest package different would suggest first make sure environment python package version,issue,negative,positive,positive,positive,positive,positive
700193735,"  But why? What is your rationale for favoring PyTorch over TF?

On Fri, Sep 25, 2020 at 3:05 PM Jeffrey Shih <notifications@github.com>
wrote:

> Hi ML-A Community!
>
> As you may have noticed, we have included PyTorch as an option for
> training in Release 7. We are planning to make PyTorch the default (and
> eventually sunset TensforFlow) in future releases.
>
> With that, we want to hear from you all before we make these big changes.
> Please try it and post issues here or general discussions in our forums.
>
> To use PyTorch, use --torch when running mlagents-learn, or add framework:
> pytorch to your trainer configuration (under the behavior name) to enable
> it.
>
> Thanks!
>
> Jeff
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/4512>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABH7VG3LS633XH5ZWLPZCIDSHTSZLANCNFSM4R2ESFTQ>
> .
>


-- 
   interFusion Games LLC - because it's fun
",rationale favoring shih wrote hi community may included option training release make default eventually sunset future want hear make big please try post general use use torch running add framework trainer configuration behavior name enable thanks jeff thread reply directly view interfusion fun,issue,positive,positive,positive,positive,positive,positive
700181849,"When starting training with the CLI, you can specify the window size in command line like `--width=1280 --height=720` to make the window bigger. There is also a checkbox in the PlayerSettings called `resizable window` that allows for the window size to be reshaped manually, please see [this doc](https://docs.unity3d.com/Manual/class-PlayerSettingsStandalone.html).
Although it is impossible to change the window size while the game is frozen/updating.",starting training specify window size command line like make window bigger also window window size manually please see doc although impossible change window size game,issue,positive,negative,negative,negative,negative,negative
699201609,"Oops I've fixed it when running experiment but somehow didn't include it. It should be fixed now.
And sure I'll add a model file.",fixed running experiment somehow include fixed sure add model file,issue,negative,positive,positive,positive,positive,positive
699193291,"Not sure if it matters, but if you look at the Camera preview, you can see the neighboring agents: 
![image](https://user-images.githubusercontent.com/6877802/94321691-32e64700-ff45-11ea-934b-467a0d02ac43.png)

Can you check in a model file (maybe once you finish with the other PR and enable stacking)?
",sure look camera preview see neighboring image check model file maybe finish enable,issue,negative,positive,positive,positive,positive,positive
697957251,"> > I am not a fan of action_output containing different type of data depending on continuous or discrete (a sample or a log_prob). I would really prefer we exported the discrete actions (and not the log probs) in the model. Barracuda now has an implementation of multinomial.
> 
> I think this would be ideal.

I agree, but this is a much larger discussion I think. We'll need to properly handle backwards compatibility of .nn/.onnx files as well. ",fan different type data depending continuous discrete sample would really prefer discrete log model barracuda implementation multinomial think would ideal agree much discussion think need properly handle backwards compatibility well,issue,positive,positive,positive,positive,positive,positive
697891804,"> Do we not mind leaving rewards unnormalized (in the paper they use the STD of full returns as a normalizer)?

Yes, you are right. I took some liberties with the original paper. For example, there is also a terminal reward for end states that I simply removed and I do not consider the Agent being done zeros the future rewards.
The argument behind it is that our current Reward Provider abstraction does not allow for either of these. I think we could implement these features, but it will a rather big refactor. It still trains the Pyramids environment correctly.",mind leaving paper use full normalizer yes right took original paper example also terminal reward end simply removed consider agent done future argument behind current reward provider abstraction allow either think could implement rather big still environment correctly,issue,positive,positive,neutral,neutral,positive,positive
697859331,Do we not mind leaving rewards unnormalized (in the paper they use the STD of full returns as a normalizer)?,mind leaving paper use full normalizer,issue,negative,positive,positive,positive,positive,positive
697694363,"Some comparison between RND in orange and Curiosity in blue on a *single* cloud training run (using PPO). SAC still does not want to hear anything related to either Curiosity or RND.
<img width=""997"" alt=""Curiosity (blue) vs RND (orange)"" src=""https://user-images.githubusercontent.com/28320361/94044531-21554180-fd83-11ea-9950-8a880e71fa00.png"">
 ",comparison orange curiosity blue single cloud training run sac still want hear anything related either curiosity curiosity blue orange,issue,negative,negative,neutral,neutral,negative,negative
697287037,"Just to concur, the Value Estimate is hugely valuable in a lot of instances, especially for inference and gameplay applications.  For instance, if making a versus AI agent like OpenAI 5 or Alphastar I can use it to display the agent's win probability. When selecting between multiple agents for a task, it's very useful to have a 'confidence estimate' of how well each agent thinks it might do. It can be used as a warning system to detect when an agent might be about to fail or succeed at a given task.",concur value estimate hugely valuable lot especially inference instance making versus ai agent like use display agent win probability multiple task useful estimate well agent might used warning system detect agent might fail succeed given task,issue,positive,positive,positive,positive,positive,positive
696909862,"> I am not a fan of action_output containing different type of data depending on continuous or discrete (a sample or a log_prob). I would really prefer we exported the discrete actions (and not the log probs) in the model. Barracuda now has an implementation of multinomial.

I think this would be ideal.",fan different type data depending continuous discrete sample would really prefer discrete log model barracuda implementation multinomial think would ideal,issue,positive,positive,positive,positive,positive,positive
696863581,"Hi @Godatplay,
Nice avatar :)

This should be a simple change to make if you're interested in submitting a pull request; you'd make RenderTextureSensorComponent.UpdateSensor() also set the sensor's m_RenderTexture, and call this from the RenderTexture set property (for an example, see how CameraSensorComponent handles setting the Camera property). One caveat is that the old and new RenderTextures need to have the same dimensions (if the sensor is non-null) because we can't change the observation size at runtime; we should either throw an exception, or warn and don't set the RenderTexture if there's a mismatch.",hi nice simple change make interested pull request make also set sensor call set property example see setting camera property one caveat old new need sensor ca change observation size either throw exception warn set mismatch,issue,positive,positive,positive,positive,positive,positive
696460449,"Thanks for considering this.

Please do not underestimate how useful the Value Estimate is when trying to incorporate trained models into a game. Developers turn to RL for solving very hard problems that are extremely difficult to solve using classic programming. The ""Value-Estimate"" is truly magic! It answers the question ""How likely is my trained agent succeed in situation X?"". So very useful.

The ML-Agents toolset is great for training, but it is surprisingly difficult to incorporate these models into game systems. The ""Value Estimate"" would be an amazing gift to game developers.",thanks considering please underestimate useful value estimate trying incorporate trained game turn hard extremely difficult solve classic truly magic question likely trained agent succeed situation useful great training surprisingly difficult incorporate game value estimate would amazing gift game,issue,positive,positive,neutral,neutral,positive,positive
696441873,I've created a ticket to track this request internally as MLA-1396 and will raise it at our next meeting.,ticket track request internally raise next meeting,issue,negative,neutral,neutral,neutral,neutral,neutral
696440814,"I spoke with the author responsible for this original change and he explained that the reason we removed GetValueEstimate is because it's unavailable when using Heuristic / Player control. It would also require a hook into mlagents_envs that is not necessarily useful when using this API directly.

Basically, value estimates are not part of the typical RL loop.

I know we closed your last request on this issue. I'll bring it up for more discussion in our next team meeting.",spoke author responsible original change reason removed unavailable heuristic player control would also require hook necessarily useful directly basically value part typical loop know closed last request issue bring discussion next team meeting,issue,positive,positive,neutral,neutral,positive,positive
696393989,"You're also going to need to add some safeguards to make sure that different versions of C# and python can work OK with each other, or give an error early when then can't. To do this, you'll need to increase the communication protocol to 1.2.0 and add a flag in the Capabilities proto message - see https://github.com/Unity-Technologies/ml-agents/pull/4462 for an example.

There's no problem using a new version of python with old C#, but if you have the latest C# using a non-trivial mapping and it's connect to an old version of the python code, you'll need to raise a warning and fall back to uncompressed observations, since the python code won't be able to decompress them correctly. Because of that, you might want consider _not_ using the ICompressibleSensor for Camera and RenderTexture sensor, since it will be harder to detect when it's OK to use compression.",also going need add make sure different python work give error early ca need increase communication protocol add flag proto message see example problem new version python old latest connect old version python code need raise warning fall back uncompressed since python code wo able decompress correctly might want consider camera sensor since harder detect use compression,issue,negative,positive,positive,positive,positive,positive
695884993,"Following up on this - have there been any updates to the roadmap for if / when MLAgents will be able to leverage GPU and train 50+ million steps within days and thus be used on more complex and generalized AI? In the meantime has the capability documentation been released?
Thanks!",following able leverage train million within day thus used complex generalized ai capability documentation thanks,issue,negative,positive,neutral,neutral,positive,positive
695077352,"> Looks good, but you'll need to make it a non-draft PR in order to get yamato tests to run (any they might not run today anyway)

Was seeing some errors locally so I made it draft and double checked. It's fine now.",good need make order get run might run today anyway seeing locally made draft double checked fine,issue,positive,positive,positive,positive,positive,positive
694362870,"Always happy to help, did you find a solution or did the issue persist when using progress? ",always happy help find solution issue persist progress,issue,positive,positive,positive,positive,positive,positive
694308141,"Thank you for the reply.
I tried the second example you suggested me but sadly i'm working in python which has some libraries that require version at least 3.6, what you linked works only with python 2. I appreciate anyway the help.

Have a good day!",thank reply tried second example sadly working python require version least linked work python appreciate anyway help good day,issue,positive,negative,neutral,neutral,negative,negative
693739338,If your goal is simple RPC with a Python component it would probably be best to implement your own socket server and channel. If your objective is just to write logic in Python within your Unity project you might want to follow this project: https://docs.unity3d.com/Packages/com.unity.scripting.python@2.0/manual/index.html,goal simple python component would probably best implement socket server channel objective write logic python within unity project might want follow project,issue,positive,positive,positive,positive,positive,positive
693683054,"""cherry pick after merge"" sounds good. I don't feel qualified to review the actual change though.",cherry pick merge good feel qualified review actual change though,issue,negative,positive,positive,positive,positive,positive
693681604,"> Should we apply this on the release branch? It won't require rerunning the yamato step, so the impact is minimal.

Sure, I can make another PR or we can cherry pick after merge. What do you say, how do you want to do this?",apply release branch wo require step impact minimal sure make another cherry pick merge say want,issue,negative,positive,positive,positive,positive,positive
693675234,"Should we apply this on the release branch? It won't require rerunning the yamato step, so the impact is minimal.",apply release branch wo require step impact minimal,issue,negative,negative,neutral,neutral,negative,negative
693672427,"> 
> 
> If I understand correctly, you do not have an agent in your scene. Is that correct?
> 
> Side channel messages are buffered and only sent when an agent step is triggered, in which case it will send any buffered messages. It is not intended for general RPC with Python and does not run in heuristic or model-based modes, only in training -- though a feature request can be made to expand the scope of the API if that's what you're after.

Hi, thanks for the reply! I appreciate it.
Yes i do not have an agent in my project because what i'm trying to do is just connect the two environments: Python and Unity. First i'd like to ask you this: since my goal is to do some basic ""if result = a"" (in python) then ""something happens"" (in Unity) could you suggest me what could be a possibile solution?

Second if there's no answer to my first question, i'm gonna make a feature request in order to achieve my goal!

Thanks in advance for feature help.",understand correctly agent scene correct side channel sent agent step triggered case send intended general python run heuristic training though feature request made expand scope hi thanks reply appreciate yes agent project trying connect two python unity first like ask since goal basic result python something unity could suggest could solution second answer first question gon na make feature request order achieve goal thanks advance feature help,issue,positive,positive,positive,positive,positive,positive
693652523,"If I understand correctly, you do not have an agent in your scene. Is that correct? 

Side channel messages are buffered and only sent when an agent step is triggered, in which case it will send any buffered messages. It is not intended for general RPC with Python and does not run in heuristic or model-based modes, only in training -- though a feature request can be made to expand the scope of the API if that's what you're after.",understand correctly agent scene correct side channel sent agent step triggered case send intended general python run heuristic training though feature request made expand scope,issue,negative,positive,neutral,neutral,positive,positive
692833566,"The IActuator interface was added after the 1.3.0-preview release. You'll need to install from the master branch, or wait for 1.4.0-preview, which should be out this week.

Copied from https://forum.unity.com/threads/actuator-missing-from-package.969159/ ",interface added release need install master branch wait week copied,issue,negative,neutral,neutral,neutral,neutral,neutral
692412953,We tried to reproduce a similar error on our example WallJump environment (also uses curriculum learning) but it seemed to work correctly. Since we can't get a reasonable reproduction could you try running with `progress`  instead of `reward` and set the `threshold` fields to a 0.0-1.0 value indicating a fraction of `max_steps`. We are trying to figure out if this may be an issue with using `reward`. Thanks ,tried reproduce similar error example environment also curriculum learning work correctly since ca get reasonable reproduction could try running progress instead reward set threshold value fraction trying figure may issue reward thanks,issue,positive,positive,positive,positive,positive,positive
692350154,"Hey Andrew,

Yes heuristic mode, parameters set to heuristic only. I'm creating
demonstrations for the nn via a minimax.

On Tue, 15 Sep 2020, 03:48 andrewcoh, <notifications@github.com> wrote:

> Hi @bbdscvr <https://github.com/bbdscvr>
>
> Are you just trying to run the agent in Heuristic mode or is there
> something else that you're trying to do? Are you setting the behavior type
> in the behavior parameters script to ""Heuristic only""?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/4481#issuecomment-692145901>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AJMCWO5ANCPJO7TZS3JT7WDSFY3NVANCNFSM4RK3XARA>
> .
>
",hey yes heuristic mode set heuristic via tue wrote hi trying run agent heuristic mode something else trying setting behavior type behavior script heuristic reply directly view,issue,negative,positive,neutral,neutral,positive,positive
692331096,"Hi @chriselion, I could create a executable file for 3DBall example. but upon calling the executable using python API, the app opens and freezes. Even env.reset() is not helping.. What should i do, could you please explain this ?",hi could create executable file example upon calling executable python even helping could please explain,issue,positive,neutral,neutral,neutral,neutral,neutral
692226187,"> Out of curiosity (no pun intended) have you tried this with Pyramids-SAC?

It does not do as well (It behaves a bit like curiosity, which is also having a bad time)
Will do more tests for SAC",curiosity pun intended tried well bit like curiosity also bad time sac,issue,negative,negative,negative,negative,negative,negative
692201857,Outdated - `develop-amrl` has the latest experimental code. ,outdated latest experimental code,issue,negative,positive,neutral,neutral,positive,positive
692194505,"I was able to reproduce your issue using our pushblock example environment.  Thank you for raising this, I will try to get a fix out soon. This is being tracked internally as MLA-1376",able reproduce issue example environment thank raising try get fix soon tracked internally,issue,negative,positive,positive,positive,positive,positive
692151466,"No, they are both using the same version of ML-Agents

On Mon, Sep 14, 2020, 8:42 AM andrewcoh <notifications@github.com> wrote:

> Hi @russellcaughey <https://github.com/russellcaughey>
>
> Was this model trained with an earlier version of ML-Agents than the
> version you are currently trying to use?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/4480#issuecomment-692142576>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABAGDUHIT4EACTXOBI4OSK3SFY2YFANCNFSM4RKZZNPQ>
> .
>
",version mon wrote hi model trained version version currently trying use reply directly view,issue,negative,positive,neutral,neutral,positive,positive
692145901,"Hi @bbdscvr 

Are you just trying to run the agent in Heuristic mode or is there something else that you're trying to do?  Are you setting the behavior type in the behavior parameters script to ""Heuristic only""?",hi trying run agent heuristic mode something else trying setting behavior type behavior script heuristic,issue,negative,neutral,neutral,neutral,neutral,neutral
692142576,"Hi @russellcaughey 

Was this model trained with an earlier version of ML-Agents than the version you are currently trying to use?",hi model trained version version currently trying use,issue,negative,neutral,neutral,neutral,neutral,neutral
691754492,"
The graph examined in tensorboard is as follows.
![Screen Shot 2020-09-14 at 9 53 11 AM](https://user-images.githubusercontent.com/42993434/93033222-486b8f00-f670-11ea-8f13-6c8d043971ba.png)
![Screen Shot 2020-09-14 at 9 53 19 AM](https://user-images.githubusercontent.com/42993434/93033224-4c97ac80-f670-11ea-82cb-ea474076d6fa.png)

",graph screen shot screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
691262037,Out of curiosity (no pun intended) have you tried this with Pyramids-SAC? ,curiosity pun intended tried,issue,negative,neutral,neutral,neutral,neutral,neutral
691203631,"Hi @jwson97,

To be able to better understand what is going wrong would you be able to share the following graphs with us from Tensorboard?

1. Environment -> Cumulative Reward
1. Losses -> Value Loss
1. Policy -> Entropy 
",hi able better understand going wrong would able share following u environment cumulative reward value loss policy entropy,issue,positive,positive,positive,positive,positive,positive
691199270,"This may not be a feature we can support in general. Environment parameters are not always float values, they can also tuples that describe a sampler i.e. https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-ML-Agents.md#supported-sampler-types.  It's not clear how we could generalize to this and future controllers for environment parameters.",may feature support general environment always float also describe sampler clear could generalize future environment,issue,positive,positive,neutral,neutral,positive,positive
690805435,Nice catch. I've run into this before and didn't think about it for pytorch.,nice catch run think,issue,negative,positive,positive,positive,positive,positive
690795918,"> Out of curiosity do you know how some of the older links weren't replaced until now? I noticed a couple from release 2 and 4.

That part of the code is new since the last release; it had copied and pasted some old links. I'm going to do a pass on some of the documentation before the release and make sure that part all flows better.",curiosity know older link couple release part code new since last release copied pasted old link going pas documentation release make sure part better,issue,positive,positive,positive,positive,positive,positive
690602796,"I guess in a sense I'm trying to do something almost like style transfer, but more physically-simulated, and with more control than the existing style transfer solutions out there. It's exciting to see it all happening in realtime, at least time-sliced, but currently the generation is just not smart enough.",guess sense trying something almost like style transfer control style transfer exciting see happening least currently generation smart enough,issue,positive,positive,neutral,neutral,positive,positive
690593438,"Aww that's unfortunate, but thanks for the heads up. IMO the most common use case for a RenderTexture, at least for newer Unity workflows, is to create it at runtime. Otherwise why not tie it to a camera like the older workflow and use that component instead?

But more specifically, if one is building something dynamically at runtime, especially in the case of procedural generation, one would generate the RT at runtime.

In my case I have an art pipeline that occurs in stages, which I'm trying to make happen during runtime. So far I have a runtime baking system that resembles Blender's Eevee with soft shadows and near realtime AO baking.

The second stage is then regenerating the baked texture using other parts of Unity to kind of physically simulate parts of the regeneration, but without ML or even AI, it's just kind of randomness, but it at least occurs at runtime just fine. I also have a test project where I'm testing this regeneration using an evolutionary algorithm with Compute Shaders. It looks great, but it's way too slow.

I want to use ML to try to get the best of both worlds. The end result might be kinda like some of Artomatix's tools, but for stylized art instead of photoreal art.

Because I'm working at runtime, I'm naturally using RenderTextures, and therefore it seemed a safe assumption that I should be able to use the dedicated sensor for that.

[EDIT: elaborated for clarity]",unfortunate thanks common use case least unity create otherwise tie camera like older use component instead specifically one building something dynamically especially case procedural generation one would generate case art pipeline trying make happen far baking system blender soft near baking second stage baked texture unity kind physically simulate regeneration without even ai kind randomness least fine also test project testing regeneration evolutionary algorithm compute great way slow want use try get best end result might like art instead art working naturally therefore safe assumption able use sensor edit clarity,issue,positive,positive,positive,positive,positive,positive
690356556,"Hi @jwson97,

Just to be sure I understand, do you mean you'd like to see the actual value of the environment parameter during training in addition to the lesson number which is currently reported on tensorboard?

Please correct me if I'm misunderstanding. I'll bring this up to the team during the next meeting.",hi sure understand mean like see actual value environment parameter training addition lesson number currently please correct misunderstanding bring team next meeting,issue,positive,positive,neutral,neutral,positive,positive
689983227,"> do we have a markdown formatter

Yes, his name is @mmattar :)

We tried prettier before https://prettier.io/blog/2017/11/07/1.8.0.html#markdown-support but it made some bad changes (i.e. not the same visual result) so didn't go far with it. If you've got one you recommend, I'd be willing to try it (ideally one that integrates with pre-commit, but we can make it work if it doesn't integrate out of the box).",markdown yes name tried made bad visual result go far got one recommend willing try ideally one make work integrate box,issue,positive,positive,positive,positive,positive,positive
689916554,This issue has been resolved (PR #4438) and will be in the next release. Thanks for reporting! ,issue resolved next release thanks,issue,positive,positive,neutral,neutral,positive,positive
689878563,"I rolled this change into https://github.com/Unity-Technologies/ml-agents/pull/4466, which also fixes the python example code.",rolled change also python example code,issue,negative,neutral,neutral,neutral,neutral,neutral
689844664,Update: we have fixed this bug on master here https://github.com/Unity-Technologies/ml-agents/pull/4463. Thank you very much for reporting this.,update fixed bug master thank much,issue,negative,positive,positive,positive,positive,positive
689832299,Glad that helped @HindBoucherit!  I'm going to close this issue but feel free to reopen or open a new issue if you continue to have trouble.,glad going close issue feel free reopen open new issue continue trouble,issue,positive,positive,positive,positive,positive,positive
689722674,Sorry for leaving this open for so long. I think adding boolean support would require much more work on the barracuda side than just this. Please discuss in https://github.com/Unity-Technologies/barracuda-release if you still need this.,sorry leaving open long think support would require much work barracuda side please discus still need,issue,positive,negative,neutral,neutral,negative,negative
689685205,"Hi @harperj ! 
Thanks for your answer, after a quick test, it seems to be the issue ! My time scale was set to 20 I guess that was what was making  the behaviour seem hectic and out of hand. I tried setting it to 1, the car was able to follow the road but it seemed pretty slow compared to the training, I guess I'll just keep trying with other values until I get one that works fine.
 
Anyhow, i finally saw a result that's very close to what had been observed during training, so thanks a lot ! ",hi thanks answer quick test issue time scale set guess making behaviour seem hectic hand tried setting car able follow road pretty slow training guess keep trying get one work fine anyhow finally saw result close training thanks lot,issue,positive,positive,positive,positive,positive,positive
689518628,"For what it's worth - it seems that the `""libgrpc_csharp_ext.x64.bundle"" cannot be opened because the developer cannot be verified` was in fact my problem. You can remove this error by following the instructions here: https://github.com/Cysharp/MagicOnion/issues/212

I then had to restart the `mlagents-learn` command with a new `run-id` and restart the Unity environment, and after that, it's training perfectly.",worth developer fact problem remove error following restart command new restart unity environment training perfectly,issue,negative,positive,positive,positive,positive,positive
689499014,"@unityjeffrey Also hitting the same issue: `Couldn't connect to trainer on port 5004 using API version 1.0.0. Will perform inference instead.`.

Tried to rebuild my conda environment by removing from existing one, and then doing a fresh `pip install mlagents` - but still hitting the same problem.

The only other message I see that may be related, when pressing `Play` on the Unity Environment, I see: `""libgrpc_csharp_ext.x64.bundle"" cannot be opened because the developer cannot be verified`. This seems to be another issues (raised [here](https://github.com/Unity-Technologies/ml-agents/issues/2306)). Not sure this is affecting anything here though, as I see no actual errors at runtime, but the Log described above regarding API version 1.0.0.

",also issue could connect trainer port version perform inference tried rebuild environment removing one fresh pip install still problem message see may related pressing play unity environment see developer another raised sure affecting anything though see actual log regarding version,issue,positive,positive,positive,positive,positive,positive
689398987,"Hey guys, I´m experiencing the same issue. 

The behavior I see in the agent after resuming is like if the model was partially correct, but for some reason some of the output values received in OnActionReceived(float[] continuousActions) were wrong.

I'm training a NN to control a chatacter's joints, and after resuming, some of the joints seem to behave more or less Ok, while others don't, and therefore I also have a massive performance drop after resuming that recovers after a few hours of re-training. 

Maybe some of the values in that array are coming in a different order than in the previous training?

I mean, correct values but in the wrong order. Something like that would definitely be compatible with the weird behavior I'm experiencing.

Thanks a lot.
",hey issue behavior see agent like model partially correct reason output received float wrong training control seem behave le therefore also massive performance drop maybe array coming different order previous training mean correct wrong order something like would definitely compatible weird behavior thanks lot,issue,negative,negative,negative,negative,negative,negative
689068175,"Thank you! 
I updated the python files and I'm not getting the UnityObservationException any longer.",thank python getting longer,issue,negative,neutral,neutral,neutral,neutral,neutral
689060928,"Hi @surfnerd ,
I will, thanks. 
I wanted to do so at first but supported operators page says to post such requests here. https://docs.unity3d.com/Packages/com.unity.barracuda@1.0/manual/SupportedOperators.html",hi thanks first page post,issue,negative,positive,positive,positive,positive,positive
689054778,"from the root directory of the repo:
```
$ python3 -m pip install -e ./ml-agents-envs/
$ python3 -m pip install -e ./ml-agents/
```",root directory python pip install python pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
689053978,"Yes, you will need to install the python packages from master. ",yes need install python master,issue,negative,neutral,neutral,neutral,neutral,neutral
689042486,"Hi @surfnerd, here is my version info:
ml-agents: 0.19.0,
ml-agents-envs: 0.19.0,
Communicator API: 1.0.0,
TensorFlow: 2.2.0
Do I need to upgrade any of this?",hi version communicator need upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
689040692,"> @dongruoping Rank is required only for distributed training which has been built keeping tensorflow usage in mind. At this point I'm unable to test this with pytorch + horovod so I suggest it stays where it is.

Got it",rank distributed training built keeping usage mind point unable test suggest stay got,issue,negative,negative,negative,negative,negative,negative
689040041,We will need to bump the communication minor version as this feature was recently added.  ,need bump communication minor version feature recently added,issue,negative,negative,neutral,neutral,negative,negative
689039654,"Hi @mbaske,
Did you happen to update the python package when you tried out these changes?  ",hi happen update python package tried,issue,negative,neutral,neutral,neutral,neutral,neutral
689025251,@dongruoping  Rank is required only for distributed training which has been built keeping tensorflow usage in mind. At this point I'm unable to test this with pytorch + horovod so I suggest it stays where it is.,rank distributed training built keeping usage mind point unable test suggest stay,issue,negative,negative,negative,negative,negative,negative
688985518,"Hi @NeonMika,
I've logged this internally as MLA-1347. We will update this issue when this happens.  Thank you for your input. ",hi logged internally update issue thank input,issue,negative,neutral,neutral,neutral,neutral,neutral
688982892,"Hi @dlindmark,
I have notified the team and we are trying to reproduce it.  We will get back to you shortly.",hi notified team trying reproduce get back shortly,issue,negative,neutral,neutral,neutral,neutral,neutral
688978392,i used ` mlagents-learn ./config/ppo/FoodCollector.yaml` to run my training session.,used run training session,issue,negative,neutral,neutral,neutral,neutral,neutral
688975334,"hi @mbaske,
I am unable to reproduce your issue. Can you give me more information?",hi unable reproduce issue give information,issue,negative,negative,negative,negative,negative,negative
688967335,can you give us more information on your use case and perhaps turn this into a request instead of a bug?  Runtime updating of the RenderTexture is not supported at this time.,give u information use case perhaps turn request instead bug time,issue,negative,neutral,neutral,neutral,neutral,neutral
688966887,"Hi @Godatplay,
The RenderTextureSensorComponent creates the underlying Sensor during agent initialization and uses the reference that is set in the inspector.  It is not expected that you set the render texture at runtime.",hi underlying sensor agent reference set inspector set render texture,issue,negative,neutral,neutral,neutral,neutral,neutral
687196927,"Ok, I overestimated the time I had in Mars...

But should have some time to do this now. If not anyone else have started on it?

In addition to what @chriselion mentioned above I am thinking about setting up a simple example with an agent using only a depth camera to train. Maybe a new scene in the Wall Jump example environment?
",time time anyone else addition thinking setting simple example agent depth camera train maybe new scene wall jump example environment,issue,negative,positive,neutral,neutral,positive,positive
686934570,"Actually my bad, repairing the C++ redistributables did not solve my problem. I still get the following:
```
Python 3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
2020-09-04 01:08:21.885272: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-09-04 01:08:21.888741: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
```

(not sure why it says win32, I'm definitely 64-bit)

EDIT: Needed to restart after CUDA install, ok good now ^_^",actually bad solve problem still get following python bit win type help copyright license information import could load dynamic library found ignore set machine sure win definitely edit restart install good,issue,positive,positive,positive,positive,positive,positive
686928564,"Could this be made more obvious in the Installation Guide? I think it should be mentioned there pretty clearly, since I only found out the solution to this problem by searching through previous issues and finding this. I didn't even consider looking for a FAQ since I interpreted Tensorflow's problem as a problem or bug rather than a question.",could made obvious installation guide think pretty clearly since found solution problem searching previous finding even consider looking since problem problem bug rather question,issue,negative,positive,neutral,neutral,positive,positive
686911745,"I also have same problem. But I change my yaml file like this : 
but when I check ""checkpoint_radius"" parameter value in tensorboard after training. The value is 0 which is set as default.
curriculum is not working. What is the problem.

behaviors:
    AircraftPlayer:
        trainer_type: ppo 
        hyperparameters: 
            batch_size : 2048
            beta: 1.0e-2
            buffer_size: 20480
            epsilon: 0.2
            lambd: 0.95
            learning_rate: 3.0e-4
            learning_rate_schedule: linear
            num_epoch: 3
        network_settings: 
            hidden_units: 256
            memory:
                memory_size : 256
                sequence_length: 64
            normalize : false
            num_layers: 2
            vis_encoder_type: simple
        reward_signals: 
            extrinsic:
                strength: 1.0
                gamma: 0.99
        max_steps: 5.0e6
        time_horizon: 128
        summary_freq: 32000
        threaded: true
          
environment_parameters:
    checkpoint_radius: 
        curriculum: 
          - name : First
            completion_criteria:
                measure : reward
                min_lesson_length : 200
                threshold : 2.0
                signal_smoothing : true
            value : 70.0
          - name : Second
            completion_criteria:
                measure : reward
                min_lesson_length : 200
                threshold : 2.0
                signal_smoothing : true
            value : 50.0
          - name : Third
            completion_criteria:
                measure : reward
                min_lesson_length : 200
                threshold : 2.0
                signal_smoothing : true
            value : 30.0
          - name : Forth
            completion_criteria:
                measure : reward
                min_lesson_length : 200
                threshold : 4.0
                signal_smoothing : true
            value : 20.0
          - name : Fifth
            completion_criteria:
                measure : reward
                min_lesson_length : 200
                threshold : 6.0
                signal_smoothing : true
            value : 10.0
          - name : Last
            value : 0.0


",also problem change file like check parameter value training value set default curriculum working problem beta epsilon linear memory normalize false simple extrinsic strength gamma threaded true curriculum name first measure reward threshold true value name second measure reward threshold true value name third measure reward threshold true value name forth measure reward threshold true value name fifth measure reward threshold true value name last value,issue,positive,positive,positive,positive,positive,positive
686825850,Made a python change on https://github.com/Unity-Technologies/ml-agents/pull/4455 and confirmed that tests are triggered (and they aren't on this PR),made python change confirmed triggered,issue,negative,positive,positive,positive,positive,positive
686690163,Hi @zookae -- thanks for the bug report. I've updated the colab notebook to load from the registry.,hi thanks bug report notebook load registry,issue,negative,positive,positive,positive,positive,positive
686061674,"Won't have any effect until https://jira.unity3d.com/browse/PAI-1291 is done, but at least it gets ahead of it.",wo effect done least ahead,issue,negative,negative,negative,negative,negative,negative
686052780,"I'll defer to @anupambhatnagar for the right behavior here. For the purposes of integrating with Horovod, it looks like there are separate tf and torch rank methods, and there doesn't appear to torch equivalent of `broadcast_global_variables`",defer right behavior like separate torch rank appear torch equivalent,issue,negative,negative,negative,negative,negative,negative
685141489,"Hi @Omniscimus -- there are some tricky things about maintaining a description about the full network architecture.  For one, we are updating the internal structure of the model (while keeping the interface the same) regularly during development. Another is that the structure is dynamic based on the behavior and settings.

One tool I think may be useful for you is Netron (https://github.com/lutzroeder/netron) -- which you can use to visualize Tensorflow, Barracuda, or ONNX model structure.  Hopefully this will help you to understand the model structure better.  With regard to inference outside Unity, we don't have an explicit plan for this at the moment but there are general purpose model serving tools for Tensorflow or ONNX models.",hi tricky description full network architecture one internal structure model keeping interface regularly development another structure dynamic based behavior one tool think may useful use visualize barracuda model structure hopefully help understand model structure better regard inference outside unity explicit plan moment general purpose model serving,issue,positive,positive,positive,positive,positive,positive
685138730,Hi @HindBoucherit -- nothing about this setup seems so different than our example environments that it should be problematic. By chance did you adjust the engine's time scale during training? We have seen some issues with inference at a lower time scale causing unexpected behavior.,hi nothing setup different example problematic chance adjust engine time scale training seen inference lower time scale causing unexpected behavior,issue,negative,positive,neutral,neutral,positive,positive
684046777,"Couple updates:
1. The `Colab-UnityEnvironment-1-Run.ipynb` works.
1. `Colab-UnityEnvironment-3-SideChannel.ipynb` also works.

The fix for `Colab-UnityEnvironment-2-Train.ipynb`:
1. go to the last cell
1. change the environment loading call to: 
```
env = default_registry[""GridWorld""].make()
# env= UnityEnvironment(file_name = None)
```
(in the default code the two are flipped to:
```
#env = default_registry[""GridWorld""].make()
env= UnityEnvironment(file_name = None)
```

",couple work also work fix go last cell change environment loading call none default code two none,issue,negative,neutral,neutral,neutral,neutral,neutral
683985936,Hi @nntsrb -- the config format for curriculum learning did in face change.  We've updated the docs on how to configure curriculum learning here: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-ML-Agents.md#curriculum,hi format curriculum learning face change configure curriculum learning,issue,negative,neutral,neutral,neutral,neutral,neutral
683152450,"@Aurimasp I replaced the wall mesh with primitives. Can you try this branch and let me know if it fixes this?
https://github.com/Unity-Technologies/ml-agents/tree/hh/develop/hallway-wall-mesh-fix",wall mesh try branch let know,issue,negative,neutral,neutral,neutral,neutral,neutral
683132575,"> Looks good, any reason not to merge, or you want me to try it on my example first?

Need to add tests for it, but from what I can tell it trains fine and can be merged. ",good reason merge want try example first need add tell fine,issue,positive,positive,positive,positive,positive,positive
682993943,"Hey Vince, can you check if setting the thread here makes TensorFlow only use one thread? I realized some of the env variables (OMP, etc.) might be shared between the two frameworks, and they'll be set on import here. ",hey check setting thread use one thread might two set import,issue,negative,neutral,neutral,neutral,neutral,neutral
682375816,"The action space is discreet with a size of two, first branch is responsible for steering, it can steer left, hold position, or steer right. Second branch is responsible for speed handling, it can add a value of 0.1f to the throttle and set brake to 0f, maintain current adjustments, or add a value of 0.1f to the brake and set throttle to 0f. 
<img width=""557"" alt=""BehaviourComponent"" src=""https://user-images.githubusercontent.com/52779094/91533245-dd038c80-e90f-11ea-895a-15af0bb5df59.png"">

My observations are these : 
<img width=""568"" alt=""Observations"" src=""https://user-images.githubusercontent.com/52779094/91533694-9c584300-e910-11ea-97b5-f88744ce9885.png"">

Its the error computed at the level of the car, and a little farther forward, the speed of the car which I normalized with maximum allowed speed, the average steepness of road within 50m (value between -1f and 1f), the dot vector of the car direction with the direction it should have (guided by the points it should follow), the steer value (a value between -1f anf 1f), and the distance between the car and the point it has to reach next. 

Unity version is 2019.3, and mlagents version is 1.1.0. 

Thanks again for reading ! 



",action space discreet size two first branch responsible steering steer left hold position steer right second branch responsible speed handling add value throttle set brake maintain current add value brake set throttle error level car little farther forward speed car maximum speed average steepness road within value dot vector car direction direction follow steer value value distance car point reach next unity version version thanks reading,issue,positive,positive,neutral,neutral,positive,positive
682340065,"@flo-wolf you can try [LostTech.TensorFlow](https://ml.blogs.losttech.software/TF-1.15-Release-Candidate/) that I just released. We have a [ML Agents demo](https://ml.blogs.losttech.software/Reinforcement-Learning-With-Unity-ML-Agents/).

However, TensorFlow does not do well with `AppDomain` unloading and reloading, that Unity editor uses extensively. So you can use it in your game, but trying to use it in the editor will simply crash the editor. The demo I mentioned connects to the editor from outside - that works fine, and, honestly, easier to set up, since you can use NuGet to reference the necessary packages.",try however well unity editor extensively use game trying use editor simply crash editor editor outside work fine honestly easier set since use reference necessary,issue,positive,positive,neutral,neutral,positive,positive
682150181,"Thanks for the detailed response @HindBoucherit.  Could you also share what the observation / action space are for your environment, and which Unity and ML-Agents package version you're using?  That might give a clue as to what's going wrong.",thanks detailed response could also share observation action space environment unity package version might give clue going wrong,issue,negative,positive,neutral,neutral,positive,positive
682147874,"`ml-agents/mlagents/trainers/policy/torch_policy.py: line 61` should be removed. It reset the default tensor type to non-cuda, which is already properly set in utils .

I tested with removing that line. It works well.",line removed reset default tensor type already properly set tested removing line work well,issue,negative,neutral,neutral,neutral,neutral,neutral
682108785,"[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/Unity-Technologies/ml-agents?pullRequest=4431) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/Unity-Technologies/ml-agents?pullRequest=4431) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: hvpeteet<br/>:x: Henry Peteeet<br/><hr/>**Henry Peteeet** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/Unity-Technologies/ml-agents?pullRequest=4431) it.</sub>",assistant check thank submission really appreciate like many open source ask sign contributor license agreement accept henry henry user need account able sign already account please add address used commit account already status still pending let u recheck,issue,positive,positive,positive,positive,positive,positive
682067308,"True, that was added somewhat recently to `UnityEnv`. Unfortunately, you'll either need to upgrade, modify the code, or live with the logs.",true added somewhat recently unfortunately either need upgrade modify code live,issue,negative,negative,neutral,neutral,negative,negative
681754152,"Hello @harperj ! Thank you for your answer.

I tried training a model from the available example environments and it worked just fine, so the bug is certainly coming from my environment. 

Here's a screen of how my environment is set up : 

<img width=""954"" alt=""env"" src=""https://user-images.githubusercontent.com/52779094/91413600-a6fcd480-e84b-11ea-93f6-c7be6d1589f8.png"">

Each car has a curve of waypoints to follow in the beginning of each episode. The waypoints are supposed to represent the road and each car should try to minimise its cross track error while navigating along the points. The curve is randomized at each episode begin. All of the agents and their waypoints are placed on a unique terrain. 
Now when I save the model and give it to a car in circular road for example, the training is very good looking, but when I save the model and give it to the agent in the circular road, the car does very random actions, not at all what it has been trained to do.

Here's a video of the performance at the end of the training : 
https://drive.google.com/file/d/1a-XgoJmyGvtBVq7dRaM9fZY_bvMTeHNo/view?usp=sharing

And here's a video of the performance when I give the agents the trained model : 
https://drive.google.com/file/d/1CRdhu9iMZInzzx-hg3ul9G5c5G7p-9CR/view?usp=sharing

Excuse the quality of the video. I'm guessing there s something wrong with my environment but the weird part is that the training is fine by the end, and the tensorboard indicators show me that the cumulative reward increases well and that the training is stable. 

Thanks for reading. 


 ",hello thank answer tried training model available example worked fine bug certainly coming environment screen environment set car curve follow beginning episode supposed represent road car try cross track error along curve episode begin unique terrain save model give car circular road example training good looking save model give agent circular road car random trained video performance end training video performance give trained model excuse quality video guessing something wrong environment weird part training fine end show cumulative reward well training stable thanks reading,issue,positive,positive,neutral,neutral,positive,positive
681190505,"Hi @HindBoucherit --

We aren't aware of any Barracuda inference bugs with ML-Agents at this time. It's difficult for us to help without more information about the environment where you saw the issue. Please add what information you can from our bug report template: https://github.com/Unity-Technologies/ml-agents/issues/new?assignees=&labels=bug&template=bug_report.md

Generally speaking we aren't able to help with issues related to custom environments because we can't easily reproduce them. If possible could you also try to reproduce this issue with one of our example environments?  If it only happens in your environment, it would be good to know more details about how your environment is set up.",hi aware barracuda inference time difficult u help without information environment saw issue please add information bug report template generally speaking able help related custom ca easily reproduce possible could also try reproduce issue one example environment would good know environment set,issue,positive,positive,positive,positive,positive,positive
681161408,"> How much of a performance hit do we expect by setting these flags?

Not sure, it could be case by case, depending on how much the model changes during training and how much optimization cudnn does.
I haven't really try to quantify the impact, but it's a general warning on PyTorch doc.
",much performance hit expect setting sure could case case depending much model training much optimization really try quantify impact general warning doc,issue,negative,positive,positive,positive,positive,positive
681160483,"> Another point missing here is that when running Torch with CuDNN, if we want to get completely reproducible results, we'll also need to set these besides setting seeds for torch and numpy:
> 
> ```
> torch.backends.cudnn.deterministic = True
> torch.backends.cudnn.benchmark = False
> ```
> 
> while these could potentially hurt the running performance.

How much of a performance hit do we expect by setting these flags?",another point missing running torch want get completely reproducible also need set besides setting torch true false could potentially hurt running performance much performance hit expect setting,issue,negative,positive,neutral,neutral,positive,positive
680989928,"> TF Policy uses the ""PyTorch"" architecture

Should we change that as well? (Not in this PR but in the future)",policy architecture change well future,issue,negative,neutral,neutral,neutral,neutral,neutral
680793019,"Hi @DVonk @vincentpierre ,
I think  the Experimental Stable Baselines (https://github.com/Stable-Baselines-Team/stable-baselines-tf2) is consistent for Tf 2 version.  The Baselines are not yet migrated to Tf 2 version and I use the experimental version to run the baseline algorithms. There is an alternate repository by  Tan Zhenyu (https://github.com/tanzhenyu) for baselines on Tf 2 , on mostly A2c and its improvisations (https://github.com/tanzhenyu/baselines-tf2). Hope this helps
Thanks",hi think experimental stable consistent version yet version use experimental version run alternate repository tan mostly hope thanks,issue,positive,positive,positive,positive,positive,positive
680323895,"> Maybe not part of this PR, but I think the `sac/optimizer.py` should also be renamed `sac/optimizer_tf.py` like the ppo one (for consistency).

Yes it's there :) ",maybe part think also like one consistency yes,issue,positive,neutral,neutral,neutral,neutral,neutral
680268578,"Another point missing here is that when running Torch with CuDNN, if we want to get completely reproducible results, we'll also need to set these besides setting seeds for torch and numpy:
```
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
```
while these could potentially hurt the running performance.",another point missing running torch want get completely reproducible also need set besides setting torch true false could potentially hurt running performance,issue,negative,negative,neutral,neutral,negative,negative
679445227,"Yes, but I think `UnityEnv` does not open that parameter in the interface. 
https://github.com/Unity-Technologies/ml-agents/blob/3d7c4b8d3c1ad17070308b4e06bb57d4a80f9a0c/gym-unity/gym_unity/envs/__init__.py#L21-L38",yes think open parameter interface,issue,negative,neutral,neutral,neutral,neutral,neutral
679406534,"Sorry, I missed your message on this before. In 0.11.1, the `args` parameter of UnityEnvironment works the same as `additional_args` in recent versions.

https://github.com/Unity-Technologies/ml-agents/blob/3d7c4b8d3c1ad17070308b4e06bb57d4a80f9a0c/ml-agents-envs/mlagents/envs/environment.py#L56",sorry message parameter work recent,issue,negative,negative,negative,negative,negative,negative
679386304,"#1743 Commented out gRPC code that made it impossible to build UWP. You should be able to build an executable with pre-trained behavior on all platforms. If it is not the case, please open a new issue.",code made impossible build able build executable behavior case please open new issue,issue,negative,negative,neutral,neutral,negative,negative
679375918,"> Looks good to me, but curious about motivation? Is the goal to migrate to Github Actions fully?

Discussed offline; evaluating moving to GH Actions since we have more capacity there.",good curious motivation goal migrate fully moving since capacity,issue,positive,positive,positive,positive,positive,positive
679257773,"Looks good to me, but curious about motivation?  Is the goal to migrate to Github Actions fully?",good curious motivation goal migrate fully,issue,positive,positive,positive,positive,positive,positive
678659483,@vincentpierre  Does  #1743 mean we can build UWP/IL2CPP for **Xbox One**? as @b-fazamb have done so for with HoloLens.,mean build one done,issue,negative,negative,negative,negative,negative,negative
678463400,"> Actually most of them looks like a single value, in those case using `tensor.item()` may be shorter and cleaner.

This is a good point, and might actually be better for typing. The stats are supposed to be of type `float`, but somehow down the line the `np.ndarray` gets converted anyway ",actually like single value case may shorter cleaner good point might actually better supposed type float somehow line converted anyway,issue,positive,positive,positive,positive,positive,positive
678431660,"Actually most of them looks like a single value, in those case using `tensor.item()` may be shorter and cleaner.
",actually like single value case may shorter cleaner,issue,positive,negative,neutral,neutral,negative,negative
678417379,"> Looks good to me, but I would like ml-agents/mlagents/torch_utils/torch.py to contain a comment stating that the file is temporary and will be removed once torch is required. (To avoid adding functionality to this file in the mean time)

The file isn't temporary (will still be needed for GPU detection) but yeah the try/except is. Added comment. ",good would like contain comment file temporary removed torch avoid functionality file mean time file temporary still detection yeah added comment,issue,positive,positive,positive,positive,positive,positive
677990247,"> I wonder if we should also change the name `NNCheckpointManager` since it's not for .nn only anymore

They're still neural nets 😛 

But yeah, we can name it maybe just CheckpointManager, or PolicyCheckpointManager. ",wonder also change name since still neural yeah name maybe,issue,negative,neutral,neutral,neutral,neutral,neutral
677977206,I wonder if we should also change the name `NNCheckpointManager` since it's not for .nn only anymore,wonder also change name since,issue,negative,neutral,neutral,neutral,neutral,neutral
677967488,"Hi @Fax3D - ah I think I see what you are asking.  If someone playing your game, could the agent learn while the person is playing the game.  Unfortunately, the trainer code is run in Python and not C#.  We are looking into potentially incorporating C# training in the future.",hi ah think see someone game could agent learn person game unfortunately trainer code run python looking potentially training future,issue,negative,negative,negative,negative,negative,negative
677947607,(I think you should make a new PR with only the useful commit),think make new useful commit,issue,positive,positive,positive,positive,positive,positive
677763070,I think you're right - I've logged it internally as a bug and will run it by the team. Feel free to submit a PR if you'd like to fix it. Thanks!,think right logged internally bug run team feel free submit like fix thanks,issue,positive,positive,positive,positive,positive,positive
676789292,I approve the PR knowing that we will do a new PR to master with exactly these changes into master right after merging add-fire to master,approve knowing new master exactly master right master,issue,negative,positive,positive,positive,positive,positive
676676158,"Note that we also need to add XR as a dependency from com.unity.ml-agents.extensions (for the Pose class), but that can be a different PR (so that this can be 🍒 ⛏️  ed to the verified branch).",note also need add dependency pose class different pick branch,issue,negative,neutral,neutral,neutral,neutral,neutral
675404967,"Hi all my friend! I also faced with this problem, you can fix it like this - 
![Screenshot_1](https://user-images.githubusercontent.com/65300126/90503854-d2e7ce00-e158-11ea-9563-13df99d913be.png)
![Screenshot_2](https://user-images.githubusercontent.com/65300126/90503861-d418fb00-e158-11ea-9ef7-1433f6f0c1f5.png)
![Screenshot_3](https://user-images.githubusercontent.com/65300126/90503862-d418fb00-e158-11ea-85a4-d01783c2a9fa.png)


",hi friend also faced problem fix like,issue,negative,neutral,neutral,neutral,neutral,neutral
675320851,"Hi @unityjeffrey,
mlagents-learn it's callable during gameplay?
I mean... it's possible for an user playing my game start a train of what he played using imitation learning? Or mlagents-learn it's callable only offline?
Thanks!",hi callable mean possible user game start train imitation learning callable thanks,issue,negative,negative,negative,negative,negative,negative
674911032,hi @Fax3D - can you clarify your request?  You should be able to do both imitation learning and training via mlagents-learn.  Let me know if there is something I am missing in your request.  Thanks!,hi clarify request able imitation learning training via let know something missing request thanks,issue,negative,positive,neutral,neutral,positive,positive
674223001,Hi @HPRIOR this has been fixed in the latest release of ML-Agents. ,hi fixed latest release,issue,negative,positive,positive,positive,positive,positive
674218757,"> Thanks for bringing this up. I was able to do some digging, and found that this is indeed a bug in our curriculum code. We'll be making a PR to fix it soon.

Any chance of a workaround or fix soon? I keep running into this bug when I try to train my agents with a curriculum
",thanks able digging found indeed bug curriculum code making fix soon chance fix soon keep running bug try train curriculum,issue,positive,positive,positive,positive,positive,positive
673855136,"Pre-commit fixed in https://github.com/Unity-Technologies/ml-agents/pull/4357, OK to ignore the failing test and just merge this.",fixed ignore failing test merge,issue,negative,positive,neutral,neutral,positive,positive
673726701,The increased number of prefabs is justified i think. It makes it much quicker/easier to update multiple scenes at one time. It does add more complexity but i think it's justified in the amount of time saved. Having a base prefab with multiple variants is current best practice .,number think much update multiple one time add complexity think amount time saved base prefab multiple current best practice,issue,positive,positive,neutral,neutral,positive,positive
673726140,"Sorry for wasting your time. It seems as though I was mistaken about the what happens when EndEpisode is invoked by an agent.  Re-reading the documentation I mistook the description of the environment setup in this [document](https://github.com/Unity-Technologies/ml-agents/blob/release_3_docs/docs/Learning-Environment-Design.md ), and the section 'The Simulation and Training Process', as the protocol for every time end episode was called - that every agents OnEpisodeBegin was called. It seems as though only the agent who called EndEpisode has it's OnEpisodeBegin called.",sorry wasting time though mistaken agent documentation mistook description environment setup document section simulation training process protocol every time end episode every though agent,issue,negative,negative,negative,negative,negative,negative
673691170,hi @HPRIOR can you provide more of the agent script as well as the debug.log file?  Trying to assess if this a bug in the implementation or something in the core ml-agents scripts.,hi provide agent script well file trying ass bug implementation something core,issue,negative,neutral,neutral,neutral,neutral,neutral
673593998,"> I approve of this PR, this does mean that we should not modify these files on `develop-add-fire` anymore or will we be merging updated versions as they change?

It seems really unlikely that we won't modify anything, but hopefully the changes are smaller than this fat one",approve mean modify change really unlikely wo modify anything hopefully smaller fat one,issue,negative,negative,negative,negative,negative,negative
673358560,"> 2\. Is there a non-1 scale on your Agent's transform? We use the ray length to determine the offset relative to the agent, then apply the transform, so if the scale is non-1, the actual ray length will be scaled.

I think you've found my problem. The agent's scale is 0.75 (0.75, 0.75, 0.75). I'll try your solution for getting the ray hit locations. Thanks for all the help.

",scale agent transform use ray length determine offset relative agent apply transform scale actual ray length scaled think found problem agent scale try solution getting ray hit thanks help,issue,positive,positive,neutral,neutral,positive,positive
673200262,"The simple_rl tests pass, but Hallway doesn't train reliably. Seems like we'll need to add previous action to get it to train. ",pas hallway train reliably like need add previous action get train,issue,negative,negative,neutral,neutral,negative,negative
673168363,"The plots will not be identical because TF adds the gradient magnitude loss to the GAIL loss (and I do not)
It also seems that my computation of the gradient magnitude loss is different in torch than in TensorFlow and I can't figure out why. (I suspect it is du to my call to `torch.autograd.grad`",identical gradient magnitude loss loss also computation gradient magnitude loss different torch ca figure suspect call,issue,negative,neutral,neutral,neutral,neutral,neutral
673162726,"> LGTM, though I'll wait for Andrew to say he sees the right plots

but how will he know if he sees the right plots",though wait say right know right,issue,negative,positive,positive,positive,positive,positive
673157746,"> Hmm, test_curriculum_no_behavior must have been part of the old PR (still would be good to have a test on this)

I will add it back before merging",must part old still would good test add back,issue,negative,positive,positive,positive,positive,positive
673144123,"Hmm, test_curriculum_no_behavior must have been part of the old PR (still would be good to have a test on this)",must part old still would good test,issue,negative,positive,positive,positive,positive,positive
673107524,"Haven't tested the commit on my codebase yet, but that if-clause looks exactly like what I've had in mind. Thank you!

Edit: Read the conversation on that commit and hadn't considered the cons til now. I don't have a problem with ""no default-behavior"". I just would like to get *some* indication that my config will probably not work as I intended.",tested commit yet exactly like mind thank edit read conversation commit considered til problem would like get indication probably work intended,issue,positive,positive,positive,positive,positive,positive
673099438,"Just to clarify, but ""non-1 scale"", I mean any scale that's not (1, 1, 1) on the Agent's transform or any other transforms in the hierarchy.",clarify scale mean scale agent transform hierarchy,issue,negative,negative,negative,negative,negative,negative
673092051,"I think the best way to get the hit location of the ray would be something like
```
var rayIndex = 0;
// Start and end location in world space
var extents = input.RayExtents(rayIndex);
var hitFraction = rayOutputs[rayIndex].HitFraction;
var hitPosition = Vector3.Lerp(extents.StartPositionWorld, extents.EndPositionWorld, hitFraction);
```

As for why the values aren't matching up:
1) Are you using a radius for the cast? The hit fraction value will put the ""sphere"" at the contact point, so that might be throwing the calculations off a bit.
2) Is there a non-1 scale on your Agent's transform? We use the ray length to determine the offset relative to the agent, then apply the transform, so if the scale is non-1, the actual ray length will be scaled. You can see the code for this here:
https://github.com/Unity-Technologies/ml-agents/blob/91465cdc59555b6fadeb5eb3fa3831360ea19274/com.unity.ml-agents/Runtime/Sensors/RayPerceptionSensor.cs#L95-L107
(This is my guess for the discrepancy)
3) Are you sure raySensor.GetRayPerceptionInput().RayLength is 20? Couldn't hurt to use that in your code instead of the hardcoded value.
 
",think best way get hit location ray would something like start end location world space matching radius cast hit fraction value put sphere contact point might throwing scale agent transform use ray length determine offset relative agent apply transform scale actual ray length scaled see code guess discrepancy sure could hurt use code instead value,issue,positive,positive,positive,positive,positive,positive
673064425,And can you also look into the issue of multiple agents?  I was unable to keep track of which agent has terminated or not. Even if only 1 agent has terminated  other agents give unexpected output. If one agent out of three has terminated than env.get_steps(behavior_name) should return 2 in decision_steps and 1 in terminal_steps instead its unpredictable what will be the return. ,also look issue multiple unable keep track agent even agent give unexpected output one agent three return instead unpredictable return,issue,negative,negative,negative,negative,negative,negative
673059487,I was able to reproduce the issue you are facing and have added it to our bug tracker. We will take a closer look at this issue. Thanks for bringing this to our attention.,able reproduce issue facing added bug tracker take closer look issue thanks attention,issue,negative,positive,positive,positive,positive,positive
673056400,"hi @HPRIOR - for the RayPerceptionSensor, we are using exposing Physics.Raycast (https://docs.unity3d.com/ScriptReference/Physics.Raycast.html).  It sounds like you are trying to compare with Ray (https://docs.unity3d.com/ScriptReference/Ray.html)

It would then likely make sense the lengths don't necessarily match up.  we will take a closer look.",hi like trying compare ray would likely make sense necessarily match take closer look,issue,negative,neutral,neutral,neutral,neutral,neutral
673033418,"This still doesn't automate it. I'd love to get the formatter in https://github.com/Unity-Technologies/git-hooks-format set up as a [pre-commit](https://pre-commit.com/) hook but that's still TBD
https://unity.slack.com/archives/CLJH4SXJB/p1597195224073600",still love get set hook still,issue,positive,positive,positive,positive,positive,positive
672748570,What should it change if you dont define a parameter in terms of Curriculum learning ?  I think you are misunderstanding the concept of it. You need to define values on your own for each lesson.,change dont define parameter curriculum learning think misunderstanding concept need define lesson,issue,negative,neutral,neutral,neutral,neutral,neutral
672704094,"> @prasengan could you please provide details(logs) for the bug that you see?

There are no logs, no error, no warnings. Though the above information should be sufficient and the bug can be easily recreated and verified please follow the mentioned steps",could please provide bug see error though information sufficient bug easily please follow,issue,positive,positive,positive,positive,positive,positive
672488250,"> Hi @Plussun I'm unable to reproduce the bug you mentioned on ML-Agents 0.20.0 on a mac with the Gridworld environment.
> here's the output that I got:
> 
> ```python
> [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin
> Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
> >>> from mlagents_envs.environment import UnityEnvironment
> >>> env = UnityEnvironment()
> >>> env.reset()
> >>> env.behavior_specs
> <mlagents_envs.base_env.BehaviorMapping object at 0x7f8cf82076a0>
> >>> name= list(env.behavior_specs)[0]
> >>> name
> 'GridWorld?team=0'
> >>> decision_steps, terminal_steps = env.get_steps(name)
> >>> print(list(decision_steps))
> [0]
> >>> print(list(terminal_steps))
> []
> ```

hi.
it will not happen at first, but after several env.reset() is called.here is the new code:
`from mlagents_envs.environment import UnityEnvironment`
`env = UnityEnvironment(file_name=None)`
press play button at Unity
```
for i in range(40):
    env.reset()
    behavior_name = list(env.behavior_specs)[0]
    decision_steps, terminal_steps = env.get_steps(behavior_name)
    print(list(terminal_steps),list(decision_steps))
```
result:

> [] [0]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [3] [0, 1, 2, 3, 4, 5, 6, 7, 8] this one
> [6] [0, 1, 2, 3, 4, 5, 6, 7, 8] this one
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [6] [0, 1, 2, 3, 4, 5, 6, 7, 8] this one
> [1] [0, 1, 2, 3, 4, 5, 6, 7, 8] this one
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [5] [0, 1, 2, 3, 4, 5, 6, 7, 8] this one
> [6] [0, 1, 2, 3, 4, 5, 6, 7, 8] this one
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [6] [0, 1, 2, 3, 4, 5, 6, 7, 8] this one
> [2, 8] [0, 1, 2, 3, 4, 5, 6, 7, 8] this one
> [] [0, 1, 2, 3, 4, 5, 6, 7, 8]
> [0] [0, 1, 2, 3, 4, 5, 6, 7, 8] this one

the GridWorld environment is download from this github, located at mlagents/Project/Assets/ML-Agents/examples/GridWorld
if i use env = default_registry[""GridWorld""].make() it works well.
And use the mlagents-learn config/ppo/GridWorld.yaml --run-id=testGridWorld works well too.
But I build the executable x86_64 file, still has this terminal steps problem.  ",hi unable reproduce bug mac environment output got python compatible apple type help copyright license information import object list name name print list print list hi happen first several new code import press play button unity range list print list list result one one one one one one one one one environment use work well use work well build executable file still terminal problem,issue,positive,negative,neutral,neutral,negative,negative
672423655,@prasengan could you please provide details(logs) for the bug that you see?,could please provide bug see,issue,negative,neutral,neutral,neutral,neutral,neutral
672414446,"Hi @Plussun I'm unable to reproduce the bug you mentioned on ML-Agents 0.20.0 on a mac with the Gridworld environment. 
here's the output that I got:

```Python 3.6.8 (v3.6.8:3c6b436a57, Dec 24 2018, 02:04:31)
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from mlagents_envs.environment import UnityEnvironment
>>> env = UnityEnvironment()
>>> env.reset()
>>> env.behavior_specs
<mlagents_envs.base_env.BehaviorMapping object at 0x7f8cf82076a0>
>>> name= list(env.behavior_specs)[0]
>>> name
'GridWorld?team=0'
>>> decision_steps, terminal_steps = env.get_steps(name)
>>> print(list(decision_steps))
[0]
>>> print(list(terminal_steps))
[]",hi unable reproduce bug mac environment output got python compatible apple type help copyright license information import object list name name print list print list,issue,negative,negative,negative,negative,negative,negative
672395201,"All python tests pass, and this won't affect any yamato tests. Going ahead with the merge.",python pas wo affect going ahead merge,issue,negative,neutral,neutral,neutral,neutral,neutral
672331980,Hi @MedhaviMonish could you please share the config file that you are using?,hi could please share file,issue,positive,neutral,neutral,neutral,neutral,neutral
672324184,"Hi @Ihsees could you please provide some more details so that this can be answered accurately.

Environment name?
Unity version?
OS?
ML-Agents version?
Tensorflow version?

It would be great if you could share the config.yaml you are using. ",hi could please provide accurately environment name unity version o version version would great could share,issue,positive,positive,positive,positive,positive,positive
672242613,"Aren't there some other places you need to check the rank, like generate_session_config() and TFPolicy.save_model()",need check rank like,issue,negative,negative,negative,negative,negative,negative
671913782,"have the similar problem, after call env.reset() sometimes there will be terminal steps.",similar problem call sometimes terminal,issue,negative,neutral,neutral,neutral,neutral,neutral
671708992,"@ervteng Sorry, mind taking another look? I pushed some other small changes after the initial approval.",sorry mind taking another look small initial approval,issue,negative,negative,negative,negative,negative,negative
671586589,"> Thanks, that works well for the latest version of ml-agent.
> 
> Is there a way to suppress the logs in ml-agents 0.11?

Is there a way to do this?",thanks work well latest version way suppress way,issue,positive,positive,positive,positive,positive,positive
671522141,@andrewcoh can you help me check if this code is doing correctly with ghost trainer? (especially the init_or_load part),help check code correctly ghost trainer especially part,issue,negative,neutral,neutral,neutral,neutral,neutral
671477982,"It doesn't show the actual value, does it?
In only shows the lesson?

It even SAYS it's the lesson... welp
![grafik](https://user-images.githubusercontent.com/17814024/89810235-05803e00-db3d-11ea-8b0d-985942089807.png)
",show actual value lesson even lesson,issue,negative,neutral,neutral,neutral,neutral,neutral
671398930,@RedTachyon Oooh. It worked. Haha. Did not see the UnityEnvironment() part. Thank you! ,worked see part thank,issue,negative,neutral,neutral,neutral,neutral,neutral
671389739,"On the line where you're calling
`env = UnityToGymWrapper(env_name)`

try instead doing

`env = UnityToGymWrapper(UnityEnvironment(env_name))` as per the instructions you linked?",line calling try instead per linked,issue,negative,neutral,neutral,neutral,neutral,neutral
671092531,"I tried the same for multiple agents, I can not understand why decision _steps and terminal_steps both have all agents when only 1 agent should be terminated.",tried multiple understand decision agent,issue,negative,neutral,neutral,neutral,neutral,neutral
670647697,"Thanks, that works well for the latest version of ml-agent.

Is there a way to suppress the logs in ml-agents 0.11? ",thanks work well latest version way suppress,issue,positive,positive,positive,positive,positive,positive
669845127,I guess I could check for the Academy's step count in an Update method and restart the environment once it has reached the max step count.,guess could check academy step count update method restart environment step count,issue,negative,neutral,neutral,neutral,neutral,neutral
669841498,"> Out of curiosity, was [this](https://forum.unity.com/threads/problem-with-the-onenvironmentreset-delegate.942262/) you? (No problem posting in both, just want to check)

Yes that was me. 

> By ""training instance"", do you mean an independent area like we discuss [in the docs](https://github.com/Unity-Technologies/ml-agents/blob/release_5/docs/Learning-Environment-Design.md#multiple-areas)? There's currently no notion of this in the SDK, and I'm not sure we want to introduce it now.

Sorry for introducing my own terminology I was struggling to describe what I meant. 

> If you have a class that manages the training instance, I would recommend putting the reset logic in this, instead of trying to have a single Agent coordinate it.

Is there a way to do this without coordinating it with an agent? As far as I'm aware, when an episode ends, each agents OnEpisodeBegin is called. The feature I am after would be a method which can be called exactly once each time an episode ends.  I'm struggling to find a way to access the restart logic without first going through an agent - although this is probably a design issue on my end. I have a class which coordinates the spawning of agents and the environment each time an episode ends. I need a way to signal to it that the episode has ended, and I am currently doing that by calling the restart class in exactly one agents OnEpisodeBegin.


",curiosity problem posting want check yes training instance mean independent area like discus currently notion sure want introduce sorry terminology struggling describe meant class training instance would recommend reset logic instead trying single agent way without agent far aware episode feature would method exactly time episode struggling find way access restart logic without first going agent although probably design issue end class spawning environment time episode need way signal episode ended currently calling restart class exactly one,issue,negative,positive,neutral,neutral,positive,positive
669669153,"Needs some tests, but this is generally ready for review.",need generally ready review,issue,negative,positive,positive,positive,positive,positive
669621631,"Out of curiosity, was [this](https://forum.unity.com/threads/problem-with-the-onenvironmentreset-delegate.942262/) you? (No problem posting in both, just want to check)

> Basically an OnEpisodeBegin for a training instance.

By ""training instance"", do you mean an independent area like we discuss [in the docs](https://github.com/Unity-Technologies/ml-agents/blob/release_5/docs/Learning-Environment-Design.md#multiple-areas)? There's currently no notion of this in the SDK, and I'm not sure we want to introduce it now.

If you have a class that manages the training instance, I would recommend putting the reset logic in this, instead of trying to have a single Agent coordinate it. ",curiosity problem posting want check basically training instance training instance mean independent area like discus currently notion sure want introduce class training instance would recommend reset logic instead trying single agent,issue,positive,positive,neutral,neutral,positive,positive
669399180,"```

Unity.Barracuda.UnsafeMatrixBlockMultiplyUnrolled8xhJob
UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)
UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])
UnityEngine.Logger:Log (UnityEngine.LogType,object)
UnityEngine.Debug:Log (object)
Unity.Jobs.LowLevel.Unsafe.JobsUtility:CreateJobReflectionData (System.Type,object,object,object) (at /home/bokken/buildslave/unity/build/Runtime/Jobs/ScriptBindings/Jobs.bindings.cs:110)
Unity.Jobs.IJobParallelForExtensions/ParallelForJobStruct`1<Unity.Barracuda.UnsafeMatrixBlockMultiplyUnrolled8xhJob>:Initialize () (at /home/bokken/buildslave/unity/build/Runtime/Jobs/Managed/IJobParallelFor.cs:23)
Unity.Jobs.IJobParallelForExtensions:Schedule<Unity.Barracuda.UnsafeMatrixBlockMultiplyUnrolled8xhJob> (Unity.Barracuda.UnsafeMatrixBlockMultiplyUnrolled8xhJob,int,int,Unity.Jobs.JobHandle) (at /home/bokken/buildslave/unity/build/Runtime/Jobs/Managed/IJobParallelFor.cs:51)
Unity.Barracuda.BurstBLAS:ScheduleSGEMM (Unity.Jobs.JobHandle,single*,int,int,single*,int,int,single*,int,int,int,bool,bool) (at Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Plugins/Burst/BurstBLAS.cs:91)
Unity.Barracuda.BurstBLAS:SGEMM (single*,int,int,single*,int,int,single*,int,int,int,bool,bool) (at Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Plugins/Burst/BurstBLAS.cs:44)
Unity.Barracuda.UnsafeArrayCPUOps:Conv2DUsingIm2ColSliced (Unity.Barracuda.Tensor,Unity.Barracuda.Tensor,Unity.Barracuda.Tensor,int[],int[]) (at Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Core/Backends/BarracudaUnsafeArrayCPU.cs:1456)
Unity.Barracuda.UnsafeArrayCPUOps:Conv2D (Unity.Barracuda.Tensor,Unity.Barracuda.Tensor,Unity.Barracuda.Tensor,int[],int[],Unity.Barracuda.Layer/FusedActivation) (at Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Core/Backends/BarracudaUnsafeArrayCPU.cs:1072)
Unity.Barracuda.StatsOps:Unity.Barracuda.IOps.Conv2D (Unity.Barracuda.Tensor,Unity.Barracuda.Tensor,Unity.Barracuda.Tensor,int[],int[],Unity.Barracuda.Layer/FusedActivation) (at Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Core/Backends/StatsOps.cs:69)
Unity.Barracuda.GenericWorker/<StartManualSchedule>d__29:MoveNext () (at Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Core/Backends/GenericWorker.cs:211)
Unity.Barracuda.GenericWorker:Execute () (at Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Core/Backends/GenericWorker.cs:118)
Unity.Barracuda.GenericWorker:Execute (System.Collections.Generic.IDictionary`2<string, Unity.Barracuda.Tensor>) (at Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Core/Backends/GenericWorker.cs:106)
Unity.MLAgents.Inference.ModelRunner:DecideBatch () (at /home/bokken/build/output/Unity-Technologies/ml-agents/upm-ci~/temp/com.unity.ml-agents-1.2.0-preview/Runtime/Inference/ModelRunner.cs:174)
Unity.MLAgents.Tests.ModelRunnerTest:TestRunModel () (at /home/bokken/build/output/Unity-Technologies/ml-agents/upm-ci~/temp/com.unity.ml-agents-1.2.0-preview/Tests/Editor/ModelRunnerTest.cs:92)
System.Reflection.MonoMethod:Invoke (object,System.Reflection.BindingFlags,System.Reflection.Binder,object[],System.Globalization.CultureInfo)
System.Reflection.MethodBase:Invoke (object,object[])
NUnit.Framework.Internal.Reflect:InvokeMethod (System.Reflection.MethodInfo,object,object[])
NUnit.Framework.Internal.MethodWrapper:Invoke (object,object[])
NUnit.Framework.Internal.Commands.TestMethodCommand:RunNonAsyncTestMethod (NUnit.Framework.Internal.ITestExecutionContext)
NUnit.Framework.Internal.Commands.TestMethodCommand:RunTestMethod (NUnit.Framework.Internal.ITestExecutionContext)
NUnit.Framework.Internal.Commands.TestMethodCommand:Execute (NUnit.Framework.Internal.ITestExecutionContext)
UnityEngine.TestRunner.NUnitExtensions.Runner.UnityLogCheckDelegatingCommand/<>c__DisplayClass2_0:<Execute>b__0 () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/UnityLogCheckDelegatingCommand.cs:25)
UnityEngine.TestRunner.NUnitExtensions.Runner.UnityLogCheckDelegatingCommand:CaptureException (NUnit.Framework.Internal.TestResult,System.Action) (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/UnityLogCheckDelegatingCommand.cs:69)
UnityEngine.TestRunner.NUnitExtensions.Runner.UnityLogCheckDelegatingCommand:ExecuteAndCheckLog (UnityEngine.TestTools.Logging.LogScope,NUnit.Framework.Internal.TestResult,System.Action) (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/UnityLogCheckDelegatingCommand.cs:80)
UnityEngine.TestRunner.NUnitExtensions.Runner.UnityLogCheckDelegatingCommand:Execute (NUnit.Framework.Internal.ITestExecutionContext) (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/UnityLogCheckDelegatingCommand.cs:25)
UnityEngine.TestRunner.NUnitExtensions.Runner.UnityLogCheckDelegatingCommand/<ExecuteEnumerable>d__3:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/UnityLogCheckDelegatingCommand.cs:36)
UnityEngine.TestTools.BeforeAfterTestCommandBase`1/<ExecuteEnumerable>d__9<NUnit.Framework.ITestAction>:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Commands/BeforeAfterTestCommandBase.cs:114)
UnityEngine.TestTools.BeforeAfterTestCommandBase`1/<ExecuteEnumerable>d__9<System.Reflection.MethodInfo>:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Commands/BeforeAfterTestCommandBase.cs:114)
UnityEngine.TestTools.ImmediateEnumerableCommand:Execute (NUnit.Framework.Internal.ITestExecutionContext) (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Commands/ImmediateEnumerableCommand.cs:19)
UnityEngine.TestTools.BeforeAfterTestCommandBase`1/<ExecuteEnumerable>d__9<System.Reflection.MethodInfo>:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Commands/BeforeAfterTestCommandBase.cs:122)
UnityEngine.TestTools.BeforeAfterTestCommandBase`1/<ExecuteEnumerable>d__9<UnityEngine.TestTools.IOuterUnityTestAction>:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Commands/BeforeAfterTestCommandBase.cs:114)
UnityEngine.TestRunner.NUnitExtensions.Runner.DefaultTestWorkItem/<PerformWork>d__2:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/DefaultTestWorkItem.cs:52)
UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<RunChildren>d__16:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/CompositeWorkItem.cs:200)
UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<PerformWork>d__12:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/CompositeWorkItem.cs:76)
UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<RunChildren>d__16:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/CompositeWorkItem.cs:200)
UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<PerformWork>d__12:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/CompositeWorkItem.cs:76)
UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<RunChildren>d__16:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/CompositeWorkItem.cs:200)
UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<PerformWork>d__12:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/CompositeWorkItem.cs:76)
UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<RunChildren>d__16:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/CompositeWorkItem.cs:200)
UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<PerformWork>d__12:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/CompositeWorkItem.cs:76)
UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<RunChildren>d__16:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/CompositeWorkItem.cs:200)
UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<PerformWork>d__12:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/CompositeWorkItem.cs:76)
UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<RunChildren>d__16:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/CompositeWorkItem.cs:200)
UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<PerformWork>d__12:MoveNext () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEngine.TestRunner/NUnitExtensions/Runner/CompositeWorkItem.cs:76)
UnityEditor.TestTools.TestRunner.EditModeRunner:MoveNextAndUpdateYieldObject () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEditor.TestRunner/TestRunner/EditModeRunner.cs:234)
UnityEditor.TestTools.TestRunner.EditModeRunner:TestConsumer () (at Library/PackageCache/com.unity.test-framework@1.1.16/UnityEditor.TestRunner/TestRunner/EditModeRunner.cs:267)
UnityEditor.EditorApplication:Internal_CallUpdateFunctions () (at /home/bokken/buildslave/unity/build/Editor/Mono/EditorApplication.cs:326)

(Filename: Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Plugins/Burst/BurstBLAS.cs Line: 91)

Caught fatal signal - signo:11 code:1 errno:0 addr:0x10
Obtained 44 stack frames.
#0  0x00559ccb6d3632 in burst_signal_handler(int, siginfo_t*, void*)
#1  0x007f154361f890 in funlockfile
#2  0x00559ccbf2b16a in bool ScriptingInvocation::Invoke<bool>(ScriptingExceptionPtr*, bool)
#3  0x00559ccb01005d in BurstCompilerService::CompileAsync(ScriptingClassPtr, ScriptingObjectPtr, void*, int, void (*)(void*, int, void*), void (*)(void*, BurstLogType, char const*, char const*, int))
#4  0x00559ccb687c01 in CreateJobReflectionData(ScriptingSystemTypeObjectPtr, ScriptingSystemTypeObjectPtr, ScriptingObjectPtr, ScriptingObjectPtr, ScriptingObjectPtr, ScriptingExceptionPtr*)
#5  0x00559ccc10191d in JobsUtility_CUSTOM_CreateJobReflectionData(ScriptingBackendNativeObjectPtrOpaque*, ScriptingBackendNativeObjectPtrOpaque*, ScriptingBackendNativeObjectPtrOpaque*, ScriptingBackendNativeObjectPtrOpaque*, ScriptingBackendNativeObjectPtrOpaque*)
#6  0x00000040e15912 in (wrapper managed-to-native) Unity.Jobs.LowLevel.Unsafe.JobsUtility:CreateJobReflectionData (System.Type,System.Type,object,object,object)
#7  0x00000040e14f78 in Unity.Barracuda.BurstBLAS:ScheduleSGEMM (Unity.Jobs.JobHandle,single*,int,int,single*,int,int,single*,int,int,int,bool,bool)
#8  0x00000040e13cd8 in Unity.Barracuda.BurstBLAS:SGEMM (single*,int,int,single*,int,int,single*,int,int,int,bool,bool)
#9  0x00000040e11a34 in Unity.Barracuda.UnsafeArrayCPUOps:Conv2DUsingIm2ColSliced (Unity.Barracuda.Tensor,Unity.Barracuda.Tensor,Unity.Barracuda.Tensor,int[],int[])
#10 0x00000040e0a31c in Unity.Barracuda.UnsafeArrayCPUOps:Conv2D (Unity.Barracuda.Tensor,Unity.Barracuda.Tensor,Unity.Barracuda.Tensor,int[],int[],Unity.Barracuda.Layer/FusedActivation)
#11 0x00000040e09ce9 in Unity.Barracuda.StatsOps:Unity.Barracuda.IOps.Conv2D (Unity.Barracuda.Tensor,Unity.Barracuda.Tensor,Unity.Barracuda.Tensor,int[],int[],Unity.Barracuda.Layer/FusedActivation)
#12 0x00000040ddfa26 in Unity.Barracuda.GenericWorker/<StartManualSchedule>d__29:MoveNext ()
#13 0x00000040dd4ee9 in Unity.Barracuda.GenericWorker:Execute ()
#14 0x00000040dc31aa in Unity.MLAgents.Inference.ModelRunner:DecideBatch ()
#15 0x00000040d8b539 in (wrapper runtime-invoke) object:runtime_invoke_void__this__ (object,intptr,intptr,intptr)
#16 0x007f141ac3bab5 in mono_print_method_from_ip
#17 0x007f141adab164 in mono_perfcounter_foreach
#18 0x007f141adb321f in mono_runtime_run_main
#19 0x007f141ad75b08 in mono_install_unhandled_exception_hook
#20 0x00000040779799 in (wrapper managed-to-native) System.Reflection.MonoMethod:InternalInvoke (System.Reflection.MonoMethod,object,object[],System.Exception&)
#21 0x00000040777188 in System.Reflection.MethodBase:Invoke (object,object[])
#22 0x00000040bf7350 in NUnit.Framework.Internal.MethodWrapper:Invoke (object,object[])
#23 0x00000040bf680c in UnityEngine.TestRunner.NUnitExtensions.Runner.UnityLogCheckDelegatingCommand:ExecuteAndCheckLog (UnityEngine.TestTools.Logging.LogScope,NUnit.Framework.Internal.TestResult,System.Action)
#24 0x00000040bf54cc in UnityEngine.TestRunner.NUnitExtensions.Runner.UnityLogCheckDelegatingCommand/<ExecuteEnumerable>d__3:MoveNext ()
#25 0x00000040bf239c in UnityEngine.TestTools.BeforeAfterTestCommandBase`1/<ExecuteEnumerable>d__9<T_REF>:MoveNext ()
#26 0x00000040bf239c in UnityEngine.TestTools.BeforeAfterTestCommandBase`1/<ExecuteEnumerable>d__9<T_REF>:MoveNext ()
#27 0x00000040bf4819 in UnityEngine.TestTools.ImmediateEnumerableCommand:Execute (NUnit.Framework.Internal.ITestExecutionContext)
#28 0x00000040bf24e9 in UnityEngine.TestTools.BeforeAfterTestCommandBase`1/<ExecuteEnumerable>d__9<T_REF>:MoveNext ()
#29 0x00000040bf239c in UnityEngine.TestTools.BeforeAfterTestCommandBase`1/<ExecuteEnumerable>d__9<T_REF>:MoveNext ()
#30 0x00000040befbe5 in UnityEngine.TestRunner.NUnitExtensions.Runner.DefaultTestWorkItem/<PerformWork>d__2:MoveNext ()
#31 0x00000040bc15ed in UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<RunChildren>d__16:MoveNext ()
#32 0x00000040bb7d46 in UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<PerformWork>d__12:MoveNext ()
#33 0x00000040bc15ed in UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<RunChildren>d__16:MoveNext ()
#34 0x00000040bb7d46 in UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<PerformWork>d__12:MoveNext ()
#35 0x00000040bc15ed in UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<RunChildren>d__16:MoveNext ()
#36 0x00000040bb7d46 in UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<PerformWork>d__12:MoveNext ()
#37 0x00000040bc15ed in UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<RunChildren>d__16:MoveNext ()
#38 0x00000040bb7d46 in UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<PerformWork>d__12:MoveNext ()
#39 0x00000040bc15ed in UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<RunChildren>d__16:MoveNext ()
#40 0x00000040bb7d46 in UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<PerformWork>d__12:MoveNext ()
#41 0x00000040bc15ed in UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<RunChildren>d__16:MoveNext ()
#42 0x00000040bb7d46 in UnityEngine.TestRunner.NUnitExtensions.Runner.CompositeWorkItem/<PerformWork>d__12:MoveNext ()
#43 0x00000040bb631f in UnityEditor.TestTools.TestRunner.EditModeRunner:MoveNextAndUpdateYieldObject ()
```
[source](https://yamato-artifactviewer.prd.cds.internal.unity3d.com/4b28143d-7790-430f-b488-6e4291cd8125%2Flogs%2Fupm-ci~%2Ftest-results%2Fisolation-3025525-com.unity.ml-agents%2Feditor%2Ftmp-198560rcZQMynNHr/UnityLog.txt) (similar in 2 other jobs)",string object log object log object object object object initialize schedule single single single bool bool single single single bool bool execute execute string invoke object object invoke object object object object invoke object object execute execute execute execute line caught fatal signal code stack void bool bool bool void void void void void void char char wrapper object object object single single single bool bool single single single bool bool execute wrapper object object wrapper object object invoke object object invoke object object execute source similar,issue,negative,negative,neutral,neutral,negative,negative
669302053,"Error:
```
[62/142] TestStart 'Unity.MLAgents.Tests.ModelRunnerTest.TestRunModel'
LogEntry
	severity: Error
	message: InvalidCastException: Specified cast is not valid.
	stacktrace: UnityEditor.AnimEditor.ExecuteShortcut (UnityEditor.ShortcutManagement.ShortcutArguments args, System.Action`1[T] exp) (at /home/bokken/buildslave/unity/build/Editor/Mono/Animation/AnimationWindow/AnimEditor.cs:868)
UnityEditor.AnimEditor.ExecuteShortcut (UnityEditor.ShortcutManagement.ShortcutArguments args, System.Action`1[T] exp) (at /home/bokken/buildslave/unity/build/Editor/Mono/Animation/AnimationWindow/AnimEditor.cs:884)
UnityEditor.AnimEditor.LastKeyFrame (UnityEditor.ShortcutManagement.ShortcutArguments args) (at /home/bokken/buildslave/unity/build/Editor/Mono/Animation/AnimationWindow/AnimEditor.cs:946)
Unity.Jobs.LowLevel.Unsafe.JobsUtility:CreateJobReflectionData(Type, Type, Object, Object, Object)
Unity.Jobs.LowLevel.Unsafe.JobsUtility:CreateJobReflectionData(Type, Object, Object, Object) (at /home/bokken/buildslave/unity/build/Runtime/Jobs/ScriptBindings/Jobs.bindings.cs:110)
Unity.Jobs.ParallelForJobStruct`1:Initialize() (at /home/bokken/buildslave/unity/build/Runtime/Jobs/Managed/IJobParallelFor.cs:23)
Unity.Jobs.IJobParallelForExtensions:Schedule(UnsafeMatrixBlockMultiplyUnrolled8xhJob, Int32, Int32, JobHandle) (at /home/bokken/buildslave/unity/build/Runtime/Jobs/Managed/IJobParallelFor.cs:51)
Unity.Barracuda.BurstBLAS:ScheduleSGEMM(JobHandle, Single*, Int32, Int32, Single*, Int32, Int32, Single*, Int32, Int32, Int32, Boolean, Boolean) (at Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Plugins/Burst/BurstBLAS.cs:91)
Unity.Barracuda.BurstBLAS:SGEMM(Single*, Int32, Int32, Single*, Int32, Int32, Single*, Int32, Int32, Int32, Boolean, Boolean) (at Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Plugins/Burst/BurstBLAS.cs:44)
Unity.Barracuda.UnsafeArrayCPUOps:Conv2DUsingIm2ColSliced(Tensor, Tensor, Tensor, Int32[], Int32[]) (at Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Core/Backends/BarracudaUnsafeArrayCPU.cs:1456)
Unity.Barracuda.UnsafeArrayCPUOps:Conv2D(Tensor, Tensor, Tensor, Int32[], Int32[], FusedActivation) (at Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Core/Backends/BarracudaUnsafeArrayCPU.cs:1072)
Unity.Barracuda.StatsOps:Unity.Barracuda.IOps.Conv2D(Tensor, Tensor, Tensor, Int32[], Int32[], FusedActivation) (at Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Core/Backends/StatsOps.cs:69)
Unity.Barracuda.<StartManualSchedule>d__29:MoveNext() (at Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Core/Backends/GenericWorker.cs:211)
Unity.Barracuda.GenericWorker:Execute() (at Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Core/Backends/GenericWorker.cs:118)
Unity.Barracuda.GenericWorker:Execute(IDictionary`2) (at Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Core/Backends/GenericWorker.cs:106)
Unity.MLAgents.Inference.ModelRunner:DecideBatch() (at /home/bokken/build/output/Unity-Technologies/ml-agents/upm-ci~/temp/com.unity.ml-agents-1.2.0-preview/Runtime/Inference/ModelRunner.cs:174)
Unity.MLAgents.Tests.ModelRunnerTest:TestRunModel() (at /home/bokken/build/output/Unity-Technologies/ml-agents/upm-ci~/temp/com.unity.ml-agents-1.2.0-preview/Tests/Editor/ModelRunnerTest.cs:92)
System.Reflection.MethodBase:Invoke(Object, Object[])
NUnit.Framework.Internal.Reflect:InvokeMethod(MethodInfo, Object, Object[])
NUnit.Framework.Internal.MethodWrapper:Invoke(Object, Object[])
NUnit.Framework.Internal.Commands.TestMethodCommand:RunNonAsyncTestMethod(ITestExecutionContext)
NUnit.Framework.Internal.Commands.TestMethodCommand:RunTestMethod(ITestExecutionContext)
NUnit.Framework.Internal.Commands.TestMethodCommand:Execute(ITestExecutionContext)
UnityEditor.EditorApplication:Internal_CallUpdateFunctions() (at /home/bokken/buildslave/unity/build/Editor/Mono/EditorApplication.cs:326)

	line: 91
	file: Library/PackageCache/com.unity.barracuda@1.0.1/Barracuda/Runtime/Plugins/Burst/BurstBLAS.cs
```
[source](https://yamato.prd.cds.internal.unity3d.com/jobs/497-ml-agents/tree/yamato-barracuda-repro/.yamato%252Fcom.unity.ml-agents-test.yml%2523test_com.unity.ml-agents_linux_2020.2_try3/3024060/job/(log:Execution))",error severity error message cast valid type type object object object type object object object initialize schedule single single single single single single tensor tensor tensor tensor tensor tensor tensor tensor tensor execute execute invoke object object object object invoke object object execute line file source,issue,negative,negative,neutral,neutral,negative,negative
668733807,"> LGTM for a TF fix - let's separately figure out how to do it for PyTorch.
> 
> Note for posterity: if the first update has none of the large observations, it's still possible to get a NaN. But this should fix the issue for most cases.

To clarify, it's possible to get a NaN if the values in successive trajectories are really different and don't follow anything near to a normal distribution (an assumption of the welford algorithm). ",fix let separately figure note posterity first update none large still possible get nan fix issue clarify possible get nan successive really different follow anything near normal distribution assumption algorithm,issue,negative,positive,positive,positive,positive,positive
668697646,"Yeah, I like the copying idea better. I'll try to get to it this week unless someone else gets to it first.",yeah like idea better try get week unless someone else first,issue,positive,positive,positive,positive,positive,positive
668649415,"Another way to handle this is to have the final model simply checkpoint the same as all of the others, and then copy the `.nn` file to its resulting location.  What do you think about that as an alternative?",another way handle final model simply copy file resulting location think alternative,issue,negative,neutral,neutral,neutral,neutral,neutral
668440095,"Since r_5 had made this change, now 
pip3 install mlagents 
would cause the incompatible error
maybe should also update the package in PyPI  ",since made change pip install would cause incompatible error maybe also update package,issue,negative,neutral,neutral,neutral,neutral,neutral
668315620,"Hi @chriselion 
I see. Thanks for your response!",hi see thanks response,issue,negative,positive,positive,positive,positive,positive
668310201,"Hi @KevinWu57,
The CSV stats have been deprecated (unofficially) for a while. There's a similar issue here https://github.com/Unity-Technologies/ml-agents/issues/3861 with a workaround on how to get the values from tensorboard.",hi unofficially similar issue get,issue,negative,neutral,neutral,neutral,neutral,neutral
668264112,This will result in inconsistency of all checkpoints having both .ckpt and .nn files except the last one only having .ckpt while its .nn file is with a different name under different path. This could work but might cause some confusion when someone look at the result folder or when writing scripts to track the checkpointing in cloud training.,result inconsistency except last one file different name different path could work might cause confusion someone look result folder writing track cloud training,issue,negative,neutral,neutral,neutral,neutral,neutral
668211105,This only works for TF PPO now. Will complete for other classes once this looks good.,work complete class good,issue,negative,positive,positive,positive,positive,positive
668177241,I did a spot check with `git grep --text m_BehaviorName` and it looks like this is the only prefab like this.,spot check git text like prefab like,issue,positive,neutral,neutral,neutral,neutral,neutral
668082777,"I did not upgrade it from an older version. I directly installed the newest version. And on another pc, I installed the older version which I believe is release 2.",upgrade older version directly version another older version believe release,issue,negative,positive,positive,positive,positive,positive
667892361,"@awjuliani Thank you very much
I notice that the file name is ""Colab-UnityEnvironment-1-Run.ipynb""
Is there have Colab-UnityEnvironment-2 or 3 ?
I tried to find them in github or search the name in google but i didn't found.
I can't even found Colab-UnityEnvironment-1-Run.ipynb in your github.
Are they exist? If yes, where are they placed.",thank much notice file name tried find search name found ca even found exist yes,issue,positive,positive,positive,positive,positive,positive
667724356,"@knmitri  @zaiyun just curious what versions were you upgrading from (that did not have this issue)?

I've seen this happen with SAC (training is going well, no errors in training environment, then all of a sudden they stop playing), and this happened whenever I used negative rewards, which @zaiyun also seems to use. 

However I don't have this issue with PPO, at least not in version 1.0.2. I can train up to 40M+ steps.",curious issue seen happen sac training going well training environment sudden stop whenever used negative also use however issue least version train,issue,negative,negative,negative,negative,negative,negative
667644560,"I had the exact same issue on my model. I don't have this problem when using another pc with an older Mlagent version. With the exact same project and same setting and everything, the newer version mlagent will collapse after certain steps. This is definitely a bug. 
![1](https://user-images.githubusercontent.com/47151605/89118900-a30daa80-d477-11ea-9c12-a8049b273b00.PNG)
",exact issue model problem another older version exact project setting everything version collapse certain definitely bug,issue,negative,positive,positive,positive,positive,positive
667359606,I think just merge this. we can try to fix the changelog on master.,think merge try fix master,issue,negative,neutral,neutral,neutral,neutral,neutral
666884641,"Sure, doing that in https://github.com/Unity-Technologies/ml-agents/pull/4290 to keep them separate. ~I think you're OK to merge this in the meantime~ Merged that it to the release branch, and release branch to here.",sure keep separate think merge release branch release branch,issue,negative,positive,positive,positive,positive,positive
666855949,"@chriselion Thanks for the heads-up. I get a merge conflict that I am not exactly sure how to resolve correctly, since I don't have experience with the yamato config files. Would you mind cherry-picking these for me? ",thanks get merge conflict exactly sure resolve correctly since experience would mind,issue,positive,positive,positive,positive,positive,positive
666848947,You're going to need to cherry-pick https://github.com/Unity-Technologies/ml-agents/pull/4278 and https://github.com/Unity-Technologies/ml-agents/pull/4282 to get yamato tests to pass.,going need get pas,issue,negative,neutral,neutral,neutral,neutral,neutral
666763293,"Training times and rewards:

master
| Job Id  | Scene Name  |  Expected Reward | Mean Reward | Max Reward  |  Inference Reward  | Training Time Hours |
|---------|-------------|------------------|-------------|-------------|--------------------|---------------------|
| crawdy-ppo  | crawlerdynamictarget  |  400 | 810.8445  |  813.6515  |  645.7593   | 10.79
| crawsta-ppo | crawlerstatictarget  |  2000    |2069.4256 |  2073.8216  | 1988.6075  | 11.01

this branch
| Job Id  | Scene Name  |  Expected Reward | Mean Reward | Max Reward  |  Inference Reward  | Training Time Hours |
|---------|-------------|------------------|-------------|-------------|--------------------|---------------------|
| crawdy-ppo  |crawlerdynamictarget    | 400 | 801.0141 |   821.2339  |  832.2614   | 10.79  |
| crawsta-ppo |crawlerstatictarget   |  2000   | 1789.9456 |  1863.3114  | 1777.4036|    13.13 |

I'm going to retrain the crawlerstatictarget scene again tonight; I got some prefabs mixed up so it was using different settings (added local space velocities too). Hopefully that will bring the training time down and the reward up a bit.

Also note that for crawlerstatictarget, master is above the expected reward and below on this branch. However, the reward on master is the outlier (unusually good) if we look at the last few weeks of data:
![image](https://user-images.githubusercontent.com/6877802/88982794-121faf00-d27e-11ea-8e54-2998fc31ff97.png)
",training time master job id scene name reward mean reward reward inference reward training time branch job id scene name reward mean reward reward inference reward training time going retrain scene tonight got mixed different added local space hopefully bring training time reward bit also note master reward branch however reward master outlier unusually good look last data image,issue,positive,positive,neutral,neutral,positive,positive
666694092,"We  are so excited about Distributed RL, please inform us a soon as you start to work on it.",excited distributed please inform u soon start work,issue,positive,positive,positive,positive,positive,positive
666019481,"> Can you where the process use the UnityEnvironment() to wrap the binary?

* learn.py creates a SubprocessEnvManager... https://github.com/Unity-Technologies/ml-agents/blob/8327ddcb2a65ccb0a76ce6390811212d1daebb6e/ml-agents/mlagents/trainers/learn.py#L129
* which starts a new subprocess using the `worker()` function: https://github.com/Unity-Technologies/ml-agents/blob/8327ddcb2a65ccb0a76ce6390811212d1daebb6e/ml-agents/mlagents/trainers/subprocess_env_manager.py#L231-L242
* that calls the env_factory callable that was passed to it: https://github.com/Unity-Technologies/ml-agents/blob/8327ddcb2a65ccb0a76ce6390811212d1daebb6e/ml-agents/mlagents/trainers/subprocess_env_manager.py#L138-L140
* which actually creates the UnityEnvironment: https://github.com/Unity-Technologies/ml-agents/blob/8327ddcb2a65ccb0a76ce6390811212d1daebb6e/ml-agents/mlagents/trainers/learn.py#L208-L217",process use wrap binary new worker function callable actually,issue,negative,positive,neutral,neutral,positive,positive
665981884,"> Do you mean when you pass it as the --env argument to mlagents-learn?

Yes. Hmm, interesting. Can you where the process use the UnityEnvironment() to wrap the binary?

> Does the file it's trying to load actually? Maybe there's a difference in permissions between when you call mlagents-learn and when you use a UnityEnvironment?

I think it's loading. And error throw out in the middle.
",mean pas argument yes interesting process use wrap binary file trying load actually maybe difference call use think loading error throw middle,issue,negative,positive,neutral,neutral,positive,positive
665966166,"> Using ml-agent cli can successfully talk to the binary

Do you mean when you pass it as the `--env` argument to `mlagents-learn`?

`mlagents-learn` uses a UnityEnvironment under the hood, so there shouldn't be anything different in how they launch the executable. You can see where it gets launched [here](https://github.com/Unity-Technologies/ml-agents/blob/release_4/ml-agents-envs/mlagents_envs/env_utils.py#L93-L101). 

Does the file it's trying to load actually? Maybe there's a difference in permissions between when you call mlagents-learn and when you use a UnityEnvironment?",successfully talk binary mean pas argument hood anything different launch executable see file trying load actually maybe difference call use,issue,negative,positive,positive,positive,positive,positive
665819388,Added a warning and a test to catch the user error originally reported.,added warning test catch user error originally,issue,negative,positive,positive,positive,positive,positive
665780158,"> My preference would be to just throw a warning and ignore the completion criteria of the last lesson, rather than throwing an error.

I agree.",preference would throw warning ignore completion criterion last lesson rather throwing error agree,issue,negative,neutral,neutral,neutral,neutral,neutral
665778171,"Hi @HPRIOR 
That's a reasonable request; it's come up before internally, but I'm not sure if we have it logged in our tracker anywhere.

Just to be clear, you would still need to have the same number of observations every step, and for all Agents with the same behavior name.

One possible downside is that it would make checking that a barracuda model was the right size for the Agent, since we wouldn't know the observation size in advance.

If keeping track of the the observation size is annoying, I have two suggestions:
1) Use SensorComponents and ISensors. These are responsible for determining their own size, so any changes in their settings don't require changes to the BehaviorParameters. The RayPerceptionSensor and RayPerceptionSensorComponent3D are good examples of this.
2) Set it too large, start training, and there will be a warning telling you the actual number of observations written (I'm not too proud to admit that I did this recently because I was too lazy to add up the number of AddObservation calls).",hi reasonable request come internally sure logged tracker anywhere clear would still need number every step behavior name one possible downside would make barracuda model right size agent since would know observation size advance keeping track observation size annoying two use responsible size require good set large start training warning telling actual number written admit recently lazy add number,issue,negative,positive,positive,positive,positive,positive
665354743,"@vincentpierre Makes sense. The question is then how to handle this. My preference would be to just throw a warning and ignore the completion criteria of the last lesson, rather than throwing an error.",sense question handle preference would throw warning ignore completion criterion last lesson rather throwing error,issue,negative,neutral,neutral,neutral,neutral,neutral
665353279,"Can you add a unit test that would have caught this before (or if you add the validation to completion_criteria, test that it fails)?

You might need to merge master to get the yamato tests passing again. Let me know if they're still failing after that and I'll take a look.",add unit test would caught add validation test might need merge master get passing let know still failing take look,issue,negative,neutral,neutral,neutral,neutral,neutral
665246809,"Yes, I was upgrading from 0.4b to 1 and before finishing I upgraded to 4. Now it is working without that file.",yes finishing working without file,issue,negative,neutral,neutral,neutral,neutral,neutral
665176638,"Thanks for bringing this up.  I was able to do some digging, and found that this is indeed a bug in our curriculum code. We'll be making a PR to fix it soon. ",thanks able digging found indeed bug curriculum code making fix soon,issue,negative,positive,positive,positive,positive,positive
665145619,"Hi @alphonse92 

For clarification, it wasn't any of our example environments which had the behavior parameter set to ""Heuristic Only,"" rather it was your own environment? As far as I know, all our examples have ""Default"" set, and can be trained directly. If that is the case, it may not be the getting started that is the best place for this documentation note, but rather one of. the documents we have specifically about making new environments. ",hi clarification example behavior parameter set heuristic rather environment far know default set trained directly case may getting best place documentation note rather one specifically making new,issue,positive,positive,positive,positive,positive,positive
664723234,Hello @seanysull the docstring in the code here describes what I believe you are looking for https://github.com/Unity-Technologies/ml-agents/blob/master/com.unity.ml-agents/Runtime/Sensors/RayPerceptionSensor.cs#L170. Let me know if that makes sense.,hello code believe looking let know sense,issue,negative,neutral,neutral,neutral,neutral,neutral
664722777,"Hi @HPRIOR 

Can you share your curriculum file?",hi share curriculum file,issue,negative,neutral,neutral,neutral,neutral,neutral
664721804,"Hello. We have a colab notebook which replaces the old getting-started notebook: https://colab.research.google.com/drive/1Qg6E5kmf9n4G8rc5lXHIM_cQzMUFGH-g#forceEdit=true&sandboxMode=true. 

Hopefully this is what you are looking for. I am going to close this issue, but if you have other questions, feel free to re-open it.",hello notebook old notebook hopefully looking going close issue feel free,issue,positive,positive,positive,positive,positive,positive
664677704,@awjuliani Might want to consider this for a patch release too.,might want consider patch release,issue,negative,neutral,neutral,neutral,neutral,neutral
664527105,"Sorry for the late reply on this. I belive this issue was fixed a few releases ago:
> When using Curriculum, the current lesson will resume if training is quit and resumed. As such, the --lesson CLI option has been removed. (#4025)

See https://github.com/Unity-Technologies/ml-agents/blob/latest_release/com.unity.ml-agents/CHANGELOG.md#110-preview---2020-06-10 for more details.
",sorry late reply belive issue fixed ago curriculum current lesson resume training quit lesson option removed see,issue,negative,negative,negative,negative,negative,negative
663764627,"Hi @awjuliani 

Thank you for logging this request! I would be more than happy to see the integration of the depth map!",hi thank logging request would happy see integration depth map,issue,positive,positive,positive,positive,positive,positive
663762105,"Yep, thanks for coming back around to this.",yep thanks coming back around,issue,positive,positive,neutral,neutral,positive,positive
663761870,"Sorry, I realized I never followed up on this. DecisionRequester was made public in https://github.com/Unity-Technologies/ml-agents/pull/3716",sorry never made public,issue,negative,negative,negative,negative,negative,negative
663756584,"@andrewcoh This was fixed by https://github.com/Unity-Technologies/ml-agents/pull/3821 right?

Also (possibly after you logged this) we changed Heuristic so that it takes a `float[]` argument to write to, instead of returning an array; this should make it harder to get the size wrong in the future.",fixed right also possibly logged heuristic float argument write instead array make harder get size wrong future,issue,negative,negative,neutral,neutral,negative,negative
663754803,"You can now also pass the `--initialize-from` flag, to restart training from an existing model.",also pas flag restart training model,issue,negative,neutral,neutral,neutral,neutral,neutral
663754006,"The behavior of the command line parameters was changed a few releases ago, so that we'll never overwrite existing runs unless you pass --force",behavior command line ago never overwrite unless pas force,issue,negative,neutral,neutral,neutral,neutral,neutral
663753583,"Sorry for the late followup on this. ML-Agents is available as a preview package currently, and will be a verified package in 2020.2.",sorry late available preview package currently package,issue,negative,negative,neutral,neutral,negative,negative
663753168,"Sorry for the late notice, but added support for custom metrics in tensorboard a few months ago. See the StatsRecorder (which is part of the Academy): https://github.com/Unity-Technologies/ml-agents/blob/latest_release/com.unity.ml-agents/Runtime/StatsRecorder.cs",sorry late notice added support custom metric ago see part academy,issue,negative,negative,negative,negative,negative,negative
663749817,"As @niskander noticed in https://github.com/Unity-Technologies/ml-agents/pull/4262, you might need to do
`.\tensorboard --logdir results`
(without the `=`) instead.",might need without instead,issue,negative,neutral,neutral,neutral,neutral,neutral
663743963,"Hi @KevinWu57 

Thanks for making this request.  It sounds like your workaround is the right one. I think the solution on our end would be to either extend the camera sensor, or add a new depth camera sensor. Others have asked for this before, and I believe it is something we on the team have internally explored, so I will make sure this request is properly logged. ",hi thanks making request like right one think solution end would either extend camera sensor add new depth camera sensor believe something team internally make sure request properly logged,issue,positive,positive,positive,positive,positive,positive
663724406,@chriselion yes apparently? That was really weird... I signed it with the other account as well. ,yes apparently really weird account well,issue,negative,negative,negative,negative,negative,negative
663370154,"@chriselion I made the change in the other files as well! 

@CLAassistant is asking me to resign the CLA (which I cannot do since it's already signed...)

<img width=""1432"" alt=""Screen Shot 2020-07-24 at 2 36 54 AM"" src=""https://user-images.githubusercontent.com/1988272/88366230-99d55d00-cd56-11ea-803c-04e616cf6fe2.png"">",made change well resign since already screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
663341458,"I just noticed the CUDA error! Yes, it is the driver issue. I have updated the GPU driver to the latest and it can now get trained successfully! 

Thank you so much for the help Vincent!",error yes driver issue driver latest get trained successfully thank much help vincent,issue,positive,positive,positive,positive,positive,positive
663332230,"Okay, I think the two issues are related. In the try/catch that was fixed on master, the culprit was probably the call to `generate_session_config()`. You can see the method fails as well this time :
```
tensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version
```
Which means this is a CUDA issue. The installation of ml-agents does not do any GPU setup, so my guess is that you have a CUDA configuration somewhere that causes this issue. 
You can try setting `export CUDA_VISIBLE_DEVICES=-1` in the terminal to force TensorFlow to use the CPU or you can try to reinstall CUDA. Have you used CUDA in other projects ?",think two related fixed master culprit probably call see method well time status driver version insufficient version issue installation setup guess configuration somewhere issue try setting export terminal force use try reinstall used,issue,negative,positive,neutral,neutral,positive,positive
663310545,"I have checkout master and pull. However, during handling, another error occured:
```
(mlagents-env) D:\ML-Agents\ml-agents\config>mlagents-learn ppo/3DBall.yaml --run-id=3DBall_test --force
2020-07-23 18:20:23.054329: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
WARNING:tensorflow:From d:\ml-agents\mlagents-env\lib\site-packages\tensorflow\python\compat\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term


                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓


 Version information:
  ml-agents: 0.19.0.dev0,
  ml-agents-envs: 0.19.0.dev0,
  Communicator API: 1.0.0,
  TensorFlow: 2.2.0
2020-07-23 18:20:25.646658: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
WARNING:tensorflow:From d:\ml-agents\mlagents-env\lib\site-packages\tensorflow\python\compat\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2020-07-23 18:20:27 INFO [environment.py:199] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
2020-07-23 18:20:29 INFO [environment.py:108] Connected to Unity environment with package version 1.2.0-preview and communication version 1.0.0
2020-07-23 18:20:29 INFO [environment.py:265] Connected new brain:
3DBall?team=0
2020-07-23 18:20:29.467836: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-07-23 18:20:29.479564: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2c24bb370f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-23 18:20:29.485485: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-23 18:20:29.490874: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-07-23 18:20:29.654236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1
coreClock: 1.683GHz coreCount: 15 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s
2020-07-23 18:20:29.662758: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-07-23 18:20:29.671851: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-07-23 18:20:29.680138: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-07-23 18:20:29.686172: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-07-23 18:20:29.694907: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-07-23 18:20:29.702204: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-07-23 18:20:29.711580: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-07-23 18:20:29.720371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-07-23 18:20:29 WARNING [stats.py:239] Could not write Hyperparameters summary for Tensorboard: {'trainer_type': 'ppo', 'hyperparameters': {'batch_size': 64, 'buffer_size': 12000, 'learning_rate': 0.0003, 'beta': 0.001, 'epsilon': 0.2, 'lambd': 0.99, 'num_epoch': 3, 'learning_rate_schedule': 'linear'}, 'network_settings': {'normalize': True, 'hidden_units': 128, 'num_layers': 2, 'vis_encode_type': 'simple', 'memory': None}, 'reward_signals': {'extrinsic': {'gamma': 0.99, 'strength': 1.0}}, 'init_path': None, 'keep_checkpoints': 5, 'checkpoint_interval': 500000, 'max_steps': 500000, 'time_horizon': 1000, 'summary_freq': 12000, 'threaded': True, 'self_play': None, 'behavioral_cloning': None}
2020-07-23 18:20:29 INFO [stats.py:131] Hyperparameters for behavior name 3DBall:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   64
          buffer_size:  12000
          learning_rate:        0.0003
          beta: 0.001
          epsilon:      0.2
          lambd:        0.99
          num_epoch:    3
          learning_rate_schedule:       linear
        network_settings:
          normalize:    True
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
        init_path:      None
        keep_checkpoints:       5
        checkpoint_interval:    500000
        max_steps:      500000
        time_horizon:   1000
        summary_freq:   12000
        threaded:       True
        self_play:      None
        behavioral_cloning:     None
2020-07-23 18:20:29.729412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1
coreClock: 1.683GHz coreCount: 15 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s
2020-07-23 18:20:29.737810: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-07-23 18:20:29.742400: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-07-23 18:20:29.746056: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-07-23 18:20:29.750547: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-07-23 18:20:29.754533: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-07-23 18:20:29.759745: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-07-23 18:20:29.763789: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-07-23 18:20:29.771611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
Traceback (most recent call last):
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\trainer_controller.py"", line 177, in start_learning
    self._reset_env(env_manager)
  File ""d:\ml-agents\ml-agents\ml-agents-envs\mlagents_envs\timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\trainer_controller.py"", line 115, in _reset_env
    self._register_new_behaviors(env_manager, env_manager.first_step_infos)
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\trainer_controller.py"", line 271, in _register_new_behaviors
    self._create_trainers_and_managers(env_manager, new_behavior_ids)
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\trainer_controller.py"", line 169, in _create_trainers_and_managers
    self._create_trainer_and_manager(env_manager, behavior_id)
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\trainer_controller.py"", line 143, in _create_trainer_and_manager
    parsed_behavior_id, env_manager.training_behaviors[name_behavior_id]
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\ppo\trainer.py"", line 205, in create_policy
    create_tf_graph=False,  # We will create the TF graph in the Optimizer
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\policy\tf_policy.py"", line 89, in __init__
    config=tf_utils.generate_session_config(), graph=self.graph
  File ""d:\ml-agents\mlagents-env\lib\site-packages\tensorflow\python\client\session.py"", line 1586, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""d:\ml-agents\mlagents-env\lib\site-packages\tensorflow\python\client\session.py"", line 701, in __init__
    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\ML-Agents\mlagents-env\Scripts\mlagents-learn-script.py"", line 33, in <module>
    sys.exit(load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')())
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\learn.py"", line 284, in main
    run_cli(parse_command_line())
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\learn.py"", line 280, in run_cli
    run_training(run_seed, options)
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\learn.py"", line 159, in run_training
    tc.start_learning(env_manager)
  File ""d:\ml-agents\ml-agents\ml-agents-envs\mlagents_envs\timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\trainer_controller.py"", line 204, in start_learning
    self._save_models()
  File ""d:\ml-agents\ml-agents\ml-agents-envs\mlagents_envs\timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\trainer_controller.py"", line 75, in _save_models
    self.trainers[brain_name].save_model()
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\trainer\rl_trainer.py"", line 133, in save_model
    policy = list(self.policies.values())[0]
IndexError: list index out of range
```",master pull however handling another error force successfully dynamic library warning removed future version long term version information dev dev communicator successfully dynamic library warning removed future version long term listening port start training pressing play button unity editor connected unity environment package version communication version connected new brain binary use service platform host guarantee used device host default version successfully dynamic library found device name successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library visible warning could write summary true none none true none none behavior name beta epsilon linear normalize true simple memory none extrinsic gamma strength none threaded true none none found device name successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library visible recent call last file line file line wrapped return file line file line file line file line file line create graph file line file line super session self target graph file line status driver version insufficient version handling exception another exception recent call last file line module file line main file line file line file line wrapped return file line file line wrapped return file line file line policy list list index range,issue,positive,positive,neutral,neutral,positive,positive
663295545,"Ok, this was to be expected. Can you checkout master and pull? I made changes on master that should fix the issue and will print some more details logs about why this is happening. Make sure to run `pip3 install -e .` as the `-e` will allow changes made on the repo to be reflected in your packages",master pull made master fix issue print happening make sure run pip install allow made reflected,issue,negative,positive,positive,positive,positive,positive
663282360,"I have tried to install `mlagents` and `mlagents-env` by cloning the repo. The `pip3 freeze` message including the commit sha is shown below:
```
(mlagents-env) D:\ML-Agents\ml-agents\ml-agents>pip3 freeze
WARNING: Could not generate requirement for distribution -ip 19.2.3 (d:\ml-agents\mlagents-env\lib\site-packages): Parse error at ""'-ip==19.'"": Expected W:(abcd...)
absl-py==0.9.0
astunparse==1.6.3
attrs==19.3.0
cachetools==4.1.1
cattrs==1.0.0
certifi==2020.6.20
chardet==3.0.4
cloudpickle==1.5.0
gast==0.3.3
google-auth==1.19.2
google-auth-oauthlib==0.4.1
google-pasta==0.2.0
grpcio==1.30.0
h5py==2.10.0
idna==2.10
importlib-metadata==1.7.0
Keras-Preprocessing==1.1.2
Markdown==3.2.2
-e git+https://github.com/Unity-Technologies/ml-agents.git@8327ddcb2a65ccb0a76ce6390811212d1daebb6e#egg=mlagents&subdirectory=ml-agents
-e git+https://github.com/Unity-Technologies/ml-agents.git@8327ddcb2a65ccb0a76ce6390811212d1daebb6e#egg=mlagents_envs&subdirectory=ml-agents-envs
numpy==1.19.1
oauthlib==3.1.0
opt-einsum==3.3.0
pi==0.1.2
Pillow==7.2.0
protobuf==3.12.2
pyasn1==0.4.8
pyasn1-modules==0.2.8
pypiwin32==223
pywin32==228
PyYAML==5.3.1
requests==2.24.0
requests-oauthlib==1.3.0
rsa==4.6
scipy==1.4.1
six==1.15.0
tensorboard==2.2.2
tensorboard-plugin-wit==1.7.0
tensorflow==2.2.0
tensorflow-estimator==2.2.0
termcolor==1.1.0
urllib3==1.25.9
Werkzeug==1.0.1
wrapt==1.12.1
zipp==3.1.0
```

And the error still exists, the message is:
```
(mlagents-env) D:\ML-Agents\ml-agents\config>mlagents-learn ppo\3DBall.yaml --run-id=3DBall_testrun
2020-07-23 16:30:43.316650: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
WARNING:tensorflow:From d:\ml-agents\mlagents-env\lib\site-packages\tensorflow\python\compat\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term


                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓


 Version information:
  ml-agents: 0.18.0,
  ml-agents-envs: 0.18.0,
  Communicator API: 1.0.0,
  TensorFlow: 2.2.0
2020-07-23 16:30:45.848914: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
WARNING:tensorflow:From d:\ml-agents\mlagents-env\lib\site-packages\tensorflow\python\compat\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2020-07-23 16:30:47 INFO [environment.py:199] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
2020-07-23 16:31:09 INFO [environment.py:108] Connected to Unity environment with package version 1.0.3 and communication version 1.0.0
2020-07-23 16:31:10 INFO [environment.py:265] Connected new brain:
3DBall?team=0
2020-07-23 16:31:10.033423: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-07-23 16:31:10.046351: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2c74563a0a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-23 16:31:10.054787: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-23 16:31:10.059908: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-07-23 16:31:10.228271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1
coreClock: 1.683GHz coreCount: 15 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s
2020-07-23 16:31:10.237983: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-07-23 16:31:10.247545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-07-23 16:31:10.256215: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-07-23 16:31:10.261961: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-07-23 16:31:10.271281: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-07-23 16:31:10.278897: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-07-23 16:31:10.290463: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-07-23 16:31:10.301647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-07-23 16:31:10 WARNING [stats.py:235] Could not write text summary for Tensorboard.
2020-07-23 16:31:10 INFO [trainer_controller.py:76] Saved Model
Traceback (most recent call last):
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\trainer_controller.py"", line 130, in _create_trainer_and_manager
    trainer = self.trainers[brain_name]
KeyError: '3DBall'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\ML-Agents\mlagents-env\Scripts\mlagents-learn-script.py"", line 33, in <module>
    sys.exit(load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')())
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\learn.py"", line 283, in main
    run_cli(parse_command_line())
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\learn.py"", line 279, in run_cli
    run_training(run_seed, options)
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\learn.py"", line 158, in run_training
    tc.start_learning(env_manager)
  File ""d:\ml-agents\ml-agents\ml-agents-envs\mlagents_envs\timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\trainer_controller.py"", line 181, in start_learning
    self._create_trainers_and_managers(env_manager, new_behavior_ids)
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\trainer_controller.py"", line 168, in _create_trainers_and_managers
    self._create_trainer_and_manager(env_manager, behavior_id)
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\trainer_controller.py"", line 132, in _create_trainer_and_manager
    trainer = self.trainer_factory.generate(brain_name)
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\trainer_util.py"", line 52, in generate
    self.multi_gpu,
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\trainer_util.py"", line 101, in initialize_trainer
    trainer_artifact_path,
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\ppo\trainer.py"", line 48, in __init__
    brain_name, trainer_settings, training, artifact_path, reward_buff_cap
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\trainer\rl_trainer.py"", line 38, in __init__
    StatsPropertyType.HYPERPARAMETERS, self.trainer_settings.as_dict()
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\stats.py"", line 321, in add_property
    writer.add_property(self.category, property_type, value)
  File ""d:\ml-agents\ml-agents\ml-agents\mlagents\trainers\stats.py"", line 216, in add_property
    self.summary_writers[category].add_summary(text, 0)
  File ""d:\ml-agents\mlagents-env\lib\site-packages\tensorflow\python\summary\writer\writer.py"", line 127, in add_summary
    for value in summary.value:
AttributeError: 'str' object has no attribute 'value'
```



",tried install pip freeze message commit sha shown pip freeze warning could generate requirement distribution parse error error still message successfully dynamic library warning removed future version long term version information communicator successfully dynamic library warning removed future version long term listening port start training pressing play button unity editor connected unity environment package version communication version connected new brain binary use service platform host guarantee used device host default version successfully dynamic library found device name successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library visible warning could write text summary saved model recent call last file line trainer handling exception another exception recent call last file line module file line main file line file line file line wrapped return file line file line file line trainer file line generate file line file line training file line file line value file line category text file line value object attribute,issue,positive,positive,neutral,neutral,positive,positive
663247955,"Okay, I got the same pip configuration as you but It runs for me. 
On the bright side, I think I got something. The fact that you got the message
```
2020-07-21 18:20:54 WARNING [stats.py:235] Could not write text summary for Tensorboard.
```
means that the try/catch [here](https://github.com/Unity-Technologies/ml-agents/blob/release_4/ml-agents/mlagents/trainers/stats.py#L224) failed and the string that was returned was a `""""`.
I have no idea why you have an error there and I don't **BUT** any error in this try / catch would give the error you are seeing. 
I will try to make some fixes but without knowing what the original error is, I am not sure I can do much.
If you could try to do the git installation by cloning the repo and reproduce the error, I can guide you to make some debug statements that will help.",got pip configuration bright side think got something fact got message warning could write text summary string returned idea error error try catch would give error seeing try make without knowing original error sure much could try git installation reproduce error guide make help,issue,negative,positive,positive,positive,positive,positive
663237218,"Here is what I get from `pip3 freeze`:
```
(mlagents-env) D:\ML-Agents\mlagents-env\Scripts>pip3 freeze
WARNING: Could not generate requirement for distribution -ip 19.2.3 (d:\ml-agents\mlagents-env\lib\site-packages): Parse error at ""'-ip==19.'"": Expected W:(abcd...)
absl-py==0.9.0
astunparse==1.6.3
attrs==19.3.0
cachetools==4.1.1
cattrs==1.0.0
certifi==2020.6.20
chardet==3.0.4
cloudpickle==1.5.0
gast==0.3.3
google-auth==1.19.2
google-auth-oauthlib==0.4.1
google-pasta==0.2.0
grpcio==1.30.0
h5py==2.10.0
idna==2.10
importlib-metadata==1.7.0
Keras-Preprocessing==1.1.2
Markdown==3.2.2
mlagents==0.18.0
mlagents-envs==0.18.0
numpy==1.19.1
oauthlib==3.1.0
opt-einsum==3.3.0
pi==0.1.2
Pillow==7.2.0
protobuf==3.12.2
pyasn1==0.4.8
pyasn1-modules==0.2.8
pypiwin32==223
pywin32==228
PyYAML==5.3.1
requests==2.24.0
requests-oauthlib==1.3.0
rsa==4.6
scipy==1.4.1
six==1.15.0
tensorboard==2.2.2
tensorboard-plugin-wit==1.7.0
tensorflow==2.2.0
tensorflow-estimator==2.2.0
termcolor==1.1.0
urllib3==1.25.9
Werkzeug==1.0.1
wrapt==1.12.1
zipp==3.1.0
```",get pip freeze pip freeze warning could generate requirement distribution parse error,issue,negative,neutral,neutral,neutral,neutral,neutral
663201160,@Ro2Be Please post on the [Unity ML-Agents forum](https://forum.unity.com/forums/ml-agents.453/). You're more likely to get a response there.,robe please post unity forum likely get response,issue,negative,neutral,neutral,neutral,neutral,neutral
663179626,"I tried to do a fresh install with Python 3.7.6, pip 20.1.1 and installed `pip3 mlagents==0.18.0`. I was able to train 3DBall without problem. Are you sure you used a clean new virtual environment ?
can you post the result of the command `pip3 freeze` into this issue?",tried fresh install python pip pip able train without problem sure used clean new virtual environment post result command pip freeze issue,issue,positive,positive,positive,positive,positive,positive
663158531,"@letec4 Hi, I'm a game dev student currently working on a similar project with little results. Did you make any progress/do you have any tips for me? You can reach me on d-robbe@hotmail.com.",hi game dev student currently working similar project little make reach,issue,negative,negative,negative,negative,negative,negative
663091404,"Hi @niskander, thanks for catching this!

It looks like there are few other places where we have a tensorboard commandline with `=` in it:
```shell
$ git grep -i tensorboard.*logdir
docs/Getting-Started.md:tensorboard --logdir=results
docs/Training-on-Microsoft-Azure.md:   `tensorboard --logdir=summaries --host 0.0.0.0`
docs/Using-Docker.md:docker exec -it <container-name> tensorboard --logdir=/unity-volume/summaries --host=0.0.0.0
docs/Using-Docker.md:docker exec -it 3DBallContainer.first.trial tensorboard --logdir=/unity-volume/summaries --host=0.0.0.0
docs/Using-Tensorboard.md:1. From the command line run: `tensorboard --logdir=results --port=6006`
```
(and some are still using the old `summaries` directory too). Would you mind updating those too? Or I can make the changes on your branch if you'd prefer.",hi thanks catching like shell git host docker docker command line run still old directory would mind make branch prefer,issue,positive,positive,positive,positive,positive,positive
662809158,The python version I am using for my virtual environment is 3.7.6. I installed `mlagents 0.18.0` and `mlagents-envs 0.18.0` using `pip install mlagents`. The pip version is 20.1.1. I didn't clone the git repo and install from there. I followed every step as instructed but it just couldn't get me work. I have reinstalled for a few times and the same thing happens. Do you think this can be a problem with python version?,python version virtual environment pip install pip version clone git install every step instructed could get work time thing think problem python version,issue,negative,neutral,neutral,neutral,neutral,neutral
662773130,"I've been trying to convert everything to use `Policy` from PPO/SAC Trainer and up (https://github.com/Unity-Technologies/ml-agents/tree/develop-add-fire-models), left some comments w.r.t the Policy class that I needed to do to get it to pass pylint. ",trying convert everything use policy trainer left policy class get pas,issue,negative,neutral,neutral,neutral,neutral,neutral
662698596,"Timing improvements weren't as good as I'd hoped (and as good as cprofile indicated they would be):
```
3DBall, --num-envs=4
master
2020-07-22 13:27:45 INFO [stats.py:112] 3DBall: Step: 492000. Time Elapsed: 193.277 s Mean Reward: 100.000. Std of Reward: 0.000. Training.

branch
2020-07-22 13:23:48 INFO [stats.py:112] 3DBall: Step: 492000. Time Elapsed: 189.707 s Mean Reward: 100.000. Std of Reward: 0.000. Training.
```
```
FoodCollector, --num-envs=4
master:
2020-07-22 13:48:50 INFO [stats.py:112] FoodCollector: Step: 1980000. Time Elapsed: 750.874 s Mean Reward: 28.950. Std of Reward: 6.407. Training.

branch:
2020-07-22 14:02:50 INFO [stats.py:112] FoodCollector: Step: 1980000. Time Elapsed: 706.588 s Mean Reward: 43.700. Std of Reward: 8.277. Training.
```",timing good hoped good would master step time mean reward reward training branch step time mean reward reward training master step time mean reward reward training branch step time mean reward reward training,issue,positive,positive,neutral,neutral,positive,positive
662624564,"@ThrusterJon  Hi, I'm a game dev student working an a mlagents implementation of connect four. I was able to set everything up and do some training but the results are really bad, the AI doesn't seem to get the game at all. I tweaked some of the hyperparameters but still no good results. Now im training an 4x4 connect three but even that one is dumb as fuck. I was wondering if you had any good results on your project? And if you have some tips for me? I feel like I should use a CNN instead of the networks provided by ml agents but I havent found a way to implement that, not sure if you succeded in doing that. You can always contact me on robbe.demey@student.howest.be.",hi game dev student working implementation connect four able set everything training really bad ai seem get game still good training connect three even one dumb wondering good project feel like use instead provided havent found way implement sure always contact,issue,negative,positive,neutral,neutral,positive,positive
662620655,"> Hi all,
> 
> In the most recent versions of ML-Agents, we have made significant changes to simplify the order of operations with respect to agent resets, steps, and observations. Please let us know if these changes have resolved the difficulties you may have run into.

I'm experiencing similar issues with the latest version of MLAgents (07/2020). The CollectDiscreteActionMask function is regularly skipped (I do a check in OnActionRecieved and just exit the function and do a new RequestDescision as a quick fix) and sometimes CollectDiscreteActionMask is even called after EndEpisode triggering an error because the mask is masking out all possible moves.",hi recent made significant simplify order respect agent please let u know resolved may run similar latest version function regularly check exit function new quick fix sometimes even error mask possible,issue,positive,positive,positive,positive,positive,positive
662602967,"Hi @AMGMNPLK 

I just loaded a fresh version of the project with the same Unity version and did not get any errors. Had you perhaps upgraded from a previous version of Unity or ML-Agents?",hi loaded fresh version project unity version get perhaps previous version unity,issue,negative,positive,neutral,neutral,positive,positive
662575831,"Ha, I thought you wanted to not do RL and only supervised.
In our trainers, we use supervised learning in the [behavioral cloning module](https://github.com/Unity-Technologies/ml-agents/blob/release_4/docs/Training-Configuration-File.md#behavioral-cloning) and GANs in the [GAIL reward module](https://github.com/Unity-Technologies/ml-agents/blob/release_4/docs/Training-Configuration-File.md#gail-intrinsic-reward). 
If you think of other generic RL components that can help learning, let us know. We currently do not have a solution for animations as we are focused on generating behaviors.",ha thought use learning behavioral module reward module think generic help learning let u know currently solution generating,issue,positive,neutral,neutral,neutral,neutral,neutral
662572325,"I think it would help us if you told us what version of Python you are using in your virtual environment and the sha of the git commit you are on. The demo environments all work on my machine, so the error is probably an installation issue.
 ",think would help u told u version python virtual environment sha git commit work machine error probably installation issue,issue,negative,neutral,neutral,neutral,neutral,neutral
662539598,"> Hi @NonoLG, you can reset your agent separately by calling ""Agent.Done()"", or you can reset your whole scene for all of your agents by calling ""Academy.Done()"". If you set a max step in the Academy. we will call the Academy.Done() for you.

Academy.Instance.Done() seems to no longer exist in the current release. How should one go about ending an episode if many agents exist? How to set maxSteps globally for all agents?",hi reset agent separately calling reset whole scene calling set step academy call longer exist current release one go ending episode many exist set globally,issue,negative,positive,positive,positive,positive,positive
662345423,"hey, vincentpierre.
I'll always be waiting for this.
Thank you for your efforts.",hey always waiting thank,issue,negative,neutral,neutral,neutral,neutral,neutral
662235128,"Thanks for the reply.

Well, the use cases can be very generic. For example, RL systems (such as [this one](https://www.youtube.com/watch?v=Rzj3k3yerDk&feature=youtu.be)) can have several supervised or unsupervised components for helping the decision making process. I'm particuarly interested in using generative models in Unity. For that, one needs a library that allows one to design the neural network architecture and choose the loss function themselves.",thanks reply well use generic example one several unsupervised helping decision making process interested generative unity one need library one design neural network architecture choose loss function,issue,positive,positive,positive,positive,positive,positive
662214767,"The ""Gesture"" behavior is my own training behavior. I thought initially the error comes from my own environment, but later when I tested it on the example environment, the same thing happened. I have updated the error message generated by the 3DBall behavior. I will try to reinstall everything from scratch and see what happens.

",gesture behavior training behavior thought initially error come environment later tested example environment thing error message behavior try reinstall everything scratch see,issue,negative,neutral,neutral,neutral,neutral,neutral
662128135,"Looks like this issue has been idle for a while. You're right that the usage of `UnityEnvironment.DEFAULT_EDITOR_PORT` was added in [0.14.0](https://github.com/Unity-Technologies/ml-agents/blob/0.14.0/gym-unity/gym_unity/envs/__init__.py#L62), but that appears to be before it was added in [0.15.0](https://github.com/Unity-Technologies/ml-agents/blob/0.15.0/ml-agents-envs/mlagents_envs/environment.py#L66). I'm not exactly sure how that happened (maybe a bad merge or cherry-pick) but everything should be OK now, so I'm going to close this issue.",like issue idle right usage added added exactly sure maybe bad merge everything going close issue,issue,negative,positive,neutral,neutral,positive,positive
661991274,"I am unable to reproduce the bug. The steps to reproduce seem to be about running 3DBall in a new virtual environment with the provided config (which works for me). The error you are getting (I am guessing) is due the summary writers being unable to deserialize the hyperparameters. I have never seen this error, and I need more information. From the error, it seems there is a ""Gesture"" behavior in the Unity scene, but it is not mentioned in the steps to reproduce.
The fact that you had it work before and now it does not, indicates that you might have something gone wrong when installing. I would recommend installing from scratch again or make sure no files have been modified (`git status`)
Can you provide more details to help me reproduce the error?",unable reproduce bug reproduce seem running new virtual environment provided work error getting guessing due summary unable never seen error need information error gesture behavior unity scene reproduce fact work might something gone wrong would recommend scratch make sure git status provide help reproduce error,issue,negative,negative,negative,negative,negative,negative
661985804,"Hi,
You are correct, ML-Agents is built around reinforcement learning. There are other tools we are working on for supervised learning like the [Unity Perception SDK](https://blogs.unity3d.com/2020/06/10/use-unitys-perception-tools-to-generate-and-analyze-synthetic-data-at-scale-to-train-your-ml-models/). If you want to use ML-Agents and do supervised learning, you can collect data with a reinforcement learning loop and then save / reuse the data to train a supervised learning model.
I will mention your interest for supervised and unsupervised learning to the team, do you have specific use cases in mind ?",hi correct built around reinforcement learning working learning like unity perception want use learning collect data reinforcement learning loop save reuse data train learning model mention interest unsupervised learning team specific use mind,issue,positive,neutral,neutral,neutral,neutral,neutral
661982479,"Hi,
Distributed training is on our roadmap and we are currently working on it. It was announced in this [blog post](https://blogs.unity3d.com/2020/05/12/announcing-ml-agents-unity-package-v1-0/) in the section **ML-Agents Cloud**. We do not yet have a release date...",hi distributed training currently working post section cloud yet release date,issue,negative,neutral,neutral,neutral,neutral,neutral
661289844,"Yeah, this would make the errors a bit less confusing.",yeah would make bit le,issue,negative,neutral,neutral,neutral,neutral,neutral
661268017,I was about to make a PR for this. It seems #4249  and #4250 are related to this change (The error is confusing due to the try/catch) 👍 ,make related change error due,issue,negative,negative,neutral,neutral,negative,negative
661248106,"I tried with visual hallway, the new collider and the scripts you provided. 
I cannot reproduce your issue. 
 ",tried visual hallway new provided reproduce issue,issue,negative,positive,neutral,neutral,positive,positive
661239473,"Hi, 
It would help greatly if I had a way to reproduce this error. Please fill out the `To Reproduce` section of the template.
The error seems to come from the tensorboard summary writer trying to write something not allowed when writing the hyperparameters. Can you provide the config yaml you used?",hi would help greatly way reproduce error please fill reproduce section template error come summary writer trying write something writing provide used,issue,negative,positive,positive,positive,positive,positive
661222465,"Hi @SamirAlSkaf 
What version of ML-Agents are you on? I suspect you are on `release_1`, can you give me your commit number to be sure. Also, did you modify any of the Python code before running it? 
The reason I ask is because according to the log, the behavior `Thesis?team=0` connected but the trainer that was created was for `PPO_Default_Thesis`. These two names should always be the same but they are not (and so the trainer is confused and thinks `Thesis` is a missing key).",hi version suspect give commit number sure also modify python code running reason ask according log behavior thesis connected trainer two always trainer confused thesis missing key,issue,negative,negative,neutral,neutral,negative,negative
660907638,"> Not sure about that warning. I think adding
> `self.summary_writer.add_graph(self.policy.graph)`
> in Trainer.save_model() might be a better place instead; that gets called if you stop training early.

I Tried doing this, however I get an error saying that PPOTrainer does not have an attribute ""summary_writer"". I also tried with ""summary_writers"" but still no luck...anyone else knows how to do this in MlAgents 1.0 Release 3? Thanks!",sure warning think might better place instead stop training early tried however get error saying attribute also tried still luck anyone else release thanks,issue,positive,positive,positive,positive,positive,positive
660848753,Is self.summary_writer.add_graph(self.policy.graph) still not present in the latest release? ,still present latest release,issue,negative,positive,positive,positive,positive,positive
660533994,ok I did many tests. The essential part is having also a camera sensor attached to the agent. Removing that I don't see the same problem. Let's say you need a the VisualHallway with those colliders.,many essential part also camera sensor attached agent removing see problem let say need,issue,negative,positive,positive,positive,positive,positive
660508280,"If it's not zero-sum, the ELO calculation will be a bit off but thats not necessarily a reflection of a broken training process.  If the agent wins/loses, it should get a reward of +1/-1 respectively. If an agent winning means another agent loses and vice versa then reward accordingly. If an agent wins and another agent doesnt technically lose then its ok to administer a +1/0, just know that ELO might become a misleading statistic.",calculation bit thats necessarily reflection broken training process agent get reward respectively agent winning another agent vice reward accordingly agent another agent doesnt technically lose administer know might become misleading statistic,issue,positive,positive,neutral,neutral,positive,positive
660469597,"Okay, thank you. You might want to add an annotation to your documentation though. Since those examples just don't work if you install ml-agents in the recommended way. 

So do you have any suggestions on a library that does work with TF2.0? I don't intend to downgrade, as I think this newer version is leagues above the one we had before. I was thinking about trying TF-Agents, but maybe you have some other suggestion.
",thank might want add annotation documentation though since work install way library work intend downgrade think version one thinking trying maybe suggestion,issue,negative,neutral,neutral,neutral,neutral,neutral
660449691,I'm trying to remove things and make it still works/happens. Probably some additional component attached to the agent are correlated with this issue (like a camera sensor). ,trying remove make still probably additional component attached agent correlated issue like camera sensor,issue,negative,neutral,neutral,neutral,neutral,neutral
660304916,"I increased the size of the colliders yes. I cannot reproduce your issue with Hallway. I am not sure how I can help, can you provide a minimal Unity project for which the issue happens?",size yes reproduce issue hallway sure help provide minimal unity project issue,issue,positive,positive,positive,positive,positive,positive
660303018,"Did you also increase the size of the colliders? I tried removing agents, and it does not happen. Even with one is fine. There is a not clear threshold value after which it happens, that is proportional to the size of the collider, the multiplicity of the agents, and probably if they concentrate in the same area (so basically when collisions or triggers happens frequently). If you have many agents with moderately large colliders, it occurs to me very often. Don't know really why it happens.",also increase size tried removing happen even one fine clear threshold value proportional size multiplicity probably concentrate area basically frequently many moderately large often know really,issue,positive,positive,positive,positive,positive,positive
660254085,"I attached the enclosed script to eye, mouth, HeadBand and AgentCube_Blue for each agent. Unity still runs fine for me. 
I think this might be a Unity physics issue since the Agents do not reference nor use the colliders.
Can you try to reproduce without Agent scripts in the scene?",attached script eye mouth headband agent unity still fine think might unity physic issue since reference use try reproduce without agent scene,issue,negative,positive,positive,positive,positive,positive
660233643,"The issue happens also **without** training, and occurs when there are **multiple** agents (with one agent I did not experience that issue). I made other tests. You can simply increase the size of the collider you have already defined with dimensions compatible with the ones I described before, and add the following script to those elements with the collider. (I use Trigger, but I have the same issue also with Collide function) 

```
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class MyScript: MonoBehaviour
{
Transform agent; // in the inspector assign the agent object here - the father -
void OnTriggerExit(Collider col)
    {
     // put a reward  to the agent
    agent.GetComponent<HallwayAgent>().AddReward(1.0f);
    }

 void OnTriggerStay(Collider col)
    {
     // put a reward  to the agent
        agent.GetComponent<HallwayAgent>().AddReward(-0.01f);
    }

    void OnTriggerEnter(Collider col)
    {
     // put a reward  to the agent 
       agent.GetComponent<HallwayAgent>().AddReward(-1.0f);
    }
}
```",issue also without training multiple one agent experience issue made simply increase size already defined compatible add following script use trigger issue also collide function public class transform agent inspector assign agent object father void col put reward agent void col put reward agent void col put reward agent,issue,positive,neutral,neutral,neutral,neutral,neutral
660225060,Forgot to update the trunk command. Will push that when the 2020.1 Windows tests have passed.,forgot update trunk command push,issue,negative,neutral,neutral,neutral,neutral,neutral
660224742,"Hi, this looks like a rather complicated setup. Can you create a minimal scene to reproduce and post it in this issue?
Does the issue happen without training? (If you press play in the editor without the training process started). Does the problem still occur if you deactivate the Agent? 
It seems rather strange to me that ML-Agents would misbehave depending on the number/shape of colliders.",hi like rather complicated setup create minimal scene reproduce post issue issue happen without training press play editor without training process problem still occur deactivate agent rather strange would misbehave depending,issue,negative,negative,negative,negative,negative,negative
660223459,Running one of the platform combos that failed last night on https://yamato.prd.cds.internal.unity3d.com/jobs/497-ml-agents/tree/update-yamato-py3/.yamato%252Fcom.unity.ml-agents-test.yml%2523test_com.unity.ml-agents_win_2020.1/2824818/job,running one platform last night,issue,negative,neutral,neutral,neutral,neutral,neutral
660089841,"Hi @vincentpierre ,
my colliders configuration probably is prone to show this issue. For example, take the box agent and create 4 nested colliders of different sizes (one that adheres to the cubic mesh, the second with size 2, the this with size 4 and the last with size 8). 
You need to put a script that does Collision (or Trigger) Enter/Exit (and possible Stay) associated to those children. 

It is hard to demonstrate but I'm doing some tests and if I reduce the size of the colliders or the number of agents, this issue occurs more rarely. 

By the way, in the test I'm doing, I noticed that Unity is still doing something even if does not respond (cause CPU is oscillating around 10%). I also increase environment timeout (in fact after 1 minute it closes) to 1 hour, but I saw that was not a matter of time needed. It looks stuck in some infinite loop, as you said waiting for something.
",hi configuration probably prone show issue example take box agent create different size one cubic mesh second size size last size need put script collision trigger possible stay associated hard demonstrate reduce size number issue rarely way test unity still something even respond cause oscillating around also increase environment fact minute hour saw matter time stuck infinite loop said waiting something,issue,negative,positive,neutral,neutral,positive,positive
660035873,"I have the same issue and I can't get to solve it
",issue ca get solve,issue,negative,neutral,neutral,neutral,neutral,neutral
660032550,"Hi, Im also getting the same issue in the last version of MlAgents, i not able to train, when I start trainning the 5004 port issue appears in console and seconds later the cmd in which I run the command gives a timeout error, I've several solutions and multiple projects but it seems to appear everytime I try to train",hi also getting issue last version able train start port issue console later run command error several multiple appear try train,issue,negative,positive,neutral,neutral,positive,positive
659720890,"That's fixed in master, and will be in the next release (sometime in August).",fixed master next release sometime august,issue,negative,positive,neutral,neutral,positive,positive
659707457,"Experiment results, picking the ""middle"" run for each variant (judged subjectively via eyeball):
![image](https://user-images.githubusercontent.com/6877802/87728531-188b3280-c778-11ea-936b-15cd80edbe97.png)
So they're more or less equivalent (within noise).

I still have hope that running this on a more complex system like Walker or Crawler will have some benefits.",experiment middle run variant subjectively via eyeball image le equivalent within noise still hope running complex system like walker crawler,issue,positive,negative,negative,negative,negative,negative
659699795,"Hi @dgiunchi,
I tried to reproduce the issue you described. I did the following to try to reproduce : 

 - I opened the hallway environment
 - I modified all the `SymbolFinderArea` by duplicating the `Agent` GameObject 10 times
 - For each `Agent`, I added a box collider on the `eye`, `mouth`, `HeadBand` and `AgentCube_Blue` children GameObjects

I then launched training. It seems to be working so far, some Agents are falling off the platform because of collisions with other Agents but I see no error.
Did you modify anything else in the Hallway environment?
I do not think collisions are at fault, from the stack you posted, it looks like gprc is waiting for something to happen (and it does not)
",hi tried reproduce issue following try reproduce hallway environment agent time agent added box eye mouth headband training working far falling platform see error modify anything else hallway environment think fault stack posted like waiting something happen,issue,negative,positive,neutral,neutral,positive,positive
659609936,Awesome! Thanks for letting us know @harperj . This will be quite useful.,awesome thanks u know quite useful,issue,positive,positive,positive,positive,positive,positive
659598676,.nn files are now saved at the same time as checkpoints (see https://github.com/Unity-Technologies/ml-agents/pull/4127). This will be in the next release (sometime in August).,saved time see next release sometime august,issue,negative,neutral,neutral,neutral,neutral,neutral
659585415,Just wanted to give an update -- we recently added an export to `.nn` for each checkpoint. It was merged in #4127 and should land in the next release.,give update recently added export land next release,issue,negative,neutral,neutral,neutral,neutral,neutral
659257360,"Hello @vincentpierre ,

In my opinion, wrapping UnityEnvironment class is especially useful to use Unity environments in a RL framework (such as rllib), with prebuilt algorithms (ML-Agents lacks multiagent algorithms such as MADDPG).

The API is not hard to use but I find the distinction between DecisionSteps and TerminateStep leads to a bit too much verbosity.
What is more, as I do not use brains, I prefer wrapping the UnityEnvironment class when I implement my algorithms. Thus, this makes the code simpler and my algorithms can work on other environments (those which implement to the rllib multiagent interface).

But it's only my own opinion, keep up the great work!",hello opinion wrapping class especially useful use unity framework hard use find distinction bit much verbosity use brain prefer wrapping class implement thus code simpler work implement interface opinion keep great work,issue,positive,positive,positive,positive,positive,positive
659163187,"@vra For that, you need to run `pip install tf2onnx` and after installing that, whenever you complete training the Unity ML Agents, the `.onnx` file will be saved along with `.nn`. 

Also, i am having a bit issue currently, that i want to test trained models in unity into real world hardware, the main issue i am having is that how can i input the observations in `frozen_graph_def.pb` or any other output file and getting the actions back, is this possible ? You can get more info about this [here](https://github.com/Unity-Technologies/ml-agents/issues/4215) . Thanks a lot 🙂   ",need run pip install whenever complete training unity file saved along also bit issue currently want test trained unity real world hardware main issue input output file getting back possible get thanks lot,issue,positive,positive,neutral,neutral,positive,positive
659160884,"> > Same issue
> 
> But solved 🙂

Hi @Shubhamai , Could you please tell me how you solve this problem? Thanks in advance",issue hi could please tell solve problem thanks advance,issue,positive,positive,positive,positive,positive,positive
658974796,"Similar to the `additional_args` argument that UnityEnvironment() takes, you should be able to pass an `additional_args` argument to the .make() method. For example
```python
default_registry['Basic'].make(additional_args=['-logFile', '-'])
```",similar argument able pas argument method example python,issue,negative,positive,positive,positive,positive,positive
658936598,"Great to hear about the capability documentation, I look forward to it!

When you say proprietary property - any idea of how much of that is ""custom algorithms"" vs using the giving PPO or SAC algorithms but with custom rewards, curriculum, hyperparameters? It would be helpful to know what the bottleneck is. It has been my understanding that PPO & SAC are still relatively state of the art and can achieve a lot, it just requires tens of millions of steps. To me the bottleneck is compute power, and being able to use GPU(s) so 10 million steps or more can be run in a day. Currently it takes me nearly 5 days to get to 5 million steps, so that's been my current blocker - no real GPU support and too slow to train, not the algorithms or anything else. Is / can that being addressed soon, or are you saying even if we could run 50 million steps easily (within days), they still wouldn't train?",great hear capability documentation look forward say proprietary property idea much custom giving sac custom curriculum would helpful know bottleneck understanding sac still relatively state art achieve lot million bottleneck compute power able use million run day currently nearly day get million current blocker real support slow train anything else soon saying even could run million easily within day still would train,issue,positive,positive,positive,positive,positive,positive
658933937,"Game developers have used ML-Agents to train agents with behaviors more complicated than those in the example environments we provide. The issue is that in most cases these games and the solutions used are proprietary property of the game developers, and not shared publicly. 

We are actually working right now to put together documentation designed to help users of ML-Agents understand the current capabilities, and get a sense of whether a given game or environment will be trainable using the current state of the toolkit. We hope to share this in the coming months externally. ",game used train complicated example provide issue used proprietary property game publicly actually working right put together documentation designed help understand current get sense whether given game environment trainable current state hope share coming externally,issue,positive,negative,negative,negative,negative,negative
658929589,"@awjuliani 
Thank you for your reply. I interpret ML-Agents as a deep reinforcement learning framework that is connected to Unity. Yes, Unity is capable of building such complex environments and even more complex; it is a game engine platform after all capable of simulations as well. However, is ML-Agents capable of solving such complex problems in general, taking out the tailored algorithms for now, giving that the hardware for computing is available? ",thank reply interpret deep reinforcement learning framework connected unity yes unity capable building complex even complex game engine platform capable well however capable complex general taking giving hardware available,issue,positive,negative,neutral,neutral,negative,negative
658926363,"Hi, 
gym_unity and mlagents_envs do not have dependencies on TensorFlow. Only mlagents (the trainers) have a dependency on TensorFlow. If you want to run Baselines or Dopamine, you do not need to mlagents package and I recommend making a new virtual environment without mlagents (and only gym_unity) to help you manage the dependencies. 
I think if you want to use Baselines or Dopamine, you should use the versions of TensorFlow these frameworks need (meaning downgrading). ",hi dependency want run need package recommend making new virtual environment without help manage think want use use need meaning,issue,negative,positive,positive,positive,positive,positive
658924709,"Thanks for the reply @awjuliani I don't expect MLAgents to be able to reach state of the art results such as Dota2 or Hide & Seek, for all the reasons you noted (custom tailored algorithms, expensive compute). However, it would be nice to understand what the current ""limit"" is for MLAgents, so people know ahead of time what's possible and what is currently out of reach. E.g. assuming we DID have budget for compute, what is the best that can be achieved?

Specifically, what is the most complex training that has been completed with MLAgents thus far (is there anything beyond the demo environments or are those currently the ""max"" limit we should expect)? And how long it takes to train (number of steps, hours to train, what hardware was used)? Just having that expectation would be hugely useful. And then in the future obviously the closer MLAgents can move toward reaching more complex generalization and models, the better.

Thanks!",thanks reply expect able reach state art hide seek noted custom expensive compute however would nice understand current limit people know ahead time possible currently reach assuming budget compute best specifically complex training thus far anything beyond currently limit expect long train number train hardware used expectation would hugely useful future obviously closer move toward reaching complex generalization better thanks,issue,positive,positive,positive,positive,positive,positive
658922655,"Hi @AdhamAlHarazi 

This question depends on how you interpret what ML-Agents is. From an environment creation aspect, it is possible to create environments with all of the properties of Dota2, Go, or the Hide and Seek game. As for training such an environment, in all the cases you listed the researchers created very specific custom tailored algorithms for their problems. They also put hundreds of thousands of dollars of compute into solving their respective problems. 

We don't think that the OpenAI/DeepMind approach is a useable solution for game developers or independent researchers using ML-Agents. As such, we are focused on providing robust algorithms that work on a broad set of environments, and plan to continue to improve them to work better for more complex tasks. ",hi question interpret environment creation aspect possible create go hide seek game training environment listed specific custom also put compute respective think approach solution game independent providing robust work broad set plan continue improve work better complex,issue,positive,negative,neutral,neutral,negative,negative
658914903,"Hi,
Thank you for the reply. @Fabien-Couthouis ! I worked on the current UnityEnvironment class and I am very interested in knowing what are the limitations of the current API. It seems that wrapping the UnityEnvironment is very common and I would like to know the reason for this. Is the API hard to use as is?",hi thank reply worked current class interested knowing current wrapping common would like know reason hard use,issue,positive,negative,neutral,neutral,negative,negative
658729455,"Hi @yijiezh, 

As a ML-Agents user, I also felt the need for a multiagent wrapper. Unfortunately, Gym interface has been designed for single agent environments. I know two solutions to extend the Gym interface to multiagent environments:

- Observations, actions, dones and rewards are lists, as OpenAI team did for [multiagent particles environment](https://github.com/openai/multiagent-particle-envs). The list index corresponds to the agent id (integer).
- Observations, actions dones and rewards are dictionaries, as ray team did for [their rllib framework](https://docs.ray.io/en/master/rllib-env.html#multi-agent-and-hierarchical). The dict key corresponds to the agent id (string or integer).

I found this second solution more convenient and I extended gym wrapper to support multiagent environments using [rllib multiagent interface](https://github.com/ray-project/ray/blob/master/rllib/env/multi_agent_env.py).



You can find the Python module I developed: [unity_wrappers.zip](https://github.com/Unity-Technologies/ml-agents/files/4924966/unity_wrappers.zip) (for ML-Agents release 3).


It works pretty much the same way as ML-Agents Gym wrapper, except that observations, rewards, dones and actions are dictionaries:

```python
# Instanciate multiagents UnityEnvironment
unity_env = UnityEnvironment(file_name=""..."")
# Wraps it with my wrapper
env = MultiUnityWrapper(unity_env)
# Use it as a Gym environment (with dictionaries instead of np arrays)
observations = env.reset()
# {""agent_1"": [0.0, 1.2, 3.0,], ""agent_2"": ...]
```

Note that my solution is not perfect and may not work
Maybe the ML-Agents team could take inspiration from this solution!

Best",hi user also felt need wrapper unfortunately gym interface designed single agent know two extend gym interface team environment list index agent id integer ray team framework key agent id string integer found second solution convenient extended gym wrapper support interface find python module release work pretty much way gym wrapper except python wrapper use gym environment instead note solution perfect may work maybe team could take inspiration solution best,issue,positive,positive,positive,positive,positive,positive
658599209,"Dears at Unity @xiaomaogy @harperj @awjuliani @dannylange 

It would be nice to know if ML-Agents can replicate OpenAI Five, AlphaGo, or Hide and Seek if the hardware requirements are met. And what is the current ML-Agents can achieve? This benchmark would help practitioners and researches set their expectations correctly when developing their projects in Unity. I do believe developing in a 3D environment such as Unity is the future of deep reinforcement learning, However, benchmarking against those demanding projects is key.",unity would nice know replicate five hide seek hardware met current achieve would help set correctly unity believe environment unity future deep reinforcement learning however demanding key,issue,negative,positive,positive,positive,positive,positive
658562893,"This is still an issue, please let me know if anyone has any recommendations on things I should try.",still issue please let know anyone try,issue,negative,neutral,neutral,neutral,neutral,neutral
658486446,"Yeah, probably the robot arm is the best testbed. To keep the number of permutations small, how about runs with
* local-space transforms only
* model-space transforms only
* local + model
* local + model + joint positions/angles
?",yeah probably robot arm best keep number small local model local model joint,issue,positive,positive,positive,positive,positive,positive
658483319,"Hi @yijiezh 

We have not started work on this feature, but it has been logged. Unfortunately there is not a ""for free"" way to get rendered images out of Unity, so we will have to make modifications to the toolkit and communication protocol in order to enable this. ",hi work feature logged unfortunately free way get unity make communication protocol order enable,issue,negative,positive,positive,positive,positive,positive
658460102,"Note that this is going into master, not the release branch. I'd rather not make a change to the release branch this close to shipping.",note going master release branch rather make change release branch close shipping,issue,negative,neutral,neutral,neutral,neutral,neutral
658449890,"> Added some comments. Not sure how thorough we should be here, since on the one hand this is only for performance evaluation, and on the other there is a good change this will form the basis for whatever pytorch SAC looks like down the line.

I actually want to have a deeper discussion about this - I made this with minimal changes to the rest of the PyTorch code, but there are a couple things that I think we should do to the `models_torch.py` that would make it more modular. For instance, separating out the normalizers from the NetworkBody, and having ActorCritic be made up of two modules, an Actor and a Critic, so that we can make an Actor-only policy. ",added sure thorough since one hand performance evaluation good change form basis whatever sac like line actually want discussion made minimal rest code couple think would make modular instance separating made two actor critic make policy,issue,positive,positive,positive,positive,positive,positive
658442239,Do you mean instead of loading the model from the .pb into a tensorflow graph?,mean instead loading model graph,issue,negative,negative,negative,negative,negative,negative
658440591,"Great! Additionally, we've added a catch for this today in #4227. Thank you for raising the issue. I am closing this issue as this has been resolved.",great additionally added catch today thank raising issue issue resolved,issue,positive,positive,positive,positive,positive,positive
658436393,"Solved - thank you.

Yes: the function is triggered by an Action that is ultimately called by OnActionReceived, which must lead to the infinite loop described in your branched conversation.

I've since prevented OnActionReceived calling EnvironmentStep() directly, and instead have it toggle the noMovesAllows boolean that's now checked on the independent FixedUpdate loop, which now calls the EnvironmentStep().

Thank you.",thank yes function triggered action ultimately must lead infinite loop branched conversation since calling directly instead toggle checked independent loop thank,issue,positive,positive,neutral,neutral,positive,positive
658403978,"If I create the environment using the the default_registry, is there any parameter that I can set to turn off the logs?",create environment parameter set turn,issue,negative,neutral,neutral,neutral,neutral,neutral
658357655,"Exactly, machine learning is all about how fast you iterate your workflow. I am already doing what you suggest regarding reward engineering and hyperparameter tuning, but it is frustrating if you could not iterate that fast. GPU is the de facto when it comes to deep learning, and DRL needs that as well. Matrix multiplication is a work best suited for GPUs.  I hope such full integration would be soon available to us along with the distributed training in the cloud.",exactly machine learning fast iterate already suggest regarding reward engineering tuning could iterate fast de come deep learning need well matrix multiplication work best hope full integration would soon available u along distributed training cloud,issue,positive,positive,positive,positive,positive,positive
658322682,"Yeah, I wouldn't even bother trying visual observations, or anything that requires more than 2 to 3 million steps to train. Instead I think (for now at least) you need to train multiple brains for specific actions and then switch to the appropriate brain as needed. Not ideal to not be able to generalize at all, but I think that's the only option currently. If you can't train with vector observations only, that likely means your model / behavior is too complex and you need to simplify it. Again not ideal, but with how slow training is and no GPU / way to speed it up, anything over 2 million or so steps seems unmanageable to train.

My new rule of thumb is if I'm not seeing consistent improvement in scores by 1 million steps, I quit and try something else (simplify the behavior even more, tweak rewards and observations, tweak hyperparameters). In much of the research you see examples where it doesn't learn much at all (no clear learning) for 10 million runs or more, and then it learns and jumps up in score. But I think that needs to take 1-2 days of training or less to reach, meaning we need much faster training; otherwise how do you know if you need to keep running it and be patient, or if it will never learn? You can't afford to wait 3 to 5 days or more to get to 10 million steps, still have it not be any better, and then quit then, losing an entire week of time. It either starts learning within a day, or I quit, make a change (simplify), and restart.",yeah would even bother trying visual anything million train instead think least need train multiple brain specific switch appropriate brain ideal able generalize think option currently ca train vector likely model behavior complex need simplify ideal slow training way speed anything million unmanageable train new rule thumb seeing consistent improvement million quit try something else simplify behavior even tweak tweak much research see learn much clear learning million score think need take day training le reach meaning need much faster training otherwise know need keep running patient never learn ca afford wait day get million still better quit losing entire week time either learning within day quit make change simplify restart,issue,positive,positive,positive,positive,positive,positive
658318869,"@chriselion thanks for the reply. We created a simple onnx-net by hand and it has an output `version_number` and we still getting the error. So we are a bit lost what to do. Do you have any idea on what we could try?

![image](https://user-images.githubusercontent.com/25705862/87458482-5b2efc80-c60a-11ea-992b-68cd8be3b745.png)
The shape seems to be wrong as well, but this shouldn't matter in line 152.

Thank you very much in advance.",thanks reply simple hand output still getting error bit lost idea could try image shape wrong well matter line thank much advance,issue,negative,negative,neutral,neutral,negative,negative
658318543,"@lukemadera Interesting information. I am glad to hear that before going to my next project that is solely based on visual observations.

My first project relied only on Rays, vector observations only, and I could not train it on my MBP 2018 despite running it till 10 million steps!",interesting information glad hear going next project solely based visual first project vector could train despite running till million,issue,positive,positive,positive,positive,positive,positive
658309725,"Hi @AdhamAlHarazi just to be clear, compute power and price are NOT the issue. The issue is that MLAgents currently does not work well with visual observations and GPU at all. Until that is addressed and MLAgents can leverage that, the CPU is the limiting factor and that can only be improved so much; the big ML training benefits come from GPUs, not CPUs.

As it stands now, only simple models that can train in around 1 to 2 million steps, using vector observations only, can be trained well. At least that is my understanding and I have never been able to train anything more complex than this (even with a Machine Learning rig with a great GPU) nor have I seen any examples that can either.

In my opinion the ""gpu"" and ""visual observation"" options should be removed until they actually work well, otherwise they mislead people like they did for me, that they can be used and will actually work. Or at least add a really big ""limitations"" / ""disclaimer"" note that they are there but don't really work yet.",hi clear compute power price issue issue currently work well visual leverage limiting factor much big training come simple train around million vector trained well least understanding never able train anything complex even machine learning rig great seen either opinion visual observation removed actually work well otherwise mislead people like used actually work least add really big disclaimer note really work yet,issue,positive,negative,neutral,neutral,negative,negative
658306141,"@lukemadera Thank you for raising this issue. For me, I spend more than a year, since I was new to Unity as well, building a complex environment such as OpenAI hide and seek. In which, I though I could achieve it. But unfortunately, I needed more computing power to train it. I learned that the hard way as you. 

I do believe Unity is shaping the future of DRL with its simulation platform and MLAgents as a DRL framework. However, the computations requirement is beyond the individual's budget. I am wondering if the upcoming unity cloud training would have reasonable price for us.

Also, I am thinking of using visual observation for my next, simpler, project, and I just read in this thread that it may not work!

With that said, I hope to see ML-Agents training going to the cloud for distributed training with reasonable prices. 

",thank raising issue spend year since new unity well building complex environment hide seek though could achieve unfortunately power train learned hard way believe unity shaping future simulation platform framework however requirement beyond individual budget wondering upcoming unity cloud training would reasonable price u also thinking visual observation next simpler project read thread may work said hope see training going cloud distributed training reasonable,issue,negative,negative,neutral,neutral,negative,negative
658288514,Is this function being manually called (either directly or indirectly) by CollectObservations or OnActionReceived? This can cause a loop that will prevent the main Update from being called.,function manually either directly indirectly cause loop prevent main update,issue,negative,positive,positive,positive,positive,positive
658253114,"@yijiezh `BaseRegistryEntry.make()` returns a `BaseEnv`, and `BaseEnv.reset()` returns `None`, so that's expected:
https://github.com/Unity-Technologies/ml-agents/blob/c75eacbdc40f9afffaedfd79d861b83a88cf3b8d/ml-agents-envs/mlagents_envs/base_env.py#L375-L379

Please create a new issue (or forum thread) if you have more problems.",none please create new issue forum thread,issue,negative,positive,positive,positive,positive,positive
658140927,"Here's the function NextPlayerTurn, which itself is triggered by an Action:

`private void NextPlayerTurn(bool isP1Turn)
	{
		noMovesAllowed = false;
		if (isPlayer1 && isP1Turn)
		{
			this.RequestDecision();
			Academy.Instance.EnvironmentStep();
		}
		else if (!isPlayer1 && !isP1Turn)// this must be player 2 and it must be player 2's turn
		{
			this.RequestDecision();
			Academy.Instance.EnvironmentStep(); //This will crash/hang unity! 
		}
	}`

",function triggered action private void bool false else must player must player turn unity,issue,negative,negative,negative,negative,negative,negative
657979533,"I tried 
```
from mlagents_envs.registry import default_registry

print(default_registry[""Basic""].description)
env = default_registry[""Basic""].make()

print(env.reset())
```

I got None in the stdout.
Didn't the env here same as the a gym env, and that should return an observation vector? Anything I missed?",tried import print basic basic print got none gym return observation vector anything,issue,negative,neutral,neutral,neutral,neutral,neutral
657945027,"That's completely fine @andrewcoh 🙂, but my alternative was that is there is any way to input the observations in `frozen_graph_def.pb` and get the actions. Is that possible ? ",completely fine alternative way input get possible,issue,negative,positive,positive,positive,positive,positive
657895356,Added discrete SAC as well. ,added discrete sac well,issue,negative,neutral,neutral,neutral,neutral,neutral
657874798,hey @surfnerd - any issue to use Apache 2.0 if we don't plan on putting extensions package through package review?,hey issue use apache plan package package review,issue,negative,neutral,neutral,neutral,neutral,neutral
657761292,"Let's do after. Currently I'm working on discrete, we can also integrate after that is done too",let currently working discrete also integrate done,issue,negative,neutral,neutral,neutral,neutral,neutral
657735216,"I tried it with 3DBall and it worked. 
Do we want to integrate this change with `experiment_torch.py` before or after merge ?",tried worked want integrate change merge,issue,negative,neutral,neutral,neutral,neutral,neutral
657697245,"Hi @Shubhamai 

Sorry but I am not sure that this is an issue we can help solve since converting `.pb` to `.h5` is entirely outside of ML-Agents.",hi sorry sure issue help solve since converting entirely outside,issue,positive,neutral,neutral,neutral,neutral,neutral
657695096,"Hi @applenicks 

Can you copy the entire function where you call EnvironmentStep?",hi copy entire function call,issue,negative,neutral,neutral,neutral,neutral,neutral
657665637,"@Segelzwerg I believe something changed in the Barracuda ONNX importer recently; we changed the model export to store the constants as outputs (https://github.com/Unity-Technologies/ml-agents/pull/4073). Unfortunately, we didn't have a good test for this, so I'm not sure when the change in Barracuda actually happened.

I think after this thread was originally created, we clarified our [policy on models that were generated outside of ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Unity-Inference-Engine.md#unity-inference-engine), namely that we do not provide support for them. You're still welcome to try, but chances are you're going to have to read through the code (especially in BarracudaModelParamLoader) to determine how to get your model hooked up.",believe something barracuda importer recently model export store unfortunately good test sure change barracuda actually think thread originally policy outside namely provide support still welcome try going read code especially determine get model hooked,issue,positive,positive,positive,positive,positive,positive
657461200,Hi @andrewcoh Thanks a lot of assigning yourself for this request. Are you working on this feature? Just asked because I didn't get any updates for this. Thanks Again!,hi thanks lot request working feature get thanks,issue,positive,positive,positive,positive,positive,positive
657079838,"Hi everyone, we tried to import a converted TF-1, TF-2 / and Keras models. We could import into Unity but failed upon drag&drop to Model Attribute in Behaviour Parameters. It seems like the error above.
```NullReferenceException: Object reference not set to an instance of an object
Unity.MLAgents.Inference.BarracudaModelParamLoader.CheckModel (Unity.Barracuda.Model model, Unity.MLAgents.Policies.BrainParameters brainParameters, Unity.MLAgents.Sensors.SensorComponent[] sensorComponents, System.Int32 observableAttributeTotalSize, Unity.MLAgents.Policies.BehaviorType behaviorType) (at Library/PackageCache/com.unity.ml-agents@1.1.0-preview/Runtime/Inference/BarracudaModelParamLoader.cs:152)
Unity.MLAgents.Editor.BehaviorParametersEditor.DisplayFailedModelChecks () (at Library/PackageCache/com.unity.ml-agents@1.1.0-preview/Editor/BehaviorParametersEditor.cs:128)
Unity.MLAgents.Editor.BehaviorParametersEditor.OnInspectorGUI () (at Library/PackageCache/com.unity.ml-agents@1.1.0-preview/Editor/BehaviorParametersEditor.cs:71)
UnityEditor.UIElements.InspectorElement+<>c__DisplayClass55_0.<CreateIMGUIInspectorFromEditor>b__0 () (at /home/builduser/buildslave/unity/build/Editor/Mono/Inspector/InspectorElement.cs:521)
UnityEngine.GUIUtility:ProcessEvent(Int32, IntPtr) (at /home/builduser/buildslave/unity/build/Modules/IMGUI/GUIUtility.cs:187)
```
We actually manage to add a constant, but we still got the same error.
![image](https://user-images.githubusercontent.com/25705862/87227282-4c421300-c39a-11ea-9aa4-b544097e8377.png)

So our question is, did anyone manage to add these constant and got the onnx network imported. We would really appreciate any help as we are stuck and the project depends on importing the network. For this project, it is not an option to train directly with ml-agents as we need the Keras models elsewhere.",hi everyone tried import converted could import unity upon drag drop model attribute behaviour like error object reference set instance object model actually manage add constant still got error image question anyone manage add constant got network would really appreciate help stuck project network project option train directly need elsewhere,issue,negative,positive,neutral,neutral,positive,positive
657032684,"Hello @andrewcoh 

I'll definitely look forward for you and your team response

And about some examples: 

 - anything that matches the definition of _""It looks natural""_, _""It's something that I like""_ or _""it feels right""_ which are definitely hard to define in a strictly mathematical and algorithmic definition, for instance, in the unity arm example I can define the reward function _""move toward the point in space X""_ in C#, but the reward function _""and move in an elegant fashion""_ may be more challenging.
I may define one using the acceleration of the joints and the end functor, the derivative of the acceleration and many other parameters, but, apart that I have no idea if these are the things I'm looking for to integrate in the reward function, they are also free parameters that I have to tweak during multiple training sessions and they may still lead toward sub-optimal solutions or reward hacking by the agent

 - Another example: A bouncer, one leg, three joints, a weight on top, I want to make it jump toward a goal with a _""natural looking""_ bouncing, I can define a relatively simple reward that takes into account the jumping length and tweak it until I like it, long but not impossible, problem arises if I want to do the train the model with the whole class of possible jumpers, using variable weights and links length, now the reward function has to be based on the mechanical parameters of the body and it needs to be heavily modified to fit the new requirements (a 20 meters tall and weighting ten tons jumper should look different from a jumper 100 times smaller and 1000 times lighter, what would be an ideal reward function to model these behaviors at the same time?)

 - Pure debug: this may be a simple side effect of the human-in-the-loop, where, during training, I can improve my requests deciding if the agent is performing well or not, without an explicit function and, then I can either derive the an old fashion reward function observing the results, or I can just use the trained model _as-is_ running the training for a few hours/days, instead of weeks and multiple re-training for the more challenging task to define, like throwing a ball: a reward function may be ""the ball has to reach the target as fast as possible"" I may realize after a few hours that the addendum ""and the agent cannot leave the throwing area"" is needed to block the agent to simply run toward the target, but it's likely that I'll realize this only after a whole training session and I'll need to update and redo everything for more hours
Then, if I want to use and extended the reward function with ""And the ball has to bounce N times on the floor before reaching the target"" I may have take into account many more variables to have the expected outcome",hello definitely look forward team response anything definition natural something like right definitely hard define strictly mathematical algorithmic definition instance unity arm example define reward function move toward point space reward function move elegant fashion may may define one acceleration end derivative acceleration many apart idea looking integrate reward function also free tweak multiple training session may still lead toward reward hacking agent another example bouncer one leg three weight top want make jump toward goal natural looking bouncing define relatively simple reward account length tweak like long impossible problem want train model whole class possible variable link length reward function based mechanical body need heavily fit new tall weighting ten jumper look different jumper time smaller time lighter would ideal reward function model time pure may simple side effect training improve agent well without explicit function either derive old fashion reward function observing use trained model running training instead multiple task define like throwing ball reward function may ball reach target fast possible may realize addendum agent leave throwing area block agent simply run toward target likely realize whole training session need update redo everything want use extended reward function ball bounce time floor reaching target may take account many outcome,issue,positive,positive,positive,positive,positive,positive
656882608,"> Do you have any examples where this is used?

I've retrained Worm partially`*`, replacing its position and rotation observations with model space position and local space rotations, and it does as well as the original (maybe slightly better). Holding off on committing a model until things stabilize a bit.

`*` 1/10 the default max steps, with constant learning rate - this runs in about 10 minutes on my laptop",used worm partially position rotation model space position local space well original maybe slightly better holding model stabilize bit default constant learning rate,issue,positive,positive,positive,positive,positive,positive
656792403,"Hi @sunirisgrace 

This type of problem may be better suited for the forums here https://forum.unity.com/forums/ml-agents.453/ since it is not so much a bug in ML-Agents but some issue with the way the problem is formulated. Please try sharing on the forums with a precise description of your reward function and the expected behavior.

",hi type problem may better since much bug issue way problem please try precise description reward function behavior,issue,positive,positive,positive,positive,positive,positive
656789630,"Hi @DrTtnk 

Thank you for the request. This is an interesting feature that I will discuss with the team. I will update this thread with a resolution in the next week.

In the meantime, do you have a particular behavior for which you cannot design a reward function?",hi thank request interesting feature discus team update thread resolution next week particular behavior design reward function,issue,positive,positive,positive,positive,positive,positive
656632357,"I tried replacing ""using Barracuda;"" with ""using Unity.Barracuda;"" in the scripts where it was showing errors. It solved most of the errors related to barracuda error. I am using unity version 2019.3.0f6. Hope it helps!",tried barracuda showing related barracuda error unity version hope,issue,negative,neutral,neutral,neutral,neutral,neutral
656482732,"@knmitri  I don't think there is a problem or bug somewhere, i would say that is normal ",think problem bug somewhere would say normal,issue,negative,positive,positive,positive,positive,positive
656444294,"I think someone has created a custom wrapper for this https://github.com/Procuste34/Unity-MLAgents/blob/master/gym_wrapper/gym_wrapper.py.

I'd like to see official support for it in the future though.

> I don't want to remove 7 agents and re-built it.

you might want to procedurally generate with script at runtime
",think someone custom wrapper like see official support future though want remove might want generate script,issue,positive,neutral,neutral,neutral,neutral,neutral
656004733,@surfnerd I was following this thread for a while now and was wondering what the status on the topic is. It would be really helpful to get access to the agent's brain to use it for artistic interpretation.,following thread wondering status topic would really helpful get access agent brain use artistic interpretation,issue,negative,positive,positive,positive,positive,positive
655877293,"> Hi @HulkMaker
> 
> Can you link your yaml?
> 
> Is it possible that you have removed the new lines from the yaml? Judging by the last line of the trace, it looks like your configuration is being read as a single line string.

yes, wrong format of the yaml is exactly the root cause. 
Solved! I can train now.
Thanks for your help!",hi link possible removed new last line trace like configuration read single line string yes wrong format exactly root cause train thanks help,issue,positive,positive,neutral,neutral,positive,positive
655857329,"Updated this to move more logic into the trainer, avoiding the Policy needing to be aware of the brain name, reward, or steps -- then merged master in.",move logic trainer policy needing aware brain name reward master,issue,positive,positive,positive,positive,positive,positive
655819911,No problem. I'm closing this issue. Please open another if you encounter any other problems.,problem issue please open another encounter,issue,negative,neutral,neutral,neutral,neutral,neutral
655796323,"> Looks like the behavior name in your yaml doesn't match the behavior name in the behavior parameters script. The yaml is looking for `TileBase` but the behavior parameters script has `TileBaseISO`

ohhh yes exactly ! thank you",like behavior name match behavior name behavior script looking behavior script yes exactly thank,issue,positive,positive,positive,positive,positive,positive
655793617,Looks like the behavior name in your yaml doesn't match the behavior name in the behavior parameters script. The yaml is looking for `TileBase` but the behavior parameters script has `TileBaseISO`,like behavior name match behavior name behavior script looking behavior script,issue,negative,neutral,neutral,neutral,neutral,neutral
655792812,"2020-07-08 21:05:33.264793: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-07-08 21:05:33.269573: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
WARNING:tensorflow:From c:\users\++++\desktop\vrtl-env\python-envs\sample-env\lib\site-packages\tensorflow\python\compat\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2020-07-08 21:05:34 INFO [environment.py:199] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
2020-07-08 21:05:38 INFO [environment.py:108] Connected to Unity environment with package version 1.1.0-preview and communication version 1.0.0
2020-07-08 21:05:38 INFO [environment.py:265] Connected new brain:
TileBaseISO?team=1
2020-07-08 21:05:38 INFO [environment.py:265] Connected new brain:
TileBaseISO?team=0
2020-07-08 21:05:38 WARNING [trainer_util.py:96] Metacurriculum enabled, but no curriculum for brain TileBaseISO. Brains with curricula: dict_keys(['TileBase']).
2020-07-08 21:05:38.568555: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-07-08 21:05:38.577295: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x20e71ddfb10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-08 21:05:38.580400: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-08 21:05:38.589405: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-07-08 21:05:39.094596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.23GiB/s
2020-07-08 21:05:39.100176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:
pciBusID: 0000:02:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.23GiB/s
2020-07-08 21:05:39.105756: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-07-08 21:05:39.111379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-07-08 21:05:39.118338: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-07-08 21:05:39.121647: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-07-08 21:05:39.128544: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-07-08 21:05:39.132443: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-07-08 21:05:39.139373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-07-08 21:05:39.141850: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-07-08 21:05:39.149195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-08 21:05:39.153181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1
2020-07-08 21:05:39.154718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y
2020-07-08 21:05:39.156062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N
2020-07-08 21:05:39.159928: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x20e721397a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-08 21:05:39.163508: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080, Compute Capability 7.5
2020-07-08 21:05:39.166463: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce RTX 2080, Compute Capability 7.5
2020-07-08 21:05:39 WARNING [stats.py:197] events.out.tfevents.1594238628.DESKTOP-VB7B3QL was left over from a previous run. Deleting.
2020-07-08 21:05:39 INFO [stats.py:130] Hyperparameters for behavior name TileBaseISO:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   1024
          buffer_size:  10240
          learning_rate:        0.0003
          beta: 0.005
          epsilon:      0.2
          lambd:        0.95
          num_epoch:    3
          learning_rate_schedule:       linear
        network_settings:
          normalize:    False
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
        init_path:      None
        keep_checkpoints:       5
        checkpoint_interval:    500000
        max_steps:      500000
        time_horizon:   64
        summary_freq:   50000
        threaded:       True
        self_play:      None
        behavioral_cloning:     None
2020-07-08 21:05:39.184578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.23GiB/s
2020-07-08 21:05:39.189616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:
pciBusID: 0000:02:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.23GiB/s
2020-07-08 21:05:39.195256: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-07-08 21:05:39.197638: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-07-08 21:05:39.199991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-07-08 21:05:39.202936: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-07-08 21:05:39.204870: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-07-08 21:05:39.206848: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-07-08 21:05:39.208753: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-07-08 21:05:39.210825: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-07-08 21:05:39.218781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-08 21:05:39.222538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1
2020-07-08 21:05:39.223943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y
2020-07-08 21:05:39.225291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N
2020-07-08 21:05:40.058013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.23GiB/s
2020-07-08 21:05:40.063121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:
pciBusID: 0000:02:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.23GiB/s
2020-07-08 21:05:40.071023: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-07-08 21:05:40.085267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-07-08 21:05:40.087924: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-07-08 21:05:40.099174: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-07-08 21:05:40.101556: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-07-08 21:05:40.103496: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-07-08 21:05:40.117907: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-07-08 21:05:40.120753: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-07-08 21:05:40.139303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-08 21:05:40.143479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1
2020-07-08 21:05:40.145738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y
2020-07-08 21:05:40.147205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N
2020-07-08 21:05:40 WARNING [trainer.py:226] Your environment contains multiple teams, but PPOTrainer doesn't support adversarial games. Enable self-play to                     train adversarial games.",could load dynamic library found ignore set machine warning removed future version long term listening port start training pressing play button unity editor connected unity environment package version communication version connected new brain connected new brain warning curriculum brain brain curriculum binary use service platform host guarantee used device host default version successfully dynamic library found device name found device name could load dynamic library found successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library please make sure missing properly would like use follow guide setup platform skipping device interconnect strength edge matrix service platform guarantee used device compute capability device compute capability warning left previous run behavior name beta epsilon linear normalize false simple memory none extrinsic gamma strength none threaded true none none found device name found device name could load dynamic library found successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library please make sure missing properly would like use follow guide setup platform skipping device interconnect strength edge matrix found device name found device name could load dynamic library found successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library please make sure missing properly would like use follow guide setup platform skipping device interconnect strength edge matrix warning environment multiple support enable train,issue,positive,positive,neutral,neutral,positive,positive
655792568,"Hi @devich123 

This seems to be an issue more suited for the barracuda issues. Please try submitting an issue here https://github.com/Unity-Technologies/barracuda-release/issues",hi issue barracuda please try issue,issue,negative,neutral,neutral,neutral,neutral,neutral
655792151,"behaviors:
  TileBase:
    trainer_type: ppo
    hyperparameters:
      batch_size: 2048
      buffer_size: 20480
      learning_rate: 0.0003
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: constant
    network_settings:
      normalize: false
      hidden_units: 512
      num_layers: 2
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    keep_checkpoints: 5
    max_steps: 50000000
    time_horizon: 1000
    summary_freq: 10000
    threaded: true
    self_play:
      save_steps: 50000
      team_change: 200000
      swap_steps: 2000
      window: 10
      play_against_latest_model_ratio: 0.5
      initial_elo: 1200.0
curriculum:
  TileBase:
    measure: reward
    thresholds: [-1.0, -0.7, -0.3, 0.3, 0.7, 1.0]
    min_lesson_length: 80
    signal_smoothing: true
    parameters:
       enemy_count: [5.0, 4.0, 3.0, 3.0, 2.0, 2.0, 2.0]

yes I have set team ids in bhv parameters 0 and 1 ",beta epsilon constant normalize false simple extrinsic gamma strength threaded true window curriculum measure reward true yes set team,issue,positive,positive,neutral,neutral,positive,positive
655791913,"Hi @HulkMaker 

Can you link your yaml?

Is it possible that you have removed the new lines from the yaml? Judging by the last line of the trace, it looks like your configuration is being read as a single line string.",hi link possible removed new last line trace like configuration read single line string,issue,negative,positive,neutral,neutral,positive,positive
655780374,"To help me understand CheckpointManager, can you edit the description of the PR and link to relevant documents?",help understand edit description link relevant,issue,negative,positive,positive,positive,positive,positive
655725678,"Hey @robinerd, the fix has been merged to `master` - quick way to workaround it on old releases of ML-Agents is to add `threaded: false` to your YAML file. ",hey fix master quick way old add threaded false file,issue,negative,positive,neutral,neutral,positive,positive
655696857,"Hi @nntsrb 

Did you set your team ids in the behavior parameters script? Also, can you share your full config file and the full output from the terminal?",hi set team behavior script also share full file full output terminal,issue,negative,positive,positive,positive,positive,positive
655684267,Thanks a lot for the fast response! I will try out the fix during our upcoming trainings. Good to hear that you found a reliable way of reproducing it.,thanks lot fast response try fix upcoming good hear found reliable way,issue,positive,positive,positive,positive,positive,positive
655646972,"Hi @robinerd,
This was reported on the [forums](https://forum.unity.com/threads/training-stops-with-runtimeerror-dictionary-changed-size-during-iteration.926297/#post-6064934) too. It's due to a conflict in multithreading on the python side, and I just got a repro last night (adding a sleep(1) in that loop and reducing the summary_freq to 1 makes it happen almost immediately).

I'll hopefully have a fix today.",hi due conflict python side got last night sleep loop reducing happen almost immediately hopefully fix today,issue,negative,negative,neutral,neutral,negative,negative
655040970,">Al escribir:
>PS C:\Users\Usuario> .\tensorboard --logdir=result

We write the data to the ""results"" directory, but you are using ""result"", which does not exist.

Please use `.\tensorboard --logdir=results`",al write data directory result exist please use,issue,negative,neutral,neutral,neutral,neutral,neutral
655005894,"👍  on this one, ran into it while trying to use the API",one ran trying use,issue,negative,neutral,neutral,neutral,neutral,neutral
654998737,"I logged the --num-envs bug as MLA-1145 in our internal tracker.

If you're still having problems with multiple areas, can you post it on the ML-Agents forum (https://forum.unity.com/forums/ml-agents.453/) ? That's a better venue for that type of questions.",logged bug internal tracker still multiple post forum better venue type,issue,negative,positive,positive,positive,positive,positive
654987655,"> Do you have enabled curiosity? What was the cumulative reward graph?
@Shubhamai 
No I did not enable curiosity. the cumulative reward is also decreasing to large negative values.",curiosity cumulative reward graph enable curiosity cumulative reward also decreasing large negative,issue,positive,negative,neutral,neutral,negative,negative
654415833,@andrewcoh Since @fedetask confirmed it for PPO I didn't try. Do you think it's a bug?,since confirmed try think bug,issue,negative,positive,positive,positive,positive,positive
654155959,"Gracias, poco a poco voy aprendiendo.

Al escribir:
PS C:\Users\Usuario> .\tensorboard --logdir=result
Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all
TensorBoard 2.0.2 at http://localhost:6006/ (Press CTRL+C to quit)

Posteriormente abro http://localhost:6006/ en mi navegador y me aparece esto de TensorBoard:

No dashboards are active for the current data set.
Probable causes:

You haven’t written any data to your event files.
TensorBoard can’t find your event files.
If you’re new to using TensorBoard, and want to find out how to add data and set up your event files, check out the README and perhaps the TensorBoard tutorial.
If you think TensorBoard is configured properly, please see the section of the README devoted to missing data problems and consider filing an issue on GitHub.

Last reload: Mon Jul 06 2020 12:37:18 GMT+0200 (hora de verano de Europa central)
Data location: result

¿Que puede hacer para ver los datos?

Muchas gracias.",poco poco al serving expose network use proxy pas press quit en mi de active current data set probable written data event find event new want find add data set event check perhaps tutorial think properly please see section devoted missing data consider filing issue last reload mon hora de de central data location result para,issue,positive,negative,neutral,neutral,negative,negative
653756466,Do you have enabled curiosity? What was the cumulative reward graph?,curiosity cumulative reward graph,issue,positive,neutral,neutral,neutral,neutral,neutral
653655597,"Abandoning. Sorry, it took me way to long to get permission for signing the CLA and subsequent changes mean this PR is no longer needed.",sorry took way long get permission subsequent mean longer,issue,negative,negative,negative,negative,negative,negative
653646977,"> We need to validate the arguments better and raise a clearer error message in this case.

Yes, it wasn't clear to me that I needed the executable from the training docs (I should've thought of it though).

> Note that --num-envs refers to the number of instances of the executable to run, not the number of training areas in the scene.

I know, I only went the multiple instance way after the problems I was facing with multiple training areas as I noted in my post",need validate better raise clearer error message case yes clear executable training thought though note number executable run number training scene know went multiple instance way facing multiple training noted post,issue,positive,positive,positive,positive,positive,positive
653644462,"> mlagents-learn config/ppo/WallJump.yaml --run-id=WallJumper --num-envs=2

This isn't a valid combination of commandline parameters: you either need to omit `--num-envs` (if you want to connect to the editor) or add `--env` and point it to an executable.

We need to validate the arguments better and raise a clearer error message in this case.

Note that `--num-envs` refers to the number of instances of the executable to run, not the number of training areas in the scene.",valid combination either need omit want connect editor add point executable need validate better raise clearer error message case note number executable run number training scene,issue,positive,positive,positive,positive,positive,positive
653108324, 2020.2 error is unrelated will be fixed in another PR,error unrelated fixed another,issue,negative,positive,neutral,neutral,positive,positive
652755596,"> Looks good. I wonder if this might even be something worth trying to contribute to pytorch itself? It seems like `torch.as_tensor` should be doing what you have in the function internally.

In the PyTorch issue they commented saying that it's a won't fix for them, but maybe us or someone could contribute the fix?",good wonder might even something worth trying contribute like function internally issue saying wo fix maybe u someone could contribute fix,issue,positive,positive,positive,positive,positive,positive
652671729,"> tensorflow_core\python\compat\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.

These messages are safe to ignore.

> Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2

This is safe to ignore. There is more information here: https://stackoverflow.com/a/47227886

The rest of the training looks good.

> PS C:\Users\Usuario> .\tensorboard --logdir = results

This is wrong. It should be `.\tensorboard --logdir=results`.",removed future version safe ignore binary use safe ignore information rest training good wrong,issue,negative,positive,positive,positive,positive,positive
652309796,"Tengo la duda porque tras parar el ejecución no puede ver los resultados en localhost:6006.

2020-07-01 11:31:54 INFO [subprocess_env_manager.py:191] UnityEnvironment worker 0: environment stopping.
2020-07-01 11:31:55 INFO [trainer_controller.py:234] Learning was interrupted. Please wait while the graph is generated.
2020-07-01 11:31:55 INFO [trainer_controller.py:108] Saved Model
2020-07-01 11:31:55 INFO [model_serialization.py:203] List of nodes to export for brain :3DBall?team=0
2020-07-01 11:31:55 INFO [model_serialization.py:205]   is_continuous_control
2020-07-01 11:31:55 INFO [model_serialization.py:205]   trainer_major_version
2020-07-01 11:31:55 INFO [model_serialization.py:205]   trainer_minor_version
2020-07-01 11:31:55 INFO [model_serialization.py:205]   trainer_patch_version
2020-07-01 11:31:55 INFO [model_serialization.py:205]   version_number
2020-07-01 11:31:55 INFO [model_serialization.py:205]   memory_size
2020-07-01 11:31:55 INFO [model_serialization.py:205]   action_output_shape
2020-07-01 11:31:55 INFO [model_serialization.py:205]   action
2020-07-01 11:31:55 INFO [model_serialization.py:205]   action_probs
Converting results\first3DBallRun\3DBall/frozen_graph_def.pb to results\first3DBallRun\3DBall.nn
IGNORED: Cast unknown layer
IGNORED: Shape unknown layer
IGNORED: StopGradient unknown layer
GLOBALS: 'is_continuous_control', 'trainer_major_version', 'trainer_minor_version', 'trainer_patch_version', 'version_number', 'memory_size', 'action_output_shape'
IN: 'vector_observation': [-1, 1, 1, 8] => 'sub_2'
OUT: 'action', 'action_probs'
DONE: wrote results\first3DBallRun\3DBall.nn file.
2020-07-01 11:31:56 INFO [model_serialization.py:83] Exported results\first3DBallRun\3DBall.nn file
PS C:\Users\Usuario> .\tensorboard --logdir = results
usage: tensorboard [-h] [--helpfull] [--logdir PATH] [--logdir_spec PATH_SPEC]
                   [--host ADDR] [--bind_all] [--port PORT]
                   [--purge_orphaned_data BOOL] [--db URI] [--db_import]
                   [--inspect] [--version_tb] [--tag TAG] [--event_file PATH]
                   [--path_prefix PATH] [--window_title TEXT]
                   [--max_reload_threads COUNT] [--reload_interval SECONDS]
                   [--reload_task TYPE] [--reload_multifile BOOL]
                   [--reload_multifile_inactive_secs SECONDS]
                   [--generic_data TYPE]
                   [--samples_per_plugin SAMPLES_PER_PLUGIN]
                   [--debugger_data_server_grpc_port PORT]
                   [--debugger_port PORT] [--master_tpu_unsecure_channel ADDR]
                   {serve,dev} ...
tensorboard: error: invalid choice: 'results' (choose from 'serve', 'dev')
PS C:\Users\Usuario> ",la el en worker environment stopping learning interrupted please wait graph saved model list export brain action converting cast unknown layer shape unknown layer unknown layer done wrote file file usage path host port port bool inspect tag tag path path text count type bool type port port serve dev error invalid choice choose,issue,negative,negative,neutral,neutral,negative,negative
652307285,"Hola de nuevo,

¿Es correcto esto?:

PS C:\Users\Usuario> .\mlagents-learn D:\ML_Unity\ml-agents-release_3\config\ppo\3DBall.yaml --run-id=first3DBallRun --resume
WARNING:tensorflow:From C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\compat\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term


                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓


 Version information:
  ml-agents: 0.17.0,
  ml-agents-envs: 0.17.0,
  Communicator API: 1.0.0,
  TensorFlow: 2.0.0
WARNING:tensorflow:From C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\compat\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2020-07-01 11:25:36 INFO [environment.py:199] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
2020-07-01 11:25:50 INFO [environment.py:108] Connected to Unity environment with package version 1.1.0-preview and communication version 1.0.0
2020-07-01 11:25:50 INFO [environment.py:265] Connected new brain:
3DBall?team=0
2020-07-01 11:25:50.511592: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-07-01 11:25:50 INFO [stats.py:130] Hyperparameters for behavior name 3DBall:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   64
          buffer_size:  12000
          learning_rate:        0.0003
          beta: 0.001
          epsilon:      0.2
          lambd:        0.99
          num_epoch:    3
          learning_rate_schedule:       linear
        network_settings:
          normalize:    True
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
        init_path:      None
        keep_checkpoints:       5
        checkpoint_interval:    500000
        max_steps:      500000
        time_horizon:   1000
        summary_freq:   12000
        threaded:       True
        self_play:      None
        behavioral_cloning:     None
2020-07-01 11:25:53 INFO [tf_policy.py:165] Loading model for brain 3DBall?team=0 from results\first3DBallRun\3DBall.
2020-07-01 11:25:53 INFO [tf_policy.py:196] Resuming training from step 233836.
2020-07-01 11:26:13 INFO [stats.py:111] 3DBall: Step: 240000. Time Elapsed: 42.431 s Mean Reward: 99.500. Std of Reward: 1.118. Training.
2020-07-01 11:26:40 INFO [stats.py:111] 3DBall: Step: 252000. Time Elapsed: 69.137 s Mean Reward: 100.000. Std of Reward: 0.000. Training.
2020-07-01 11:26:56 INFO [stats.py:111] 3DBall: Step: 264000. Time Elapsed: 85.440 s Mean Reward: 100.000. Std of Reward: 0.000. Training.
2020-07-01 11:27:21 INFO [stats.py:111] 3DBall: Step: 276000. Time Elapsed: 109.471 s Mean Reward: 100.000. Std of Reward: 0.000. Training.
2020-07-01 11:27:41 INFO [stats.py:111] 3DBall: Step: 288000. Time Elapsed: 130.070 s Mean Reward: 92.462. Std of Reward: 26.114. Training.",de resume warning removed future version long term version information communicator warning removed future version long term listening port start training pressing play button unity editor connected unity environment package version communication version connected new brain binary use behavior name beta epsilon linear normalize true simple memory none extrinsic gamma strength none threaded true none none loading model brain training step step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training,issue,positive,negative,neutral,neutral,negative,negative
652005111,"Hello, I was planning on creating a similar application to play around with some cool behaviours. Any update on this?",hello similar application play around cool update,issue,positive,positive,positive,positive,positive,positive
651930235,"It does seem like a file encoding issue given the funny characters printed with ""behaviors"" in the terminal. Try saving as `UTF-8` in your editor to get it to load properly. ",seem like file issue given funny printed terminal try saving editor get load properly,issue,positive,positive,positive,positive,positive,positive
651831561,"I solved the problem by using Layers. I added a new layer for the agent prefab and only added this to the parent object. I took the cube collider and put that as a child. Then I changed only parent's layer. I closed IgnoreRaycast from Project Settings/Physics (InfectionLayer is the agent layer)

![image](https://user-images.githubusercontent.com/32769732/86138640-6442a880-baf7-11ea-83d4-98e74d411583.png)

I unchecked the Ray Layer Mask > InfectiousLayer in RayPerceptionSensor3D

![image](https://user-images.githubusercontent.com/32769732/86138664-6b69b680-baf7-11ea-9831-302536a197c3.png)


and voila. Now it hit as it should.

![image](https://user-images.githubusercontent.com/32769732/86138774-8b997580-baf7-11ea-99ef-83ea90b3a475.png)

",problem added new layer agent prefab added parent object took cube put child parent layer closed project agent layer image unchecked ray layer mask image hit image,issue,negative,positive,neutral,neutral,positive,positive
651436895,"@ervteng @sankalp04 -- merged in changes to move the checkpointing and model save logic into TFPolicy.  The rewards associated with the checkpoints / final model are now also calculated from `RLTrainer.cumulative_returns_since_policy_update` so we should get all rewards for episodes from that policy.  Unfortunately we don't actually have any way to ensure we have collected rewards from the policy when we save the model; I have it save as `None` if there are no rewards.

@ervteng and I discussed a bit whether we should change or remove the reward outputs.  I believe @chriselion requested this so he might have some thoughts on how it will be used / what the best value would be to use for the reward.",move model save logic associated final model also calculated get policy unfortunately actually way ensure collected policy save model save none bit whether change remove reward believe might used best value would use reward,issue,positive,positive,positive,positive,positive,positive
651433473,"Did all the renames, and backed out the sensor changes for now.
@surfnerd @vincentpierre mind taking another look?",backed sensor mind taking another look,issue,negative,neutral,neutral,neutral,neutral,neutral
651350122,"I created the yaml file using another application and it is working.
The first non-working yaml is created on ""Rider""
I think this is a file encoding issue.",file another application working first rider think file issue,issue,negative,positive,positive,positive,positive,positive
651289125,"When you create the UnityEnvironment, it accepts an `additional_args` parameter, which is a list of strings that will get passed to the executable. You can try passing some of the ones here https://docs.unity3d.com/Manual/CommandLineArguments.html (notably, `-nolog` might help).

However, I'm not 100% sure this will get rid of them if the executable is writing to stdout.

When you build the executable, there are some settings to adjust the logging under Player Settings > Other Settings > Logging (screenshot is from 2018.4):
![image](https://user-images.githubusercontent.com/6877802/86042722-6ecc4780-b9fc-11ea-9f09-32c05a52eef1.png)
",create parameter list get executable try passing notably might help however sure get rid executable writing build executable adjust logging player logging image,issue,positive,positive,positive,positive,positive,positive
651277753,"I see, anything can be done on the UnityGymWrapper level to suppress the verbose logs?",see anything done level suppress verbose,issue,negative,neutral,neutral,neutral,neutral,neutral
651272366,They're all generated from the Unity executable that you're connecting to. None of them are coming directly from python.,unity executable none coming directly python,issue,negative,positive,neutral,neutral,positive,positive
650872853,"I would absolutely love the feature to automatically save .nn files, particularly if one of them was always the one with the highest current reward for this session.
",would absolutely love feature automatically save particularly one always one highest current reward session,issue,positive,positive,positive,positive,positive,positive
650473240,"Hi @Jean-Diddy, happy to answer your questions but since this isn't a bug report, would you mind posting it on the ML-Agents Forums? https://forum.unity.com/forums/ml-agents.453/ 

I can help you there. Thanks!",hi happy answer since bug report would mind posting help thanks,issue,positive,positive,positive,positive,positive,positive
650473051,"The first one is pretty easy - just add `--force`. 

The second one might be an actual bug - I've added it to our internal tracker with ID 1124 and we'll investigate. The workaround is to remove `initialize_from: null` in your YAML file. ",first one pretty easy add force second one might actual bug added internal tracker id investigate remove null file,issue,positive,positive,positive,positive,positive,positive
650444565,"this would make the code simpler, less `if env_name in unity_env_names: unity_env = UnityEnvironment(PATH_LOOKUP[env_name])` ",would make code simpler le,issue,negative,neutral,neutral,neutral,neutral,neutral
650174655,"Ok, that sounds good. If this issue is still open in a few weeks, I will try to change this myself, but at the moment I don't have enough time to do that.

For now, we will handle this in C#, like you said. Should work just fine.",good issue still open try change moment enough time handle like said work fine,issue,positive,positive,positive,positive,positive,positive
649870803,"Ah of course. I knew it was something obvious.
It doesn't give me any incorrect path errors but does give me: `raise UnityTrainerException(
mlagents.trainers.exception.UnityTrainerException: Previous data from this run ID was not found. Train a new run by removing the --resume flag.`
when I run `mlagents-learn Documents\config.yaml --run-id=BugTest`

Further, after I start a new neural network without the modified config (by running `mlagents-learn --run-id=BugTest`), when I try and resume with the modified config (by running `mlagents-learn Documents\config.yaml --run-id=BugTest --resume`), I get this error: `raise UnityTrainerException(
mlagents.trainers.exception.UnityTrainerException: Could not initialize from results\None. Make sure models have already been saved with that run ID.`

What could be causing this problem?",ah course knew something obvious give incorrect path give raise previous data run id found train new run removing resume run start new neural network without running try resume running resume get error raise could initialize make sure already saved run could causing problem,issue,negative,positive,positive,positive,positive,positive
649867989,"> Should we put this test in a separate script ? It kind of tests something very specific.

I think it's OK in `test_settings.py` since it tests the `ExportableSettings` mixin and is basically an end-to-end test of the structure/unstructure mechanism. But we could put it in `test_learn.py` too maybe?",put test separate script kind something specific think since basically test mechanism could put maybe,issue,positive,positive,positive,positive,positive,positive
649867047,"Ah, apologies for the confusion, you shouldn't use <> when specifying a path - so just `mlagents-learn Documents\config.yaml --run-id=BugTest`",ah confusion use path,issue,negative,neutral,neutral,neutral,neutral,neutral
649866553,"Maybe I'm missing something but when I do that, I get this error: `Config file could not be found at C:\Users\My Username\=BugTest`

This happens when I run this command: `mlagents-learn <Documents\config.yaml> --run-id=BugTest` from this directory: `C:\Users\My Username`

I copied the changed config file to my Documents folder to avoid any complications in inputting the directory. It's probably something obvious but what am I missing?
",maybe missing something get error file could found run command directory copied file folder avoid directory probably something obvious missing,issue,negative,negative,negative,negative,negative,negative
649856137,Should we put this test in a separate script ? It kind of tests something very specific.,put test separate script kind something specific,issue,positive,positive,positive,positive,positive,positive
649851914,"Hi,
I had another look at the code and I think it would be pretty straightforward to prevent the overwriting. We'd need to change the handling in StatsSideChannel to store a list values for each key, instead of a single value, and make sure this gets propagated to the StatsReporter.

If you'd like to try to make this change, we can definitely merge it. Otherwise, I'll try to get to it soon, but no promises on when.

Alternatively, you can maintain a list of values in C#, and average them and send them to the StatsRecorder in a FixedUpdate step.",hi another look code think would pretty straightforward prevent need change handling store list key instead single value make sure like try make change definitely merge otherwise try get soon alternatively maintain list average send step,issue,positive,positive,positive,positive,positive,positive
649768412,"Hi @kbonzon, the `configuration.yaml` that's generated in your run isn't the configuration loaded in the next run, it's the configuration used in your _previous run_. It's intended as an output file for record-keeping, not input. So changing those values won't change anything. 

If you want to change the settings, I'd copy `configuration.yaml` to a safe location, make your changes, and run `mlagents-learn <path_to_changed_configuration_file> --run-id=BugTest --resume`. ",hi run configuration loaded next run configuration used intended output file input wo change anything want change copy safe location make run resume,issue,negative,positive,positive,positive,positive,positive
649145224,Glad that worked; I added a note to the FAQ in https://github.com/Unity-Technologies/ml-agents/pull/4159,glad worked added note,issue,negative,positive,positive,positive,positive,positive
649108934,"Alright, as usual, the error is my own fault, and could have been prevented by simply reading the documentation more carefully. Great.. But let me explain.

Since we are creating a SIR model, we of course have more than one agent. Our goal was to track the average number of collisions of all agents in each simulation. Luckily, ml agents provides us with a method that takes the value of each agent and sends the average to the tensorboard, right? Well, kind of. If I had read the summary of `StatAggregationMethod.Average`, I would have noticed that it clearly stated that _values from the same C# environment in the same step may replace each other_.  This means that we now know, that all of our recent tensorboard statistics are wrong because only the last value supplied is actually being used. Whoops.

But why was it always 0? Well, the statistics are being added in the `Kill()` method which is called at the end of each episode for each agent in the same order as during the instantiation. But the last agent created is always already infected from the start, which is why he could not collide with other agents while being in the susceptible state. Therefore, the tensorboard always showed 0.

I would not have been able to discover this, and still trust incorrect statistics without your help. Your instructions were really easy to follow and led me precisely to my mistake and this conclusion. Thank you for that!

On a side note though: Are there any plans to allow adding multiple values per environment and step in the future?",alright usual error fault could simply reading documentation carefully great let explain since sir model course one agent goal track average number simulation luckily u method value agent average right well kind read summary would clearly stated environment step may replace know recent statistic wrong last value actually used whoop always well statistic added kill method end episode agent order last agent always already infected start could collide susceptible state therefore always would able discover still trust incorrect statistic without help really easy follow led precisely mistake conclusion thank side note though allow multiple per environment step future,issue,positive,positive,positive,positive,positive,positive
649093693,"Hi @StefanoCecere 

Thank you for the doc improvement. I've made a suggestion to clean up the contribution.",hi thank doc improvement made suggestion clean contribution,issue,positive,positive,positive,positive,positive,positive
649062670,"> Maybe try the ""Let us recheck it."" link in the message from the bot?

i clicked that twice just now, but have no idea what tasks it does or doesn't spawn 🤷 ",maybe try let u recheck link message bot twice idea spawn,issue,negative,neutral,neutral,neutral,neutral,neutral
649061834,"Maybe try the ""Let us recheck it."" link in the message from the bot?",maybe try let u recheck link message bot,issue,negative,neutral,neutral,neutral,neutral,neutral
649059784,"> Hi @codethulu
> 
> Can you please sign our CLA so that we can consider merging this?
> 
> Thanks.

@andrewcoh also it looks like i signed this on the 6th? not sure how to sign it again or why SAP hasn't processed it since it displays as signed when i click through..",hi please sign consider thanks also like th sure sign sap since click,issue,positive,positive,positive,positive,positive,positive
649058671,"> Hi @codethulu
> 
> Can you please sign our CLA so that we can consider merging this?
> 
> Thanks.

uhhh... sure? thought it would be covered by the whole employment thing though :S",hi please sign consider thanks sure thought would covered whole employment thing though,issue,positive,positive,positive,positive,positive,positive
649050507,"Hi @codethulu 

Can you please sign our CLA so that we can consider merging this?

Thanks.",hi please sign consider thanks,issue,positive,positive,positive,positive,positive,positive
648997793,"A few other questions:
* Are you on Mac or Linux?
* Can you try passing `no_graphics=True` to the `make()` method?",mac try passing make method,issue,negative,neutral,neutral,neutral,neutral,neutral
648992521,"Hmm, still not sure what's going on. I might need you to debug a bit into the python code.

Can you try clone the repo (you probably want the release_3 tag) and run the local development steps here? https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md#advanced-local-installation-for-development-2

I think the relevant parts of the code to see where the information is going bad are:
 * Where python receives the data from C#: https://github.com/Unity-Technologies/ml-agents/blob/release_3/ml-agents-envs/mlagents_envs/side_channel/stats_side_channel.py#L39
 * Where we aggregate the values for the StatsReporter: https://github.com/Unity-Technologies/ml-agents/blob/release_3/ml-agents/mlagents/trainers/agent_processor.py#L321
 * Where we actually write to Tensorboard: https://github.com/Unity-Technologies/ml-agents/blob/release_3/ml-agents/mlagents/trainers/stats.py#L177

Can you add some print statements around there and see if the values are what you'd expect? The last one is probably the most important; if the values look right there, then there's something wrong with Tensorboard displaying them (and thus, beyond my capacity to help).

Let me know how that goes, and based on what you find, I'll figure out how to debug further (but this should narrow the scope of the problem a lot).",still sure going might need bit python code try clone probably want tag run local development think relevant code see information going bad python data aggregate actually write add print around see expect last one probably important look right something wrong thus beyond capacity help let know go based find figure narrow scope problem lot,issue,negative,positive,neutral,neutral,positive,positive
648961827,"@ChristianCoenen Yes you are on the right track. When you have 30 environment, you can get 0 - 30 agent step during 1 academy step (since some of the agent might not finish during that academy step). And the ratio between agent step vs trainer step is determined by the decision period. So if the decision period is 1, you can get 0 - 30 trainer steps. 

I do agree that this is quite hard for many user to understand, and we are working on a new code refactor to make it more understandable. Hopefully that will help. 

",yes right track environment get agent step academy step since agent might finish academy step ratio agent step trainer step determined decision period decision period get trainer agree quite hard many user understand working new code make understandable hopefully help,issue,positive,positive,positive,positive,positive,positive
648961566,"Hi @mbaske, that is intended behavior - it's for improved efficiency in communication and inference, as it ensures all agents in a multi-agent scene can RequestDecision at the same time, even if they EndEpisode at different times. Otherwise, the agents would eventually not be in sync anymore. ",hi intended behavior efficiency communication inference scene time even different time otherwise would eventually sync,issue,negative,neutral,neutral,neutral,neutral,neutral
648942904,Can you please post on the Unity forum (https://forum.unity.com/forums/ml-agents.453/) for the other question?,please post unity forum question,issue,negative,neutral,neutral,neutral,neutral,neutral
648671557,"That was my first thought, too, but I just do not see where the error could be since the relevant code for that is quite simple. Outputting the value to the console before calling `StatsRecorder.Add()` produces the correct result while the tensorboard still shows 0.
```csharp
foreach (HealthStatus healthStatus in Enum.GetValues(typeof(HealthStatus))) {
    Debug.Log($""{healthStatus}: {_collisionCount[healthStatus]}"");
    Academy.Instance.StatsRecorder.Add($""Collisions/{healthStatus}"", _collisionCount[healthStatus]);
}
```
![Console Log](https://user-images.githubusercontent.com/35924749/85517595-b5d2cb00-b5ff-11ea-96de-81f9fb33a774.png)
This is just a small part of the log, but the rest is similar with just a few 0s.

![Tensorboard Statistics](https://user-images.githubusercontent.com/35924749/85517520-a18ece00-b5ff-11ea-89eb-54b5ffaaae48.png)
",first thought see error could since relevant code quite simple value console calling correct result still console log small part log rest similar statistic,issue,negative,positive,neutral,neutral,positive,positive
648626114,"It was my bad. I think I had the trainer running when I tried it a few days ago. I must have moved it into the 'trainer is not running' category because it doesn't receive data from the environment and crashes after 30s - but cause it connects, it starts the environment with approx. 20x speed.

All right, thanks!",bad think trainer running tried day ago must running category receive data environment cause environment speed right thanks,issue,negative,negative,neutral,neutral,negative,negative
648559805,"Thanks for your reply!  The  ""is not defined"" can be cleared manually in the console which .
Can I ask for another help-wanted? Can the mlagent be trained  correctly when I change the timescale in Unity for an acceleration? Because I use the ML to 2D game  which contains too much trigger and initialization by scripts. As a layman, it's hard for me to modify the scripts to use multiple training areas within the same scene or launch scenes asynchronously by dragging. So it trains slowly and I try for increasing timescale that really makes the game speed up. it seems that I have to change the decision request period. I don't know clearly whether this method can work! Maybe I have to change some setting about the ML or there exsits other methods for this situation.  Thanks again for your patience!
",thanks reply defined manually console ask another trained correctly change unity acceleration use game much trigger layman hard modify use multiple training within scene launch dragging slowly try increasing really game speed change decision request period know clearly whether method work maybe change setting situation thanks patience,issue,positive,negative,neutral,neutral,negative,negative
648528890,"PR to fix the exceptions are here: https://github.com/Unity-Technologies/ml-agents/pull/4155 (unfortunately I don't think we can easily prevent the ""is not defined"" messages from spamming the console).",fix unfortunately think easily prevent defined console,issue,negative,negative,neutral,neutral,negative,negative
648469469,"> It's what I am observing. When changing from 'default' to 'inference only', speed will increase (I am not exactly sure how much - but I guess 20x). I don't have a problem with that behavior though.

Hmm, I tried to reproduce this on our 3DBall scene;  I tried setting a single agent to Inference Only, as well as the prefab that all the agents use, and in both cases, running (without python) happened at the normal speed. If you can reproduce the behavior in one of our scenes, I'll look into it more.

The person responding on #4125 has been on vacation but will be back tomorrow. I'll remind them about it in the morning.",observing speed increase exactly sure much guess problem behavior though tried reproduce scene tried setting single agent inference well prefab use running without python normal speed reproduce behavior one look person vacation back tomorrow remind morning,issue,negative,positive,positive,positive,positive,positive
648467628,"I tried and was unable to reproduce this. My code was extremely simple:
```
        var statsRecorder = Academy.Instance.StatsRecorder;
        statsRecorder.Add(""Susceptible"", 2.0f + Random.value);
        statsRecorder.Add(""Infected"", 3.0f + Random.value);
        statsRecorder.Add(""Recovered"", 4.0f + Random.value);
```
and on the 3DBall scene, it produced this:
![image](https://user-images.githubusercontent.com/6877802/85473755-8c1a9500-b568-11ea-9bca-239a2cccfdda.png)

At this point, I suspect the error is in your code, but hard to say without seeing more of what you're doing.
",tried unable reproduce code extremely simple susceptible infected scene produced image point suspect error code hard say without seeing,issue,negative,negative,negative,negative,negative,negative
648348909,"Thanks for the callstacks, I think I see the problems now (and I can reproduce them with the 3D Raycasts in our examples).
The internal tracker IDs for these are MLA-1106 and MLA-1109.",thanks think see reproduce internal tracker,issue,negative,positive,neutral,neutral,positive,positive
648338955,"Are there any of these that you think are caused by ML-Agents? The only one I see is ""Connected to Unity environment ...""",think one see connected unity environment,issue,negative,neutral,neutral,neutral,neutral,neutral
647980800,"> @smshehryar i fixed the problem! thank you!

how did u fix it?",fixed problem thank fix,issue,negative,positive,neutral,neutral,positive,positive
647945262,"Thanks a lot! I didn't consider the --initialize-from argument. It's indeed covering usecase 2.

For now, I don't miss any metrics - it might change when I shift to more complex problems over time. Thanks for the mentioning of the StatsRecorder. Will look into it then.

It's what I am observing. When changing from 'default' to 'inference only', speed will increase (I am not exactly sure how much - but I guess 20x). I don't have a problem with that behavior though.

<br>
Off-topic: Another user and I still waiting for a final reply to my issue #4125 - I think it's a quick answer for you and would close the issue. Would be great if you could check it out. Thanks!

",thanks lot consider argument indeed covering miss metric might change shift complex time thanks look observing speed increase exactly sure much guess problem behavior though another user still waiting final reply issue think quick answer would close issue would great could check thanks,issue,positive,positive,positive,positive,positive,positive
647904877,"Thanks a lot ! The problem seems to be solved by changing Project Settings > Physics 2D > Queries Start In Colliders. The rays can normally hit the other objects. But when I add RayPerceptionSensor2D component, the error is still reported. It is like the detectable tag is null when added although it doesn't matter.

the error is:
NullReferenceException: Object reference not set to an instance of an object
Unity.MLAgents.Sensors.RayPerceptionSensor.PerceiveSingleRay (Unity.MLAgents.Sensors.RayPerceptionInput input, System.Int32 rayIndex, Unity.MLAgents.Sensors.DebugDisplayInfo+RayInfo& debugRayOut) (at E:/important data/ml-agents-release_2/ml-agents-release_2/com.unity.ml-agents/Runtime/Sensors/RayPerceptionSensor.cs:465)
Unity.MLAgents.Sensors.RayPerceptionSensorComponentBase.OnDrawGizmosSelected () (at E:/important data/ml-agents-release_2/ml-agents-release_2/com.unity.ml-agents/Runtime/Sensors/RayPerceptionSensorComponentBase.cs:287)
UnityEngine.GUIUtility:ProcessEvent(Int32, IntPtr)

The next error is also reported when I am typing the tag name. 
Tag: Ite is not defined.
UnityEngine.GameObject:CompareTag(String)
Unity.MLAgents.Sensors.RayPerceptionSensor:PerceiveSingleRay(RayPerceptionInput, Int32, RayInfo&) (at E:/important data/ml-agents-release_2/ml-agents-release_2/com.unity.ml-agents/Runtime/Sensors/RayPerceptionSensor.cs:467)
Unity.MLAgents.Sensors.RayPerceptionSensorComponentBase:OnDrawGizmosSelected() (at E:/important data/ml-agents-release_2/ml-agents-release_2/com.unity.ml-agents/Runtime/Sensors/RayPerceptionSensorComponentBase.cs:287)
UnityEngine.GUIUtility:ProcessEvent(Int32, IntPtr)

The above errors can be cleared when I finish the tag add. ",thanks lot problem project physic start normally hit add component error still like detectable tag null added although matter error object reference set instance object input next error also tag name tag defined string finish tag add,issue,negative,positive,positive,positive,positive,positive
647886481,"Could you please provide the full callstack from the NullReferenceException? I can't reproduce the error.

2D raycasts behave differently than 3D ones by default - they will ""hit"" objects that they start in. You can change this behavior in Project Settings > Physics 2D > Queries Start In Colliders. If that's too big of a change for your project, you can change the start position of the rays by adding another GameObject parented to the Agent's GameObject, adding the RayPerceptionSensorComponent2D to that, and adjusting it's transform (and make sure the Use Child Sensors checkbox is checked in the Agent component in the Inspector).
",could please provide full ca reproduce error behave differently default hit start change behavior project physic start big change project change start position another agent transform make sure use child checked agent component inspector,issue,negative,positive,positive,positive,positive,positive
647882802,"I updated the documentation to reflect the actual behavior. If you have further problems, please open a new issue (and fill out the template next time).",documentation reflect actual behavior please open new issue fill template next time,issue,negative,positive,neutral,neutral,positive,positive
647808627,"Thank you for your suggestion.
- Cause: There may be no agent in the scene 
This env is provided by default_registry so I assume there is an agent in the scene.

- Cause: On OSX, the firewall may be preventing communication with the environment. Solution: Add the built environment binary to the list of exceptions on the firewall by following instructions.
I have checked there is no firewall.

- Cause: An error happened in the Unity Environment preventing communication. Solution: Look into the log files generated by the Unity Environment to figure what error happened.
This was the log 
'''
Mono path[0] = '/tmp/ml-agents-binaries/binaries/Basic-b1b94a0ae13eef9d91f9d8db1e5770c5/Startup/Startup_Data/Managed'
Mono config path = '/tmp/ml-agents-binaries/binaries/Basic-b1b94a0ae13eef9d91f9d8db1e5770c5/Startup/Startup_Data/MonoBleedingEdge/etc'
PlayerConnection initialized from /tmp/ml-agents-binaries/binaries/Basic-b1b94a0ae13eef9d91f9d8db1e5770c5/Startup/Startup_Data (debug = 0)
PlayerConnection initialized network socket : 0.0.0.0 55404
Multi-casting ""[IP] 172.31.17.102 [Port] 55404 [Flags] 2 [Guid] 946481988 [EditorId] 429131514 [Version] 1048832 [Id] LinuxPlayer(172.31.17.102) [Debug] 0 [PackageName] LinuxPlayer [ProjectName] UnityEnvironment"" to [225.0.0.222:54997]...
Started listening to [0.0.0.0:55404]
Preloaded 'libgrpc_csharp_ext.x64.so'
Unable to preload the following plugins:
	lib_burst_generated.so
PlayerConnection already initialized - listening to [0.0.0.0:55404]
Unable to load player prefs
Desktop is 0 x 0 @ 0 Hz
Invalid initial resolution 80 x 80 - forcing to 100 x 100
'''

- Cause: You have assigned HTTP_PROXY and HTTPS_PROXY values in your environment variables. Solution: Remove these values and try again.
I have checked the environment variables but they are not set.",thank suggestion cause may agent scene provided assume agent scene cause may communication environment solution add built environment binary list following checked cause error unity environment communication solution look log unity environment figure error log mono path mono path network socket port version id listening unable following already listening unable load player invalid initial resolution forcing cause assigned environment solution remove try checked environment set,issue,negative,negative,negative,negative,negative,negative
647791929,"I think both of your usecases are supported. Here are tensorboard graphs for 3 runs:
```
mlagents-learn config/ppo/3DBall.yaml --run-id=3dball_new
mlagents-learn config/ppo/3DBall.yaml --run-id=3dball_new --resume --inference
mlagents-learn config/ppo/3DBall.yaml --run-id=3dball_newer  --initialize-from=3dball_new  --inference
```
for about 60K steps each. 

![image](https://user-images.githubusercontent.com/6877802/85339091-48585a80-b498-11ea-8202-9cb7bf16f3f2.png)

So at least the rewards are tracked when resuming (usecase 1), and you can use --initialize-from to provide a new run id (usecase 2). Are there other tensorboard metrics you expected to see during inference?

For your scenario 1, are you expecting the 20x playback speed, or that's what you're observing? I don't think this will happen currently.

Also note that you can use the StatsRecorder interface in C# to write custom metrics from C# to appear in tensorboard.",think resume inference inference image least tracked use provide new run id metric see inference scenario playback speed observing think happen currently also note use interface write custom metric appear,issue,negative,negative,neutral,neutral,negative,negative
647790856,"thanks :)

On Tue, 23 Jun 2020 at 00:38, kahabal <notifications@github.com> wrote:

> @chriselion <https://github.com/chriselion> sorry, it was solved, i had a
> problem of versioning and for some reasons i was not able to use the
> updated version of the package. thank you!!!
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/3956#issuecomment-647782419>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AI75KO6VFE2UOFX3SF43G63RX7FLRANCNFSM4M7LUHFA>
> .
>
",thanks tue wrote sorry problem able use version package thank reply directly view,issue,negative,positive,neutral,neutral,positive,positive
647782419,"@chriselion sorry, it was solved, i had a problem of versioning and for some reasons i was not able to use the updated version of the package. thank you!!!",sorry problem able use version package thank,issue,negative,neutral,neutral,neutral,neutral,neutral
647781682,@kahabal This should be fixed in the 0.16.1 version of the mlagents python package (see https://github.com/Unity-Technologies/ml-agents/releases/tag/release_2). Are you still seeing this problem?,fixed version python package see still seeing problem,issue,negative,positive,neutral,neutral,positive,positive
647773710,Hey just a little bump to let the wonderful BugFixers know that the Hummingbird Premium Learn course shows this error in the videos.  I ran the course using Windows 10-1909 64 bit; Anaconda env Python 3.7 ; Unity 2019.3 then 2019.4; the latest MLAgents from the Package Manager ... about 3 days ago..  This **5004 port issue persists** -- but does not stop the training - able to get a nice ppo Hummingbird.NN... Would really like to be able to control port listening for security purposes.... Thanks.,hey little bump let wonderful know hummingbird premium learn course error ran course bit anaconda python unity latest package manager day ago port issue stop training able get nice would really like able control port listening security thanks,issue,positive,positive,positive,positive,positive,positive
647762932,"I'm not sure what's causing that, but you don't need that package in your project.

I'm removing it from the ML-Agents project here: https://github.com/Unity-Technologies/ml-agents/pull/4146",sure causing need package project removing project,issue,negative,positive,positive,positive,positive,positive
647680372,"This is a warning not an error, so it's not considered a breaking change (you could make it an error by passing `true` as a second argument).",warning error considered breaking change could make error passing true second argument,issue,negative,positive,positive,positive,positive,positive
647551191,"> I think there may be more straightforward examples like uploading a picture and doing some image classification. However, let me know if I am missing some additional context.

@unityjeffrey - You're definitely right that there are examples of browser-based ML which are more straightforward to implement.  Image Classification, doodle classifiers, etc. are all available as part of [ml5.js](https://learn.ml5js.org/docs/#/) or TFJS directly and very easy to implement.  From a student perspective, however, reinforcement learning presents a really valuable opportunity for learning ML because an 'agent' whose actions are being controlled by an RL brain is a much more visible analogy than traditional ML 'Hello World' projects. What I mean is that the 'decision making' process happens in real time and happens over time.  Rather than being presented with a single input and an output, we are presented with a continuous series of inputs and outputs, which, taken in aggregate, seems to form the 'behavior' of an agent.  Because of this, RL is also more fun!


Reinforcement Learning has (up until now) been quite challenging to implement in the browser. 
For context, the browser-based RL libraries which include training capabilities ([ConvnetJS](https://cs.stanford.edu/people/karpathy/convnetjs/), [REINFORCEjs](https://cs.stanford.edu/people/karpathy/reinforcejs/), among others) seem mostly to be stale. Tensorflow.js _does_ include some 'official' RL examples ([CartPole](https://github.com/tensorflow/tfjs-examples/tree/master/cart-pole) and [SnakeDQN](https://github.com/tensorflow/tfjs-examples/tree/master/snake-dqn)), which are exciting examples but difficult to train / implement.  This is an area in which Unity ML Agents would allow teachers / curriculum developers to develop custom models for specific demonstrations. 

ML-Agents could really lower the barrier to entry for teaching about RL, and I think interoperability with TFJS would help that.  

I think this is just one of numerous places where having a Unity-trained RL model in the browser (outside of a WebGL build) would be exciting and fun.  I am happy to brainstorm others (the first one that comes to mind is having an agent interact directly with DOM elements) if it would be helpful for this feature-request / discussion.



",think may straightforward like picture image classification however let know missing additional context definitely right straightforward implement image classification doodle available part directly easy implement student perspective however reinforcement learning really valuable opportunity learning whose brain much visible analogy traditional world mean making process real time time rather single input output continuous series taken aggregate form agent also fun reinforcement learning quite implement browser context include training among seem mostly stale include exciting difficult train implement area unity would allow curriculum develop custom specific could really lower barrier entry teaching think would help think one numerous model browser outside build would exciting fun happy first one come mind agent interact directly dom would helpful discussion,issue,positive,positive,positive,positive,positive,positive
647346552,"¿Alguien sabe como puedo solucionar este oos errores en unity que no me permiten correr la aplicación?

Errores:
Library\PackageCache\com.unity.package-validation-suite@0.7.15-preview\Editor\Extension\Packages\ValidationSuiteExtension.cs(3,34): error CS0234: The type or namespace name 'UI' does not exist in the namespace 'UnityEditor.PackageManager' (are you missing an assembly reference?)

Library\PackageCache\com.unity.package-validation-suite@0.7.15-preview\Editor\Extension\Packages\ValidationSuiteExtension.cs(13,47): error CS0246: The type or namespace name 'IPackageManagerExtension' could not be found (are you missing a using directive or an assembly reference?)

Gracias.

",sabe en unity la error type name exist missing assembly reference error type name could found missing directive assembly reference,issue,negative,negative,negative,negative,negative,negative
646841790,"The ```flatten_branched``` flag needed to be set to ```True``` so the space type would be Discrete instead of Multidiscrete.

Simply change:
```env = UnityToGymWrapper(unity_env, 0, uint8_visual=True)```
With:
```env = UnityToGymWrapper(unity_env, flatten_branched = True, uint8_visual=True)```

The README.md should be updated",flag set true space type would discrete instead simply change true,issue,positive,positive,positive,positive,positive,positive
646728935,"SOLVED: unfortunately to get the new version i had to uninstall the old one first.
you can close it",unfortunately get new version old one first close,issue,negative,negative,neutral,neutral,negative,negative
646578987,"one more question, when i tipi mlagents-learn --version i dont got the version number that i am currently running... how can i see which one is really running?

thanks",one question version dont got version number currently running see one really running thanks,issue,negative,positive,positive,positive,positive,positive
646368033,"Alright then, using this in ML-Agents Release 3 would work!
```
using Unity.MLAgents.SideChannels;

void Awake()
{
    mazeChannel = new MazeChannel();
    SideChannelsManager.RegisterSideChannel(mazeChannel);
}
```",alright release would work void awake new,issue,negative,positive,positive,positive,positive,positive
646292991,"@chriselion Thank you for your help, I found that my code was responsible for the freezing. I used recursive function for calculation it was working while playing normally, yet it produces freezing while training. ",thank help found code responsible freezing used recursive function calculation working normally yet freezing training,issue,positive,positive,positive,positive,positive,positive
646276303,"Hi @AidanNelson - it feels like ML-A + Unity would not be a good solution for what you are describing (teaching Javascript with a small RL component).  I think there may be more straightforward examples like uploading a picture and doing some image classification.  However, let me know if I am missing some additional context.",hi like unity would good solution teaching small component think may straightforward like picture image classification however let know missing additional context,issue,positive,positive,positive,positive,positive,positive
646213294,"Sure!  

Part of what is great about the Unity ML-Agents Framework is the ability to quickly spin up a training environment and get started experimenting with RL **without** a deep knowledge of the difference between SAC and PPO or the ability to tune the various training hyperparameters.  ML-Agents provides an easier (and more powerful) access point to training RL than I've seen elsewhere, in an environment which many people are already familiar with.  That said, there are a number of contexts in which someone might want the final version of their project to exist online outside of a Unity WebGL build.  The first one that comes to mind is a classroom environment.

Teachers teaching Javascript (in a middle or high school environment, for instance) with the ability to incorporate a simple RL model into their curriculum would be able to:
* live-code on an environment in javascript and see results without needing to first compile,
* have students build applications using these models without needing to know how to use Unity,
* have students build applications using these models without needing to have Unity installed on their machines (in computer lab environment, for example),
* interact with other Javascript libraries / frameworks / browser APIs (webcam, etc.) in a familiar manner,
* host javascript sketches on various platforms ([p5.js web editor](https://editor.p5js.org/), [Glitch](https://glitch.com/), etc.)

Personally, I want to work with incorporating models into projects using [p5.js](https://github.com/processing/p5.js/) and [ml5.js](https://github.com/ml5js/ml5-library), libraries which have large and welcoming communities around them and are specifically geared towards accessibility and education.

Any thoughts?  

",sure part great unity framework ability quickly spin training environment get without deep knowledge difference sac ability tune various training easier powerful access point training seen elsewhere environment many people already familiar said number someone might want final version project exist outside unity build first one come mind classroom environment teaching middle high school environment instance ability incorporate simple model curriculum would able environment see without needing first compile build without needing know use unity build without needing unity computer lab environment example interact browser familiar manner host various web editor personally want work large welcoming around specifically geared towards accessibility education,issue,positive,positive,positive,positive,positive,positive
646197121,"Does this happen with the provided example scenes? If not, the problem is likely due to a bug in your code. If you post it, I can try to help debug it",happen provided example problem likely due bug code post try help,issue,negative,negative,neutral,neutral,negative,negative
646146558,hi @AidanNelson - quick question for you.  Can you describe a use case where you would want to run an ML-Agents model via Tensorflow.js?  Barracuda allows for ML-Agents models to be run in WebGL games.,hi quick question describe use case would want run model via barracuda run,issue,negative,positive,positive,positive,positive,positive
646141341,"Hi,
Thanks for catching this; the actual class name is [SideChannel**s**Manager](https://github.com/Unity-Technologies/ml-agents/blob/release_3/com.unity.ml-agents/Runtime/SideChannels/SideChannelsManager.cs).

I'll either rename the class to be consistent with the python version, or update the docs.",hi thanks catching actual class name manager either rename class consistent python version update,issue,negative,positive,positive,positive,positive,positive
646130171,"The gym and baseline versions are compatible according to pip. I also posted the issue in the baseline repo. The deepq function of baseline takes in the environment variable which is set equal to UnityToGymWrapper() function. Ie, env = UnityToGymWrapper(args**).

It is when deepq calls on env.action_space.n do I get the error. Could it be possible the error is due to something that happens in the UnityToGymWrapper function?

my gym version: 0.15.4
baselines version: 0.1.6",gym compatible according pip also posted issue function environment variable set equal function ie get error could possible error due something function gym version version,issue,negative,negative,neutral,neutral,negative,negative
646071697,"You need put more information to let others help you with this issue, follow the template of the post new issue, show version of your package, at least show others how to reproduce the issue.",need put information let help issue follow template post new issue show version package least show reproduce issue,issue,negative,negative,neutral,neutral,negative,negative
646061117,"@unityjeffrey I would love to open this thread up again now that MLAgents is in an official release. 
 
The ability to export models as Tensorflow `SavedModel` format would allow for interoperability with Tensorflow.js, a very well supported web-based inference engine with a large community around it.  The ability to use models trained in Unity on the web (even outside of the Unity ecosystem on the web) would open up a lot of possibilities for Unity ML Agents as a training platform.  The advantages of web-based inference are numerous.  My particular interest is in creating web based educational tools around machine learning.  

Here is a model trained in Unity, then exported as a Tensorflow SavedModel, converted for use with Tensorflow.js, and loaded in a javascript sketch: https://www.aidanjnelson.com/balance-ball/

Thoughts?",would love open thread official release ability export format would allow well inference engine large community around ability use trained unity web even outside unity ecosystem web would open lot unity training platform inference numerous particular interest web based educational around machine learning model trained unity converted use loaded sketch,issue,positive,positive,positive,positive,positive,positive
645519172,"Also @harperj  while I understand you don't have time to troubleshoot my specific agent, do you have a general answer to the question of how to best encode arbitrary complexity in vector observations? Specifically would this work:
For each raycast store two observations:
1. distance to hit (or -1 if no hit)
2. a float value that is a map to a specific object and state, e.g.
1 if switch A on
2 if switch A off
3 if switch B on
4 if switch B off
5 if enemy A
6 if ally A
7 if road
8 if obstacle
...

That way infinite states can be stored in just 2 vector observations per raycast, which is the only way I see to use vector observations without having 1000+ vector observations. If you instead store 1 or 0 for each state, then with just 10 different states and 100 raycasts, you already are at 1000 vector observations, which obviously does not scale. This is how the example environments were set up, but because they were so simple you could get away with just 3 or so states per ray cast (e.g. block or wall) but for any non trivial environment there's tens or hundreds of states the agent needs to distinguish between.

If there's not a simple answer or if there's a better place to ask this question, feel free to point me there. Thanks!",also understand time specific agent general answer question best encode arbitrary complexity vector specifically would work store two distance hit hit float value map specific object state switch switch switch switch enemy ally road obstacle way infinite vector per way see use vector without vector instead store state different already vector obviously scale example set simple could get away per ray cast block wall non trivial environment agent need distinguish simple answer better place ask question feel free point thanks,issue,positive,positive,positive,positive,positive,positive
645486452,"This issue is explained here: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/FAQ.md#visual-c-dependency-windows-users

You need to install the [Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019.(https://support.microsoft.com/en-my/help/2977003/the-latest-supported-visual-c-downloads).
",issue need install visual visual studio,issue,negative,neutral,neutral,neutral,neutral,neutral
645485882,"Great to know! Again just to make sure - to use multiple parallel environments, you need to do a build and have an executable right, rather than using the Unity editor?

What are some examples of the ""fairly complex agents""? Very eager to see the examples!",great know make sure use multiple parallel need build executable right rather unity editor fairly complex eager see,issue,positive,positive,positive,positive,positive,positive
645283828,"> WARNING: The scripts mlagents-learn.exe and mlagents-run-experiment.exe are installed in 'C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\Scripts' which is not on PATH.
> Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.

He movido todos los scipts que indican los warning (mlagents-learn.exe, mlagents-run-experiment.exe, estimator_ckpt_converter.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe) a mi directorio raíz (C:\Users\Usuario\).
Posteriormente con el comando .\mlagents-learn --help me indica esto:
PS C:\Users\Usuario> .\mlagents-learn --help
Traceback (most recent call last):
  File ""C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.2032.0_x64__qbz5n2kfra8p0\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.2032.0_x64__qbz5n2kfra8p0\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: No se puede encontrar el módulo especificado.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.2032.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.2032.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Usuario\mlagents-learn.exe\__main__.py"", line 4, in <module>
  File ""C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\mlagents\trainers\learn.py"", line 12, in <module>
    from mlagents import tf_utils
  File ""C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\mlagents\tf_utils\__init__.py"", line 1, in <module>
    from mlagents.tf_utils.tf import tf as tf  # noqa
  File ""C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\mlagents\tf_utils\tf.py"", line 3, in <module>
    import tensorflow as tf  # noqa I201
  File ""C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.2032.0_x64__qbz5n2kfra8p0\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.2032.0_x64__qbz5n2kfra8p0\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: No se puede encontrar el módulo especificado.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Visite la web pero no logré solucionar el problema.

Muchas gracias por la ayuda.",warning path consider directory path prefer suppress warning use indican warning mi con el help help recent call last file line module import file line module file line description file line return name file file line return spec load se el handling exception another exception recent call last file line file line code file line module file line module import file line module import file line module import file line module import file line module import file line module raise recent call last file line module import file line module file line description file line return name file file line return spec load se el load native see common include entire stack trace error message help visite la web el la,issue,negative,negative,neutral,neutral,negative,negative
645081649,"> Secondly, could you share what hardware you used? As I mentioned, even with a <1 year old gaming PC I was at 112% CPU usage and the GPU was barely being used so I'm not sure I could have run more Unity instances since I already seemed maxed out CPU wise.

I'm not sure of the system specifics, but generally 112% CPU is referring to 1.12 CPU cores. Modern gaming PCs frequently have 8+ cores. I have specifically tested multiple environments in parallel on a MacBook Pro 13"" (2018 I believe) and seen the benefit of parallel environments.  Since each environment is an independent application with different compute requirements, the specific number of environments / areas within an environment is something you'll need to tune.

> Thank you for the prompt replies and all your work on MLAgents! I'd love to see it make it to the next level of being able to train more complex agents, but I do appreciate all the hard work you have done thus far!

Thanks, :-) -- we've already seen success training fairly complex agents, but since different games / environments can require fairly different observations/actions/rewards it can be difficult to provide general advice to someone working on a new environment. We are working on more complex examples, so stay tuned.",secondly could share hardware used even year old gaming usage barely used sure could run unity since already wise sure system generally modern gaming frequently specifically tested multiple parallel pro believe seen benefit parallel since environment independent application different compute specific number within environment something need tune thank prompt work love see make next level able train complex appreciate hard work done thus far thanks already seen success training fairly complex since different require fairly different difficult provide general advice someone working new environment working complex stay tuned,issue,positive,positive,neutral,neutral,positive,positive
645060439,"Thanks! Good to know a balance of multiple arenas and multiple Unity instances may work best. Just to clarify - to use multiple Unity instances we must first build an executable right? Once the model and rewards are all worked out that can be fine, but in the beginning I often change things and watch in the Editor to see where the AI is struggling and how to adjust.

Secondly, could you share what hardware you used? As I mentioned, even with a <1 year old gaming PC I was at 112% CPU usage and the GPU was barely being used so I'm not sure I could have run more Unity instances since I already seemed maxed out CPU wise.

Thanks! I'd love to help save future people the time with those limitation notes, including the hardware it was trained on, since that can make a big difference. I think it's crucial people have correctly set expectations of what MLAgents can do, what they can do on their current hardware before they invest the time and money to build a machine learning rig just to find out it doesn't help!
And I opened a forum post here as per your suggestion. If you know any other places or people who could point me to more complex example environments and agents I'd appreciate it!
https://forum.unity.com/threads/what-is-the-most-complex-ai-agent-you-have-trained-and-how-long-did-it-take.913745/

Thank you for the prompt replies and all your work on MLAgents! I'd love to see it make it to the next level of being able to train more complex agents, but I do appreciate all the hard work you have done thus far!",thanks good know balance multiple multiple unity may work best clarify use multiple unity must first build executable right model worked fine beginning often change watch editor see ai struggling adjust secondly could share hardware used even year old gaming usage barely used sure could run unity since already wise thanks love help save future people time limitation hardware trained since make big difference think crucial people correctly set current hardware invest time money build machine learning rig find help forum post per suggestion know people could point complex example appreciate thank prompt work love see make next level able train complex appreciate hard work done thus far,issue,positive,positive,positive,positive,positive,positive
645058394,"> That said, as per this comment, it seems just having one Unity instance with multiple arenas is better?

Actually, in our experience multiple Unity instances with multiple arenas often results in the best performance. In practice most environments have work that needs to be synchronized in a main thread which becomes a bottleneck at some point. We found improvements with parallel environments for almost all of our examples environments.

Also thanks for the suggestions. I think (1) makes a lot of sense and I'll track it as a potential addition to the docs.  (2) feels more like something that would be better maintained by the community.",said per comment one unity instance multiple better actually experience multiple unity multiple often best performance practice work need synchronized main thread becomes bottleneck point found parallel almost also thanks think lot sense track potential addition like something would better community,issue,positive,positive,positive,positive,positive,positive
645054172,"Thanks for the blog post link, though again that's a much simpler agent than what I've been trying though it did confirm the timing of taking 11 hours to train and about 1 million runs being a realistic max for MLAgents. That said, as per this comment, it seems just having one Unity instance with multiple arenas is better? I'm already doing that. As per the comment question, it's unclear to me why `num-envs` would ever be used, unless in that game there wasn't a good way to create multiple arenas within the same Unity instance. Especially since using num-envs requires pre-building an executable each time, so if you change anything you have to rebuild, which is time consuming. Or am I missing something here?
https://github.com/Unity-Technologies/ml-agents/issues/2298#issuecomment-513895001

Yep, GPU config seemed to be working for me, but as things were CPU bound, it didn't matter.

Yes, my intuition was the same - too many vector observations.
I totally understand you can't troubleshoot my particular use case; I didn't expect you to :) What I would ask for is to add one or both of:

1. a limitation note mentioning all the above (that trying to train more complex agents than the example environments likely will not work, that visual observations are unlikely to work, that 1 million runs can take about 12 hours to complete and that GPUs don't really help). That information would have saved me a month of time :)

2. more complex example environments (and how many steps were required to train each one). Maybe this can be crowdsourced? I'd be happy to share my experiences and I'd love to hear the experiences of other people as well. Last I checked the forum it didn't seem very active and searching for ""gpu"" gives an error, but I'm very curious how far others have been able to push MLAgents and what AIs have been created, and how they were trained!",thanks post link though much simpler agent trying though confirm timing taking train million realistic said per comment one unity instance multiple better already per comment question unclear would ever used unless game good way create multiple within unity instance especially since executable time change anything rebuild time consuming missing something yep working bound matter yes intuition many vector totally understand ca particular use case expect would ask add one limitation note trying train complex example likely work visual unlikely work million take complete really help information would saved month time complex example many train one maybe happy share love hear people well last checked forum seem active searching error curious far able push trained,issue,positive,positive,positive,positive,positive,positive
645047045,"> it seems that even though visual observations are an option, practically speaking they should not be used?

Generally speaking, visual observations are simpler to set up but result in slower training (though there may be exceptions).  [This blog post](https://blogs.unity3d.com/2019/04/15/unity-ml-agents-toolkit-v0-8-faster-training-on-real-games/) we shared last year is an example of an environment in which we found visual observations to be the best approach.

> Re: GPU acceleration - as I noted I did NOT see improvement in performance - do I need to change a setting?

See the [Tensorflow GPU support page](https://www.tensorflow.org/install/gpu) for the trainer side of this. The GPU should be used by the Unity environment without additional configuration.

> Are thousands or tens of thousands of vector observations expected and what we should be using?

It's really hard to say, unfortunately. We don't yet have good answers as to the best observation space setup for any given environment. I would be concerned with that large of an observation space and my intuition is that there is a better way.

Unfortunately, though, we really don't have the bandwidth to try to troubleshoot how to get custom environments training. You might consider asking on [the forum](https://forum.unity.com/forums/ml-agents.453/), as other folks in the community may have practical advice to share.",even though visual option practically speaking used generally speaking visual simpler set result training though may post last year example environment found visual best approach acceleration noted see improvement performance need change setting see support page trainer side used unity environment without additional configuration vector really hard say unfortunately yet good best observation space setup given environment would concerned large observation space intuition better way unfortunately though really try get custom training might consider forum community may practical advice share,issue,positive,positive,positive,positive,positive,positive
645021695,"Yes, clarifications would be very helpful @harperj 
It would be super helpful to have some non trivial example environments, ideally easy (which are what I consider the tasks I'm currently trying to train for but have been struggling for over a month now), medium and hard. And / or a statement that MLAgents currently only works (well) for trivial (small model, simple behaviors) agents currently, but more complex behaviors are being worked on.

Re: GPU acceleration - as I noted I did NOT see improvement in performance - do I need to change a setting? Also as mentioned, my CPU was at 112% so it still seemed CPU bound, and that was with just 1 environment. Are there any working examples of GPU with visual observations and / or parallel environments? I did notice that most of the example environments that had visual observations said they ""do not train"" - so it seems that even though visual observations are an option, practically speaking they should not be used?

Yes, I read about visual observations being more difficult but also that to capture ""arbitrary complexity"" you may need them. Where is the ""break even"" point for when there's so many vector observations that visual would be better (e.g. if there are 1000+ vector observations?). Again all the example environments seemed very simple. In my (still pretty simple) case I have a target to shoot vs a hook switch vs a switch to carry a box onto vs a switch to jump on AND each of those have an ""on"" and ""off"" state - so that's already at least 8 states just for simple switches. In the examples I saw a general strategy of storing the distance to the raycast hit AND a key for EACH object type that is 1 or 0 to tell what is hit. So for 250 raycasts, that means each would need to store at least 9 observations right? So already 2500 vector observations. Or is there a more compact way to store observations? Such as assigning a float number to each state, so it's always only 2 observations per raycast but can encode infinite states? This is likely required since you can't change the number of observations later so if you add in a new element to the game, you'd have to retrain from scratch. So we need a way for an agent to differentiate between all game objects (road vs grass vs switch A on vs switch A off vs switch B on vs ..). For the example environments and the self driving car I used vector observations for it was very simple - the car is either on the road or not, and that still required over 500 vector observations to drive well, and that was just on a single track. Are thousands or tens of thousands of vector observations expected and what we should be using?",yes would helpful would super helpful non trivial example ideally easy consider currently trying train struggling month medium hard statement currently work well trivial small model simple currently complex worked acceleration noted see improvement performance need change setting also still bound environment working visual parallel notice example visual said train even though visual option practically speaking used yes read visual difficult also capture arbitrary complexity may need break even point many vector visual would better vector example simple still pretty simple case target shoot hook switch switch carry box onto switch jump state already least simple saw general strategy distance hit key object type tell hit would need store least right already vector compact way store float number state always per encode infinite likely since ca change number later add new element game retrain scratch need way agent differentiate game road grass switch switch switch example self driving car used vector simple car either road still vector drive well single track vector,issue,positive,negative,neutral,neutral,negative,negative
644998400,"Hi @lukemadera -- I think some clarification might be needed here with regard to ML-Agents performance. ML-Agents training includes Python/Tensorflow trainers as well as the Unity environment.

For the trainers, we are generally using relatively small models for our examples. Tensorflow supports GPU acceleration out of the box -- but the improvement provided is going to be small when the network and batch size are small. That said, we support both larger fully-connected networks and two different visual encoders (CNN and ResNet). You may see some improvement in performance by using GPU acceleration.

The environments are built using Unity, which naturally is able to take advantage of GPUs for rendering. If you're using visual observations, Unity will need to render them and you should see a benefit from having a powerful GPU.  Even better, with a powerful GPU you'll be able to run more parallel environments without performance degrading.  If the environment uses vector observations, the GPU won't generally provide any benefit (and you can turn off the rendering with the ""no graphics"" option).

In our experience the limiting factor in most training scenarios is the environment / simulation time. This can be due to the physics simulation, rendering, or other simulation logic.  If you need steps quickly, one of the best first steps is to increase the parallel environments.

Another thing to consider is that it is more difficult to learn from visual observations than from vector observations.  If you're looking to reduce the number of steps necessary, consider whether you can provide better/simpler observations or a better reward function.
",hi think clarification might regard performance training well unity environment generally relatively small acceleration box improvement provided going small network batch size small said support two different visual may see improvement performance acceleration built unity naturally able take advantage rendering visual unity need render see benefit powerful even better powerful able run parallel without performance degrading environment vector wo generally provide benefit turn rendering graphic option experience limiting factor training environment simulation time due physic simulation rendering simulation logic need quickly one best first increase parallel another thing consider difficult learn visual vector looking reduce number necessary consider whether provide better reward function,issue,positive,positive,positive,positive,positive,positive
644977481,"@xiaomaogy thanks for your answer!

I did some more research and would like to know if I am on the right track.

1. A DecisionPeriod of 5 means that the Agent will request a decision every 5 Academy steps.
    - _found in the decision requester script as a comment_
2. AgentScript -> max_step == Academy steps
    - _Tested_
3. Tensorboard -> Episode Length == Decision steps
    - _Tested_

**Conclusion:**
When I have 30 environments (1 agent in each environment) in one scene, then I can get 0-30 trainer steps during **1** Academy step? And the trainer step counter is increased by **1** each time a decision is requested from one of the 30 agents right?

That would be the only way which explains why the Academy MaxStepCount is lower than the trainer count and that that gap increases the lower the decision period / the more agents in a scene.

**Note:**
I think this is quite hard for many users to grasp (I got no answer in the Forum despite quite a few views before opening it here as an issue). The best I could find about this in the ML Agents doc was [The Simulation and Training Process](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Design.md). But that didn't answer many of my questions. I think some additional documentation might help some users.",thanks answer research would like know right track agent request decision every academy decision requester script academy episode length decision conclusion agent environment one scene get trainer academy step trainer step counter time decision one right would way academy lower trainer count gap lower decision period scene note think quite hard many grasp got answer forum despite quite opening issue best could find doc simulation training process answer many think additional documentation might help,issue,positive,positive,positive,positive,positive,positive
644914069,"(Sorry, my Spanish is not very good)

I think this is the important warning message:
```
WARNING: The scripts mlagents-learn.exe and mlagents-run-experiment.exe are installed in 'C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\Scripts' which is not on PATH.
Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
```
The scripts is not in your PATH, so it doesn't know where to look for `mlagents-learn`. I don't user PowerShell, but I think this is how you can add it to your path: https://stackoverflow.com/a/37663645/224264

",sorry good think important warning message warning path consider directory path prefer suppress warning use path know look user think add path,issue,negative,positive,positive,positive,positive,positive
644910766,"> It sounds like your path name is too long: https://stackoverflow.com/a/55189256/224264

Finalmente creo que pude instalar el paquete mlagents cambiando la variable LongPathsEnabled a 1, pero al usar el comando mlagents-learn no me lo reconoce:

PS C:\Users\Usuario> pip3 install --user D:\ML_Unity\ml-agents-release_3\ml-agents\
Processing d:\ml_unity\ml-agents-release_3\ml-agents
Requirement already satisfied: grpcio>=1.11.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents==0.17.0) (1.29.0)
Requirement already satisfied: h5py>=2.9.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents==0.17.0) (2.10.0)
Requirement already satisfied: mlagents_envs==0.17.0 in d:\ml_unity\ml-agents-release_3\ml-agents-envs (from mlagents==0.17.0) (0.17.0)
Requirement already satisfied: numpy<2.0,>=1.13.3 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents==0.17.0) (1.18.5)
Requirement already satisfied: Pillow>=4.2.1 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents==0.17.0) (7.1.2)
Requirement already satisfied: protobuf>=3.6 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents==0.17.0) (3.12.2)
Requirement already satisfied: pyyaml>=3.1.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents==0.17.0) (5.3.1)
Collecting tensorflow<3.0,>=1.7
  Using cached tensorflow-2.2.0-cp37-cp37m-win_amd64.whl (459.2 MB)
Collecting cattrs>=1.0.0
  Using cached cattrs-1.0.0-py2.py3-none-any.whl (14 kB)
Collecting attrs>=19.3.0
  Using cached attrs-19.3.0-py2.py3-none-any.whl (39 kB)
Requirement already satisfied: six>=1.12.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents==0.17.0) (1.15.0)
Requirement already satisfied: pypiwin32==223 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents==0.17.0) (223)
Requirement already satisfied: cloudpickle in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents_envs==0.17.0->mlagents==0.17.0) (1.4.1)
Requirement already satisfied: setuptools in c:\program files\windowsapps\pythonsoftwarefoundation.python.3.7_3.7.2032.0_x64__qbz5n2kfra8p0\lib\site-packages (from protobuf>=3.6->mlagents==0.17.0) (41.2.0)
Requirement already satisfied: wheel>=0.26; python_version >= ""3"" in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorflow<3.0,>=1.7->mlagents==0.17.0) (0.34.2)
Collecting tensorflow-estimator<2.3.0,>=2.2.0
  Using cached tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)
Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.1.2)
Requirement already satisfied: opt-einsum>=2.3.2 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorflow<3.0,>=1.7->mlagents==0.17.0) (3.2.1)
Requirement already satisfied: termcolor>=1.1.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.1.0)
Requirement already satisfied: wrapt>=1.11.1 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.12.1)
Collecting tensorboard<2.3.0,>=2.2.0
  Using cached tensorboard-2.2.2-py3-none-any.whl (3.0 MB)
Requirement already satisfied: google-pasta>=0.1.8 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorflow<3.0,>=1.7->mlagents==0.17.0) (0.2.0)
Requirement already satisfied: astunparse==1.6.3 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.6.3)
Requirement already satisfied: scipy==1.4.1; python_version >= ""3"" in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.4.1)
Requirement already satisfied: gast==0.3.3 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorflow<3.0,>=1.7->mlagents==0.17.0) (0.3.3)
Requirement already satisfied: absl-py>=0.7.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorflow<3.0,>=1.7->mlagents==0.17.0) (0.9.0)
Requirement already satisfied: pywin32>=223 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from pypiwin32==223->mlagents==0.17.0) (228)
Requirement already satisfied: requests<3,>=2.21.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (2.23.0)
Requirement already satisfied: werkzeug>=0.11.15 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.0.1)
Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (0.4.1)
Collecting tensorboard-plugin-wit>=1.6.0
  Using cached tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)
Requirement already satisfied: markdown>=2.6.8 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (3.2.2)
Requirement already satisfied: google-auth<2,>=1.6.3 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.16.1)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (2020.4.5.2)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.25.9)
Requirement already satisfied: chardet<4,>=3.0.2 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (3.0.4)
Requirement already satisfied: idna<3,>=2.5 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (2.9)
Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.3.0)
Requirement already satisfied: importlib-metadata; python_version < ""3.8"" in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.6.1)
Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (4.1.0)
Requirement already satisfied: rsa<4.1,>=3.1.4 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (4.0)
Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (0.2.8)
Requirement already satisfied: oauthlib>=3.0.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (3.1.0)
Requirement already satisfied: zipp>=0.5 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from importlib-metadata; python_version < ""3.8""->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (3.1.0)
Requirement already satisfied: pyasn1>=0.1.3 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (0.4.8)
Building wheels for collected packages: mlagents
  Building wheel for mlagents (setup.py) ... done
  Created wheel for mlagents: filename=mlagents-0.17.0-py3-none-any.whl size=142504 sha256=e4099d3ad8c16d34f714ff9ec98bd27c740d97b5923d53c4e79946777853865d
  Stored in directory: c:\users\usuario\appdata\local\pip\cache\wheels\2a\1d\59\dbb9b65104c8cec980a2dc73367c0421100df12733d8cd4455
Successfully built mlagents
Installing collected packages: tensorflow-estimator, tensorboard-plugin-wit, tensorboard, tensorflow, attrs, cattrs, mlagents
  WARNING: The script tensorboard.exe is installed in 'C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\Scripts' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
  WARNING: The scripts estimator_ckpt_converter.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\Scripts' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
  WARNING: The scripts mlagents-learn.exe and mlagents-run-experiment.exe are installed in 'C:\Users\Usuario\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\Scripts' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
Successfully installed attrs-19.3.0 cattrs-1.0.0 mlagents-0.17.0 tensorboard-2.2.2 tensorboard-plugin-wit-1.6.0.post3 tensorflow-2.2.0 tensorflow-estimator-2.2.0
PS C:\Users\Usuario> pip3 list
Package                Version     Location
---------------------- ----------- ----------------------------------------------
absl-py                0.9.0
astunparse             1.6.3
attrs                  19.3.0
cachetools             4.1.0
cattrs                 1.0.0
certifi                2020.4.5.2
chardet                3.0.4
cloudpickle            1.4.1
gast                   0.3.3
google-auth            1.16.1
google-auth-oauthlib   0.4.1
google-pasta           0.2.0
grpcio                 1.29.0
h5py                   2.10.0
idna                   2.9
importlib-metadata     1.6.1
Keras-Preprocessing    1.1.2
Markdown               3.2.2
mlagents               0.17.0
mlagents-envs          0.17.0      d:\ml_unity\ml-agents-release_3\ml-agents-envs
numpy                  1.18.5
oauthlib               3.1.0
opt-einsum             3.2.1
Pillow                 7.1.2                                                                                                                                                                    pip                    20.1.1                                                                                                                                                                   protobuf               3.12.2                                                                                                                                                                   pyasn1                 0.4.8                                                                                                                                                                    pyasn1-modules         0.2.8                                                                                                                                                                    pypiwin32              223                                                                                                                                                                      pywin32                228                                                                                                                                                                      PyYAML                 5.3.1                                                                                                                                                                    requests               2.23.0
requests-oauthlib      1.3.0
rsa                    4.0
scipy                  1.4.1
six                    1.15.0
tensorboard            2.2.2
tensorboard-plugin-wit 1.6.0.post3
tensorflow             2.2.0
tensorflow-estimator   2.2.0
termcolor              1.1.0
urllib3                1.25.9
Werkzeug               1.0.1
wheel                  0.34.2
wrapt                  1.12.1
zipp                   3.1.0
PS C:\Users\Usuario> mlagents-learn
mlagents-learn : El término 'mlagents-learn' no se reconoce como nombre de un cmdlet, función, archivo de script o programa ejecutable. Compruebe si escribió correctamente el nombre o, si
incluyó una ruta de acceso, compruebe que dicha ruta es correcta e inténtelo de nuevo.
En línea: 1 Carácter: 1
+ mlagents-learn
+ ~~~~~~~~~~~~~~
    + CategoryInfo          : ObjectNotFound: (mlagents-learn:String) [], CommandNotFoundException
    + FullyQualifiedErrorId : CommandNotFoundException
",like path name long el la variable al usar el lo pip install user requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied pillow requirement already satisfied requirement already satisfied requirement already satisfied six requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied wheel requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied markdown requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied markdown requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied markdown requirement already satisfied building collected building wheel done wheel directory successfully built collected warning script path consider directory path prefer suppress warning use warning path consider directory path prefer suppress warning use warning path consider directory path prefer suppress warning use successfully post pip list package version location gast markdown pillow pip six post wheel el se de un de script si el si de e de en string,issue,positive,positive,positive,positive,positive,positive
644900182,"It also would be great to provide suggested alternatives in the meantime for how to best train non trivial behaviors. The multi-brain swapping (break down the task into a set of simple behaviors that each can be trained in 2 million steps or less) is the best way I could think of, but do you recommend something else? 2 examples of behaviors I've been stuck on for weeks:

1. walking up a ramp. I can train the humanoid character to move to a switch, to move to a box, lift it, and carry it to a switch, but if the switch is on a ramp, it will just move directly toward the switch, no matter what side it is on, and then just get stuck running into the wall, below the switch, because it does not know to run away from the target to get to the ramp to run up to it.
I've tried many curriculums - first starting on the ramp, then at the bottom of the ramp (it can do these - just needs to run straight) but as soon as I start it slightly to the side of the ramp, it still runs straight to the target, missing the ramp. It seemed to get stuck and after 1.25 million steps of no apparent learning (2 days of training) I gave up. My thought based on reading was that it needs to randomly move onto the ramp and succeed, and then over time it slowly learns a path to success (go to the bottom of the ramp). It does this sometimes, and I think it may have been learning, but as with other examples in literature, it needed a few million more runs to master it. But I need to be able to do those few million runs in a day or less, rather than 3+ days. Especially since it seems tricky to figure out with AI training if it just won't learn with the current system OR it just needs to run (a lot) more to figure it out. Much of the literature, including the MLAgents guide, recommends keeping things (rewards) simple, which makes sense. But that requires lots of runs and exploration to figure it out. It seems I should be expecting at least 10 million runs to learn some of these behaviors, yet because of how long it takes to train, the most I've ever done is 4.5 million. The AI may just need more time to learn.

2. managing 4 characters. 1 to shoot (targets), 1 to hook, 1 to lift and carry boxes, 1 to jump. I originally tried 1 brain to manage all characters (as it is for the human player in the game) and for very simple tasks I had moderate success in about 3 million runs, but again if there was any directionality required (e.g. shooting or hooking to a target on a wall, so you need to approach from a particular angle, or you must back up far enough to have line of sight to the target) it would get stuck and fail. It also had a hard time changing characters and I had to add code to force it (via action masking) to change to the appropriate character for the given task.
If I were to switch to 1 brain per character it would be easier to train (less generalization) BUT what about common behaviors such as movement? It would essentially need a subset of multiple brains at once (e.g. one for movement they all have, then one for the current character). Does MLAgents support multiple brains? Or is there a better way to train an AI to do this?
This is just the tip of the iceberg - the original goal was to be able to train an AI to play the game, just like a human would - being able to get in and drive a car or vehicle for example (though again I could switch to a different brain once in the vehicle) and lots of other nuances similar to the ramp issue with navigating terrain and solving more challenging tasks and puzzles.  This is starting to feel impossible with MLAgents now. But hopefully there is, or soon will be, a solution I'm missing! Thanks again for all your work on MLAgents!",also would great provide best train non trivial swapping break task set simple trained million le best way could think recommend something else stuck walking ramp train humanoid character move switch move box lift carry switch switch ramp move directly toward switch matter side get stuck running wall switch know run away target get ramp run tried many first starting ramp bottom ramp need run straight soon start slightly side ramp still straight target missing ramp get stuck million apparent learning day training gave thought based reading need randomly move onto ramp succeed time slowly path success go bottom ramp sometimes think may learning literature million master need able million day le rather day especially since tricky figure ai training wo learn current system need run lot figure much literature guide keeping simple sense lot exploration figure least million learn yet long train ever done million ai may need time learn shoot hook lift carry jump originally tried brain manage human player game simple moderate success million shooting target wall need approach particular angle must back far enough line sight target would get stuck fail also hard time add code force via action change appropriate character given task switch brain per character would easier train le generalization common movement would essentially need subset multiple brain one movement one current character support multiple brain better way train ai tip iceberg original goal able train ai play game like human would able get drive car vehicle example though could switch different brain vehicle lot similar ramp issue terrain starting feel impossible hopefully soon solution missing thanks work,issue,positive,positive,neutral,neutral,positive,positive
644889312,"Thanks for the prompt reply @xiaomaogy though that's too bad to hear. Do you have an idea of how many steps the example environments took to train? And what a realistic upper limit is? It sounds like without being able to leverage the GPU, that means MLAgents is limited to just 1 to 5 million steps to reach training proficiency, meaning we can't train anything other than simple agents and behaviors? Definitely no generalization; would need to make agent actions very specific and then change brains, similar to Wall Jump, since training for even small levels of generalizations (e.g. in my case I can get the agent to learn to shoot OR to carry a box to a switch, but due to catastrophic forgetting, it can't learn both without running them in tandem and doing many millions of runs).

Are there plans to setup MLAgents to leverage GPUs and start to work toward being able to train more complex behaviors? One of the reasons I started using MLAgents in the first place was to work on generalization. Several articles I read said an issue is having realistic simulation environments beyond simple Atari games and OpenAI gym and Unity with MLAgents sounded like the perfect solution! But if we're basically performance capped to only a few million runs, that seems to take all the wind out of the sails unfortunately.

Simply put: my goal is to be able to start to replicate things like OpenAIs multi agent hide and seek (or even better OpenAI Five, but obviously that's much harder). I knew it would be costly to train and take tens or hundreds of millions of runs, but it needs to be able to be done in a reasonable amount of time. More specifically, we're building an AI speech therapy game that will have several mini games that leverage AI, so we need to train a lot of agents with a lot of different behaviors. It sounds like at present I'll have to use the multi-brain approach and be limited to simple behaviors for now. It would be awesome to be able to do more!

For now though, this seems like a huge limitation and it would be nice to have it noted on the main readme page, so others know up front what to expect and don't have to figure it out the hard way after weeks of failed training as I went through.",thanks prompt reply though bad hear idea many example took train realistic upper limit like without able leverage limited million reach training proficiency meaning ca train anything simple definitely generalization would need make agent specific change brain similar wall jump since training even small case get agent learn shoot carry box switch due catastrophic forgetting ca learn without running tandem many million setup leverage start work toward able train complex one first place work generalization several read said issue realistic simulation beyond simple gym unity like perfect solution basically performance capped million take wind unfortunately simply put goal able start replicate like agent hide seek even better five obviously much harder knew would costly train take million need able done reasonable amount time specifically building ai speech therapy game several leverage ai need train lot lot different like present use approach limited simple would awesome able though like huge limitation would nice noted main page know front expect figure hard way training went,issue,positive,positive,positive,positive,positive,positive
644887557,The class was made public. Closing this issue.,class made public issue,issue,negative,neutral,neutral,neutral,neutral,neutral
644885073,It sounds like your path name is too long: https://stackoverflow.com/a/55189256/224264,like path name long,issue,negative,negative,neutral,neutral,negative,negative
644880585,"Hi @lukemadera, you are right, mlagents isn't setup to leverage GPU, so unless you have a scene that heavily uses visual and doing lots of model updates (such as SAC) and has a really large neural net, in which case you will be bound by model update computation, you won't benefit much from using better GPU. 

To make training faster, you could use our multi environment argument to launch multiple unity instances to make the speed of generating steps faster. In this case more and better CPU definitely helps a lot. To go beyond that, you would need some kind of distributed training setting, which becomes a lot more complicated. Internally we are still working on such a solution and hoping to integrate into our mlagents cloud service in the future. ",hi right setup leverage unless scene heavily visual lot model sac really large neural net case bound model update computation wo benefit much better make training faster could use environment argument launch multiple unity make speed generating faster case better definitely lot go beyond would need kind distributed training setting becomes lot complicated internally still working solution integrate cloud service future,issue,positive,positive,positive,positive,positive,positive
644637011,"> El `pip`error generado sugiere que la instalación de pip no se realizó correctamente. Puede intentar instalar usando el `--user`parámetro
> `pip install --user <package-name>`
> Una vez hecho esto, también puede verificar si el paquete está instalado usando el `pip list`comando.
> ¡Espero eso ayude!

Muchas gracias, pero sigue sin instalarse el paquete mlagents.

Se instala todos los demas paquetes menos ese:

PS C:\Users\Usuario> pip3 install --user D:\ML_Unity\ml-agents-release_3\ml-agents\
Processing d:\ml_unity\ml-agents-release_3\ml-agents
Requirement already satisfied: grpcio>=1.11.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents==0.17.0) (1.29.0)
Requirement already satisfied: h5py>=2.9.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents==0.17.0) (2.10.0)
Requirement already satisfied: mlagents_envs==0.17.0 in d:\ml_unity\ml-agents-release_3\ml-agents-envs (from mlagents==0.17.0) (0.17.0)
Requirement already satisfied: numpy<2.0,>=1.13.3 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents==0.17.0) (1.18.5)
Requirement already satisfied: Pillow>=4.2.1 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents==0.17.0) (7.1.2)
Requirement already satisfied: protobuf>=3.6 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents==0.17.0) (3.12.2)
Requirement already satisfied: pyyaml>=3.1.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents==0.17.0) (5.3.1)
Collecting tensorflow<3.0,>=1.7
  Using cached tensorflow-2.2.0-cp37-cp37m-win_amd64.whl (459.2 MB)
Collecting cattrs>=1.0.0
  Using cached cattrs-1.0.0-py2.py3-none-any.whl (14 kB)
Collecting attrs>=19.3.0
  Using cached attrs-19.3.0-py2.py3-none-any.whl (39 kB)
Requirement already satisfied: six>=1.12.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents==0.17.0) (1.15.0)
Requirement already satisfied: pypiwin32==223 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents==0.17.0) (223)
Requirement already satisfied: cloudpickle in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from mlagents_envs==0.17.0->mlagents==0.17.0) (1.4.1)
Requirement already satisfied: setuptools in c:\program files\windowsapps\pythonsoftwarefoundation.python.3.7_3.7.2032.0_x64__qbz5n2kfra8p0\lib\site-packages (from protobuf>=3.6->mlagents==0.17.0) (41.2.0)
Processing c:\users\usuario\appdata\local\pip\cache\wheels\3f\e3\ec\8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\termcolor-1.1.0-py3-none-any.whl
Requirement already satisfied: wrapt>=1.11.1 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.12.1)
Collecting opt-einsum>=2.3.2
  Using cached opt_einsum-3.2.1-py3-none-any.whl (63 kB)
Requirement already satisfied: google-pasta>=0.1.8 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorflow<3.0,>=1.7->mlagents==0.17.0) (0.2.0)
Requirement already satisfied: gast==0.3.3 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorflow<3.0,>=1.7->mlagents==0.17.0) (0.3.3)
Collecting scipy==1.4.1; python_version >= ""3""
  Using cached scipy-1.4.1-cp37-cp37m-win_amd64.whl (30.9 MB)
Requirement already satisfied: absl-py>=0.7.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorflow<3.0,>=1.7->mlagents==0.17.0) (0.9.0)
Collecting tensorflow-estimator<2.3.0,>=2.2.0
  Using cached tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)
Requirement already satisfied: astunparse==1.6.3 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.6.3)
Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.1.2)
Requirement already satisfied: wheel>=0.26; python_version >= ""3"" in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorflow<3.0,>=1.7->mlagents==0.17.0) (0.34.2)
Collecting tensorboard<2.3.0,>=2.2.0
  Using cached tensorboard-2.2.2-py3-none-any.whl (3.0 MB)
Requirement already satisfied: pywin32>=223 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from pypiwin32==223->mlagents==0.17.0) (228)
Requirement already satisfied: werkzeug>=0.11.15 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.0.1)
Requirement already satisfied: markdown>=2.6.8 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (3.2.2)
Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (0.4.1)
Requirement already satisfied: google-auth<2,>=1.6.3 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.16.1)
Requirement already satisfied: requests<3,>=2.21.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (2.23.0)
Collecting tensorboard-plugin-wit>=1.6.0
  Using cached tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)
Requirement already satisfied: importlib-metadata; python_version < ""3.8"" in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.6.1)
Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.3.0)
Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (0.2.8)
Requirement already satisfied: rsa<4.1,>=3.1.4 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (4.0)
Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (4.1.0)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (1.25.9)
Requirement already satisfied: chardet<4,>=3.0.2 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (3.0.4)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (2020.4.5.2)
Requirement already satisfied: idna<3,>=2.5 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (2.9)
Requirement already satisfied: zipp>=0.5 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from importlib-metadata; python_version < ""3.8""->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (3.1.0)
Requirement already satisfied: oauthlib>=3.0.0 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (3.1.0)
Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\users\usuario\appdata\local\packages\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\localcache\local-packages\python37\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<3.0,>=1.7->mlagents==0.17.0) (0.4.8)
Building wheels for collected packages: mlagents
  Building wheel for mlagents (setup.py) ... done
  Created wheel for mlagents: filename=mlagents-0.17.0-py3-none-any.whl size=142504 sha256=da182557c03775b349d8085b1258be16ba2903e50a6ed2d27b29bc3151cc7bec
  Stored in directory: c:\users\usuario\appdata\local\pip\cache\wheels\2a\1d\59\dbb9b65104c8cec980a2dc73367c0421100df12733d8cd4455
Successfully built mlagents
Installing collected packages: termcolor, opt-einsum, scipy, tensorflow-estimator, tensorboard-plugin-wit, tensorboard, tensorflow, attrs, cattrs, mlagents
ERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\Users\\Usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\linear_optimizer\\python\\utils\\__pycache__\\sharded_mutable_dense_hashtable.cpython-37.pyc'

PS C:\Users\Usuario> pip3 list
Package              Version    Location
-------------------- ---------- ----------------------------------------------
absl-py              0.9.0
astunparse           1.6.3
cachetools           4.1.0
certifi              2020.4.5.2
chardet              3.0.4
cloudpickle          1.4.1
gast                 0.3.3
google-auth          1.16.1
google-auth-oauthlib 0.4.1
google-pasta         0.2.0
grpcio               1.29.0
h5py                 2.10.0
idna                 2.9
importlib-metadata   1.6.1
Keras-Preprocessing  1.1.2
Markdown             3.2.2
mlagents-envs        0.17.0     d:\ml_unity\ml-agents-release_3\ml-agents-envs
numpy                1.18.5
oauthlib             3.1.0
opt-einsum           3.2.1
Pillow               7.1.2
pip                  20.1.1
protobuf             3.12.2
pyasn1               0.4.8
pyasn1-modules       0.2.8
pypiwin32            223
pywin32              228
PyYAML               5.3.1
requests             2.23.0
requests-oauthlib    1.3.0
rsa                  4.0
scipy                1.4.1
six                  1.15.0
termcolor            1.1.0
urllib3              1.25.9
Werkzeug             1.0.1
wheel                0.34.2
wrapt                1.12.1
zipp                 3.1.0
PS C:\Users\Usuario>",el pip error la de pip se el user pip install user si el el pip list sin el se pip install user requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied pillow requirement already satisfied requirement already satisfied requirement already satisfied six requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied wheel requirement already satisfied requirement already satisfied requirement already satisfied markdown requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied markdown requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied markdown requirement already satisfied requirement already satisfied building collected building wheel done wheel directory successfully built collected error could install due file directory pip list package version location gast markdown pillow pip six wheel,issue,positive,positive,positive,positive,positive,positive
644498277,"Hi @xiaomaogy, so can you explain when the step count (which is printed on the console) is updated (like after a set of observations, actions, rewards are collected and then it is updated?). And can I access this value in Unity using the ML-Agents runtime scripts?",hi explain step count printed console like set collected access value unity,issue,positive,neutral,neutral,neutral,neutral,neutral
644467861,"Hi @ChristianCoenen, the MaxStepCount in Unity is related to the number of steps for the environment, while the step count in console is related to the step number for the trainer, so they are different.  ",hi unity related number environment step count console related step number trainer different,issue,negative,neutral,neutral,neutral,neutral,neutral
644466950,"Hi @theInsomniacGameMaker could you please fill out the full template of the bug report, and include all of the relevant information such as the OS you are on?",hi could please fill full template bug report include relevant information o,issue,negative,positive,positive,positive,positive,positive
644466469,"Hi @MWaqas1512 please open up a new issue with filled template for the bug report, so that we can help accordingly. ",hi please open new issue filled template bug report help accordingly,issue,positive,positive,positive,positive,positive,positive
644417751,"This snippet is actually responsible for sending the observations when some agent reaches its terminal condition.
https://github.com/Unity-Technologies/ml-agents/blob/3c2fa4d8d1cd981e9cef6b2e2fdb2f77757983c3/gym-unity/gym_unity/envs/__init__.py#L175-L180

In a multi-agent/vectorized setting would it be okay to return the observations/rewards/dones considering both `decision_step` and `terminal_step` rather than only one?

P.S.: One workaround that I can think of for my particular use case is to not call the `EndEpisode()` method in the C# script for the agents. But then, I do need the information whether the agent terminates or not. I don't know if that makes sense!",snippet actually responsible sending agent terminal condition setting would return considering rather one one think particular use case call method script need information whether agent know sense,issue,negative,positive,positive,positive,positive,positive
644411871,"Hi @xiaomaogy,
So I've currently managed to get the data from multiple agents by bypassing the above mentioned check. For stepping I change the following line
https://github.com/Unity-Technologies/ml-agents/blob/3c2fa4d8d1cd981e9cef6b2e2fdb2f77757983c3/gym-unity/gym_unity/envs/__init__.py#L169

using `(-1, action_size)` instead of `(1, action_size)`. The check for whether the number of agents match is done in the `set_actions` method of the `UnityEnvironment` class so I didn't enforce any checks as of now.
https://github.com/Unity-Technologies/ml-agents/blob/20527d10121b68c60b490468eafed0465df498e3/ml-agents-envs/mlagents_envs/environment.py#L338-L345

The issue that I am facing now is that when the episode ends, I only get the observations from the agent that is responsible for termination.

My use case however is quite different. I want the episode to be agent dependent, and even if some agent might ""die"", the rest of the agent should continue. The dead agent would spawn somewhere else in the map! Is this achievable? And could you give me some pointers on some of the possible pitfalls that I should look out for. Thanks in advance!!",hi currently get data multiple check stepping change following line instead check whether number match done method class enforce issue facing episode get agent responsible termination use case however quite different want episode agent dependent even agent might die rest agent continue dead agent would spawn somewhere else map achievable could give possible look thanks advance,issue,negative,positive,neutral,neutral,positive,positive
644298544,"@DVonk Yes the doc seems outdated, thanks for pointing him in the right direction. ",yes doc outdated thanks pointing right direction,issue,positive,positive,neutral,neutral,positive,positive
644292398,"The `pip` generated error suggests that the pip installation did not succeed. You can try installing using the `--user` parameter.
`pip install --user <package-name>`
Once done you can also check to see if the package is installed using `pip list` command.
Hope it helps!",pip error pip installation succeed try user parameter pip install user done also check see package pip list command hope,issue,positive,neutral,neutral,neutral,neutral,neutral
643759616,"As of now I have zeroed in to this snippet of code that raises an exception whenever the number of agents is greater than one in the `UnityEnvironment`

https://github.com/Unity-Technologies/ml-agents/blob/3c2fa4d8d1cd981e9cef6b2e2fdb2f77757983c3/gym-unity/gym_unity/envs/__init__.py#L267-L272

Is there any reason why this is the norm? Is it just to make sure that the environments are compatible with the standard RL libraries like OpenAI's baselines and Dopamine?",snippet code exception whenever number greater one reason norm make sure compatible standard like,issue,positive,positive,positive,positive,positive,positive
643718811,"Facing Issue in agent.cs file
NullReferenceException: Object reference not set to an instance of an object
MLAgents.Agent.OnEnable () (at Assets/ML-Agents/Scripts/Agent.cs:255)
",facing issue file object reference set instance object,issue,negative,neutral,neutral,neutral,neutral,neutral
643472249,"Hi @DVonk,
This has to do with Unity's package process. ML-Agents v1.0.2 will be a verified package in the Unity 2020.2 release, which is why we dropped the ""preview"" suffix there. Any bug fixes between now and then can also be put into new patch releases, for example v1.0.3. We will continue to release new features and improvements as 1.x.y-preview packages (roughly once a month).

For the Barracuda dependency, verified packages aren't allowed to depend on preview packages, so Barracuda will also be a verified package in 2020.2. This is why ML-Agents 1.0.2 depends on Barracuda 1.0.0.

The fact that ML-Agents 1.1.0 depends on Barracuda 0.7.1 is an oversight; we should have updated the dependency before we released. If we do a patch release (1.1.1), we'll update the dependency there; otherwise 1.2.0 will depend on the latest Barracuda release at the time.",hi unity package process package unity release preview suffix bug also put new patch example continue release new roughly month barracuda dependency depend preview barracuda also package barracuda fact barracuda oversight dependency patch release update dependency otherwise depend latest barracuda release time,issue,negative,positive,positive,positive,positive,positive
643360216,"Turns out this error was resolved and in newer versions summaries are saved to /results.

https://github.com/Unity-Technologies/ml-agents/pull/4085",turn error resolved saved,issue,negative,neutral,neutral,neutral,neutral,neutral
642825114,"Okay, thanks for the information. If you can update the tutorial would be nice though. 
Anyway, thanks a lot.",thanks information update tutorial would nice though anyway thanks lot,issue,positive,positive,positive,positive,positive,positive
642804994,"Hi @Dionysus326 

There have been some changes to the gym wrapper recently which may have made the DQN tutorial no longer compatible. I'll log this as a bug, and have someone on the team take a look.",hi gym wrapper recently may made tutorial longer compatible log bug someone team take look,issue,negative,neutral,neutral,neutral,neutral,neutral
642788823,"Thank you can I use GridWorld as a continues environment, it looks discrete ? @xiaomaogy ",thank use environment discrete,issue,negative,neutral,neutral,neutral,neutral,neutral
642766160,"I agree that it would be a good idea to include at least one simple 2D environment. Maybe something like in the link, or an agent that avoids obstacles. Or maybe Pong - I just implemented that and it worked quite well.",agree would good idea include least one simple environment maybe something like link agent maybe pong worked quite well,issue,positive,positive,positive,positive,positive,positive
642763843,"The documentation is quite outdated in parts - especially the example environments. It seems like you need to have a separate config for every environment now, and it has to say 'behaviors:' at the top. See here:
https://github.com/Unity-Technologies/ml-agents/blob/master/config/ppo/3DBall.yaml",documentation quite outdated especially example like need separate every environment say top see,issue,positive,positive,neutral,neutral,positive,positive
642762534,"The structure of the yaml files has changed (also mentioned in the migration guide). Just download this one manually:

https://github.com/Unity-Technologies/ml-agents/blob/master/config/ppo/3DBall.yaml",structure also migration guide one manually,issue,negative,neutral,neutral,neutral,neutral,neutral
642594446,"I am getting the same error while following the Getting Started guide in the repository.

Unity version 2019.3.13f1
Windows 10 64-bit
Python 3.8.3 64-bit
MLAgents release_2 branch",getting error following getting guide repository unity version python branch,issue,negative,neutral,neutral,neutral,neutral,neutral
642471070,"I also got the same error: 
`TypeError: __init__() got multiple values for argument 'use_visual'`
Is this has something to do with the multi-agents?
I deleted all the other agents except one but still got the same error",also got error got multiple argument something except one still got error,issue,negative,neutral,neutral,neutral,neutral,neutral
642360564,"I created a PR here: https://github.com/Unity-Technologies/ml-agents/pull/4111
It turns out that storing the Collider isn't a great solution, because 2D raycasting would need a Collider2D. So instead I'm storing the GameObject, which you would be able to use to get the Collider if you wanted it.",turn great solution would need instead would able use get,issue,positive,positive,positive,positive,positive,positive
642337177,"Each ISensor.Update() is called in the Agent's update:
https://github.com/Unity-Technologies/ml-agents/blob/9d57ff95f36811d4710db3b41644e6c4fed4f6a2/com.unity.ml-agents/Runtime/Agent.cs#L920-L926
This gets called every time the Agent requests a decision, which could be less often than each FixedUpdate - if you're using DecisionRequester, it will be whatever the DecisionPeriod is.

ISensor.Write() (where the raycasting happens for RayPerceptionSensor) is also called each time a decision is requested. This is called from different locations, depending on whether you're doing training or inference.",agent update every time agent decision could le often whatever also time decision different depending whether training inference,issue,negative,neutral,neutral,neutral,neutral,neutral
642324132,"Hi @MrStashley, for installation issues like this, please open a forum post here: https://forum.unity.com/forums/ml-agents.453/",hi installation like please open forum post,issue,positive,neutral,neutral,neutral,neutral,neutral
642323845,"Hi @we00037890 for issues like this (specific to your custom environment), please open a Forum post on our unity forum https://forum.unity.com/forums/ml-agents.453/. ",hi like specific custom environment please open forum post unity forum,issue,positive,neutral,neutral,neutral,neutral,neutral
642323467,"Yes it is really easy to add that. Our GridWorld example environment is sort of our 2D Environment, you maybe you can take a look at that one. @Hsgngr ",yes really easy add example environment sort environment maybe take look one,issue,positive,positive,positive,positive,positive,positive
642172135,How can I know if the env implemented the rendering given that the env file is a Unity binary?,know rendering given file unity binary,issue,negative,neutral,neutral,neutral,neutral,neutral
642163772,"As long as the environment implements the `env.render(mode='rgb_array')`, Monitor can be applied.",long environment monitor applied,issue,negative,negative,neutral,neutral,negative,negative
642157399,"Thanks @vwxyzjn
I am pretty new to Unity so correct me if I am wrong.
I think gym monitor can only be applied when the env was created by `gym.make`.
It's not applied to the envs that created by `UnityToGymWrapper`?",thanks pretty new unity correct wrong think gym monitor applied applied,issue,negative,positive,neutral,neutral,positive,positive
642149002,"Thanks @awjuliani ! This looks wonderful.

Will this support gym 'monitor' interface for video replay?",thanks wonderful support gym interface video replay,issue,positive,positive,positive,positive,positive,positive
642132414,"Hi @vwxyzjn and @yijiezh 

Our next release of ML-Agents (coming this week) will actually include an environment registry. You can read about it on the documentation for our master branch: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Unity-Environment-Registry.md.",hi next release coming week actually include environment registry read documentation master branch,issue,negative,neutral,neutral,neutral,neutral,neutral
642111403,"Hi,

I made [this notebook on colab](https://colab.research.google.com/drive/1Qg6E5kmf9n4G8rc5lXHIM_cQzMUFGH-g#forceEdit=true&sandboxMode=true) and I would like to know if it satisfies your use case. I am using the binaries hosted on the default registry and using a virtual frame buffer with xvfb to render. 
If this solution works, we can use regular environments on hosted and local colab",hi made notebook would like know use case default registry virtual frame buffer render solution work use regular local,issue,positive,neutral,neutral,neutral,neutral,neutral
641740376,"> Hi @egedursun are you able to get training going? is this message just popping up when you open the project?
@unityjeffrey 
I hava the same problem. I can't get traing going. I think the problem is the connection between unity and mlagents. I found a similar problem,https://github.com/Unity-Technologies/ml-agents/issues/3667. But I have no idea how to set the base_port in the instatiation of the UnityEnvironment to 5004. 
It would be nice if I could get some help too.",hi able get training going message open project problem ca get going think problem connection unity found similar problem idea set would nice could get help,issue,negative,positive,positive,positive,positive,positive
641644220,"+1 for the request.

Another benefit brought by using gym.make("""") will be video replay.

Without that, is there anyway for for video replay at the moment? @awjuliani ",request another benefit brought video replay without anyway video replay moment,issue,negative,neutral,neutral,neutral,neutral,neutral
641548984,"> > Should we add a unit test for EnvironmentReset?
> 
> We do not have a way to unit test when the Python communication is happening. This bug does not happen at inference

Sounds good. I guess this sounds like a Yamato test but it's probably out-of-scope for this PR. ",add unit test way unit test python communication happening bug happen inference good guess like test probably,issue,positive,positive,positive,positive,positive,positive
641545914," > Should we add a unit test for EnvironmentReset?

We do not have a way to unit test when the Python communication is happening. This bug does not happen at inference",add unit test way unit test python communication happening bug happen inference,issue,negative,neutral,neutral,neutral,neutral,neutral
641521062,"> > Why did the original PR break just FoodCollector?
> 
> Because of all our example environments, only FoodCollector has a dependency on EnvironmentReset

Should we add a unit test for EnvironmentReset?",original break example dependency add unit test,issue,negative,positive,positive,positive,positive,positive
641518766,"> Why did the original PR break just FoodCollector?

Because of all our example environments, only FoodCollector has a dependency on EnvironmentReset",original break example dependency,issue,negative,positive,positive,positive,positive,positive
641499579,"> Do you think it is safe to cherry pick such a large change into this release?

I agree it's a bit of a risk. But I think the tradeoff in terms of user experience is worth it. What do you think?",think safe cherry pick large change release agree bit risk think user experience worth think,issue,positive,positive,positive,positive,positive,positive
641457099,"Hi @locdoan12121997 

These. issues should be finally fixed in the upcoming release this month. Apologies for the issues in the mean time.",hi finally fixed upcoming release month mean time,issue,negative,negative,negative,negative,negative,negative
641438524,"Ah, that looks to be an issue on the gym side since its trying to access an attribute `n` of the action space.  Perhaps the version of gym doesn't align with the version of baselines?",ah issue gym side since trying access attribute action space perhaps version gym align version,issue,negative,positive,neutral,neutral,positive,positive
641400017,"Ok, that's what happened for me too ! I tried again yesterday and it worked but I do not understand why because I set the default parameters given in the RollerBall tutorial (as I did when it did not work)...I  don't understand, sometimes it works but most of the time it seems to fail...
I tried an other tutorial ""Wall Jump"" and my agent seems to learn pretty well with the Small Jump configuration but stays as ""dumb"", without learning, with the Big Wall configuration. The agent doesn't want to try jumping on the cube...
@DVonk did you try this tutorial and did you have the same issue ?
And about RollerBall, if you try over and over again, is there sometimes a great learning rate ?",tried yesterday worked understand set default given tutorial work understand sometimes work time fail tried tutorial wall jump agent learn pretty well small jump configuration stay dumb without learning big wall configuration agent want try cube try tutorial issue try sometimes great learning rate,issue,negative,negative,neutral,neutral,negative,negative
641385418,"I just noticed the exact same thing. I haven't used ML-Agents since 0.13 and it never took very long to learn RollerBall. But now I'm not even getting close to the performance shown in TensorBoard in the example. 

After 10k steps I was at a mean of 0.194. After 20k at 0.214. And it increased extremely slowly from then on. At 130k steps I was at 0.715, but then it went down again and hovered there for a very long time. At 240k it finally broke out to 0.794, but then down again. I'm at 300k steps right now with 0.785. I have no idea if it'll ever reach 1.

When is the last time you've tried this tutorial? Because it doesn't seem to work as well.",exact thing used since never took long learn even getting close performance shown example mean extremely slowly went long time finally broke right idea ever reach last time tried tutorial seem work well,issue,negative,negative,neutral,neutral,negative,negative
641266244,"I hava same problem with @egedursun ,and I almost try everything @egedursun  had tried .But the problem is still there.
",problem almost try everything tried problem still,issue,negative,neutral,neutral,neutral,neutral,neutral
641149385,"@awjuliani Hi, the current installation guide does not work. There are conflict between the guide and training config. I follow c0deminded work around and it works ok.",hi current installation guide work conflict guide training follow work around work,issue,negative,neutral,neutral,neutral,neutral,neutral
641115872,The problem is solved with the Unity 2018.4.17.   I am new to Unity and don't realize the incompatible between versions,problem unity new unity realize incompatible,issue,negative,positive,positive,positive,positive,positive
641005672,"I want to create a unity environment and run my own RL algorithm, but it never succeeds in win10. whichever version of mlagents doc i followed , there always  show :
 ""The Unity environment took too long to respond. Make sure that :\n""
unityagents.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that :
	 The environment does not need user interaction to launch
	 The Academy and the External Brain(s) are attached to objects in the Scene
	 The environment and the Python interface have compatible versions.",want create unity environment run algorithm never win whichever version doc always show unity environment took long respond make sure unity environment took long respond make sure environment need user interaction launch academy external brain attached scene environment python interface compatible,issue,negative,positive,neutral,neutral,positive,positive
640903754,"one more question, the RaySensor.Update() is already called in the FixedUpdate() by default??
thanks",one question already default thanks,issue,negative,positive,positive,positive,positive,positive
640902860,"thank you for your clarification !!!! it helped a lot

to your question: yes, RayOut can be a good place to store infos, in fact i checked and some infos are already there, the only different is that we don't know who's the collider that get the hits now",thank clarification lot question yes good place store fact checked already different know get,issue,positive,positive,positive,positive,positive,positive
640901352,"You didn't answer my question before - would storing the hit collider in `RayOutput` be helpful?

In response to your questions:

a) Perceive isn't called directly, but it does the equivalent work. RayPerceptionSensorComponent3D creates a RayPerceptionSensor, and RayPerceptionSensor.Write calls PerceiveSingleRay:
https://github.com/Unity-Technologies/ml-agents/blob/9d57ff95f36811d4710db3b41644e6c4fed4f6a2/com.unity.ml-agents/Runtime/Sensors/RayPerceptionSensor.cs#L316

This is the same method called inside Perceive:
https://github.com/Unity-Technologies/ml-agents/blob/9d57ff95f36811d4710db3b41644e6c4fed4f6a2/com.unity.ml-agents/Runtime/Sensors/RayPerceptionSensor.cs#L376

The difference is that in RayPerceptionSensor.Write, we convert the output to an array of floats right away.

b) You don't need to call Perceive if you have a RayPerceptionSensorComponent3D attached to the Agent. This will automatically result in an observation being created for the Agent, which will be used for training and inference.

c) RayPerceptionSensor.Update() is empty :)
https://github.com/Unity-Technologies/ml-agents/blob/9d57ff95f36811d4710db3b41644e6c4fed4f6a2/com.unity.ml-agents/Runtime/Sensors/RayPerceptionSensor.cs#L332-L334
but all of the raycasting work happens in the Write() method. As I said above, this should be identical to what happens in Perceive(), except that RayPerceptionSensor.Write also stores some debug information that the Gizmo drawing uses.",answer question would hit helpful response perceive directly equivalent work method inside perceive difference convert output array right away need call perceive attached agent automatically result observation agent used training inference empty work write method said identical perceive except also information drawing,issue,negative,positive,neutral,neutral,positive,positive
640850855,"Sorry, I forgot to update it. Also previously forgot to clarify I used the baseline deepq algorithm as shown in the first example on gym_unity's READMD.md saved as train_unity.py . I used the updated ml-agents library on python and the ml-agents package. I install using pip install -e ./ from the updated ml-agents directory. The current master's repo has been labeled unstable.  I get the following error on cmd:

```
(ml-agents) C:\Users\hridoy\ml-agents\Project\env>python -m train_unity
2020-06-08 15:40:45 INFO [environment.py:108] Connected to Unity environment with package version 1.1.0-preview and communication version 1.0.0
2020-06-08 15:40:47 INFO [environment.py:265] Connected new brain:
GridWorld?team=0
Logging to ./logs
Traceback (most recent call last):
  File ""C:\Users\hridoy\Anaconda3\envs\ml-agents\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\hridoy\Anaconda3\envs\ml-agents\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\hridoy\ml-agents\Project\env\train_unity.py"", line 35, in <module>
    main()
  File ""C:\Users\hridoy\ml-agents\Project\env\train_unity.py"", line 29, in main
    dueling=True
  File ""c:\users\hridoy\documents\baselines\baselines\deepq\deepq.py"", line 204, in learn
    num_actions=env.action_space.n,
AttributeError: 'MultiDiscrete' object has no attribute 'n'
2020-06-08 15:40:48 INFO [environment.py:418] Environment shut down with return code 0 (CTRL_C_EVENT).

```

Here is the unity setup from which I built the environment:

![image](https://user-images.githubusercontent.com/59928531/84073795-78cfcd00-a99f-11ea-9397-9af4205bf208.png)

I had to delete the agent view area or I get the error there is more than 1 agent in the scene.


Here are the files in my env directory:

![image](https://user-images.githubusercontent.com/59928531/84073920-b2a0d380-a99f-11ea-9016-ee7e0d1965bc.png)


Update: I repeated the procedure with the latest stable release: ""release 2"" and got the same multi discrete error.",sorry forgot update also previously forgot clarify used algorithm shown first example saved used library python package install pip install directory current master unstable get following error python connected unity environment package version communication version connected new brain logging recent call last file line file line code file line module main file line main file line learn object attribute environment shut return code unity setup built environment image delete agent view area get error agent scene directory image update repeated procedure latest stable release release got discrete error,issue,negative,positive,neutral,neutral,positive,positive
640808387,"From the stack trace, it seems like the `Agent script` is missing from your agent. In 3DBall, this is called `Ball 3D Agent`. Is this true?",stack trace like agent script missing agent ball agent true,issue,negative,positive,neutral,neutral,positive,positive
640803854,"Hi @hridoylego 

It seems you're reading docs from `master` but using version 0.14 of ML-Agents. Can you try upgrading to the most recent release and also using the corresponding docs?",hi reading master version try recent release also corresponding,issue,negative,neutral,neutral,neutral,neutral,neutral
640785888,"Hi @wang88256187 

Please follow our installation instructions here: https://github.com/Unity-Technologies/ml-agents/blob/release_2/docs/Installation.md and instructions for running with an executable here: https://github.com/Unity-Technologies/ml-agents/blob/release_2/docs/Learning-Environment-Executable.md.

If you encounter any specific problems, feel free to raise them as issues.  For more general questions, please use the forums https://forum.unity.com/forums/ml-agents.453/
",hi wang please follow installation running executable encounter specific feel free raise general please use,issue,positive,positive,positive,positive,positive,positive
640781185,"Hi @sobassy 

I believe you are using a .yaml from our `master` branch and not from the release branch.  If this is intended, can you please share the .yaml that you are using?",hi believe master branch release branch intended please share,issue,positive,neutral,neutral,neutral,neutral,neutral
640779190,"Hi @WazWaz 

Thank you for pointing this out. This is something of which we are aware. It's a holdover/artifact from earlier versions of ML-Agents. For the problem as it is, it doesn't cause an issue because things move slowly enough. However, as you point out, it doesn't support extensibility to more complex variants. 

I've logged this internally as issue 1060 and we will discuss a fix.  Thank you again!",hi thank pointing something aware problem cause issue move slowly enough however point support extensibility complex logged internally issue discus fix thank,issue,positive,negative,neutral,neutral,negative,negative
640771087,"Hi @danmac03 

This seems like an issue thats more appropriate for the barracuda project. Please open an issue there  https://github.com/Unity-Technologies/barracuda-release/issues ",hi like issue thats appropriate barracuda project please open issue,issue,positive,positive,positive,positive,positive,positive
640769919,"Hi @mateolopezareal 

I believe this is more appropriate for a forum post since this is not a bug in the repo. I encourage you to post this there (https://forum.unity.com/forums/ml-agents.453/) with a short description of what the goal of an agent is as well as how you build the observation and action space and reward function.",hi believe appropriate forum post since bug encourage post short description goal agent well build observation action space reward function,issue,positive,positive,positive,positive,positive,positive
640767566,"TensorflowSharp is no longer supported by ML-Agents (and Tensorflow.NET never was). If you want to follow that old guide, you'll likely need a version of Unity from that timeframe (November 2018).",longer never want follow old guide likely need version unity,issue,negative,positive,neutral,neutral,positive,positive
640763243,It looks like you're using version 0.16 of ml-agents-envs but reading the docs of 0.15. The `get_agent_group` function was removed.,like version reading function removed,issue,negative,neutral,neutral,neutral,neutral,neutral
640255895,"I tried to decrease the time scale in the project settings and it gives me a better learning but it took a longer time to reach 1.00 of mean reward. I understood that the time scale can affect physic in ml-agents tool, and I wonder if there can be a huge difference in learning rate according to this Time scale and the CPU of the computer. Can the CPU limit a lot the learning if it is not so ""powerful"" ? Do the max time scale one can put without damaging the learning depends a lot on the CPU of one's computer ?
(I have a 2,1 GHz CPU frequency )",tried decrease time scale project better learning took longer time reach mean reward understood time scale affect physic tool wonder huge difference learning rate according time scale computer limit lot learning powerful time scale one put without learning lot one computer frequency,issue,positive,positive,positive,positive,positive,positive
640231780,"No idea, it seems tensorflow.NET is the growing framework but no idea why it doesn't work. ",idea growing framework idea work,issue,negative,neutral,neutral,neutral,neutral,neutral
640127276,"dear @chriselion , i tried and i have one more question...

I created a very simple scene, my agent with a RayPerceptionSensorComponent3D in front of a tagged ball,

in start:
r1 = this.GetComponent< RayPerceptionSensorComponent3D >();
r2 = r1.GetRayPerceptionInput();

in update:
r3 = RayPerceptionSensor.Perceive(r2);
foreach (RayPerceptionOutput.RayOutput ro in r3.RayOutputs) {
            Debug.Log(ro.HasHit+"" ""+ro.HitTaggedObject+"" ""+ro.HitTagIndex);
}

and it works: True, True, 0 (is the number of the Tag of the ball)
False, False, -1 if i move the object away from the ball.

QUESTIONs:

a) in normal agent scripting, the Perceive is done automatically?

because i am trying since days to create an agent with several sensors (like a matrix of 5x5 sensors with just 1 ray) and i receive the same behaviour (the reward and learning curves of my agent on the TensorBoard) also WITHOUT the RayPerceptionSensorComponent3D... so i doubt that it is working... there are simple ways to verify if the sensors are capting something?

b) should i always use Perceive in the update (fixed?) to receive NEW infos from the sensors?

c) what is in case the difference between Perceive() and Update() that is in the RaySensor?

Thanks a lot,
Dario",dear tried one question simple scene agent front tagged ball start update work true true number tag ball false false move object away ball normal agent perceive done automatically trying since day create agent several like matrix ray receive behaviour reward learning agent also without doubt working simple way verify something always use perceive update fixed receive new case difference perceive update thanks lot,issue,positive,positive,neutral,neutral,positive,positive
640090862,"I found the problem using debugger, agent was set to dynamic in opponent class, cause I wanted to add player as opponent with expandoobject.

This line was causing the problem:
opponents[i].Update(fwd, opponents[i].Agent.transform.position - pos); (where agent was dynamic type).

Thanks for the help",found problem agent set dynamic opponent class cause add player opponent line causing problem agent dynamic type thanks help,issue,positive,positive,neutral,neutral,positive,positive
639874776,"Sorry for the delay. I think we could store the collider of the hit in the `RayOutput` struct. We wouldn't use it by default, but if you're using the static version of `Perceive()`, it could still be useful. Does cover what you need?",sorry delay think could store hit would use default static version perceive could still useful cover need,issue,negative,positive,neutral,neutral,positive,positive
639858043,"The only thing that jumps out at me is the `Array.Sort` call - do you still get the error if you comment that out? 

If it's not that, can you try a development build (or anything with a lower optimization level) in case inlining is obscuring the callstack?",thing call still get error comment try development build anything lower optimization level case,issue,negative,neutral,neutral,neutral,neutral,neutral
639832531,"Ok, I see! Please see the code of CollectObservation below. The base.CollectObservation calls the base class and that code is also found below.
```
    public override void CollectObservations(VectorSensor sensor)
    {
        base.CollectObservations(sensor);

        Vector3 pos = transform.position;
        Vector3 fwd = transform.forward;
        boostAcceleration = 0;

        // Ideally, it should be up to the agent to decide which opponent it follows.
        // That would require observing a variable number (up to 19 in this case) of 
        // opponents though. To simplify things, we focus on only two - one in front, 
        // and one behind the agent.

        for (int i = 0; i < opponents.Length; i++)
        {
            opponents[i].Update(fwd, opponents[i].Agent.transform.position - pos);
        }
        Array.Sort(opponents); // by distance

        bool hasFrontOpponent = false;
        bool hasRearOpponent = false;
        // Iterate from closest to farthest.
        for (int i = 0; i < opponents.Length; i++)
        {
            if (opponents[i].DirDot > 0)
            {
                if (!hasFrontOpponent)
                {
                    oppFront = opponents[i];
                    hasFrontOpponent = true;
                }
            }
            else if (!hasRearOpponent)
            {
                oppRear = opponents[i];
                hasRearOpponent = true;
            }

            if (hasFrontOpponent && hasRearOpponent)
            {
                break;
            }
        }

        AdvancedAgent oppAgent;

        if (hasFrontOpponent)
        {
            const float followMaxSqrDistance = 10000;
            if (oppFront.DirDot < oppCurrent.DirDot && oppCurrent.SqrDistance < followMaxSqrDistance)
            {
                // Keep following oppCurrent if that requires less steering,
                // even if another opponent is now closer.
                oppFront = oppCurrent;
            }
            else
            {
                // oppFront.DirDot < oppCurrent.DirDot is false if closest opponent didn't change.
                oppCurrent = oppFront;
            }

            oppAgent = oppFront.Agent;

            if (hasAgentDetection)
            {
                // Another agent was detected straight ahead (raycast).
                // +1 -> Clear shot.
                // -1 -> Blocked by team member.
                bool hasClearShot = detectedCollider == oppAgent.AgentPhysics.Collider;
                sensor.AddObservation((hasClearShot ? 1 : -1));
            }
            else
            {
                sensor.AddObservation((0));
            }

            float distance = Mathf.Sqrt(oppFront.SqrDistance);
            sensor.AddObservation((NormalizeDistance(distance)));

            Vector3 direction = Localize(oppFront.Direction);
            sensor.AddObservation((Util.ToPolar(direction) / 90f)); // front hemisphere -90/+90 deg

            Vector3 orientation = Localize(oppAgent.transform.forward);
            sensor.AddObservation((Util.ToPolar(orientation) / 180f));

            Vector3 velocity = Localize(oppAgent.AgentPhysics.Rigidbody.velocity) * velocityScale;
            sensor.AddObservation((Util.Sigmoid(velocity)));

            // Reward forward velocity like with BasicAgent, but focus reward on opponent
            // direction. Training should start with a low exponent, can be increased later.
            const int followRewardExp = 4;
            float reward = scaledLocalVelocity.z * velocityRewardFactor;
            reward *= Util.PowInt(direction.z, followRewardExp);
            AddReward(reward);

            // Boost acceleration if ship is pointed towards opponent.
            const int boostExp = 1;
            const float boostFactor = 0.25f;
            boostAcceleration = Util.PowInt(direction.z, boostExp) * boostFactor;
        }
        else
        {
            // Neutral values.
            sensor.AddObservation((0));
            sensor.AddObservation((1));
            sensor.AddObservation((Vector2.zero));
            sensor.AddObservation((Vector2.zero));
            sensor.AddObservation((Vector3.zero));
        }

        sensor.AddObservation((hasFrontOpponent ? 1 : -1));

        if (hasRearOpponent)
        {
            oppAgent = oppRear.Agent;

            float distance = Mathf.Sqrt(oppRear.SqrDistance);
            sensor.AddObservation((NormalizeDistance(distance)));

            Vector3 direction = Localize(oppRear.Direction);
            if (!hasFrontOpponent)
            {
                // Turn around to face rear opponent, direction.z is negative here.
                AddReward(direction.z);
            }
            direction.z *= -1f; // rear -> flip 
            sensor.AddObservation((Util.ToPolar(direction) / 90f)); // rear hemisphere -90/+90 deg

            Vector3 orientation = Localize(oppAgent.transform.forward);
            sensor.AddObservation((Util.ToPolar(orientation) / 180f));

            Vector3 velocity = Localize(oppAgent.AgentPhysics.Rigidbody.velocity) * velocityScale;
            sensor.AddObservation((Util.Sigmoid(velocity)));
        }
        else
        {
            // Neutral values.
            sensor.AddObservation((1));
            sensor.AddObservation((Vector2.zero));
            sensor.AddObservation((Vector2.zero));
            sensor.AddObservation((Vector3.zero));
        }
    }
```

```
Base class code:

public override void CollectObservations(VectorSensor sensor)
    {
        Rigidbody rb = AgentPhysics.Rigidbody;
        // Measured max z is 48 with drag = 2, force multiplier = 2
        // Can be higher with AdvancedAgent's additional boost.
        scaledLocalVelocity = Localize(rb.velocity) * velocityScale;
        sensor.AddObservation((Util.Sigmoid(scaledLocalVelocity)));
        // Measured max is 7 with angular drag = 5, torque multiplier = 1
        sensor.AddObservation((Util.Sigmoid(Localize(rb.angularVelocity) * 0.5f)));

        Vector3 pos = transform.position;
        CastRays(pos, sensor);

        if (isBasic)
        {
            // Neutral values for initial training without opponents.
            sensor.AddObservation(0); // detected agent (raycast)
            sensor.AddObservation(1); // front opponent distance
            sensor.AddObservation(Vector2.zero); // front opponent direction
            sensor.AddObservation(Vector2.zero); // front opponent orientation
            sensor.AddObservation(Vector3.zero); // front opponent velocity
            sensor.AddObservation(1); // has front opponent
            sensor.AddObservation(1); // rear opponent distance
            sensor.AddObservation(Vector2.zero); // rear opponent direction 
            sensor.AddObservation(Vector2.zero); // rear opponent orientation 
            sensor.AddObservation(Vector3.zero); // rear opponent velocity 

            // Reward forward speed.
            AddReward(scaledLocalVelocity.z * velocityRewardFactor);

            // One agent per asteroid field.
            asteroidField.UpdateBounds(pos);
        }
    }

```",see please see code base class code also found public override void sensor sensor vector vector ideally agent decide opponent would require observing variable number case though simplify focus two one front one behind agent distance bool false bool false iterate farthest true else true break float keep following le steering even another opponent closer else false opponent change another agent straight ahead clear shot blocked team member bool else float distance distance vector direction localize direction front hemisphere deg vector orientation localize orientation vector velocity localize velocity reward forward velocity like focus reward opponent direction training start low exponent later float reward reward reward boost acceleration ship pointed towards opponent float else neutral float distance distance vector direction localize turn around face rear opponent negative rear flip direction rear hemisphere deg vector orientation localize orientation vector velocity localize velocity else neutral base class code public override void sensor measured drag force multiplier higher additional boost localize measured angular drag torque multiplier localize vector sensor neutral initial training without agent front opponent distance front opponent direction front opponent orientation front opponent velocity front opponent rear opponent distance rear opponent direction rear opponent orientation rear opponent velocity reward forward speed one agent per asteroid field,issue,positive,negative,neutral,neutral,negative,negative
639829888,"We use `List<float>` in lots of other places, not just here. Can you please post your code? I think that's the source of the problem, since AdvancedAgent.CollectObservations is the last thing in the callstack before the system calls.",use list float lot please post code think source problem since last thing system,issue,negative,neutral,neutral,neutral,neutral,neutral
639825659,"> Can you post the contents of your `AdvancedAgent.CollectObservations` method? Can you try commenting out the contents of that method to see if that makes the error go away (since that's where the exception is coming from)?

chriselion, I commented out the method and the error does not occur, the agent is running now. I need to use VectorSensor any chance we could convert the variable from List to array so it will work on ios?",post content method try content method see error go away since exception coming method error occur agent running need use chance could convert variable list array work,issue,negative,neutral,neutral,neutral,neutral,neutral
639658786,I even changed the agents yet I still get this error. I searched it and found the other ml-agents users that encountered the same error yet there is no solution,even yet still get error found error yet solution,issue,negative,neutral,neutral,neutral,neutral,neutral
639606494,Can you post the contents of your `AdvancedAgent.CollectObservations` method? Can you try commenting out the contents of that method to see if that makes the error go away (since that's where the exception is coming from)?,post content method try content method see error go away since exception coming,issue,negative,neutral,neutral,neutral,neutral,neutral
639219943,"Branched off from master, leading to lot of unnecessary commits showing up. Will recreate this and upload a new pull request.",branched master leading lot unnecessary showing recreate new pull request,issue,negative,negative,negative,negative,negative,negative
639161285,Waiting on ~#4070~ and #4073 before merging this and splitting release branch. ,waiting splitting release branch,issue,negative,neutral,neutral,neutral,neutral,neutral
638971806,"Having similar issues, but for me I can't even get ""using tensorflow"" to work in visual studio. (Tensorflow.NET doesn't work for me either)",similar ca even get work visual studio work either,issue,negative,neutral,neutral,neutral,neutral,neutral
638847639,"@guillefix what step did you take to install the python ```mlagents``` package?

I remember getting something like this because I installed via pip remote:

```pip install mlagents```

This package normally isn't compatible if you have pulled the ML Agents repo, instead you have to **install the python packages from the repo, locally**. So **from within your repo**, in a shell:

```pip install -e ml-agents```
```pip install -e ml-agents-envs```

P.s. If you did install from pip remote (no ```-e```) then you might need to ```pip uninstall mlagents``` first.

Any other errors I got when building were just warnings about certain cuda dlls which didn't impede my progress.

",step take install python package remember getting something like via pip remote pip install package normally compatible instead install python locally within shell pip install pip install install pip remote might need pip first got building certain impede progress,issue,positive,positive,neutral,neutral,positive,positive
638805117,"No,I just try to open the ""Project"" Folder with Unity as the Guide describe.",try open project folder unity guide describe,issue,negative,neutral,neutral,neutral,neutral,neutral
638744214,"I have the same issue when training 4 agents. The other three agents are fine but on the fourth one, I get this issue.",issue training three fine fourth one get issue,issue,negative,positive,positive,positive,positive,positive
638688518,why should you use 2019.1.0a8 ? can't you at least try with a stable release like Unity 2019.1.14 ?,use ca least try stable release like unity,issue,positive,negative,negative,negative,negative,negative
638627917,"Check. We fix that locally then and wait for the next release. 

Thank you!",check fix locally wait next release thank,issue,negative,neutral,neutral,neutral,neutral,neutral
638508557,"For bookkeeping, would you mind letting us know which project you were trying to load?  Are you trying to load DevProject?",bookkeeping would mind u know project trying load trying load,issue,negative,neutral,neutral,neutral,neutral,neutral
638491083,Don't forget to remove the extra demo file !,forget remove extra file,issue,negative,neutral,neutral,neutral,neutral,neutral
638363339,"Hi @dlindmark, you're absolutely right, this is a known bug. It has been fixed on `master`: https://github.com/Unity-Technologies/ml-agents/pull/4038 and will be fixed in the next release. 
Your fix is also correct. Technically you should add `self.update_steps = policy.get_current_step()/self.steps_per_update()` and do the same for `self.reward_signal_update_steps`. ",hi absolutely right known bug fixed master fixed next release fix also correct technically add,issue,negative,positive,neutral,neutral,positive,positive
638354122,"I think we can include it with a regular expression if we want, but not sure it's a big deal.",think include regular expression want sure big deal,issue,negative,positive,positive,positive,positive,positive
638316417,Is there any other debugging information I can provide here to try and fix this issue?,information provide try fix issue,issue,negative,neutral,neutral,neutral,neutral,neutral
638278193,"hi, @andrewcoh 

Here is the whole stack trace;

(mlagentsrelease1) C:\Users\user\Desktop\ml-agents-15.0-capstone\ml-agents-release_1>mlagents-learn config/space_shooter_trainer_config.yaml --run-id=theRun1 --train
WARNING:tensorflow:From C:\Users\user\anaconda3\envs\mlagentsrelease1\lib\site-packages\tensorflow_core\python\compat\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term


                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓


 Version information:
  ml-agents: 0.16.0,
  ml-agents-envs: 0.16.0,
  Communicator API: 1.0.0,
  TensorFlow: 2.0.0
2020-06-03 18:37:13 WARNING [learn.py:534] The --train option has been deprecated. Train mode is now the default. Use --inference to run in inference mode.
WARNING:tensorflow:From C:\Users\user\anaconda3\envs\mlagentsrelease1\lib\site-packages\tensorflow_core\python\compat\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2020-06-03 18:37:15 INFO [environment.py:201] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
2020-06-03 18:37:20 INFO [environment.py:111] Connected to Unity environment with package version 1.0.0-preview and communication version 1.0.0
2020-06-03 18:37:21 INFO [environment.py:342] Connected new brain:
WarAgent?team=3
2020-06-03 18:37:21 INFO [environment.py:342] Connected new brain:
ContinuousAgent?team=1
2020-06-03 18:37:21.398975: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-06-03 18:37:21 INFO [stats.py:130] Hyperparameters for behavior name theRun1_WarAgent:
        summary_path:   theRun1_WarAgent
        model_path:     ./models/theRun1/WarAgent
        keep_checkpoints:       5
        trainer:        ppo
        batch_size:     1024
        beta:   0.005
        buffer_size:    10240
        epsilon:        0.2
        hidden_units:   128
        lambd:  0.95
        learning_rate:  0.0003
        learning_rate_schedule: linear
        max_steps:      0
        memory_size:    128
        normalize:      False
        num_epoch:      3
        num_layers:     2
        time_horizon:   64
        sequence_length:        64
        summary_freq:   10000
        use_recurrent:  False
        vis_encode_type:        simple
        save_steps:     40000
        window: 10
        reward_signals:
          extrinsic:
            strength:   1.0
            gamma:      0.99
2020-06-03 18:37:23 INFO [stats.py:130] Hyperparameters for behavior name theRun1_ContinuousAgent:
        summary_path:   theRun1_ContinuousAgent
        model_path:     ./models/theRun1/ContinuousAgent
        keep_checkpoints:       5
        trainer:        ppo
        batch_size:     2048
        buffer_size:    20480
        hidden_units:   512
        learning_rate_schedule: linear
        max_steps:      0
        time_horizon:   2048
        num_epoch:      3
        save_steps:     40000
        summary_freq:   10000
        window: 10
        reward_signals:
          extrinsic:
            strength:   1.0
            gamma:      0.99
          curiosity:
            strength:   0.02
            gamma:      0.99
            encoding_size:      128
2020-06-03 18:37:23 INFO [model_serialization.py:221] List of nodes to export for brain :WarAgent?team=3
2020-06-03 18:37:23 INFO [model_serialization.py:223]   is_continuous_control
2020-06-03 18:37:23 INFO [model_serialization.py:223]   version_number
2020-06-03 18:37:23 INFO [model_serialization.py:223]   memory_size
2020-06-03 18:37:23 INFO [model_serialization.py:223]   action_output_shape
2020-06-03 18:37:23 INFO [model_serialization.py:223]   action
Process Process-1:
Traceback (most recent call last):
  File ""C:\Users\user\anaconda3\envs\mlagentsrelease1\lib\multiprocessing\connection.py"", line 312, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
BrokenPipeError: [WinError 109] Boru sonlandı

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\user\anaconda3\envs\mlagentsrelease1\lib\multiprocessing\process.py"", line 297, in _bootstrap
    self.run()
  File ""C:\Users\user\anaconda3\envs\mlagentsrelease1\lib\multiprocessing\process.py"", line 99, in run
    self._target(*self._args, **self._kwargs)
  File ""c:\users\user\desktop\ml-agents-15.0-capstone\ml-agents-release_1\ml-agents\mlagents\trainers\subprocess_env_manager.py"", line 151, in worker
    req: EnvironmentRequest = parent_conn.recv()
  File ""C:\Users\user\anaconda3\envs\mlagentsrelease1\lib\multiprocessing\connection.py"", line 250, in recv
    buf = self._recv_bytes()
  File ""C:\Users\user\anaconda3\envs\mlagentsrelease1\lib\multiprocessing\connection.py"", line 321, in _recv_bytes
    raise EOFError
EOFError",hi whole stack trace train warning removed future version long term version information communicator warning train option train mode default use inference run inference mode warning removed future version long term listening port start training pressing play button unity editor connected unity environment package version communication version connected new brain connected new brain binary use behavior name trainer beta epsilon linear normalize false false simple window extrinsic strength gamma behavior name trainer linear window extrinsic strength gamma curiosity strength gamma list export brain action process recent call last file line err true handling exception another exception recent call last file line file line run file line worker file line file line raise,issue,positive,positive,neutral,neutral,positive,positive
638260593,"Currently, 2018.4 is the official version supported by ML-Agents.  Additionally, I see you're using `develop`. We have changed our gitflow and `develop` is no longer maintained. Our leading edge is `master`. We encourage you to use our official releases (or `master` to try new features before they are released). Closing this for now.",currently official version additionally see develop develop longer leading edge master encourage use official master try new,issue,positive,positive,neutral,neutral,positive,positive
638257297,"Hi @hecticmonke 

Can you provide the whole stack trace including the command you used to launch?",hi provide whole stack trace command used launch,issue,negative,positive,positive,positive,positive,positive
638219463,"dear @unityjeffrey and @chriselion  is the Ray sensor based on the Physics.Raycast? if so, do you think it will be possible to have the same as Physics.SphereCast ? thanks",dear ray sensor based think possible thanks,issue,positive,positive,neutral,neutral,positive,positive
637931439,Sorry for the review tag spam; I think both Arthur and Marwan have modified some of this recently.,sorry review tag think recently,issue,negative,negative,negative,negative,negative,negative
637707805,"Hi @ugurkanates 

I am assigning @vincentpierre to this issue, as he has been working on the registry solution, and doing some experimentation with colab notebooks. ",hi issue working registry solution experimentation,issue,negative,neutral,neutral,neutral,neutral,neutral
637334989,"I changed to the 0.15.1 version and the error disapeared, but there are also waring like
""The referenced script on this Behaviour (Game Object 'Agent') is missing""
and
""Couldn't connect to trainer on port 5004 using API version 0.15.0. Will perform inference instead.
UnityEngine.Debug:Log(Object)""",version error also like script behaviour game object missing could connect trainer port version perform inference instead log object,issue,negative,negative,negative,negative,negative,negative
637242633,"<img width=""1440"" alt=""スクリーンショット 2020-06-02 11 58 05"" src=""https://user-images.githubusercontent.com/54519381/83475209-d314ed80-a4c8-11ea-8269-e33da8a4641f.png"">
<img width=""1440"" alt=""スクリーンショット 2020-06-02 12 00 15"" src=""https://user-images.githubusercontent.com/54519381/83475224-dc9e5580-a4c8-11ea-9d8a-9a174b8a79e6.png"">

I got training going and there is no error message on my console. I couldn't understand why the log disappeared.

My issue is resolved.",got training going error message console could understand log issue resolved,issue,negative,neutral,neutral,neutral,neutral,neutral
637101682,"@awjuliani 

So I was updating notebook to work with current registry system(that's available in master branch). 
But couldn't find a file provider to hold files with their original file prefixes. I was trying Google Drive but that doesn't provide a file prefix. Only their convoluted codenames for upload.

Also for example I uploaded one for continuous actions and one for discrete actions but yeah. 
Hope you guys can help with hosting these files in Google Cloud services you have.

Another thing was depending on situation I will have one Notebook file and one yaml. If Colab examples added to default registry I wouldn't need yaml. Current default registry ones are all built with visual servers which doesn't work with headless Google Colab services.

Headless versions work fine(I provided zips below).  So if uploading them to Google Cloud Buckets and adding them to yaml is possible that would be nice. 

Where should Colab notebook be hold in repository by the way? couldn't find a proper place for it honestly. I was even thinking dropping into main repository files along with readme
Utils and Docs also could be a possibility but yeah.


[](environments:
  - BasicColab:
     expected_reward: 0.93
     description: https://github.com/Unity-Technologies/ml-agents/blob/release_1/docs/Learning-Environment-Examples.md#basic
     linux_url: https://drive.google.com/u/0/uc?export=download&confirm=scux&id=19xQab1vJet4zZKWewpyDyEyiflcodx1d
     darwin_url: """"
     win_url: """"
   
  - CrawlerStaticTargetColab:
     expected_reward: 2000
     description: https://github.com/Unity-Technologies/ml-agents/blob/release_1/docs/Learning-Environment-Examples.md
     linux_url: https://drive.google.com/u/0/uc?export=download&confirm=P2Bu&id=14HcSMDmofd-SP2Q1BANViePFZGrkhneB
     darwin_url: """"
     win_url: """")",notebook work current registry system available master branch could find file provider hold original file trying drive provide file prefix convoluted also example one continuous one discrete yeah hope help hosting cloud another thing depending situation one notebook file one added default registry would need current default registry built visual work headless headless work fine provided cloud possible would nice notebook hold repository way could find proper place honestly even thinking dropping main repository along also could possibility yeah description description,issue,positive,positive,positive,positive,positive,positive
637085868,"> @anupambhatnagar Thank you. You think that without improving the performance of tensorflow by adding this AVX2 I should get the same results than displayed in the RollerBall tutorial ?

",thank think without improving performance get displayed tutorial,issue,negative,neutral,neutral,neutral,neutral,neutral
637041518,@fedetask if the training runs long enough after resuming there should be plenty of swaps between the teams. Would it really matter if the training does not resume from the team where it stopped.,training long enough plenty would really matter training resume team stopped,issue,negative,positive,neutral,neutral,positive,positive
637036456,@Inza124 the demo model is probably trained with a different set of parameters. I suggest trying `--num-envs=8` if your machine has the resources. ,model probably trained different set suggest trying machine,issue,negative,neutral,neutral,neutral,neutral,neutral
637034172,@thelittleone1984 could you please run the mlagents-learn command from the root of the repo and place the config file in a subfolder. Then the models and summaries will be generated in the repo folder.,could please run command root place file folder,issue,negative,neutral,neutral,neutral,neutral,neutral
637034160,"Ok, in that case, going to close this. Thanks for the contributions on the other PR though.",case going close thanks though,issue,negative,positive,positive,positive,positive,positive
637032071,@ervteng do we store the active team when Ctrl-C is called? Should the user expect to resume training from team-0 or from the last active team?,store active team user expect resume training last active team,issue,positive,negative,neutral,neutral,negative,negative
637021884,"Were there additional changes to the content, or just the line break changes?",additional content line break,issue,negative,neutral,neutral,neutral,neutral,neutral
637012909,"Okay, I'll keep this in mind from next time. you may reject this PR.",keep mind next time may reject,issue,negative,neutral,neutral,neutral,neutral,neutral
636771765,"Hey! Thanks for your answer and what you are saying makes sense. To be honest, what I liked about it was mostly the showcasing potential. When presenting ML-Agents in a lecture or a video, it was a great way to quickly demonstrate the ability of imitation learning. ",hey thanks answer saying sense honest mostly potential lecture video great way quickly demonstrate ability imitation learning,issue,positive,positive,positive,positive,positive,positive
636515797,Please sign the CLA and also try to improve your PR Description.,please sign also try improve description,issue,positive,neutral,neutral,neutral,neutral,neutral
636130596,"I messed around with branch_release_1, branch_release_2, and master. I also tinkered with adding in the default tag and it didn't seem to make any difference whatsoever",around master also default tag seem make difference whatsoever,issue,negative,neutral,neutral,neutral,neutral,neutral
636015632,"[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/Unity-Technologies/ml-agents?pullRequest=4041) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/Unity-Technologies/ml-agents?pullRequest=4041) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: yanchaosun<br/>:x: Yanchao Sun<br/><hr/>**Yanchao Sun** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/Unity-Technologies/ml-agents?pullRequest=4041) it.</sub>",assistant check thank submission really appreciate like many open source ask sign contributor license agreement accept sun sun user need account able sign already account please add address used commit account already status still pending let u recheck,issue,positive,positive,positive,positive,positive,positive
635844158,I am facing a similar issue though I have pulled the master repository instead of branch_release_2. Could it be that in the master branch the config files are not parsed properly and the default parameter in the .yaml file is still needed?,facing similar issue though master repository instead could master branch properly default parameter file still,issue,negative,neutral,neutral,neutral,neutral,neutral
635695738,"I'll submit a PR for at least the FAQ. I'll see how the guard code could work.
Thanks again for your response and suggestion.",submit least see guard code could work thanks response suggestion,issue,negative,negative,neutral,neutral,negative,negative
635690695,"Glad that fixed it!

I think we should definitely add it to the FAQ here: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/FAQ.md#tensorflow-dependency

An additional option would be to do a try-catch around the tensorflow import here https://github.com/Unity-Technologies/ml-agents/blob/801c05bacc73ff03da2265340c88e6c2b8f3a5df/ml-agents/mlagents/tf_utils/tf.py#L3
and log a better error message if the exception contains ""DLL load failed"". I like this option since it keeps the solution close to the problem, unfortunately I'm not on Windows so it's hard for me to test.

As for the installation instructions, @mmattar (who's currently on PTO until the end of the week) has some opinions on what should and shouldn't go in the install guide, and recently rewrote some large swaths of it.

If you want to make a pull request for either the FAQ or the try-catch, please do. You're welcome to for the installation instructions too, it just might take a few days to get it reviewed.",glad fixed think definitely add additional option would around import log better error message exception load like option since solution close problem unfortunately hard test installation currently end week go install guide recently large want make pull request either please welcome installation might take day get,issue,positive,positive,positive,positive,positive,positive
635676405,"> @jdunwoody This appears to be a general tensorflow problem, and not specific to ML-Agents.
> 
> Have you tried this? [tensorflow/tensorflow#22794 (comment)](https://github.com/tensorflow/tensorflow/issues/22794#issuecomment-573297027) (which I found by following the link to https://www.tensorflow.org/install/errors)

That was exactly it! Thankyou very much!
Here is the solution in case the tensorflow link breaks:

For this problem
```
ImportError: DLL load failed: The specified module could not be found.
```
Install this
https://support.microsoft.com/en-my/help/2977003/the-latest-supported-visual-c-downloads


I also looked into the Tensorflow messageboards for reasons for this error but didn't get to the bottom of the discussion - to find this gem! Too many suggestions from the Tensorflow community suggesting various combinations of `protobuf`, `tensorflow` and `cuda` that I couldn't find a combination that works.

Would it be worthwhile adding this dependency to the mlagents installation docs, given that this requirement is now default for Linux and Windows Tensforflow?

I can raise a pull request if you'd like me to do it.",general problem specific tried comment found following link exactly much solution case link problem load module could found install also error get bottom discussion find gem many community suggesting various could find combination work would dependency installation given requirement default raise pull request like,issue,negative,positive,positive,positive,positive,positive
635516009,@andrewcoh will this break the saving in the ghost trainer?,break saving ghost trainer,issue,negative,neutral,neutral,neutral,neutral,neutral
635472753,"@jdunwoody This appears to be a general tensorflow problem, and not specific to ML-Agents. 

Have you tried this? https://github.com/tensorflow/tensorflow/issues/22794#issuecomment-573297027 (which I found by following the link to https://www.tensorflow.org/install/errors)",general problem specific tried found following link,issue,negative,positive,neutral,neutral,positive,positive
635350454,"Αnd I came to the conclusion that this might be a bug, so I posted here.",came conclusion might bug posted,issue,negative,neutral,neutral,neutral,neutral,neutral
635346626,"I have also tried with Anaconda.

```
> python --version
Python 3.7.7
```

```
> pip list (before install)

Package      Version
------------ -------------------
certifi      2020.4.5.1
pip          20.0.2
setuptools   46.4.0.post20200518
wheel        0.34.2
wincertstore 0.2
```

```
> git status
On branch release_2_branch
Your branch is up to date with 'origin/release_2_branch'.
```

```
> unity version
2018.4.23f1
```

```
> ML Agents (com.unity.ml-agents)
1.0.2 preview
```

```
> pip install mlagents

(Python37MLAgents) C:\Projects\Unity\ml-agents>pip list
Package                Version
---------------------- -------------------
absl-py                0.9.0
astunparse             1.6.3
cachetools             4.1.0
certifi                2020.4.5.1
chardet                3.0.4
cloudpickle            1.4.1
gast                   0.3.3
google-auth            1.15.0
google-auth-oauthlib   0.4.1
google-pasta           0.2.0
grpcio                 1.29.0
h5py                   2.10.0
idna                   2.9
importlib-metadata     1.6.0
Keras-Preprocessing    1.1.2
Markdown               3.2.2
mlagents               0.16.1
mlagents-envs          0.16.1
numpy                  1.18.4
oauthlib               3.1.0
opt-einsum             3.2.1
Pillow                 7.1.2
pip                    20.0.2
protobuf               3.12.2
pyasn1                 0.4.8
pyasn1-modules         0.2.8
pypiwin32              223
pywin32                227
PyYAML                 5.3.1
requests               2.23.0
requests-oauthlib      1.3.0
rsa                    4.0
scipy                  1.4.1
setuptools             46.4.0.post20200518
six                    1.15.0
tensorboard            2.2.1
tensorboard-plugin-wit 1.6.0.post3
tensorflow             2.2.0
tensorflow-estimator   2.2.0
termcolor              1.1.0
urllib3                1.25.9
Werkzeug               1.0.1
wheel                  0.34.2
wincertstore           0.2
wrapt                  1.12.1
zipp                   3.1.0
```

```
(Python37MLAgents) C:\Projects\Unity\ml-agents>mlagents-learn --help
Traceback (most recent call last):
  File ""c:\python\anaconda3\envs\python37mlagents\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\python\anaconda3\envs\python37mlagents\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\python\anaconda3\envs\python37mlagents\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""c:\python\anaconda3\envs\python37mlagents\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""c:\python\anaconda3\envs\python37mlagents\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\python\anaconda3\envs\python37mlagents\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\python\anaconda3\envs\python37mlagents\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\python\anaconda3\envs\Python37MLAgents\Scripts\mlagents-learn.exe\__main__.py"", line 4, in <module>
  File ""c:\python\anaconda3\envs\python37mlagents\lib\site-packages\mlagents\trainers\learn.py"", line 12, in <module>
    from mlagents import tf_utils
  File ""c:\python\anaconda3\envs\python37mlagents\lib\site-packages\mlagents\tf_utils\__init__.py"", line 1, in <module>
    from mlagents.tf_utils.tf import tf as tf  # noqa
  File ""c:\python\anaconda3\envs\python37mlagents\lib\site-packages\mlagents\tf_utils\tf.py"", line 3, in <module>
    import tensorflow as tf  # noqa I201
  File ""c:\python\anaconda3\envs\python37mlagents\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""c:\python\anaconda3\envs\python37mlagents\lib\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""c:\python\anaconda3\envs\python37mlagents\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""c:\python\anaconda3\envs\python37mlagents\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\python\anaconda3\envs\python37mlagents\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\python\anaconda3\envs\python37mlagents\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""c:\python\anaconda3\envs\python37mlagents\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""c:\python\anaconda3\envs\python37mlagents\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```",also tried anaconda python version python pip list install package version pip post wheel git status branch branch date unity version preview pip install pip list package version gast markdown pillow pip post six post wheel help recent call last file line module import file line module file line description file line return name file file line return spec load module could found handling exception another exception recent call last file line file line code file line module file line module import file line module import file line module import file line module import file line module import file line module raise recent call last file line module import file line module file line description file line return name file file line return spec load module could found load native see common include entire stack trace error message help,issue,negative,negative,neutral,neutral,negative,negative
635257901,"I did the modifications you suggested me, but when I release the jump button, in most of the cases 
`actionsOut[3] = m_jumpRelease;
        if (actionsOut[3] == 1f)
        {
            Debug.Log(""jump release action true"");
        }`

Doesn't print in console, and the agent doesn't recieve in `OnActionReceived` the right value, I don't understand why it get 0 from actions[3], even if m_jumpRelease un `Update` method is equal to 1",release jump button jump release action true print console agent right value understand get even un update method equal,issue,positive,positive,positive,positive,positive,positive
635223832,"Problem solved! I just uninstalled the package validation suite and it is running well! 
Thank you",problem uninstalled package validation suite running well thank,issue,negative,neutral,neutral,neutral,neutral,neutral
635023724,"My higher level concern is that both the trainers and this are using the same ""global"" np.random generator. We should probably consider moving to np.random.Generators in both places eventually, but I think this is OK for now.",higher level concern global generator probably consider moving eventually think,issue,negative,positive,positive,positive,positive,positive
635017798,"> I have a slight preference towards option 1 

@chriselion Should I change this implementation to be option 1 or is it good as is?",slight preference towards option change implementation option good,issue,negative,positive,positive,positive,positive,positive
635011910,I have a slight preference towards option 1 (new class ActionSampler) since that makes it easier to separate out what's part of the API and what's example code to get you up and running.,slight preference towards option new class since easier separate part example code get running,issue,negative,negative,neutral,neutral,negative,negative
634962160,"> Is there a particular use case you have in mind?

Usability when using the LL-API. We used to have the notebooks and the logic to sample random actions was taking a lot of space in the notebook. I am trying to make it simpler to get started with an environment from the registry",particular use case mind usability used logic sample random taking lot space notebook trying make simpler get environment registry,issue,negative,negative,negative,negative,negative,negative
634961310,Is there a particular use case you have in mind?,particular use case mind,issue,negative,positive,positive,positive,positive,positive
634900152,"Hi @rsta80,
Can you elaborate on what you mean when you say ""it doesn't work properly?""",hi elaborate mean say work properly,issue,negative,positive,neutral,neutral,positive,positive
634896132,"pre-commit sounds fine too.  But I like it in CI since you don't have to install any pre-commit hooks locally in order to catch it.  We could do both I guess but this seems like the best ""catch-all"" version. ",fine like since install locally order catch could guess like best version,issue,positive,positive,positive,positive,positive,positive
634856503,"Hi @Ordepedro,
Is there a reason you are installing the package validation suite?  The documentation states that it is an internal tool and has no guaranteed support.",hi reason package validation suite documentation internal tool support,issue,negative,neutral,neutral,neutral,neutral,neutral
634806194,Hi!!! I had posted my question in the forum and  is ***HALF TRAINED NETWORK***.,hi posted question forum half trained network,issue,negative,negative,negative,negative,negative,negative
634797458,@surfnerd Any thoughts on keeping this like it is (manual script) vs converting it to a pre-commit hook? We can also extend the check to make sure that the corresponding file is tracked in git...,keeping like manual script converting hook also extend check make sure corresponding file tracked git,issue,positive,positive,positive,positive,positive,positive
634779266,@Ordepedro are you using a clean project from the Unity hub?  I am not sure on your issue as it seems to be something with the package validation suite.,clean project unity hub sure issue something package validation suite,issue,positive,positive,positive,positive,positive,positive
634651630,"Hi @unityjeffrey ! Thank you for your help. 

I already did that but the same thing is happening: 

![image](https://user-images.githubusercontent.com/11221970/83023020-45e31c00-a024-11ea-9f4a-5057104ec004.png)
![image](https://user-images.githubusercontent.com/11221970/83023067-56939200-a024-11ea-97d1-9c94d587885f.png)
![image](https://user-images.githubusercontent.com/11221970/83023102-61e6bd80-a024-11ea-8931-ecf82ca2266b.png)
",hi thank help already thing happening image image image,issue,positive,neutral,neutral,neutral,neutral,neutral
634611213,"Hi @surfnerd 

I tried caching it in update method, but when I test it, doesn't work properly

I did this:

```
void Update() {
        m_jumpHeld = Input.GetButton(""Jump"") ? 1.0f : 0.0f;
        m_jumpRelease = Input.GetButtonUp(""Jump"") ? 1.0f : 0.0f;
        if (m_jumpRelease == 1f)
        {
            Debug.Log(""Button jump release pressed"");
        }
}
```
And my heuristic method

```
public override void Heuristic(float[] actionsOut)
    {
        actionsOut[0] = Input.GetAxisRaw (""Horizontal"");
        //Jumping
        actionsOut[1] = m_jumpHeld;
        //Jump Release
        actionsOut[3] = m_jumpRelease;
        if (actionsOut[3] == 1f)
        {
            Debug.Log(""jump release action true"");
        }
    }
```

When I run the game I have the following behavior:

![image](https://user-images.githubusercontent.com/20643610/83015852-5c3ba880-a021-11ea-8694-95137ac7fbdb.png)

I don't know if the both methods can work simultaneously, i was searching in the ml-agents examples but there are not this kind of implementation,  and why it happens  only with Input.GetButtonUp, Input.GetButtonDown, Input.GetKeyUp, Input.GetKeyDown

Thank you for your reply :) ",hi tried update method test work properly void update jump jump button jump release heuristic method public override void heuristic float horizontal release jump release action true run game following behavior image know work simultaneously searching kind implementation thank reply,issue,positive,positive,neutral,neutral,positive,positive
634382072,"It definitely works if I remove self_play entirely.  But it seems self_play is a desirable feature, so I'd like to get it working.

I put the prefab directly into the scene, but it didn't make any difference.

```
% mlagents-learn configs/sheep_wolf_config2.yaml --run-id=WolfSheep16
WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term


                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓


 Version information:
  ml-agents: 0.16.0,
  ml-agents-envs: 0.16.0,
  Communicator API: 1.0.0,
  TensorFlow: 2.2.0
2020-05-26 21:59:35 INFO [environment.py:201] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
2020-05-26 21:59:40 INFO [environment.py:111] Connected to Unity environment with package version 1.0.2-preview and communication version 1.0.0
2020-05-26 21:59:41 INFO [environment.py:343] Connected new brain:
Sheep?team=0
2020-05-26 21:59:41.025429: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-26 21:59:41.043269: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc61f263d30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-26 21:59:41.043289: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-26 21:59:41 INFO [stats.py:130] Hyperparameters for behavior name WolfSheep16_Sheep:
	summary_path:	WolfSheep16_Sheep
	model_path:	./models/WolfSheep16/Sheep
	keep_checkpoints:	5
	trainer:	ppo
	batch_size:	10
	beta:	0.005
	buffer_size:	100
	epsilon:	0.2
	hidden_units:	128
	lambd:	0.95
	learning_rate:	0.0003
	learning_rate_schedule:	linear
	max_steps:	5.0e6
	memory_size:	128
	normalize:	False
	num_epoch:	3
	num_layers:	2
	self_play:
	  window:	10
	  play_against_latest_model_ratio:	0.5
	  save_steps:	500
	  swap_steps:	1000
	  team_change:	1000
	sequence_length:	128
	summary_freq:	1000
	time_horizon:	64
	use_recurrent:	False
	reward_signals:
	  extrinsic:
	    strength:	1.0
	    gamma:	0.99
2020-05-26 21:59:44 INFO [environment.py:343] Connected new brain:
Wolf?team=1
2020-05-26 21:59:44 WARNING [env_manager.py:109] Agent manager was not created for behavior id Wolf?team=1.
2020-05-26 21:59:44 INFO [stats.py:130] Hyperparameters for behavior name WolfSheep16_Wolf:
	summary_path:	WolfSheep16_Wolf
	model_path:	./models/WolfSheep16/Wolf
	keep_checkpoints:	5
	trainer:	ppo
	batch_size:	10
	beta:	0.005
	buffer_size:	100
	epsilon:	0.2
	hidden_units:	128
	lambd:	0.95
	learning_rate:	0.0003
	learning_rate_schedule:	linear
	max_steps:	5.0e6
	memory_size:	128
	normalize:	False
	num_epoch:	3
	num_layers:	2
	self_play:
	  window:	10
	  play_against_latest_model_ratio:	0.5
	  save_steps:	500
	  swap_steps:	1000
	  team_change:	1000
	sequence_length:	128
	summary_freq:	1000
	time_horizon:	64
	use_recurrent:	False
	reward_signals:
	  extrinsic:
	    strength:	1.0
	    gamma:	0.99
2020-05-26 22:02:04 INFO [stats.py:111] WolfSheep16_Sheep: Step: 1000. Time Elapsed: 149.164 s Mean Reward: 0.381. Std of Reward: 0.925. Training.
2020-05-26 22:02:04 INFO [stats.py:116] WolfSheep16_Sheep ELO: 1203.347.
2020-05-26 22:03:04 INFO [subprocess_env_manager.py:191] UnityEnvironment worker 0: environment stopping.
2020-05-26 22:03:04 INFO [model_serialization.py:221] List of nodes to export for brain :Sheep?team=0
2020-05-26 22:03:04 INFO [model_serialization.py:223] 	is_continuous_control
2020-05-26 22:03:04 INFO [model_serialization.py:223] 	version_number
2020-05-26 22:03:04 INFO [model_serialization.py:223] 	memory_size
2020-05-26 22:03:04 INFO [model_serialization.py:223] 	action_output_shape
2020-05-26 22:03:04 INFO [model_serialization.py:223] 	action
Traceback (most recent call last):
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 232, in start_learning
    self.reset_env_if_ready(env_manager, global_step)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 300, in reset_env_if_ready
    self.end_trainer_episodes(env, lessons_incremented)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 265, in end_trainer_episodes
    self._reset_env(env)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents-envs/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 154, in _reset_env
    env.reset(config=sampled_reset_param)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/env_manager.py"", line 67, in reset
    self.first_step_infos = self._reset_env(config)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/subprocess_env_manager.py"", line 295, in _reset_env
    ew.previous_step = EnvironmentStep(ew.recv().payload, ew.worker_id, {}, {})
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/subprocess_env_manager.py"", line 92, in recv
    raise env_exception
mlagents_envs.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that :
	 The environment does not need user interaction to launch
	 The Agents are linked to the appropriate Brains
	 The environment and the Python interface have compatible versions.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/bin/mlagents-learn"", line 11, in <module>
    load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')()
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/learn.py"", line 554, in main
    run_cli(parse_command_line())
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/learn.py"", line 550, in run_cli
    run_training(run_seed, options)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/learn.py"", line 407, in run_training
    tc.start_learning(env_manager)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents-envs/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 260, in start_learning
    self._export_graph()
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 126, in _export_graph
    self.trainers[brain_name].export_model(name_behavior_id)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/ghost/trainer.py"", line 315, in export_model
    self.trainer.export_model(brain_name)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer/trainer.py"", line 134, in export_model
    export_policy_model(settings, policy.graph, policy.sess)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/model_serialization.py"", line 71, in export_policy_model
    f.write(frozen_graph_def.SerializeToString())
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write
    self._prewrite_check()
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check
    compat.as_bytes(self.__name), compat.as_bytes(self.__mode))
tensorflow.python.framework.errors_impl.NotFoundError: ./models/WolfSheep16/Sheep/frozen_graph_def.pb; No such file or directory
```",definitely work remove entirely desirable feature like get working put prefab directly scene make difference warning removed future version long term version information communicator listening port start training pressing play button unity editor connected unity environment package version communication version connected new brain sheep binary use service platform host guarantee used device host default version behavior name trainer beta epsilon linear normalize false window false extrinsic strength gamma connected new brain wolf warning agent manager behavior id wolf behavior name trainer beta epsilon linear normalize false window false extrinsic strength gamma step time mean reward reward training worker environment stopping list export brain sheep action recent call last file line file line file line file line wrapped return file line file line reset file line file line raise unity environment took long respond make sure environment need user interaction launch linked appropriate brain environment python interface compatible handling exception another exception recent call last file line module file line main file line file line file line wrapped return file line file line file line file line file line file line write file line file directory,issue,positive,negative,neutral,neutral,negative,negative
634377692,"Wow, didn't even need the other test, because I forgot `EnumReflectionSensor.cs.meta`",wow even need test forgot,issue,positive,positive,neutral,neutral,positive,positive
634315381,"I made those changes, and also figured out how to support Flags. Non-flags will use all zeros for unknown values. Flags will [OR](https://docs.microsoft.com/en-us/dotnet/api/system.enum.hasflag?view=netcore-3.1) the value with each defined flag.",made also figured support use unknown value defined flag,issue,positive,negative,neutral,neutral,negative,negative
634298100,"For debugging purposes, would it be possible to create a variant of this game where the prefab is present in the scene when you begin training? My intuition is that it's a peculiarity in the way the game is built that we didn't anticipate.

Alternatively, to determine if its actually self_play, you can try removing the self_play hyperparameters and then each agent will just train individually without coordination among teams.",would possible create variant game prefab present scene begin training intuition peculiarity way game built anticipate alternatively determine actually try removing agent train individually without among,issue,negative,negative,negative,negative,negative,negative
634159049,"Also, looking at the docs for [`GetButtonDown`](https://docs.unity3d.com/ScriptReference/Input.GetButtonDown.html) it seems that it only works in the `Update` method.  This means you have to cache your action value in the update method and then set it when your `Heuristic` method is called.  For example:

```
void Update()
{
    m_JumpValue = Input.GetButtonDown(""Jump"") ? 1.0f : 0.0f;
    m_DashValue = Input.GetButtonDown(""Dash"") ? 1.0f : 0.0f;
    ...
}

void Heuristic(float[] actions)
{
    actions[4] = m_DashValue;
    actions[5] = m_JumpValue;
    ...
}
```

Let me know if this works for you. Thanks.",also looking work update method cache action value update method set heuristic method example void update jump dash void heuristic float let know work thanks,issue,positive,positive,positive,positive,positive,positive
634148793,"Hi @rsta80, 
What does your action function look like?",hi action function look like,issue,negative,positive,neutral,neutral,positive,positive
634130471,"Hi. It seems like I have been able to solve the problem, and even though the tutorial steps are very clear, I will share the exact steps I did here for the others to be able to try the same steps.

1. Created a folder in my development folder
2. I cloned the ml-agents-release_2 inside the development folder.
3. I have created my Unity project inside the development folder, next to the ml-agents-release_2 folder.
4. I opened the Unity project inside the editor and from Window->Package Manager, I checked the ""Show Preview Packages"" from the dropdown box named ""Advanced"".
5. I found the ML-AGENTS package and imported it.
6. I opened an example scene from the example projects from ml-agents-release_2 folder.
7. I have removed the default brain from the agents.
8. From the console, I have created a conda environment via "" conda create -n [virtual-env-name] python=3.x.y ""
8. Activated the environment with ""conda activate [virtual-env-name]
9. Upgraded pip3 : "" pip3 install --upgrade pip ""
10. Upgraded setuptools : "" pip3 install --upgrade setuptools ""
11. Installed mlagents : "" pip3 install mlagents ""
12. Changed directory inside ml-agents-release_2 directory : "" cd [current_directory_to_mlagents_parent] /ml-agents-release_2 ""
12. Started training : "" mlagents-learn config/trainer_config.yaml --run-id:wormsTestOne --train ""
13. Pressed play button from the Unity Editor. 
14. Works without any problems!

When problems turn back:
- When I setup the /ml-agents and /ml-agents-envs locally, the problem still happens.
- When I use a python venv instead of conda environment, problem still persists.
- When I turn on Mac OSx firewall, problem still persists.
- Even though the training is running as you can see in the image below, the warning message that's shown in the title of the issue is still showing in the Unity Editor.

<img width=""568"" alt=""Screenshot 2020-05-26 at 19 26 12"" src=""https://user-images.githubusercontent.com/28991396/82925712-c3f0e580-9f86-11ea-8ed6-9a2cf5efac98.png"">

FYI.
Best,
Ege",hi like able solve problem even though tutorial clear share exact able try folder development folder inside development folder unity project inside development folder next folder unity project inside editor package manager checked show preview box advanced found package example scene example folder removed default brain console environment via create environment activate pip pip install upgrade pip pip install upgrade pip install directory inside directory training train play button unity editor work without turn back setup locally problem still use python instead environment problem still turn mac problem still even though training running see image warning message shown title issue still showing unity editor best,issue,positive,positive,positive,positive,positive,positive
634093678,also - are you able to get training going?  is this message just popping up when you open the project?,also able get training going message open project,issue,negative,positive,positive,positive,positive,positive
634093414,Hi @egedursun are you able to get training going?  is this message just popping up when you open the project?,hi able get training going message open project,issue,negative,positive,positive,positive,positive,positive
634093004,hi @Ordepedro - this looks to be an issue with the package validate suite.  Can you update to the latest version - https://docs.unity3d.com/Packages/com.unity.package-validation-suite@0.10/manual/index.html,hi issue package validate suite update latest version,issue,negative,positive,positive,positive,positive,positive
634087582,"hi @dkal3 - can you post your question on Forums and please include more information

https://forum.unity.com/forums/ml-agents.453/",hi post question please include information,issue,negative,neutral,neutral,neutral,neutral,neutral
634084330,hi @745976804 - please use the Unity Forum (https://forum.unity.com/forums/ml-agents.453/) and provide more details there on your issue.,hi please use unity forum provide issue,issue,negative,neutral,neutral,neutral,neutral,neutral
634076750,hi @yuta0306 - can you copy / paste the screenshot when you run mlagents-learn from the command line?,hi copy paste run command line,issue,negative,neutral,neutral,neutral,neutral,neutral
634073878,"hi @cabesd - can you open you request on the forums for help (https://forum.unity.com/forums/ml-agents.453/)?  Unfortunately, we can't debug setups that are not standard (i.e. using a different trainer).",hi open request help unfortunately ca standard different trainer,issue,negative,neutral,neutral,neutral,neutral,neutral
633810540,"Changed Unity version to 2018.4.23f1 and now 3DBall and PushBlock started working.
When I try to do Basic environment, the instantiation is still a blocking statment.",unity version working try basic environment still blocking,issue,negative,neutral,neutral,neutral,neutral,neutral
633759751,I think that this will work if you use the Unity function `Instantiate` with the prefab/gameObject of the agent somewhere in your game code i.e. area controller.  The main training loop should recognize a new behavior name and create the new trainer etc.,think work use unity function agent somewhere game code area controller main training loop recognize new behavior name create new trainer,issue,negative,positive,neutral,neutral,positive,positive
633751797,I couldn't really understand. Did you rebuild the anaconda environment?,could really understand rebuild anaconda environment,issue,negative,positive,positive,positive,positive,positive
633738627,"Rebuilding the same environment, it works well. but the logs message is remained. ",environment work well message,issue,negative,neutral,neutral,neutral,neutral,neutral
633717201,"The yaml file for the ""making a new learning environment"" still seems to be missing these entries:

```
sequence_length: 64
memory_size: 128
```

After adding these, the project works.",file making new learning environment still missing project work,issue,negative,negative,neutral,neutral,negative,negative
633714626,Having the same issue on the same version of Mac OSx. Still couldn't manage to solve it.,issue version mac still could manage solve,issue,negative,neutral,neutral,neutral,neutral,neutral
633687811,I can no longer reproduce this bug. I wish I knew why.,longer reproduce bug wish knew,issue,negative,neutral,neutral,neutral,neutral,neutral
633350734,Couldn't this be done thru the side_channels (configuration or environment parameters)?,could done configuration environment,issue,negative,neutral,neutral,neutral,neutral,neutral
633350689,"Same result (see below).

```
% mlagents-learn configs/sheep_wolf_config2.yaml --run-id=WolfSheep14
WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term


                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓


 Version information:
  ml-agents: 0.16.0,
  ml-agents-envs: 0.16.0,
  Communicator API: 1.0.0,
  TensorFlow: 2.2.0
2020-05-24 22:48:12 INFO [environment.py:201] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
2020-05-24 22:48:50 INFO [environment.py:111] Connected to Unity environment with package version 1.0.0-preview and communication version 1.0.0
2020-05-24 22:48:51 INFO [environment.py:343] Connected new brain:
Sheep?team=0
2020-05-24 22:48:51.351221: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-24 22:48:51.368726: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8d19b98180 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-24 22:48:51.368746: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-24 22:48:51 INFO [stats.py:130] Hyperparameters for behavior name WolfSheep14_Sheep:
	summary_path:	WolfSheep14_Sheep
	model_path:	./models/WolfSheep14/Sheep
	keep_checkpoints:	5
	trainer:	ppo
	batch_size:	10
	beta:	0.005
	buffer_size:	100
	epsilon:	0.2
	hidden_units:	128
	lambd:	0.95
	learning_rate:	0.0003
	learning_rate_schedule:	linear
	max_steps:	5.0e6
	memory_size:	128
	normalize:	False
	num_epoch:	3
	num_layers:	2
	self_play:
	  window:	10
	  play_against_latest_model_ratio:	0.5
	  save_steps:	500
	  swap_steps:	1000
	  team_change:	10000
	sequence_length:	128
	summary_freq:	1000
	time_horizon:	64
	use_recurrent:	False
	reward_signals:
	  extrinsic:
	    strength:	1.0
	    gamma:	0.99
2020-05-24 22:48:54 INFO [environment.py:343] Connected new brain:
Wolf?team=1
2020-05-24 22:48:54 WARNING [env_manager.py:109] Agent manager was not created for behavior id Wolf?team=1.
2020-05-24 22:48:54 INFO [stats.py:130] Hyperparameters for behavior name WolfSheep14_Wolf:
	summary_path:	WolfSheep14_Wolf
	model_path:	./models/WolfSheep14/Wolf
	keep_checkpoints:	5
	trainer:	ppo
	batch_size:	10
	beta:	0.005
	buffer_size:	100
	epsilon:	0.2
	hidden_units:	128
	lambd:	0.95
	learning_rate:	0.0003
	learning_rate_schedule:	linear
	max_steps:	5.0e6
	memory_size:	128
	normalize:	False
	num_epoch:	3
	num_layers:	2
	self_play:
	  window:	10
	  play_against_latest_model_ratio:	0.5
	  save_steps:	500
	  swap_steps:	1000
	  team_change:	10000
	sequence_length:	128
	summary_freq:	1000
	time_horizon:	64
	use_recurrent:	False
	reward_signals:
	  extrinsic:
	    strength:	1.0
	    gamma:	0.99
2020-05-24 22:49:08 INFO [stats.py:111] WolfSheep14_Sheep: Step: 1000. Time Elapsed: 55.477 s Mean Reward: 0.463. Std of Reward: 0.886. Training.
2020-05-24 22:49:08 INFO [stats.py:116] WolfSheep14_Sheep ELO: 1204.079.
2020-05-24 22:49:19 INFO [stats.py:111] WolfSheep14_Sheep: Step: 2000. Time Elapsed: 66.273 s Mean Reward: 0.500. Std of Reward: 0.866. Training.
2020-05-24 22:49:19 INFO [stats.py:116] WolfSheep14_Sheep ELO: 1214.286.
2020-05-24 22:49:27 INFO [stats.py:111] WolfSheep14_Sheep: Step: 3000. Time Elapsed: 74.558 s Mean Reward: 0.750. Std of Reward: 0.661. Training.
2020-05-24 22:49:27 INFO [stats.py:116] WolfSheep14_Sheep ELO: 1225.421.
2020-05-24 22:49:35 INFO [stats.py:111] WolfSheep14_Sheep: Step: 4000. Time Elapsed: 83.123 s Mean Reward: 0.561. Std of Reward: 0.828. Training.
2020-05-24 22:49:35 INFO [stats.py:116] WolfSheep14_Sheep ELO: 1236.238.
2020-05-24 22:49:44 INFO [stats.py:111] WolfSheep14_Sheep: Step: 5000. Time Elapsed: 91.305 s Mean Reward: 0.659. Std of Reward: 0.753. Training.
2020-05-24 22:49:44 INFO [stats.py:116] WolfSheep14_Sheep ELO: 1243.298.
2020-05-24 22:49:52 INFO [stats.py:111] WolfSheep14_Sheep: Step: 6000. Time Elapsed: 99.732 s Mean Reward: 0.600. Std of Reward: 0.800. Training.
2020-05-24 22:49:52 INFO [stats.py:116] WolfSheep14_Sheep ELO: 1251.554.
2020-05-24 22:50:03 INFO [stats.py:111] WolfSheep14_Sheep: Step: 7000. Time Elapsed: 110.498 s Mean Reward: 0.476. Std of Reward: 0.879. Training.
2020-05-24 22:50:03 INFO [stats.py:116] WolfSheep14_Sheep ELO: 1258.928.
2020-05-24 22:50:11 INFO [stats.py:111] WolfSheep14_Sheep: Step: 8000. Time Elapsed: 118.667 s Mean Reward: 0.300. Std of Reward: 0.954. Training.
2020-05-24 22:50:11 INFO [stats.py:116] WolfSheep14_Sheep ELO: 1262.214.
2020-05-24 22:50:20 INFO [stats.py:111] WolfSheep14_Sheep: Step: 9000. Time Elapsed: 127.409 s Mean Reward: 0.302. Std of Reward: 0.953. Training.
2020-05-24 22:50:20 INFO [stats.py:116] WolfSheep14_Sheep ELO: 1263.285.
2020-05-24 22:50:28 INFO [stats.py:111] WolfSheep14_Sheep: Step: 10000. Time Elapsed: 135.982 s Mean Reward: 0.659. Std of Reward: 0.753. Training.
2020-05-24 22:50:28 INFO [stats.py:116] WolfSheep14_Sheep ELO: 1266.750.
2020-05-24 22:51:28 INFO [subprocess_env_manager.py:191] UnityEnvironment worker 0: environment stopping.
2020-05-24 22:51:28 INFO [model_serialization.py:221] List of nodes to export for brain :Sheep?team=0
2020-05-24 22:51:28 INFO [model_serialization.py:223] 	is_continuous_control
2020-05-24 22:51:28 INFO [model_serialization.py:223] 	version_number
2020-05-24 22:51:28 INFO [model_serialization.py:223] 	memory_size
2020-05-24 22:51:28 INFO [model_serialization.py:223] 	action_output_shape
2020-05-24 22:51:28 INFO [model_serialization.py:223] 	action
Traceback (most recent call last):
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 232, in start_learning
    self.reset_env_if_ready(env_manager, global_step)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 300, in reset_env_if_ready
    self.end_trainer_episodes(env, lessons_incremented)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 265, in end_trainer_episodes
    self._reset_env(env)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents-envs/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 154, in _reset_env
    env.reset(config=sampled_reset_param)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/env_manager.py"", line 67, in reset
    self.first_step_infos = self._reset_env(config)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/subprocess_env_manager.py"", line 295, in _reset_env
    ew.previous_step = EnvironmentStep(ew.recv().payload, ew.worker_id, {}, {})
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/subprocess_env_manager.py"", line 92, in recv
    raise env_exception
mlagents_envs.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that :
	 The environment does not need user interaction to launch
	 The Agents are linked to the appropriate Brains
	 The environment and the Python interface have compatible versions.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/bin/mlagents-learn"", line 11, in <module>
    load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')()
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/learn.py"", line 554, in main
    run_cli(parse_command_line())
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/learn.py"", line 550, in run_cli
    run_training(run_seed, options)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/learn.py"", line 407, in run_training
    tc.start_learning(env_manager)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents-envs/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 260, in start_learning
    self._export_graph()
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 126, in _export_graph
    self.trainers[brain_name].export_model(name_behavior_id)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/ghost/trainer.py"", line 315, in export_model
    self.trainer.export_model(brain_name)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer/trainer.py"", line 134, in export_model
    export_policy_model(settings, policy.graph, policy.sess)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/model_serialization.py"", line 71, in export_policy_model
    f.write(frozen_graph_def.SerializeToString())
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write
    self._prewrite_check()
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check
    compat.as_bytes(self.__name), compat.as_bytes(self.__mode))
tensorflow.python.framework.errors_impl.NotFoundError: ./models/WolfSheep14/Sheep/frozen_graph_def.pb; No such file or directory
dant@imacz :( [22:51:29] [~/Unity Local/ML]
```",result see warning removed future version long term version information communicator listening port start training pressing play button unity editor connected unity environment package version communication version connected new brain sheep binary use service platform host guarantee used device host default version behavior name trainer beta epsilon linear normalize false window false extrinsic strength gamma connected new brain wolf warning agent manager behavior id wolf behavior name trainer beta epsilon linear normalize false window false extrinsic strength gamma step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training worker environment stopping list export brain sheep action recent call last file line file line file line file line wrapped return file line file line reset file line file line raise unity environment took long respond make sure environment need user interaction launch linked appropriate brain environment python interface compatible handling exception another exception recent call last file line module file line main file line file line file line wrapped return file line file line file line file line file line file line write file line file directory,issue,positive,negative,negative,negative,negative,negative
633231759,"Hello everyone,

Is this available now? to pass a 2D array as an observation?",hello everyone available pas array observation,issue,negative,positive,positive,positive,positive,positive
633210508,"We were developing a Terrarium with different creatures, predatory and preys, and we thought it would have been interesting to put together RL with evolutionary operators. In particular, each species has different reward functions for doing different things. When a new specie is created, I need to instantiate its new agents under a new behavior name, since their reward function is different. 

I can imagine it can require quite some changes in the code and I understand something like this is not a priority  ",terrarium different predatory thought would interesting put together evolutionary particular specie different reward different new specie need new new behavior name since reward function different imagine require quite code understand something like priority,issue,positive,positive,positive,positive,positive,positive
633115694,"Thanks for looking into this.  I made the numbers smaller so I can trigger the failure faster. I've just re-run it with a buffer_size of 100 and batch_size of 10, and the same problems comes up (see paste below).

I have a Prefab made for the entire training area, which is not in the scene when it starts up. The prefab (which includes the agents and everything else) is instantiated in the Start() method of GameManager.

I call `Academy.Instance.EnvironmentStep();` at the very end of this method.

If it helps, here's [the package](https://drive.google.com/file/d/1akL4mAGvSU8MZNp8xU5INxLCaRQu1KBb/view?usp=sharing) itself.

```
% mlagents-learn configs/sheep_wolf_config2.yaml --run-id=WolfSheep13
WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term


                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓


 Version information:
  ml-agents: 0.16.0,
  ml-agents-envs: 0.16.0,
  Communicator API: 1.0.0,
  TensorFlow: 2.2.0
2020-05-23 14:38:57 INFO [environment.py:201] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
2020-05-23 14:39:12 INFO [environment.py:111] Connected to Unity environment with package version 1.0.0-preview and communication version 1.0.0
2020-05-23 14:39:13 INFO [environment.py:343] Connected new brain:
Sheep?team=0
2020-05-23 14:39:13.280284: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-23 14:39:13.299053: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbbed249720 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-23 14:39:13.299085: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-23 14:39:13 INFO [stats.py:130] Hyperparameters for behavior name WolfSheep13_Sheep:
	summary_path:	WolfSheep13_Sheep
	model_path:	./models/WolfSheep13/Sheep
	keep_checkpoints:	5
	trainer:	ppo
	batch_size:	10
	beta:	0.005
	buffer_size:	100
	epsilon:	0.2
	hidden_units:	128
	lambd:	0.95
	learning_rate:	0.0003
	learning_rate_schedule:	linear
	max_steps:	5.0e6
	memory_size:	128
	normalize:	False
	num_epoch:	3
	num_layers:	2
	self_play:
	  window:	10
	  play_against_latest_model_ratio:	0.5
	  save_steps:	500
	  swap_steps:	1000
	  team_change:	1000
	sequence_length:	128
	summary_freq:	1000
	time_horizon:	64
	use_recurrent:	False
	reward_signals:
	  extrinsic:
	    strength:	1.0
	    gamma:	0.99
2020-05-23 14:39:16 INFO [environment.py:343] Connected new brain:
Wolf?team=1
2020-05-23 14:39:16 WARNING [env_manager.py:109] Agent manager was not created for behavior id Wolf?team=1.
2020-05-23 14:39:16 INFO [stats.py:130] Hyperparameters for behavior name WolfSheep13_Wolf:
	summary_path:	WolfSheep13_Wolf
	model_path:	./models/WolfSheep13/Wolf
	keep_checkpoints:	5
	trainer:	ppo
	batch_size:	10
	beta:	0.005
	buffer_size:	100
	epsilon:	0.2
	hidden_units:	128
	lambd:	0.95
	learning_rate:	0.0003
	learning_rate_schedule:	linear
	max_steps:	5.0e6
	memory_size:	128
	normalize:	False
	num_epoch:	3
	num_layers:	2
	self_play:
	  window:	10
	  play_against_latest_model_ratio:	0.5
	  save_steps:	500
	  swap_steps:	1000
	  team_change:	1000
	sequence_length:	128
	summary_freq:	1000
	time_horizon:	64
	use_recurrent:	False
	reward_signals:
	  extrinsic:
	    strength:	1.0
	    gamma:	0.99
2020-05-23 14:39:31 INFO [stats.py:111] WolfSheep13_Sheep: Step: 1000. Time Elapsed: 34.223 s Mean Reward: 0.394. Std of Reward: 0.876. Training.
2020-05-23 14:39:31 INFO [stats.py:116] WolfSheep13_Sheep ELO: 1204.302.
2020-05-23 14:40:31 INFO [subprocess_env_manager.py:191] UnityEnvironment worker 0: environment stopping.
2020-05-23 14:40:31 INFO [model_serialization.py:221] List of nodes to export for brain :Sheep?team=0
2020-05-23 14:40:31 INFO [model_serialization.py:223] 	is_continuous_control
2020-05-23 14:40:31 INFO [model_serialization.py:223] 	version_number
2020-05-23 14:40:31 INFO [model_serialization.py:223] 	memory_size
2020-05-23 14:40:31 INFO [model_serialization.py:223] 	action_output_shape
2020-05-23 14:40:31 INFO [model_serialization.py:223] 	action
Traceback (most recent call last):
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 232, in start_learning
    self.reset_env_if_ready(env_manager, global_step)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 300, in reset_env_if_ready
    self.end_trainer_episodes(env, lessons_incremented)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 265, in end_trainer_episodes
    self._reset_env(env)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents-envs/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 154, in _reset_env
    env.reset(config=sampled_reset_param)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/env_manager.py"", line 67, in reset
    self.first_step_infos = self._reset_env(config)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/subprocess_env_manager.py"", line 295, in _reset_env
    ew.previous_step = EnvironmentStep(ew.recv().payload, ew.worker_id, {}, {})
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/subprocess_env_manager.py"", line 92, in recv
    raise env_exception
mlagents_envs.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that :
	 The environment does not need user interaction to launch
	 The Agents are linked to the appropriate Brains
	 The environment and the Python interface have compatible versions.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/bin/mlagents-learn"", line 11, in <module>
    load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')()
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/learn.py"", line 554, in main
    run_cli(parse_command_line())
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/learn.py"", line 550, in run_cli
    run_training(run_seed, options)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/learn.py"", line 407, in run_training
    tc.start_learning(env_manager)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents-envs/mlagents_envs/timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 260, in start_learning
    self._export_graph()
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer_controller.py"", line 126, in _export_graph
    self.trainers[brain_name].export_model(name_behavior_id)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/ghost/trainer.py"", line 315, in export_model
    self.trainer.export_model(brain_name)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/trainers/trainer/trainer.py"", line 134, in export_model
    export_policy_model(settings, policy.graph, policy.sess)
  File ""/Users/dant/Unity Local/ML/ml-agents/ml-agents/mlagents/model_serialization.py"", line 71, in export_policy_model
    f.write(frozen_graph_def.SerializeToString())
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write
    self._prewrite_check()
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check
    compat.as_bytes(self.__name), compat.as_bytes(self.__mode))
tensorflow.python.framework.errors_impl.NotFoundError: ./models/WolfSheep13/Sheep/frozen_graph_def.pb; No such file or directory
dant@imacz :( [14:40:32] [~/Unity Local/ML]
```
",thanks looking made smaller trigger failure faster come see paste prefab made entire training area scene prefab everything else start method call end method package warning removed future version long term version information communicator listening port start training pressing play button unity editor connected unity environment package version communication version connected new brain sheep binary use service platform host guarantee used device host default version behavior name trainer beta epsilon linear normalize false window false extrinsic strength gamma connected new brain wolf warning agent manager behavior id wolf behavior name trainer beta epsilon linear normalize false window false extrinsic strength gamma step time mean reward reward training worker environment stopping list export brain sheep action recent call last file line file line file line file line wrapped return file line file line reset file line file line raise unity environment took long respond make sure environment need user interaction launch linked appropriate brain environment python interface compatible handling exception another exception recent call last file line module file line main file line file line file line wrapped return file line file line file line file line file line file line write file line file directory,issue,positive,negative,neutral,neutral,negative,negative
633032565,"I'm here looking for the same answers. Not sure if anyone was successful in figuring out how to name the consts. My other question is, if I converted from tensorflow to onnx would it then work?",looking sure anyone successful name question converted would work,issue,positive,positive,positive,positive,positive,positive
632918068,"hi @Sebastian-Schuchmann - we deprecated this features since most games training with BC require more than just one demonstration (and that you can always save the recording and train offline).

With that, could you describe why it would be important to have this in your scenario?  We didn't hear pushback when we deprecated and assumed that most users would use the offline mode.",hi since training require one demonstration always save recording train could describe would important scenario hear assumed would use mode,issue,positive,positive,positive,positive,positive,positive
632914168,"Yes, but regardless of if I set Behavior Type to ""Inference Only"" (which shouldn't require a Python process for training, right?), it still freezes.",yes regardless set behavior type inference require python process training right still,issue,negative,positive,positive,positive,positive,positive
632913781,"Hi @DanTulovsky 

I tried to reproduce with our asymmetric self-play environment by using hyperparameters similar to yours but it seemed to work just fine.  My first thought was that maybe something would break if team change was less than the buffer size. I'd advise you to use a team change/save step that are greater than the buffer size.  Saving a policy every 500 steps when the policy only gets updated every 20000 steps means you'll save 40 instances of the exact same policy!  Unless there's a good reason for this, I'd recommend increasing this to at least the buffer size if not greater so that you only save policies once theyve been updated.  Additionally, if team change is less than the buffer size, that means that we don't actually make any updates to the policy and so swapping the team yields training against the same opponent. I'd advise setting team change to be some multiple of save step. 

That being said, I'm not sure its your problem.  I notice this warning `2020-05-21 16:12:34 WARNING [env_manager.py:109] Agent manager was not created for behavior id Wolf?team=1.` in your trace which may be the a symptom of the issue.  I'm not sure what the precise cause of this could be without more implementation details. Are both agents present in the scene upon initialization?",hi tried reproduce asymmetric environment similar work fine first thought maybe something would break team change le buffer size advise use team step greater buffer size saving policy every policy every save exact policy unless good reason recommend increasing least buffer size greater save additionally team change le buffer size actually make policy swapping team training opponent advise setting team change multiple save step said sure problem notice warning warning agent manager behavior id wolf trace may symptom issue sure precise cause could without implementation present scene upon,issue,positive,positive,positive,positive,positive,positive
632911939,"hi @Sebastian-Schuchmann - to confirm, are you referring the the Online (in-editor) behavioral cloning features in the past?  ",hi confirm behavioral past,issue,negative,negative,negative,negative,negative,negative
632907098,"hi @increpare as of version Release 1, you technically do not need to install from the Github repo (you install the C# package from the Unity Package manager and the Python using PIP).

However, you are correct in that it is not clear.  I will update the documentation and copy / paste the PR here.",hi version release technically need install install package unity package manager python pip however correct clear update documentation copy paste,issue,negative,positive,neutral,neutral,positive,positive
632906239,"that is interesting - can you elaborate a bit more detail on the request for this type of functionality.  you are right in that you would likely need to modify the python and C# code.

CC @ervteng @awjuliani @andrewcoh 
",interesting elaborate bit detail request type functionality right would likely need modify python code,issue,negative,positive,positive,positive,positive,positive
632905228,"hi @mhr - to confirm, it does not look like you are running python process to start training.  ",hi confirm look like running python process start training,issue,negative,neutral,neutral,neutral,neutral,neutral
632904758,hi @DanTulovsky - checking on this for you.  it may have to do with the fact that you are having two different agent types in self-play,hi may fact two different agent,issue,negative,neutral,neutral,neutral,neutral,neutral
632618631," I am also trying to get a PyTorch model to load into an MLAgent. I got stuck after converting to Onnx then Barracuda. After completing the conversion successfully I noticed I was not able to drag the model into the MLAgents inspector.

Is this ""Barracuda"" plug-in the solution to my problem? Does it allow MLAgents to accept Barracuda models?",also trying get model load got stuck converting barracuda conversion successfully able drag model inspector barracuda solution problem allow accept barracuda,issue,positive,positive,positive,positive,positive,positive
632552891,"> Did you run the mlagents-learn script first? That's what it's trying to connect to.

thanks, now I figure out it works now",run script first trying connect thanks figure work,issue,negative,positive,positive,positive,positive,positive
632429625,Did you run the mlagents-learn script first?  That's what it's trying to connect to.,run script first trying connect,issue,negative,positive,positive,positive,positive,positive
632273967,"Hi @GProulx ,

Sorry for my late reply...

I can't upgrade, because I have to deliver this project soon, I dont'have time to upgrade and change some things to make it works.

Is it possible to know how do you fix it in version 0.15.1 to apply to my 0.13.1?",hi sorry late reply ca upgrade deliver project soon time upgrade change make work possible know fix version apply,issue,negative,negative,negative,negative,negative,negative
632225386,"@harperj Sorry, I put the error on slack but not here. This happens during the docker build:
```
Step 19/28 : RUN pip install -e .
 ---> Running in 78b3a1ef52d7
...
Building wheels for collected packages: pyyaml
  Building wheel for pyyaml (setup.py): started
  Building wheel for pyyaml (setup.py): finished with status 'done'
  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=45919 sha256=965004bd2f7c6345862a9c5897ae7821e17459fb00147ffd4fd3d066424fdd33
  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd
Successfully built pyyaml
Installing collected packages: Pillow, cloudpickle, grpcio, numpy, protobuf, pyyaml, mlagents-envs
  Found existing installation: PyYAML 3.12
ERROR: Cannot uninstall 'PyYAML'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.
```",sorry put error slack docker build step run pip install running building collected building wheel building wheel finished status wheel directory successfully built collected pillow found installation error project thus accurately determine belong would lead partial,issue,negative,positive,positive,positive,positive,positive
632223780,"@chriselion I'm not sure I understand -- why would this version be incompatible with nightly training?

In any case, having the more flexible bounds for our requirements is a good thing.",sure understand would version incompatible nightly training case flexible good thing,issue,positive,positive,positive,positive,positive,positive
631788308,"Good call, I'll fix those on release_2_branch and update the release_2_docs tag.",good call fix update tag,issue,negative,positive,positive,positive,positive,positive
631658713,"Maybe there are too many moving parts with this change. Users have not complained their terminal has too many messages anyway.
Maybe we should not merge this since apparently it has some use.",maybe many moving change terminal many anyway maybe merge since apparently use,issue,negative,positive,positive,positive,positive,positive
631654568,"@chriselion This is fine with me. The mlagents-cloud-internal (gke_learn.py) will log whatever is printed on the console. If we assume these stdout messages are not helpful, then we don't need them. ",fine log whatever printed console assume helpful need,issue,positive,positive,positive,positive,positive,positive
631651938,Thanks for your help! I'm looking forward to having this feature released :),thanks help looking forward feature,issue,positive,positive,positive,positive,positive,positive
631649556,"@chriselion We should capture Player.log with cloud training, but it will certainly change the normal ML-Agents log file.

I don't have very strong feelings about this change -- a lot of the player output wasn't logged to STDOUT/STDERR anyway -- but I also don't find the Player logging to be problematic in any way.",capture cloud training certainly change normal log file strong change lot player output logged anyway also find player logging problematic way,issue,negative,positive,positive,positive,positive,positive
631639992,"> Memory leaks, we usually look at the cloud run and see the out-of-memory error from the executable. Not sure if this is logged to Player.log

These are not in Player.log? Weird...",memory usually look cloud run see error executable sure logged weird,issue,negative,negative,neutral,neutral,negative,negative
631636975,"> I will make a new branch off master and merge this PR into this new branch and make edits there.

Correction : After reviewing our guidelines, I duplicated your branch and made an internal pull request for it. You will be marked as co-author on this PR and I will be able to do more tests and changes directly.",make new branch master merge new branch make correction branch made internal pull request marked able directly,issue,negative,positive,positive,positive,positive,positive
631627129,"I am sure some yamato tests will be failing on master because of lines [like this](https://github.com/Unity-Technologies/ml-agents/blob/e869b5c393c920ec2fe7876d9b11019bfb2ae99c/ml-agents/tests/yamato/scripts/run_gym.py#L13) in our CI. 
I will make a new branch off master and merge this PR into this new branch and make edits there.",sure failing master like make new branch master merge new branch make,issue,negative,positive,positive,positive,positive,positive
631620048,"> I changed it but I am pretty sure no one will be able to give me an example of a bug that was fixed by looking at theses console outputs

Memory leaks, we usually look at the cloud run and see the out-of-memory error from the executable. Not sure if this is logged to Player.log",pretty sure one able give example bug fixed looking thesis console memory usually look cloud run see error executable sure logged,issue,positive,positive,positive,positive,positive,positive
631617140,"I can come in another iteration, but I think we should have some notion of versioning of a given environment (separate from the communicator version) to indicate changes/iterations on the scene itself.",come another iteration think notion given environment separate communicator version indicate scene,issue,negative,neutral,neutral,neutral,neutral,neutral
631591134,"Can you share the code you are running? This error in the trace seems to be the essential one:

`mlagents_envs.exception.UnityWorkerInUseException: Couldn't start socket communication because worker number 0 is still in use. You may need to manually close a previously opened environment or use a different worker number.`

This suggests that the same port number is being used for more than one Unity environment, thus causing the issue.",share code running error trace essential one could start socket communication worker number still use may need manually close previously environment use different worker port number used one unity environment thus causing issue,issue,negative,negative,neutral,neutral,negative,negative
631289036,"Done. Let me know what you think. 
There are some examples in the gym-unity Readme, that should be tested again. But I don't have the time to setup everything. Maybe someone can check on it who has everything installed already?",done let know think tested time setup everything maybe someone check everything already,issue,negative,neutral,neutral,neutral,neutral,neutral
631229455,Going to merge this so tomorrow morning's job picks it up.,going merge tomorrow morning job,issue,negative,neutral,neutral,neutral,neutral,neutral
631152892,@harperj @xiaomaogy Is this going to break logging on cloud training runs?,going break logging cloud training,issue,negative,neutral,neutral,neutral,neutral,neutral
631137939,I changed it but I am pretty sure no one will be able to give me an example of a bug that was fixed by looking at theses console outputs,pretty sure one able give example bug fixed looking thesis console,issue,positive,positive,positive,positive,positive,positive
631135087,Sounds fine to me. I think there are some messages that hit the console that aren't printed to Player.log (mostly during a crash). Maybe we can null out the stdout but not the stderr? I'm going to assume most of the times we want to see these are in cloud training. ,fine think hit console printed mostly crash maybe null going assume time want see cloud training,issue,negative,positive,positive,positive,positive,positive
631090251,hi @ShettyHarapanahalli - we did release a robotics env (UR3 arm) prototype a couple of weeks ago.  See the link here - https://github.com/Unity-Technologies/articulations-robot-demo/tree/mlagents,hi release ur arm prototype couple ago see link,issue,negative,neutral,neutral,neutral,neutral,neutral
631041921,"We do not use these at all. If there is a problem in the environment, we look at PlayerLogs. What is displayed in the console when an executable starts is not used at all. So I want to remove it. If we use it, then we should not remove it put pass those to our own logger, not the default one.",use problem environment look displayed console executable used want remove use remove put pas logger default one,issue,negative,neutral,neutral,neutral,neutral,neutral
630955912,@awjuliani  Yeah that should fix storage problem relating to binaries. I will keep eye on that PR ,yeah fix storage problem keep eye,issue,negative,neutral,neutral,neutral,neutral,neutral
630949234,"Hi @ugurkanates 

Thanks for making this PR. We actually have another open PR which will hopefully provide just the functionality needed to easily download binaries from the python api itself. It is still being worked on, but it should perfectly complement the work you have here: https://github.com/Unity-Technologies/ml-agents/pull/3967",hi thanks making actually another open hopefully provide functionality easily python still worked perfectly complement work,issue,positive,positive,positive,positive,positive,positive
630506955,"Just curious, what's the rationale for this? Is it that we don't want random stuff printed by the executable to show up in the logs?",curious rationale want random stuff printed executable show,issue,negative,negative,negative,negative,negative,negative
630425568,"I have cleaned up my example code to work like old Unity ML agent notebooks. No training or agent related code.
Just showing how to make random moves in both discrete & continuous environments.

Though I have possible question where to store headless/server binaries. In current code I'm holding them in my repository. Obviously this has to change for PR related

https://github.com/Unity-Technologies/ml-agents/pull/3990

",example code work like old unity agent training agent related code showing make random discrete continuous though possible question store current code holding repository obviously change related,issue,negative,negative,neutral,neutral,negative,negative
630325265,"Hi @seboz123 

Are you able to connect to and train the environment using the `mlagents-learn` command? That can give some context as to whether it is an issue with the gym wrapper/baselines or if it is an issue with the environment itself. ",hi able connect train environment command give context whether issue gym issue environment,issue,negative,positive,positive,positive,positive,positive
630324510,"Hi @ugurkanates 

Thanks for sharing this. We are very open to PRs around collab notebooks to use as example for our users. I think we would want the notebook to be relatively simple and demonstrate basic functionality of the `mlagents` or `mlagents-envs` packages. If you are willing to create a modified version of the repo you shared with just that functionality, we'd be happy to take a look at a PR based on it. ",hi thanks open around use example think would want notebook relatively simple demonstrate basic functionality willing create version functionality happy take look based,issue,positive,positive,positive,positive,positive,positive
630322139,Thanks @sangue18 for catching this. I've got a fix in https://github.com/Unity-Technologies/ml-agents/pull/3987,thanks catching got fix,issue,negative,positive,positive,positive,positive,positive
630320387,"Hello @autoli 

This is very possible with the current ML-Agents. All you have to do is give different ""Behavior Names"" to your different agents within the Unity scene, and then in your `.yaml` configuration file, add separate entries for each of the behavior names. For example, if you have a soccer game with a goalie and striker behavior names, you can set the striker to use ppo and the goalie to use sac. ",hello possible current give different behavior different within unity scene configuration file add separate behavior example soccer game goalie striker behavior set striker use goalie use sac,issue,negative,negative,neutral,neutral,negative,negative
630077335,"Sorry for the mess. I forgot about the tests and just learned about pre-commit. 

I think adjusting the test in that way is fine, because as discussed in the linked issue, vector observations should not be omitted when visual observations are also used. I added a test where both types of observations are used. In that case the observations are stored in a list with the last element containing the vectors observations. ",sorry mess forgot learned think test way fine linked issue vector visual also used added test used case list last element,issue,negative,negative,neutral,neutral,negative,negative
630043357,"Sorry, can't downgrade the label from Bug to anything less dramatic.
Complete File reference is
 ml-agents/com.unity.ml-agents/Tests/Editor/Sensor/SensorShapeValidatorTests.cs 
",sorry ca downgrade label bug anything le dramatic complete file reference,issue,negative,negative,negative,negative,negative,negative
629677981,"Hi ShivanshuPurohit, I managed to run it after shutting down run time on Colab and then running just env = UnityEnvironment(file_name='Reacher_Linux_NoVis/Reacher.x86_x64')",hi run shutting run time running,issue,negative,neutral,neutral,neutral,neutral,neutral
629501540,"I think the argument you are making is a reasonable one. So long as we can properly convey compatibility to users of the gym wrapper, it makes sense to allow this. If you would like to make a PR, we can review it. ",think argument making reasonable one long properly convey compatibility gym wrapper sense allow would like make review,issue,negative,positive,neutral,neutral,positive,positive
629449666,"Hey @awjuliani, 

thank you for your reply. But to be honest, I don't really understand your argument. Why does the Gym interface need to discard observations? If the RL algorithm that someone wants to use, only supports a single type of observation (which probably most of them do, I admit), one could just implement the environment to only provide this exact type of observations. This is super easy to do in Unity thanks to ml-agents.

But the other way round, if somebody (like me) needs more than one type of observations, they spend quite some time to make everything work. First, you do a lot of research to make sure that this indeed is not possible (it's quite unexpected behaviour for me). Then they try to figure out another way to ""hack"" this and from now on maintain their own fork. 

As far as I understand, adding this feature (or rather removing this restriction) would not influence compatibility to other RL algortithms at all. It would be nice if you can explain this in more detail. 

Sidenote: My proposed commit above would indeed break compatibility. But that can be easily resolved. ",hey thank reply honest really understand argument gym interface need discard algorithm someone use single type observation probably admit one could implement environment provide exact type super easy unity thanks way round somebody like need one type spend quite time make everything work first lot research make sure indeed possible quite unexpected behaviour try figure another way hack maintain fork far understand feature rather removing restriction would influence compatibility would nice explain detail sidenote commit would indeed break compatibility easily resolved,issue,positive,positive,positive,positive,positive,positive
629442813,"Oh lol yeah didn't see it  in master so I assumed it was typo. Sorry
",oh yeah see master assumed typo sorry,issue,negative,negative,negative,negative,negative,negative
629440015,"The API changed on master, if you are using a released version of ML-Agents, use 
`https://github.com/Unity-Technologies/ml-agents/blob/<release_tag>/docs/Readme.md`
to get the matching documentation.",master version use get matching documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
629432536,"This is solved with 1.0.0 release considering they revamped Python API and terminal state held in another object type entirely.
",release considering python terminal state another object type entirely,issue,negative,neutral,neutral,neutral,neutral,neutral
629412646,"@chriselion I do think custom CLI args is great to have as a feature of UnityEnvironment as well, though.",think custom great feature well though,issue,positive,positive,positive,positive,positive,positive
629411398,"Also, this wasn't meant to be a ""feature"" of UnityEnvironment - it was created so that a user could pass arguments to mlagents-learn and these would ""pass through"" to the executable args.",also meant feature user could pas would pas executable,issue,negative,neutral,neutral,neutral,neutral,neutral
629409213,"I don't like this approach. I agree with @harperj about the confusion for `-` and `--` - the latter is more standard outside of Unity, and it's actually what we use passing ml agents args to Academy.",like approach agree confusion latter standard outside unity actually use passing academy,issue,negative,neutral,neutral,neutral,neutral,neutral
629404953,"IMO the ""additional_args"" is more explicit so it's clearer.  The main worry I have with it is that `additional_args` is sort of self documenting -- telling you that it's possible to pass more args.  Seeing that a method takes `kwargs` doesn't tell you what they do.

A secondary thought is that this forces the environment args to be a certain format.  Unity Player uses the format `-flag` or `-argName value` but there's no reason a developer couldn't add `--arg-name=value`.",explicit clearer main worry sort self telling possible pas seeing method tell secondary thought environment certain format unity player format value reason developer could add,issue,negative,positive,neutral,neutral,positive,positive
628968968,"Made some updates : 
 - UnityEnvRegistry is no longer static and needs to be instantiated
 - Providing a `default_registry` instance that will contains our environments
 - Added a functionality to register RemoteRegistryEntry with a yaml file

yaml file looks like : 

```yaml
environments:
  - 3DBall:
     expected_reward: 100
     description: | ## 3DBall: 3D Balance Ball
      ...
     linux_url: null
     darwin_url: https://storage.googleapis.com/mlagents-test-environments/1.0.0/darwin/3DBall.zip
     win_url: null

  - Basic:
     expected_reward: 0.93
     description: | ## Basic
       ...
     linux_url: null
     darwin_url: https://storage.googleapis.com/mlagents-test-environments/1.0.0/darwin/Basic.zip
     win_url: null

```",made longer static need providing instance added functionality register file file like description balance ball null null basic description basic null null,issue,negative,positive,positive,positive,positive,positive
628930829,"Here's a sketch of what I had in mind for the remote yaml files:
```
class UnityEnvRegistry(Mapping):
    def __init__(manifest_loc_or_url: str)
        self._manifest_loc_or_url = manifest_loc_or_url
        self._synced = False

    def _populate_from_manifest():
        if self._synced:
            return

        if is_remote_url(self._manifest_loc_or_url):
            manifest = download(self._manifest_loc_or_url)
        else:
            manifest = yaml_load(self._manifest_loc_or_url)

        for env in manifest[""environments""]:
            self.register(RemoteRegistryEntry.from_manifest_entry(env))
        self._synced = True

    def clear():
        self._synced = False # Sync next time we do something
        # clear internal state

    def __getitem__(self, identifier: str) -> BaseRegistryEntry:
        self._populate_from_manifest()
        # do work
    
    def __len__(self) -> int:
        self._populate_from_manifest()
        # do work
    
    def __iter__(self) -> Iterator[Any]:
        self._populate_from_manifest()
        # do work
```",sketch mind remote class false return manifest else manifest manifest true clear false sync next time something clear internal state self identifier work self work self work,issue,positive,negative,neutral,neutral,negative,negative
628906429,"Thanks for pointing this out, @DanTulovsky 

I was thinking of the original issue. What you bring up also needs to be addressed though. I will make a PR to do so right now.",thanks pointing thinking original issue bring also need though make right,issue,positive,positive,positive,positive,positive,positive
628900921,"Do you mean in:  https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Create-New.md ?

I still see the same error:

reward_signals:
    extrinsic:
    strength: 1.0
    gamma: 0.99

",mean still see error extrinsic strength gamma,issue,negative,negative,negative,negative,negative,negative
628854429,"@awjuliani I see. Thought what you were referring to is having all files synced with master branch. 
I installed mlagents through PyPi, guess which isn't up-to-date. Is the recommended way to install mlagents locally from cloned repo then? ",see thought master branch guess way install locally,issue,negative,neutral,neutral,neutral,neutral,neutral
628848849,"Hi @henryyuanheng-wang 

According to this, it looks like you aren't actually on `master` as the current mlagents version is `0.17_dev`. ",hi according like actually master current version,issue,negative,neutral,neutral,neutral,neutral,neutral
628839036,"@c0deminded It seems the change works for me as well. Thanks! 

@awjuliani thanks for investigating. here is the version print out when running the command:

Version information:
  ml-agents: 0.16.0,
  ml-agents-envs: 0.16.0,
  Communicator API: 1.0.0,
  TensorFlow: 2.1.0
2020-05-14 15:16:43 INFO [environment.py:111] Connected to Unity environment with package version 1.0.0-preview and communication version 1.0.0
2020-05-14 15:16:43 INFO [environment.py:342] Connected new brain:
",change work well thanks thanks investigating version print running command version information communicator connected unity environment package version communication version connected new brain,issue,positive,positive,positive,positive,positive,positive
628830724,"@henryyuanheng-wang  and @Kyunghyunn 

Can you share your version information that prints when running `mlagents-learn`? I am unable to reproduce this using the latest version of master.",share version information running unable reproduce latest version master,issue,negative,neutral,neutral,neutral,neutral,neutral
628830059,"Hi all.

This should be fixed now on the last version of `master`. Thanks for calling it out though. ",hi fixed last version master thanks calling though,issue,negative,positive,positive,positive,positive,positive
628828656,"I also faced this issue when doing [this](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Create-New.md) guide.
I'v just compared the **rollerball_config.yaml** and **trainer_config.yaml** which is used to train example models and figured out some differences. I've reformatted it like **trainer_config.yaml** and it worked!

So my config file now looks like this.
```
default:
    trainer: ppo
    batch_size: 1024
    beta: 5.0e-3
    buffer_size: 10240
    epsilon: 0.2
    hidden_units: 128
    lambd: 0.95
    learning_rate: 3.0e-4
    learning_rate_schedule: linear
    max_steps: 5.0e5
    memory_size: 128
    normalize: false
    num_epoch: 3
    num_layers: 2
    time_horizon: 64
    sequence_length: 64
    summary_freq: 10000
    use_recurrent: false
    vis_encode_type: simple
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99

RollerBall:
    batch_size: 10
    buffer_size: 100
    memory_size: 128

```
",also faced issue guide used train example figured like worked file like default trainer beta epsilon linear normalize false false simple extrinsic strength gamma,issue,positive,negative,negative,negative,negative,negative
628731971,I'm having the same issue with the same error messages using the latest master branch. Both training using Editor and passing environment file fail. Thanks for the reply in advance. ,issue error latest master branch training editor passing environment file fail thanks reply advance,issue,negative,positive,neutral,neutral,positive,positive
628680237,"Additionally, please fix:

    reward_signals:
        extrinsic:
        strength: 1.0
        gamma: 0.99

should be:

    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99",additionally please fix extrinsic strength gamma extrinsic strength gamma,issue,positive,neutral,neutral,neutral,neutral,neutral
628398958,"Sorry for the delay.

Steps to reproduce the error (I encountered this error in colab):
`#get the Unity reacher environment for a ""headless"" version`

`!wget https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux_NoVis.zip`

`!unzip ""content/Reacher_Linux_NoVis.zip""`

`!pip install unityagents`

`from unityagents import UnityEnvironment`

`env = UnityEnvironment(file_name='Reacher_Linux_NoVis/Reacher.x86_x64', worker_id=5)`

This leads to the following error message:
`---------------------------------------------------------------------------`

`AttributeError                            Traceback (most recent call last)`

`/usr/local/lib/python3.6/dist-packages/unityagents/rpc_communicator.py in initialize(self, inputs)
     47             # Establish communication grpc
---> 48             self.server = grpc.server(ThreadPoolExecutor(max_workers=10))
     49             self.unity_to_external = UnityToExternalServicerImplementation()`

`/usr/local/lib/python3.6/dist-packages/grpc/__init__.py in server(thread_pool, handlers, interceptors, options, maximum_concurrent_rpcs, compression)`

`AttributeError: module 'grpc._server' has no attribute 'create_server'`


`During handling of the above exception, another exception occurred:`

`UnityTimeOutException                     Traceback (most recent call last)`

`<ipython-input-4-524433f10541> in <module>()`
`----> 1 env = UnityEnvironment(file_name='Reacher_Linux_NoVis/Reacher.x86_64', worker_id=5)`

`/usr/local/lib/python3.6/dist-packages/unityagents/environment.py in __init__(self, file_name, worker_id, base_port, curriculum, seed, docker_training, no_graphics)
     62         )
     63         try:
---> 64             aca_params = self.send_academy_parameters(rl_init_parameters_in)
     65         except UnityTimeOutException:
     66             self._close()`

`/usr/local/lib/python3.6/dist-packages/unityagents/environment.py in send_academy_parameters(self, init_parameters)
    503         inputs = UnityInput()
    504         inputs.rl_initialization_input.CopyFrom(init_parameters)
--> 505         return self.communicator.initialize(inputs).rl_initialization_output
    506 
    507     def wrap_unity_input(self, rl_input: UnityRLInput) -> UnityOutput:`

`/usr/local/lib/python3.6/dist-packages/unityagents/rpc_communicator.py in initialize(self, inputs)
     55                 ""Couldn't start socket communication because worker number {} is still in use. ""
     56                 ""You may need to manually close a previously opened environment ""
---> 57                 ""or use a different worker number."".format(str(self.worker_id)))
     58         if not self.unity_to_external.parent_conn.poll(30):
     59             raise UnityTimeOutException(
UnityTimeOutException: Couldn't start socket communication because worker number 5 is still in use. You may need to manually close a previously opened environment or use a different worker number.`
`",sorry delay reproduce error error get unity reacher environment headless version pip install import following error message recent call last initialize self establish communication server compression module attribute handling exception another exception recent call last module self curriculum seed try except self return self initialize self could start socket communication worker number still may need manually close previously environment use different worker number raise could start socket communication worker number still use may need manually close previously environment use different worker,issue,negative,negative,neutral,neutral,negative,negative
628270138,"> Hate to ask this, but does it work on Windows CMD?

I was going to ask if it works on all terminals",hate ask work going ask work,issue,negative,negative,negative,negative,negative,negative
628261277,"> @vincentpierre What about people with white background terminals?

We link to a tutorial on how to use better background color for their terminals.

![Screen Shot 2020-05-13 at 2 42 00 PM](https://user-images.githubusercontent.com/28320361/81868887-ee26ba00-9527-11ea-97d6-dea8f033955f.png)
",people white background link tutorial use better background color screen shot,issue,negative,positive,positive,positive,positive,positive
628254169,"Hi @dgiunchi 

This is possible, but not officially supported. You can use our python API to connect an ml-agents environment to any python training code. That said, only specific network architectures can be re-imported and used directly with ML-Agents. I would recommend looking more into the barracuda github documentation to learn about what models are supported in Unity: https://github.com/Unity-Technologies/barracuda-release. ",hi possible officially use python connect environment python training code said specific network used directly would recommend looking barracuda documentation learn unity,issue,negative,positive,neutral,neutral,positive,positive
628252981,"![Unity-1200X630_crop](https://user-images.githubusercontent.com/28320361/81867079-01845600-9525-11ea-9e35-2ca2f5e84f7a.png)
I found the image [here](https://unity3d.com/public-relations/downloads) and I cropped it 

Looks like this in my terminal
![Screen Shot 2020-05-13 at 2 22 06 PM](https://user-images.githubusercontent.com/28320361/81867184-2b3d7d00-9525-11ea-8536-6e53326d5a69.png)
",found image like terminal screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
628247200,"@vincentpierre - can you attach the image to the PR Description?

Also, where did you get in online?",attach image description also get,issue,negative,neutral,neutral,neutral,neutral,neutral
628177849,"Thanks for your reply @awjuliani 

I'm using the latest master release because the release-1 version doesn't have the file, config/ppo/3DBall.yaml. I checked the trainer_config.yaml from release-1 version, but I couldn't find any solution. It seems like 3DBall.yaml includes a section for the brain name (3DBall). The problem still happens.



This is the entire log.

Traceback (most recent call last):
  File ""c:\users\kyunghyunkim\anaconda3\lib\site-packages\mlagents\trainers\trainer_controller.py"", line 175, in _create_trainer_and_manager
    trainer = self.trainers[brain_name]
KeyError: '3DBall'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\kyunghyunkim\anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\kyunghyunkim\anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\KyunghyunKim\anaconda3\Scripts\mlagents-learn.exe\__main__.py"", line 7, in <module>
  File ""c:\users\kyunghyunkim\anaconda3\lib\site-packages\mlagents\trainers\learn.py"", line 554, in main
    run_cli(parse_command_line())
  File ""c:\users\kyunghyunkim\anaconda3\lib\site-packages\mlagents\trainers\learn.py"", line 550, in run_cli
    run_training(run_seed, options)
  File ""c:\users\kyunghyunkim\anaconda3\lib\site-packages\mlagents\trainers\learn.py"", line 407, in run_training
    tc.start_learning(env_manager)
  File ""c:\users\kyunghyunkim\anaconda3\lib\site-packages\mlagents_envs\timers.py"", line 305, in wrapped
    return func(*args, **kwargs)
  File ""c:\users\kyunghyunkim\anaconda3\lib\site-packages\mlagents\trainers\trainer_controller.py"", line 227, in start_learning
    self._create_trainers_and_managers(env_manager, new_behavior_ids)
  File ""c:\users\kyunghyunkim\anaconda3\lib\site-packages\mlagents\trainers\trainer_controller.py"", line 213, in _create_trainers_and_managers
    self._create_trainer_and_manager(env_manager, behavior_id)
  File ""c:\users\kyunghyunkim\anaconda3\lib\site-packages\mlagents\trainers\trainer_controller.py"", line 177, in _create_trainer_and_manager
    trainer = self.trainer_factory.generate(brain_name)
  File ""c:\users\kyunghyunkim\anaconda3\lib\site-packages\mlagents\trainers\trainer_util.py"", line 61, in generate
    self.multi_gpu,
  File ""c:\users\kyunghyunkim\anaconda3\lib\site-packages\mlagents\trainers\trainer_util.py"", line 100, in initialize_trainer
    f'Trainer config must have either a ""default"" section, or a section for the brain name ({brain_name}). '
mlagents.trainers.exception.TrainerConfigError: Trainer config must have either a ""default"" section, or a section for the brain name (3DBall). See config/trainer_config.yaml for an example.
",thanks reply latest master release version file checked version could find solution like section brain name problem still entire log recent call last file line trainer handling exception another exception recent call last file line file line code file line module file line main file line file line file line wrapped return file line file line file line trainer file line generate file line must either default section section brain name trainer must either default section section brain name see example,issue,positive,positive,positive,positive,positive,positive
628156701,"Hi @shakenes 

We currently do not support this because we want the gym wrapper to be as compatible with the majority of RL algorithms written for ATARI/Mujoco style environments. Of course, users are free to fork our implementation and make their own, but we wanted what we officially shared to maintain only the basic functionality for optimal compatibility. ",hi currently support want gym wrapper compatible majority written style course free fork implementation make officially maintain basic functionality optimal compatibility,issue,positive,positive,positive,positive,positive,positive
628151511,"Hi @Kyunghyunn 

I just tried the command you used from the `master` branch and did not get this error. Could you try pulling the latest master release, or using the `release-1` version? It is likely that you may have config files and/or code that is not all up to date. ",hi tried command used master branch get error could try latest master release version likely may code date,issue,negative,positive,positive,positive,positive,positive
628147887,"Hi @teelrc, this is indeed a bug and will be fixed soon. If you want to fix it yourself in the meantime, adding `self._set_step(0)` at line 141 in `tf_policy.py` will fix the issue. Thanks for reporting it!",hi indeed bug fixed soon want fix line fix issue thanks,issue,negative,positive,positive,positive,positive,positive
628147610,"Hi @teelrc 

This is indeed a bug. We will be addressing it shortly in a hotfix. Thanks for brining it to our attention. ",hi indeed bug shortly thanks attention,issue,negative,positive,positive,positive,positive,positive
628146220,"I'm still unclear on exactly what changed. I think the deprecation is incidental (it happened on Feb 4), and it wasn't flake8 that got deprecated, just the hook (the hook is now in the same repo as the actual package).",still unclear exactly think deprecation incidental flake got hook hook actual package,issue,negative,positive,positive,positive,positive,positive
628142455,"Hi @vwxyzjn 

We actually don't use bare sockets, but instead use gRPC to communicate protobuf objects. We have evaluated a number of different options, and decided on this as a good balance between usability, performance, and maintainability. We are of course always looking to improve performance, and will take this into account as we do so.",hi actually use bare instead use communicate number different decided good balance usability performance course always looking improve performance take account,issue,positive,positive,positive,positive,positive,positive
628140449,"HI @vwxyzjn 

Thanks for making this request. This is actually a feature we are in the early stages of putting together right now. We agree that it would be very useful for users like yourself, and we hope to have more to share in the coming months. ",hi thanks making request actually feature early together right agree would useful like hope share coming,issue,positive,positive,positive,positive,positive,positive
628139670,"Hi @vwxyzjn 

Thanks for making this request. Because of the way in which rendering takes place in Unity, it is not straightforward to pass the rendered screen to gym every step of the simulation. I agree though that this is a useful feature, and we will keep this request in mind going forward. ",hi thanks making request way rendering place unity straightforward pas screen gym every step simulation agree though useful feature keep request mind going forward,issue,positive,positive,positive,positive,positive,positive
628132755,"Hi @vwxyzjn 

This is due to the fact that the `GridWorld` environment contains nine agents, and the gym wrapper is only designed to work with single agent environments, such as `Basic`. It is possible to simply remove the additional agents in scenes such as `GridWorld` and rebuild the binary with only a single agent. That said, I agree that this is not a very informative error message, and will make a work item to have it improved.

(Logged as MLA-1004)",hi due fact environment nine gym wrapper designed work single agent basic possible simply remove additional rebuild binary single agent said agree informative error message make work item logged,issue,negative,negative,neutral,neutral,negative,negative
627902799,"Hi Chris,

I just tested it and that fixed the issue.
Sorry I couldn't test it any sooner, have a great day! :)
Claudio

Il giorno mar 12 mag 2020 alle ore 23:14 Chris Goy <notifications@github.com>
ha scritto:

> hi @evercloud <https://github.com/evercloud>,
> could you try the patch in #3949
> <https://github.com/Unity-Technologies/ml-agents/pull/3949> to see if
> that fixed the issue for you?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/3932#issuecomment-627601763>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AE6HT4EMLVM44EUZQDZJAELRRG33PANCNFSM4M3QTFNA>
> .
>


-- 
Arch. Claudio Scamporlino
Tel. +39 392 239 4010
claudio.scamporlino@gmail.com
",hi tested fixed issue sorry could test sooner great day mar mag ore goy ha hi could try patch see fixed issue reply directly view arch,issue,positive,positive,positive,positive,positive,positive
627733187,"it must be saving data to somewhere, check cfg, txt files and player prefs
You re welcome :)",must saving data somewhere check player welcome,issue,negative,positive,positive,positive,positive,positive
627718865,"Hey @ervteng I believe there may be a bug in this PR where the step isn't being reset to zero when using --initialize-from and instead it just resumes from where the model you initialized left off.  I created the bug below with repro steps:

https://github.com/Unity-Technologies/ml-agents/issues/3956
",hey believe may bug step reset zero instead model left bug,issue,negative,neutral,neutral,neutral,neutral,neutral
627621727,"> Looks good. Any documentation/examples that need to change too?

@chriselion Made the documentation changes.",good need change made documentation,issue,negative,positive,positive,positive,positive,positive
627614365,"Hi @ShivanshuPurohit @Locke637 -- there is a known issue with some Linux distributions where a port is reserved for up to 60 seconds after the application is closed.  We can't do much about that issue, unfortunately.

@ShivanshuPurohit are you instantiating the environment multiple times?  or will an environment never actually launch?  We'd appreciate any specific steps to reproduce the issue.",hi known issue port reserved application closed ca much issue unfortunately environment multiple time environment never actually launch appreciate specific reproduce issue,issue,negative,negative,neutral,neutral,negative,negative
627601763,"hi @evercloud,
could you try the patch in #3949 to see if that fixed the issue for you?",hi could try patch see fixed issue,issue,negative,positive,neutral,neutral,positive,positive
627533893,"> Looks good. Any documentation/examples that need to change too?

Yes, coming soon
",good need change yes coming soon,issue,positive,positive,positive,positive,positive,positive
627499712,"Hi @RedTachyon -- these references in the docs are correct for the master branch, which is under active development.  You can find the [docs for Release 1 here](https://github.com/Unity-Technologies/ml-agents/blob/release_1/docs/Readme.md).",hi correct master branch active development find release,issue,negative,negative,negative,negative,negative,negative
627486299,"I think BehaviorMapping is overkill, but I'm not strongly opposed to it. I think having `Mapping` as part of the interface is the important part.",think strongly opposed think part interface important part,issue,negative,positive,positive,positive,positive,positive
627330464,"Hi @evercloud,
It looks like you’ve unearthed a bug in the editor.  They are working on a fix.  In the mean time, we will be adding a workaround today so this doesn’t happen in the future.  Thank you for finding and taking the time to report this.  It helps ml-agents and unity become better with feedback like this.  
Cheers,
Chris",hi like unearthed bug editor working fix mean time today happen future thank finding taking time report unity become better feedback like,issue,positive,positive,neutral,neutral,positive,positive
627279477,"Ok, somehow this bug only happens when I install the repo with the source files. If I install it with pypi without the source files, it seems to work
",somehow bug install source install without source work,issue,negative,neutral,neutral,neutral,neutral,neutral
627054789,"> Mapping is read-only (as opposed to MutableMapping), so using Mapping as the return type should be sufficient and you could just return self._env_specs in UnityEnvironment. It wouldn't be enforced at runtime, just by type checking and IDE syntax highlighting (but a user determined to mess things up could modify BehaviorMapping _dict too).

Would you recommend dropping the `BehaviorMapping` alltogether?
I think it is safer to do it this way, it makes it harder to mess up (while not impossible) which is better in my opinion.",opposed return type sufficient could return would enforced type ide syntax user determined mess could modify would recommend dropping think way harder mess impossible better opinion,issue,negative,negative,negative,negative,negative,negative
627040217,"> Using Mapping allows me to have this property be read only.

Mapping is read-only (as opposed to MutableMapping), so using Mapping as the return type should be sufficient and you could just return `self._env_specs` in UnityEnvironment. It wouldn't be enforced at runtime, just by type checking and IDE syntax highlighting (but a user determined to mess things up could modify `BehaviorMapping _dict` too).",property read opposed return type sufficient could return would enforced type ide syntax user determined mess could modify,issue,negative,negative,negative,negative,negative,negative
627020244,"> Making it a property is much cleaner. Just curious as to the reason for creating BehaviorMapping. Is it just to have more explicit naming? From what I can tell it has the same functionality as a dictionary.

Using Mapping allows me to have this property be read only. ",making property much cleaner curious reason explicit naming tell functionality dictionary property read,issue,negative,positive,neutral,neutral,positive,positive
627016627,"Hi @evercloud,
I see that the editor keeps reloading a native plugin.  I'll ask with our editor and prefab teams to see if this is a known issue.  Thanks",hi see editor native ask editor prefab see known issue thanks,issue,negative,positive,positive,positive,positive,positive
627014496,"Yes, I sent you an email with the file. I hope this helps! :)
",yes sent file hope,issue,positive,neutral,neutral,neutral,neutral,neutral
627012743,Making it a property is much cleaner.  Just curious as to the reason for creating `BehaviorMapping`. Is it just to have more explicit naming? From what I can tell it has the same functionality as a dictionary.,making property much cleaner curious reason explicit naming tell functionality dictionary,issue,negative,positive,neutral,neutral,positive,positive
626849432,Do either of you ( @evercloud / @tarikkarsi ) have the editor.log from these sessions you could share with us?,either session could share u,issue,negative,neutral,neutral,neutral,neutral,neutral
626837957,"Same issue here, after updating ML-Agents 1.0.0 and Unity 2019.3.12f1 and 2019.3.13f1 agent prefabs started flickering and open prefab button does not work. ",issue unity agent flickering open prefab button work,issue,negative,neutral,neutral,neutral,neutral,neutral
626834014,"Hi @Fagin-H 

When you resume an existing training run with self-play, the ELO is reset to the default value.  However, the loaded policy should still be correct.  We are aware that this is not ideal and are working on a fix. We have noted this as a limitation in this [README](https://github.com/Unity-Technologies/ml-agents/tree/release_1_branch/ml-agents). 

Also, if you are not already doing so, I encourage you to use the recent 1.0 release as there were some self-play bug fixes.  ",hi resume training run reset default value however loaded policy still correct aware ideal working fix noted limitation also already encourage use recent release bug,issue,positive,positive,positive,positive,positive,positive
626680591,Also the folder envs under `C:\Users\...\.conda\envs\tensorflow_2_src_gpu\lib\site-packages\gym_unity\envs\__init__.py` only contains an __init__.py and __pycache__ directory. Shouldn't there be the file UnityToGymWrapper?,also folder directory file,issue,negative,neutral,neutral,neutral,neutral,neutral
626655912,As @zako42 already mentioned we also use different map sizes and it would be great to resize the vector observation for each episode.,already also use different map size would great resize vector observation episode,issue,positive,positive,positive,positive,positive,positive
626602824,"So I installed it via pip with pip install gym-unity==0.15.1
It installed correctly, but when I try to import it in the python interpreter I still get the Error:
`ImportError: cannot import name 'UnityToGymWrapper' from 'gym_unity.envs' (C:\Users\...\.conda\envs\tensorflow_2_src_gpu\lib\site-packages\gym_unity\envs\__init__.py)`",via pip pip install correctly try import python interpreter still get error import name,issue,negative,neutral,neutral,neutral,neutral,neutral
626337744,So are you trying to install 0.15.1 or 0.16.0? Either way maybe try reinstalling the packages specifying the correct version?,trying install either way maybe try correct version,issue,negative,neutral,neutral,neutral,neutral,neutral
625939498,"I'll give it a try when possible! Thanks for the update!
",give try possible thanks update,issue,negative,positive,neutral,neutral,positive,positive
625934714,"Awesome, great you've got it working!",awesome great got working,issue,positive,positive,positive,positive,positive,positive
625775922,"Hi :) 
Thank you for taking the time to help. 
I've updated to the v0.16 release and transform the data structure Agent.Heuristic() as the migration documentation says . 

And then I found this 
`RuntimeError: The action dimensions [1] in demonstration do not match the policy's [6]. `

So the issue was due to coherence in between action dimensions in recording and training

<img width=""284"" alt=""Screenshot 2020-05-06 at 16 54 24"" src=""https://user-images.githubusercontent.com/24204714/81402178-33667880-9131-11ea-8281-4d982903c14f.png"">
<img width=""273"" alt=""Screenshot 2020-05-06 at 17 01 07"" src=""https://user-images.githubusercontent.com/24204714/81402182-37929600-9131-11ea-8a04-24ded51e6ea7.png"">

It's working now! 💯 
Thanks for your support, and for making this change, it definetly makes the backtracking easier and describes the approach in a very semantic way ! 👍 

Keep up the good work 🥇 

",hi thank taking time help release transform data structure migration documentation found action demonstration match policy issue due coherence action recording training working thanks support making change easier approach semantic way keep good work,issue,positive,positive,positive,positive,positive,positive
625560909,"Hi @evercloud, we're trying to reproduce this.  Could you share what Unity version you're using?",hi trying reproduce could share unity version,issue,negative,neutral,neutral,neutral,neutral,neutral
625559253,"Hi @JayGameHive -- we can reproduce this bug, but have fixed it in [the latest release](https://github.com/Unity-Technologies/ml-agents/releases/tag/release_1).  Is it possible for you to update your ML-Agents version?",hi reproduce bug fixed latest release possible update version,issue,negative,positive,positive,positive,positive,positive
625545507,"Hi @SoyGema, does the same happen with v0.16 (the release 1 version of the Python trainers?) We added some better checking of demo file action/observation spaces that should throw a more verbose error if there's a size mismatch. ",hi happen release version python added better file throw verbose error size mismatch,issue,negative,positive,positive,positive,positive,positive
625540367,"Hi @AcelisWeaven, this is correct in the latest `master` branch. However the `--curriculum` is still needed for the latest release version (Release 1, Trainers v0.16). ",hi correct latest master branch however curriculum still latest release version release,issue,negative,positive,positive,positive,positive,positive
625516658,"I don't know how it got there, but the Agent (Script) attached was causing the problem. I removed it and now it works",know got agent script attached causing problem removed work,issue,negative,neutral,neutral,neutral,neutral,neutral
625360935,"Ml agent release 1.0 does not have docker target name anymore, which is causing problems when running the environment in a docker container. 

Quick Solution: I made a merge between release 1.0 and 0.15.1, where I added to the python scripts of release 1.0 the needed docker target name functions to make it work. 

Is there a better fix for that, or am I missing something to make docker run mlagents 1.0?",agent release docker target name causing running environment docker container quick solution made merge release added python release docker target name make work better fix missing something make docker run,issue,negative,positive,positive,positive,positive,positive
624760177,"I logged this as MLA-990 in our internal tracker; it's something we should handle more gracefully. Glad you got things working for now, though!",logged internal tracker something handle gracefully glad got working though,issue,positive,positive,positive,positive,positive,positive
624513146,"env_name = ""../Project/Assets/ML-Agents/Examples/3DBall"" 
env = UnityEnvironment(file_name=env_name, side_channels = [engine_configuration_channel])

UnityEnvironmentException: Couldn't launch the 3DBall environment. Provided filename does not match any environments.
I got this error，is anyone know how to solve it? Is env_name'path wrroy?",could launch environment provided match got anyone know solve,issue,negative,neutral,neutral,neutral,neutral,neutral
624426432,"Hi ,

:) Great the fix works  in  latest release . Please close the issue .

Thanks 👍 


",hi great fix work latest release please close issue thanks,issue,positive,positive,positive,positive,positive,positive
624305788,"Hi @strikeman1,
If you'd like to quickly visual the network you can also grab the .pb file and use this tool https://github.com/lutzroeder/netron",hi like quickly visual network also grab file use tool,issue,negative,neutral,neutral,neutral,neutral,neutral
624274485,"Hi @graybob,
This issue was addressed in the [latest release](https://github.com/Unity-Technologies/ml-agents/releases).  Could you give it a try and see if the issue persists?",hi issue latest release could give try see issue,issue,negative,positive,positive,positive,positive,positive
624252185,"Awesome, thanks a lot! :) will close it now",awesome thanks lot close,issue,positive,positive,positive,positive,positive,positive
624251777,"Hey @thebillkidy,
Yes calling `reset` is the way to get the environment state.  I will update the doc to add these lines of code to make that more clear.  Thank you for your diligence.  If you feel like this issue is resolved, please close it and have fun with ml-agents!",hey yes calling reset way get environment state update doc add code make clear thank diligence feel like issue resolved please close fun,issue,positive,positive,positive,positive,positive,positive
624238210,Agent Done is on the C# side. It is supposed to be called when the episode is finished.,agent done side supposed episode finished,issue,negative,neutral,neutral,neutral,neutral,neutral
624236164,@JoachimVeulemans thanks for the feedback. we will take your comments into consideration while updating the dockerfile.,thanks feedback take consideration,issue,negative,positive,positive,positive,positive,positive
624232712,"Hi @zergy,
The demonstration recorder is recording actions for each step.  It is expected that you would record multiple W's if you are holding down the W key.  The trainer needs information on a per step basis, not an event based one.  If you are taking no actions, then those get recorded as well.  

As for no-ops, those are valid recordings as well.  You might want your agent to not do anything to avoid hitting a moving obstacle, etc.  

If you have questions about the demonstration recorder please bring them to the [ML-Agents forum](https://forum.unity.com/forums/ml-agents.453/).  I don't believe this is a bug and I will close this issue for the time being. 

Cheers,
Chris",hi demonstration recorder recording step would record multiple holding key trainer need information per step basis event based one taking get well valid well might want agent anything avoid moving obstacle demonstration recorder please bring forum believe bug close issue time,issue,negative,neutral,neutral,neutral,neutral,neutral
624211247,"Hi ,

I also figured out that Ray Perception Sensor Component 2D attached to bird stops working as soon as ""SceneManager.LoadScene(SceneManager.GetActiveScene().buildIndex);"" is triggered.

I could see raycast fine till bird died(once) , once bird dies Scecemanager is triggered to reload the scene , once this happens the raycast vanishes .

I cant train without raycast.

Thanks
",hi also figured ray perception sensor component attached bird working soon triggered could see fine till bird bird triggered reload scene cant train without thanks,issue,negative,positive,positive,positive,positive,positive
624130711,"The - stupid - issue has been found! This happens when no `.reset()` call is presented to the environment. Or in more details, when having these lines:

```python
from mlagents_envs.environment import UnityEnvironment
unity_env = UnityEnvironment(f""./envs/3DBall.x86_64"", seed=1, side_channels=[])
```

The environment is non blocking, so the gRPC communicator server automatically closes since it thinks it's ""done"" (which it actually is of course...)

Transforming it into the following solves this issue and I see an observation coming in!

```python
from mlagents_envs.environment import UnityEnvironment
unity_env = UnityEnvironment(f""./envs/3DBall.x86_64"", seed=1, side_channels=[])

env_info = unity_env.reset()
print(env_info)
```

Observation:

```bash
rl_output {
  agentInfos {
    key: ""3DBall?team=0""
    value {
      value {
        observations {
          shape: 8
          float_data {
            data: -0.04766776040196419
            data: -0.08700116723775864
            data: -0.5429515838623047
            data: 4.0
            data: 0.11863136291503906
            data: 0.0
            data: 0.0
            data: 0.0
          }
        }
      }
      value {
        id: 1
        observations {
          shape: 8
          float_data {
            data: -0.04967791959643364
            data: -0.016459552571177483
            data: -1.2431511878967285
            data: 4.0
            data: 1.087937355041504
            data: 0.0
            data: 0.0
            data: 0.0
          }
        }
      }
      # ... trimmed
    }
  }
}
rl_initialization_output {
  brain_parameters {
    vector_action_size: 2
    vector_action_space_type: continuous
    brain_name: ""3DBall?team=0""
    is_training: true
  }
}
```

So for me the main question is: is this as intended, or should a feature be added that alerts the user that a server is opened but that no commands have been received yet? 😊 I would classify this as a ""nice to have"" but it makes things more clearer 😀 ",stupid issue found call environment python import environment non blocking communicator server automatically since done actually course transforming following issue see observation coming python import print observation bash key value value shape data data data data data data data data value id shape data data data data data data data data continuous true main question intended feature added user server received yet would nice clearer,issue,positive,positive,neutral,neutral,positive,positive
624006121,@anupambhatnagar I have made my suggestions on [PR#3900](https://github.com/Unity-Technologies/ml-agents/pull/3900). I hope you find them useful.,made hope find useful,issue,positive,positive,positive,positive,positive,positive
623797876,"but the question have not bean solved right now,do you know how to solve it?






------------------&nbsp;原始邮件&nbsp;------------------
发件人:&nbsp;""Chris Goy""<notifications@github.com&gt;;
发送时间:&nbsp;2020年5月5日(星期二) 凌晨1:12
收件人:&nbsp;""Unity-Technologies/ml-agents""<ml-agents@noreply.github.com&gt;;
抄送:&nbsp;""奈何 瞧！""<1481902338@qq.com&gt;;""Mention""<mention@noreply.github.com&gt;;
主题:&nbsp;Re: [Unity-Technologies/ml-agents] tensorboard:about how to display mlagent modle graph in tensorboard (#3920)





 
Hi @strikeman1, this is not an appropriate request for github.  The request label is meant for feature requests.  Please ask these types of questions in the ml-agents forum.
 Also this question has been answered in #2062 already.  Please search for answers before posting a new issue.
 Cheers,
 Chris
 
—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or unsubscribe.",question bean right know solve goy mention mention display graph hi appropriate request request label meant feature please ask forum also question already please search posting new issue reply directly view,issue,positive,positive,positive,positive,positive,positive
623792093,"@JoachimVeulemans If you would like to use a docker image with the latest version of ML-Agents, you can use [this](https://github.com/Unity-Technologies/ml-agents/blob/develop-dockerfile/ml-agents.Dockerfile) Dockerfile. The command to build the file is given at the bottom of the file. I would like to hear your feedback on it.",would like use docker image latest version use command build file given bottom file would like hear feedback,issue,positive,positive,positive,positive,positive,positive
623725244,"After an offline discussion (Thanks a lot for helping so far @surfnerd !!) Current progress: 

Python Port Check: OK
Python GRPC Server Start: OK

Currently the gRPC server seems to start and then stops almost instantly (after `netstat -tulpn`). Question I have there is that if it's due to an error or due to not receiving commands. I remember that when I created a gRPC server myself I had to have a while loop since `server.start()` is not blocking. 

AFAIK when utilizing gRPC I utilized a Servicer that I add to the server, then at the bottom of the main server.py file I always added a try while true, time.sleep loop.

Will continue tomorrow and update here ",discussion thanks lot helping far current progress python port check python server start currently server start almost instantly question due error due remember server loop since blocking add server bottom main file always added try true loop continue tomorrow update,issue,positive,positive,neutral,neutral,positive,positive
623691795,"Update: going to `RpcCommunicator.cs` on `m_Client.Exchange()` it is showing an NULL value for `unityInput` and when running `m_Client.Exchange(WrapMessage(null, 200))` separately I get: `Status(StatusCode=Unavailable, Detail=""Connect Failed"")`",update going showing null value running null separately get status connect,issue,negative,neutral,neutral,neutral,neutral,neutral
623684013,"So I added 2 exception handlers, one in the RpcCommunicator and one in the Academy (where it logged the message for ""Will perform inference instead"". This is what I got (wrapped it with DEBUG ON and DEBUG OFF):

**RpcCommunicator.cs**

```csharp
DEBUG ON
UnityEngine.DebugLogHandler:Internal_Log(LogType, LogOption, String, Object)
UnityEngine.DebugLogHandler:LogFormat(LogType, Object, String, Object[])
UnityEngine.Logger:Log(LogType, Object)
UnityEngine.Debug:LogWarning(Object)
Unity.MLAgents.RpcCommunicator:Initialize(CommunicatorInitParameters) (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Communicator\RpcCommunicator.cs:162)
Unity.MLAgents.Academy:InitializeEnvironment() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:378)
Unity.MLAgents.Academy:LazyInitialize() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:218)
Unity.MLAgents.Academy:.ctor() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:206)
Unity.MLAgents.<>c:<.cctor>b__80_0() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:78)
System.Lazy`1:CreateValue()
System.Lazy`1:LazyInitValue()
System.Lazy`1:get_Value()
Unity.MLAgents.Academy:get_Instance() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:93)
Unity.MLAgentsExamples.ProjectSettingsOverrides:Awake() (at F:\ml-agents\ml-agents-release_1\Project\Assets\ML-Agents\Examples\SharedAssets\Scripts\ProjectSettingsOverrides.cs:52)

(Filename: F Line: 0)

ICommunicator.Initialize() failed.
UnityEngine.DebugLogHandler:Internal_Log(LogType, LogOption, String, Object)
UnityEngine.DebugLogHandler:LogFormat(LogType, Object, String, Object[])
UnityEngine.Logger:Log(LogType, Object)
UnityEngine.Debug:LogWarning(Object)
Unity.MLAgents.RpcCommunicator:Initialize(CommunicatorInitParameters) (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Communicator\RpcCommunicator.cs:163)
Unity.MLAgents.Academy:InitializeEnvironment() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:378)
Unity.MLAgents.Academy:LazyInitialize() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:218)
Unity.MLAgents.Academy:.ctor() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:206)
Unity.MLAgents.<>c:<.cctor>b__80_0() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:78)
System.Lazy`1:CreateValue()
System.Lazy`1:LazyInitValue()
System.Lazy`1:get_Value()
Unity.MLAgents.Academy:get_Instance() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:93)
Unity.MLAgentsExamples.ProjectSettingsOverrides:Awake() (at F:\ml-agents\ml-agents-release_1\Project\Assets\ML-Agents\Examples\SharedAssets\Scripts\ProjectSettingsOverrides.cs:52)

(Filename: F Line: 0)

DEBUG OFF
```

**Academy.cs**

```csharp
DEBUG ON
UnityEngine.DebugLogHandler:Internal_Log(LogType, LogOption, String, Object)
UnityEngine.DebugLogHandler:LogFormat(LogType, Object, String, Object[])
UnityEngine.Logger:Log(LogType, Object)
UnityEngine.Debug:LogWarning(Object)
Unity.MLAgents.Academy:InitializeEnvironment() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:399)
Unity.MLAgents.Academy:LazyInitialize() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:218)
Unity.MLAgents.Academy:.ctor() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:206)
Unity.MLAgents.<>c:<.cctor>b__80_0() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:78)
System.Lazy`1:CreateValue()
System.Lazy`1:LazyInitValue()
System.Lazy`1:get_Value()
Unity.MLAgents.Academy:get_Instance() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:93)
Unity.MLAgentsExamples.ProjectSettingsOverrides:Awake() (at F:\ml-agents\ml-agents-release_1\Project\Assets\ML-Agents\Examples\SharedAssets\Scripts\ProjectSettingsOverrides.cs:52)

(Filename: F Line: 0)

The Communicator was unable to connect. Please make sure the External process is ready to accept communication with Unity.
UnityEngine.DebugLogHandler:Internal_Log(LogType, LogOption, String, Object)
UnityEngine.DebugLogHandler:LogFormat(LogType, Object, String, Object[])
UnityEngine.Logger:Log(LogType, Object)
UnityEngine.Debug:LogWarning(Object)
Unity.MLAgents.Academy:InitializeEnvironment() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:400)
Unity.MLAgents.Academy:LazyInitialize() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:218)
Unity.MLAgents.Academy:.ctor() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:206)
Unity.MLAgents.<>c:<.cctor>b__80_0() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:78)
System.Lazy`1:CreateValue()
System.Lazy`1:LazyInitValue()
System.Lazy`1:get_Value()
Unity.MLAgents.Academy:get_Instance() (at F:\ml-agents\ml-agents-release_1\com.unity.ml-agents\Runtime\Academy.cs:93)
Unity.MLAgentsExamples.ProjectSettingsOverrides:Awake() (at F:\ml-agents\ml-agents-release_1\Project\Assets\ML-Agents\Examples\SharedAssets\Scripts\ProjectSettingsOverrides.cs:52)

(Filename: F Line: 0)

DEBUG OFF
```",added exception one one academy logged message perform inference instead got wrapped string object object string object log object object initialize awake line string object object string object log object object initialize awake line string object object string object log object object awake line communicator unable connect please make sure external process ready accept communication unity string object object string object log object object awake line,issue,positive,positive,neutral,neutral,positive,positive
623682781,"@chriselion of course thanks
interrupted training session 
[frozen_graph_def.zip](https://github.com/Unity-Technologies/ml-agents/files/4577189/frozen_graph_def.zip)

trained to 10m steps
[frozen_graph_def.zip](https://github.com/Unity-Technologies/ml-agents/files/4577188/frozen_graph_def.zip)
",course thanks interrupted training session trained,issue,negative,positive,positive,positive,positive,positive
623662990,"I know this is asking a lot, but i'm going to ask if you could print the exception in the C# code where we print out the ""Will perform inference instead."" message. 

This should be the place where you could catch an actual exception and print it:
https://github.com/Unity-Technologies/ml-agents/blob/92163c8031a90891962baa12089e35187b8093b6/com.unity.ml-agents/Runtime/Communicator/RpcCommunicator.cs#L162

So you'd need to add `(Exception e)` to the `catch` portion then,
`Debug.Log(e.message)` or whatever you see fit.",know lot going ask could print exception code print perform inference instead message place could catch actual exception print need add exception catch portion whatever see fit,issue,negative,positive,positive,positive,positive,positive
623656856,"Hi @surfnerd, just checked it and still the same (Python Library 0.16.0 now :) 

logs:

```bash
Found path: /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall.x86_64
Mono path[0] = '/mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Managed'
Mono config path = '/mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/MonoBleedingEdge/etc'
Preloaded 'libgrpc_csharp_ext.x64.so'
Initialize engine version: 2019.3.11f1 (ceef2d848e70)
[Subsystems] Discovering subsystems at path /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/UnitySubsystems
Forcing GfxDevice: Null
GfxDevice: creating device client; threaded=0
NullGfxDevice:
    Version:  NULL 1.0 [1.0]
    Renderer: Null Device
    Vendor:   Unity Technologies
Begin MonoManager ReloadAssembly
- Completed reload, in  2.466 seconds
WARNING: Shader Unsupported: 'Autodesk Interactive' - All passes removed
WARNING: Shader Did you use #pragma only_renderers and omit this platform?
UnloadTime: 4.327100 ms
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libcoreclr.so
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libcoreclr.so
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libcoreclr.so
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libSystem.dylib
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libSystem.dylib.so
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libSystem.dylib
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libcoreclr.so
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libcoreclr.so
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libcoreclr.so
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libSystem.dylib
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libSystem.dylib.so
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libSystem.dylib
Unknown communication error between Python. Python communication protocol: 1.0.0, Python library version: 0.16.0.
(Filename: ./Runtime/Export/Debug/Debug.bindings.h Line: 35)

Couldn't connect to trainer on port 5005 using API version 1.0.0. Will perform inference instead.
(Filename: ./Runtime/Export/Debug/Debug.bindings.h Line: 35)
```

Note: I start with `python3 test.py` and the following content:

```python
from mlagents_envs.environment import UnityEnvironment
unity_env = UnityEnvironment(f""./envs/3DBall.x86_64"", seed=1, side_channels=[])
```",hi checked still python library bash found path mono path mono path initialize engine version path forcing null device client version null renderer null device vendor unity begin reload warning shader unsupported interactive removed warning shader use omit platform fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library unknown communication error python python communication protocol python library version line could connect trainer port version perform inference instead line note start python following content python import,issue,negative,negative,neutral,neutral,negative,negative
623619086,"😱 Omg, thanks a lot ! I will try re-downloading and retrying right away!!! Very strange though since I'm using the download from release_1... Maybe it's somewhere cached.

I do not override the main port BTW, just 2 lines with an import and the Unity Environment start pointing to the compiled scene.

Get Outlook for Android<https://aka.ms/ghei36>

________________________________
From: Chris Goy <notifications@github.com>
Sent: Monday, May 4, 2020 8:04:55 PM
To: Unity-Technologies/ml-agents <ml-agents@noreply.github.com>
Cc: xavier geerinck <thebillkidy@hotmail.com>; Mention <mention@noreply.github.com>
Subject: Re: [Unity-Technologies/ml-agents] Receiving Unknown Communication Error between Python (#3919)


Hi @thebillkidy<https://github.com/thebillkidy>, I noticed this in your python log Python library version: 0.17.0.dev0. This tells me you are working off of master and not off of release_1 in your python codebase.

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub<https://github.com/Unity-Technologies/ml-agents/issues/3919#issuecomment-623617219>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AAEQLMRYBUQPM7EAI5KTBS3RP37UPANCNFSM4MYC3UFQ>.
",thanks lot try right away strange though since maybe somewhere override main port import unity environment start pointing scene get outlook android goy sent may mention mention subject unknown communication error python hi python log python library version dev working master python reply directly view,issue,negative,positive,neutral,neutral,positive,positive
623617992,What's the code in your `main.py`?  Do you override the base port?,code override base port,issue,negative,negative,negative,negative,negative,negative
623617219,"Hi @thebillkidy, I noticed this in your python log `Python library version: 0.17.0.dev0.` This tells me you are working off of master and not off of release_1 in your python codebase.  Though, the communication should still work. ",hi python log python library version working master python though communication still work,issue,negative,neutral,neutral,neutral,neutral,neutral
623592028,"This happens both when:
* Running on windows -> non containerized, just python main.py
* Running on WSL 2 through sidecar injection (unity and python are in the same container here)",running non python running sidecar injection unity python container,issue,negative,neutral,neutral,neutral,neutral,neutral
623591124,@sterlingcrispin Can you attach the `frozen_graph_def.pb` file that was generated? That should help us debug the conversion to .nn file more easily.,attach file help u conversion file easily,issue,positive,positive,positive,positive,positive,positive
623590149,"Hi @strikeman1, this is not an appropriate request for github.  The `request` label is meant for feature requests.  Please ask these types of questions in the [ml-agents forum](https://forum.unity.com/forums/ml-agents.453/).  
Also this question has been answered in #2062 already.  Please search for answers before posting a new issue. 
Cheers,
Chris",hi appropriate request request label meant feature please ask forum also question already please search posting new issue,issue,positive,positive,positive,positive,positive,positive
623584822,@thebillkidy are you running both python and unity in the container?,running python unity container,issue,negative,neutral,neutral,neutral,neutral,neutral
623583820,"@RLranger @TristanBertin - the file you have for your model (`continuous2vis8vec2action.nn`) is a sample file we use for unit tests; it's not supposed to be used for the example.

For the heurisitc, please make sure you have the Heuristic method defined for your Agent. Note that the signature of the method changed between the 0.15 release and the latest release (1.0.0):
https://github.com/Unity-Technologies/ml-agents/blob/0.15.1/docs/Learning-Environment-Create-New.md#testing-the-environment
https://github.com/Unity-Technologies/ml-agents/blob/release_1_docs/docs/Learning-Environment-Create-New.md#testing-the-environment
",file model sample file use unit supposed used example please make sure heuristic method defined agent note signature method release latest release,issue,positive,positive,positive,positive,positive,positive
623562899,"@JPhilipp You can install the `com.unity.coding` package (it's in the public registry, but not visible by default), and once you do that, there's a copy of the file in 
`{your project}/Library/PackageCache/com.unity.coding@0.1.0-preview.13/Coding~/Conventions/CSharpReference.cs`

Not exactly obvious :/ but hopefully that's enough to go on. Here's the line to add it to your list of dependencies: 
https://github.com/Unity-Technologies/ml-agents/blob/eedc3f9c052295d89bed0ac40a8e82a8fd17fead/DevProject/Packages/manifest.json#L7
",install package public registry visible default copy file project exactly obvious hopefully enough go line add list,issue,negative,neutral,neutral,neutral,neutral,neutral
623481811,"Hi @OcinProgrammer , 

It's working fine for me with version 0.15.1.  Possible for you to upgrade and test it?",hi working fine version possible upgrade test,issue,negative,positive,positive,positive,positive,positive
623455547,"Hi , this happened to me when I use Master branch , use tag release_1 instead and give a try.
git clone --branch release_1   https://github.com/Unity-Technologies/ml-agents.git

Thanks

  ",hi use master branch use tag instead give try git clone branch thanks,issue,negative,positive,positive,positive,positive,positive
623143359,"@surfnerd: Thanks for your swift reply, here are the terminal logs I see:

```bash
xavier@<MASKED>:/mnt/f/project-reinforcement-learning/src/Servers/ML-Agents$ python3 test.py
Found path: /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall.x86_64
Mono path[0] = '/mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Managed'
Mono config path = '/mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/MonoBleedingEdge/etc'
Preloaded 'libgrpc_csharp_ext.x64.so'
Initialize engine version: 2019.3.11f1 (ceef2d848e70)
[Subsystems] Discovering subsystems at path /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/UnitySubsystems
Forcing GfxDevice: Null
GfxDevice: creating device client; threaded=0
NullGfxDevice:
    Version:  NULL 1.0 [1.0]
    Renderer: Null Device
    Vendor:   Unity Technologies
Begin MonoManager ReloadAssembly
- Completed reload, in  3.616 seconds
WARNING: Shader Unsupported: 'Autodesk Interactive' - All passes removed
WARNING: Shader Did you use #pragma only_renderers and omit this platform?
UnloadTime: 5.149800 ms
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libcoreclr.so
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libcoreclr.so
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libcoreclr.so
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libSystem.dylib
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libSystem.dylib.so
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libSystem.dylib
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libcoreclr.so
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libcoreclr.so
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libcoreclr.so
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libSystem.dylib
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libSystem.dylib.so
Fallback handler could not load library /mnt/f/project-reinforcement-learning/src/Servers/ML-Agents/./envs/3DBall_Data/Mono/libSystem.dylib
Unknown communication error between Python. Python communication protocol: 1.0.0, Python library version: 0.17.0.dev0.
(Filename: ./Runtime/Export/Debug/Debug.bindings.h Line: 35)

Couldn't connect to trainer on port 5005 using API version 1.0.0. Will perform inference instead.
(Filename: ./Runtime/Export/Debug/Debug.bindings.h Line: 35)
```",thanks swift reply terminal see bash masked python found path mono path mono path initialize engine version path forcing null device client version null renderer null device vendor unity begin reload warning shader unsupported interactive removed warning shader use omit platform fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library unknown communication error python python communication protocol python library version dev line could connect trainer port version perform inference instead line,issue,negative,positive,neutral,neutral,positive,positive
623142867,"Hi @thebillkidy,
Do you happen to have the python terminal logs you can share?  Off the top of my head I can’t think of what it might be. ",hi happen python terminal share top head think might,issue,negative,positive,positive,positive,positive,positive
622702806,"hey @surfnerd the tensor name is ```action```  if I print it here in ```CheckOutputTensorShape``` which appears to be the relevant area,
```
            // If the model expects an output but it is not in this list
            foreach (var name in model.outputs)
            {
                if (tensorTester.ContainsKey(name))
                {
                    var tester = tensorTester[name];
                    Debug.Log(name);
                    var error = tester.Invoke(brainParameters, model.GetShapeByName(name), modelActionSize);
                    if (error != null)
                    {
                        failedModelChecks.Add(error);
                    }
                }
            }
```",hey tensor name action print relevant area model output list name name tester name name error name error null error,issue,negative,positive,positive,positive,positive,positive
622655642,"> Have you fixed your error ? I have the same issue...

I didn't solve this, so I turn back to the previous version.",fixed error issue solve turn back previous version,issue,negative,negative,neutral,neutral,negative,negative
622591611,"You should be able to print out the tensor name in this function:
`MLAgents.InferenceBrain.BarracudaModelParamLoader.CheckOutputTensorShape`",able print tensor name function,issue,negative,positive,positive,positive,positive,positive
622591382,"Hi @sterlingcrispin,
Could you debug in the unity editor, or print out the name of the output tensor it's trying to get?

@mantasp, any thoughts?",hi could unity editor print name output tensor trying get,issue,negative,neutral,neutral,neutral,neutral,neutral
622541922,There were two fairly small things that I noticed could be wrong in the release docs as well. I'll PR those in into `release_1_branch` separately. ,two fairly small could wrong release well separately,issue,negative,negative,negative,negative,negative,negative
622418522,all of the code changes look good to me.  I haven't read through the doc changes yet,code look good read doc yet,issue,negative,positive,positive,positive,positive,positive
622195000,"> Now I remember that we decided this was too cumbersome, e.g. TerminalStep.state == StateType.done. Having two flags seems fine.

We no longer need two flags since the done flag is now implicit if the Agent is in the TerminationSteps

I will wait on release to be merged into master to merge this.",remember decided cumbersome two fine longer need two since done flag implicit agent wait release master merge,issue,negative,positive,positive,positive,positive,positive
622188548,"@JPhilipp I'll find out - AFAIK the only version is in Unity's internal repo, but there might be a public copy somewhere.",find version unity internal might public copy somewhere,issue,negative,neutral,neutral,neutral,neutral,neutral
622162049,"~Did we ever decide to propagate the enum (`done` or `interrupted` for TerminalSteps) into the Python side?~

Now I remember that we decided this was too cumbersome, e.g. `TerminalStep.state == StateType.done`. Having two flags seems fine. ",ever decide propagate done interrupted python side remember decided cumbersome two fine,issue,negative,positive,positive,positive,positive,positive
622158553,"> Unity's C# style guide

@chriselion Is this document publicly available by any chance? I'd be interested to have a look (I understand it's also Microsoft C# convention to go for .PascalCase for public fields, whereas I'm always used to Unity fields being .camelCase; which means Unity developers need to choose either-or, resulting in mixes). Thanks!",unity style guide document publicly available chance interested look understand also convention go public whereas always used unity unity need choose resulting thanks,issue,positive,positive,positive,positive,positive,positive
622155907,"Hi @surfnerd,
I didn't know the descriptions couldn't be null. I added this line to my script and got it working: `brain.vectorActionDescriptions = new string[] {""x"", ""y"", ""z""};`. Thank you for your quick and wonderful solution!",hi know could null added line script got working new string thank quick wonderful solution,issue,positive,positive,positive,positive,positive,positive
622151459,"Hi @JoachimVeulemans,
It looks like it's trying to add the BrainParameters.VectorActionDescriptions to the protobuf, but it is null. If you make this non-null it won't throw the exception.  We will fix this so the default behavior doesn't throw this ArgumentNullException.   Let me know if the workaround works for you. ",hi like trying add null make wo throw exception fix default behavior throw let know work,issue,negative,neutral,neutral,neutral,neutral,neutral
621987191,OK to close this? I think all the changes got made on the release branch...,close think got made release branch,issue,negative,neutral,neutral,neutral,neutral,neutral
621947723,"Hi I get a similar warning when I run my game, which is a board game of two agents. The RedAgent (TeamID=1) and the BlueAgent (TeamID=2)


```
(MLAgentUnityEnv) user-mbp:myData user$ mlagents-learn notebooks/trainer_config.yaml --run-id=RLGame
WARNING:tensorflow:From /Users/user/Programming/MLAgentUnityEnv/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term


                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓

        
 Version information:
  ml-agents: 0.15.1,
  ml-agents-envs: 0.15.1,
  Communicator API: 0.15.0,
  TensorFlow: 2.0.1
2020-04-30 18:45:21 INFO [environment.py:132] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
2020-04-30 18:45:27 INFO [environment.py:160] Connected to Unity environment with package version 0.15.1-preview and communication version 0.15.0
2020-04-30 18:45:27 INFO [environment.py:305] Connected new brain:
RedAgent?team=1
2020-04-30 18:45:27 INFO [trainer_controller.py:167] Hyperparameters for the GhostTrainer of brain RedAgent: 
	trainer:	ppo
	batch_size:	2048
	beta:	0.001
	buffer_size:	20480
	epsilon:	0.2
	hidden_units:	5
	lambd:	0.99
	learning_rate:	0.0003
	learning_rate_schedule:	constant
	max_steps:	6.0e4
	memory_size:	128
	normalize:	True
	num_epoch:	2
	num_layers:	2
	time_horizon:	1000
	sequence_length:	64
	summary_freq:	1000
	use_recurrent:	False
	vis_encode_type:	simple
	reward_signals:	
	  extrinsic:	
	    strength:	1.0
	    gamma:	0.99
	summary_path:	RLGame_RedAgent
	model_path:	./models/RLGame/RedAgent
	keep_checkpoints:	5
	self_play:	
	  window:	10
	  play_against_latest_model_ratio:	0.5
	  save_steps:	1000
	  swap_steps:	25000
	  team_change:	200000
2020-04-30 18:45:27.497444: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-04-30 18:45:27.510553: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fcbef0cfa00 executing computations on platform Host. Devices:
2020-04-30 18:45:27.510571: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2020-04-30 18:45:30 INFO [environment.py:305] Connected new brain:
BlueAgent?team=2
2020-04-30 18:45:30 WARNING [env_manager.py:101] Agent manager was not created for behavior id BlueAgent?team=2.
2020-04-30 18:45:30 INFO [trainer_controller.py:167] Hyperparameters for the GhostTrainer of brain BlueAgent: 
	trainer:	ppo
	batch_size:	2048
	beta:	0.001
	buffer_size:	20480
	epsilon:	0.2
	hidden_units:	5
	lambd:	0.99
	learning_rate:	0.0003
	learning_rate_schedule:	constant
	max_steps:	6.0e4
	memory_size:	128
	normalize:	True
	num_epoch:	2
	num_layers:	2
	time_horizon:	1000
	sequence_length:	64
	summary_freq:	1000
	use_recurrent:	False
	vis_encode_type:	simple
	reward_signals:	
	  extrinsic:	
	    strength:	1.0
	    gamma:	0.99
	summary_path:	RLGame_BlueAgent
	model_path:	./models/RLGame/BlueAgent
	keep_checkpoints:	5
	self_play:	
	  play_against_latest_model_ratio:	0.5
	  save_steps:	1000
	  swap_steps:	25000
	  team_change:	200000
2020-04-30 18:45:45 INFO [trainer.py:214] RLGame: BlueAgent: Step: 1000. Time Elapsed: 14.710 s Mean Reward: 0.387. Std of Reward: 0.539. Not Training.
2020-04-30 18:45:45 INFO [trainer.py:103]  Learning brain BlueAgent?team=2 ELO: 1201.496
Mean Opponent ELO: 1199.864 Std Opponent ELO: 0.430
2020-04-30 18:45:47 INFO [trainer.py:214] RLGame: RedAgent: Step: 1000. Time Elapsed: 20.199 s Mean Reward: -0.145. Std of Reward: 0.000. Not Training.
2020-04-30 18:45:47 INFO [trainer.py:103]  Learning brain RedAgent?team=1 ELO: 1200.500
Mean Opponent ELO: 1199.955 Std Opponent ELO: 0.144
```

In addition to that when I export my game and try it with the python code the python code recognizes only the one agent

```python
import matplotlib.pyplot as plt
import numpy as np
import sys

from mlagents_envs.environment import UnityEnvironment
from mlagents_envs.side_channel.engine_configuration_channel import EngineConfig, EngineConfigurationChannel

if (sys.version_info[0] < 3):
      raise Exception(""ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3"")

env_name = ""myGame""
engine_configuration_channel = EngineConfigurationChannel()
env = UnityEnvironment(file_name=env_name, side_channels = [engine_configuration_channel])

env.reset()

group_name = env.get_agent_groups()
print(group_name)
```
output => 

```
 ['RedAgent?team=1']
```

this is my yaml file 


```yaml
efault:
    trainer: ppo
    batch_size: 1024
    beta: 5.0e-3
    buffer_size: 10240
    epsilon: 0.2
    hidden_units: 5
    lambd: 0.95
    learning_rate: 3.0e-4
    learning_rate_schedule: linear
    max_steps: 5.0e5
    memory_size: 128
    normalize: false
    num_epoch: 3
    num_layers: 2
    time_horizon: 64
    sequence_length: 64
    summary_freq: 10000
    use_recurrent: false
    vis_encode_type: simple
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99

BlueAgent:
    max_steps: 6.0e4
    learning_rate_schedule: constant
    normalize: true
    batch_size: 2048
    buffer_size: 20480
    hidden_units: 5
    num_epoch: 2
    summary_freq: 1000
    time_horizon: 1000
    lambd: 0.99
    beta: 0.001
    self_play:
        play_against_latest_model_ratio: 0.5
        save_steps: 1000
        swap_steps: 25000
        team_change: 200000

RedAgent:
    max_steps: 6.0e4
    learning_rate_schedule: constant
    normalize: true
    batch_size: 2048
    buffer_size: 20480
    hidden_units: 5
    num_epoch: 2
    summary_freq: 1000
    time_horizon: 1000
    lambd: 0.99
    beta: 0.001
    self_play:
        window: 10
        play_against_latest_model_ratio: 0.5
        save_steps: 1000
        swap_steps: 25000
        team_change: 200000
```
",hi get similar warning run game board game two user warning removed future version long term version information communicator listening port start training pressing play button unity editor connected unity environment package version communication version connected new brain brain trainer beta epsilon constant normalize true false simple extrinsic strength gamma window binary use service platform host device host default version connected new brain warning agent manager behavior id brain trainer beta epsilon constant normalize true false simple extrinsic strength gamma step time mean reward reward training learning brain mean opponent opponent step time mean reward reward training learning brain mean opponent opponent addition export game try python code python code one agent python import import import import import raise exception error onwards python print output file trainer beta epsilon linear normalize false false simple extrinsic strength gamma constant normalize true beta constant normalize true beta window,issue,positive,negative,neutral,neutral,negative,negative
621467360,"Sorry, don't feel like I know enough about Dockerfiles to give this a review.",sorry feel like know enough give review,issue,negative,negative,negative,negative,negative,negative
621456384,"Example from 3DBall with the fix
```json
{
    ""name"": ""root"",
    ""gauges"": {
        ""ppo_3DBall.Policy.Entropy.mean"": {
            ""value"": 1.3003549575805664,
            ""min"": 1.3003549575805664,
            ""max"": 1.4189382791519165,
            ""count"": 12
        },
        ""ppo_3DBall.Environment.EpisodeLength.mean"": {
            ""value"": 971.5,
            ""min"": 21.158379373848987,
            ""max"": 971.5,
            ""count"": 12
        },
        ""ppo_3DBall.Policy.ExtrinsicValueEstimate.mean"": {
            ""value"": 3.8920631408691406,
            ""min"": -0.03331759199500084,
            ""max"": 3.8920631408691406,
            ""count"": 12
        },
        ""ppo_3DBall.Environment.CumulativeReward.mean"": {
            ""value"": 97.06668154398601,
            ""min"": 1.113284105744309,
            ""max"": 97.06668154398601,
            ""count"": 12
        },
        ""ppo_3DBall.Policy.ExtrinsicReward.mean"": {
            ""value"": 97.06668154398601,
            ""min"": 1.113284105744309,
            ""max"": 97.06668154398601,
            ""count"": 12
        },
        ""ppo_3DBall.IsTraining.mean"": {
            ""value"": 1.0,
            ""min"": 1.0,
            ""max"": 1.0,
            ""count"": 12
        },
        ""ppo_3DBall.Losses.ValueLoss.mean"": {
            ""value"": 8.972146034240723,
            ""min"": 0.3149506449699402,
            ""max"": 11.991447448730469,
            ""count"": 11
        },
        ""ppo_3DBall.Losses.PolicyLoss.mean"": {
            ""value"": 0.10021013021469116,
            ""min"": 0.09831372648477554,
            ""max"": 0.10745824873447418,
            ""count"": 11
        },
        ""ppo_3DBall.Policy.LearningRate.mean"": {
            ""value"": 0.0002184834738727659,
            ""min"": 0.0002184834738727659,
            ""max"": 0.0002927808091044426,
            ""count"": 11
        }
    },
    ""metadata"": {
        ""timer_format_version"": ""0.1.0"",
        ""start_time_seconds"": ""1588192751"",
        ""python_version"": ""3.7.6 (default, Dec 30 2019, 19:38:28) \n[Clang 11.0.0 (clang-1100.0.33.16)]"",
        ""command_line_arguments"": ""/Users/foo/code/ml-agents/venv/bin/mlagents-learn config/trainer_config.yaml --force --env=Project/Build/ball --no-graphics"",
        ""mlagents_version"": ""0.16.0"",
        ""mlagents_envs_version"": ""0.16.0"",
        ""communication_protocol_version"": ""0.17.0"",
        ""tensorflow_version"": ""2.0.1"",
        ""end_time_seconds"": ""1588192850""
    },
    ""total"": 99.22794142299999,
    ""count"": 1,
    ""self"": 0.004021215999983951,
    ""children"": {
        ""run_training.setup"": {
            ""total"": 0.01560470500000033,
            ""count"": 1,
            ""self"": 0.01560470500000033
        },
        ""TrainerController.start_learning"": {
            ""total"": 99.208315502,
            ""count"": 1,
            ""self"": 9.104267448999181,
            ""children"": {
                ""TrainerController._reset_env"": {
                    ""total"": 0.8152601319999997,
                    ""count"": 1,
                    ""self"": 0.8152601319999997
                },
                ""TrainerController.advance"": {
                    ""total"": 89.07477324800081,
                    ""count"": 14848,
                    ""self"": 0.18365643900101247,
                    ""children"": {
                        ""env_step"": {
                            ""total"": 88.8911168089998,
                            ""count"": 14848,
                            ""self"": 77.19064573999994,
                            ""children"": {
                                ""SubprocessEnvManager._take_step"": {
                                    ""total"": 11.51412037099989,
                                    ""count"": 14848,
                                    ""self"": 0.47601072899954566,
                                    ""children"": {
                                        ""NNPolicy.evaluate"": {
                                            ""total"": 11.038109642000345,
                                            ""count"": 13030,
                                            ""self"": 11.038109642000345
                                        }
                                    }
                                },
                                ""workers"": {
                                    ""total"": 0.18635069799996806,
                                    ""count"": 14847,
                                    ""self"": 0.0,
                                    ""children"": {
                                        ""worker_root"": {
                                            ""total"": 97.55444978000044,
                                            ""count"": 14847,
                                            ""is_parallel"": true,
                                            ""self"": 51.16557370200104,
                                            ""children"": {
                                                ""run_training.setup"": {
                                                    ""total"": 0.0,
                                                    ""count"": 0,
                                                    ""is_parallel"": true,
                                                    ""self"": 0.0,
                                                    ""children"": {
                                                        ""steps_from_proto"": {
                                                            ""total"": 0.004443772999999762,
                                                            ""count"": 1,
                                                            ""is_parallel"": true,
                                                            ""self"": 0.0003309589999993534,
                                                            ""children"": {
                                                                ""_process_vector_observation"": {
                                                                    ""total"": 0.004112814000000409,
                                                                    ""count"": 2,
                                                                    ""is_parallel"": true,
                                                                    ""self"": 0.004112814000000409
                                                                }
                                                            }
                                                        },
                                                        ""UnityEnvironment.step"": {
                                                            ""total"": 0.017111769000000443,
                                                            ""count"": 1,
                                                            ""is_parallel"": true,
                                                            ""self"": 0.0001352060000003874,
                                                            ""children"": {
                                                                ""UnityEnvironment._generate_step_input"": {
                                                                    ""total"": 0.0002027519999998617,
                                                                    ""count"": 1,
                                                                    ""is_parallel"": true,
                                                                    ""self"": 0.0002027519999998617
                                                                },
                                                                ""communicator.exchange"": {
                                                                    ""total"": 0.016472735999999877,
                                                                    ""count"": 1,
                                                                    ""is_parallel"": true,
                                                                    ""self"": 0.016472735999999877
                                                                },
                                                                ""steps_from_proto"": {
                                                                    ""total"": 0.00030107500000031706,
                                                                    ""count"": 1,
                                                                    ""is_parallel"": true,
                                                                    ""self"": 0.00010562699999994152,
                                                                    ""children"": {
                                                                        ""_process_vector_observation"": {
                                                                            ""total"": 0.00019544800000037554,
                                                                            ""count"": 2,
                                                                            ""is_parallel"": true,
                                                                            ""self"": 0.00019544800000037554
                                                                        }
                                                                    }
                                                                }
                                                            }
                                                        }
                                                    }
                                                },
                                                ""UnityEnvironment.step"": {
                                                    ""total"": 46.3888760779994,
                                                    ""count"": 14846,
                                                    ""is_parallel"": true,
                                                    ""self"": 1.6719174050012242,
                                                    ""children"": {
                                                        ""UnityEnvironment._generate_step_input"": {
                                                            ""total"": 1.6747899139994242,
                                                            ""count"": 14846,
                                                            ""is_parallel"": true,
                                                            ""self"": 1.6747899139994242
                                                        },
                                                        ""communicator.exchange"": {
                                                            ""total"": 39.38887078999902,
                                                            ""count"": 14846,
                                                            ""is_parallel"": true,
                                                            ""self"": 39.38887078999902
                                                        },
                                                        ""steps_from_proto"": {
                                                            ""total"": 3.653297968999733,
                                                            ""count"": 14846,
                                                            ""is_parallel"": true,
                                                            ""self"": 1.4095413739998186,
                                                            ""children"": {
                                                                ""_process_vector_observation"": {
                                                                    ""total"": 2.2437565949999145,
                                                                    ""count"": 29692,
                                                                    ""is_parallel"": true,
                                                                    ""self"": 2.2437565949999145
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                ""trainer_threads"": {
                    ""total"": 7.220700000232227e-05,
                    ""count"": 1,
                    ""self"": 7.220700000232227e-05,
                    ""children"": {
                        ""thread_root"": {
                            ""total"": 0.0,
                            ""count"": 0,
                            ""is_parallel"": true,
                            ""self"": 0.0,
                            ""children"": {
                                ""trainer_advance"": {
                                    ""total"": 95.04654379600397,
                                    ""count"": 206214,
                                    ""is_parallel"": true,
                                    ""self"": 3.6411367380041924,
                                    ""children"": {
                                        ""process_trajectory"": {
                                            ""total"": 59.422005785999794,
                                            ""count"": 206214,
                                            ""is_parallel"": true,
                                            ""self"": 59.422005785999794
                                        },
                                        ""_update_policy"": {
                                            ""total"": 31.98340127199998,
                                            ""count"": 12,
                                            ""is_parallel"": true,
                                            ""self"": 13.672576812999822,
                                            ""children"": {
                                                ""PPOOptimizer.update"": {
                                                    ""total"": 18.31082445900016,
                                                    ""count"": 6948,
                                                    ""is_parallel"": true,
                                                    ""self"": 18.31082445900016
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                ""TrainerController._save_model"": {
                    ""total"": 0.213942466000006,
                    ""count"": 1,
                    ""self"": 0.213942466000006
                }
            }
        }
    }
}
```",example fix name root value min count value min count value min count value min count value min count value min count value min count value min count value min count default clang force total count self total count self total count self total count self total count self total count self total count self total count self total count self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count true self total count self total count true self total count true self total count true self total count true self total count true self total count self,issue,positive,positive,positive,positive,positive,positive
621377540,"> Looks good. can you update the changelog too? Forgot to mention it on the other PR.

@chriselion added. ",good update forgot mention added,issue,negative,positive,positive,positive,positive,positive
620811416,"> Hi @Luhen1
> 
> We have changed the way that ML-Agents works. There are no longer brains, but instead a ""Behavior Parameters"" component. Please check out the latest version of the getting started guide: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started.md

Hi  @awjuliani!

Thank you so much for the response.",hi way work longer brain instead behavior component please check latest version getting guide hi thank much response,issue,positive,positive,positive,positive,positive,positive
620722970,"Hi @Luhen1 

We have changed the way that ML-Agents works. There are no longer brains, but instead a ""Behavior Parameters"" component. Please check out the latest version of the getting started guide: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started.md",hi way work longer brain instead behavior component please check latest version getting guide,issue,negative,positive,positive,positive,positive,positive
620530406,"Also should my agent be rewarded for another agents failure? 
```
For example AgentA moves to a valid space, no food eaten and no collisions, therefore 0 points.
at the same step, AgentB hits its own body, therefore SetReward (-1) new game.
```

Should AgentA get a reward? or is it ok for the game scores to be unbalanced in this way. (not Zero sum)",also agent another failure example valid space food eaten therefore step body therefore new game get reward game unbalanced way zero sum,issue,negative,negative,negative,negative,negative,negative
620391329,Me too. cant find ml-agent -> learning brain from asset -> create. Would love to see any solution to this.,cant find learning brain asset create would love see solution,issue,positive,positive,positive,positive,positive,positive
620313318,"Hi,

We logged the bug (MLA-960) and pushed the following fix on our next release branch : #3874

",hi logged bug following fix next release branch,issue,negative,neutral,neutral,neutral,neutral,neutral
620310162,"I finally got to reproduce the error in 0.15.0
This is a very elusive bug but basically, C# wants to send information about the two behaviors to Python. Problem is : The information is incomplete because no decisions have been requested yet (Python ignores the incomplete data received). So in the end, C# thinks it sent the data and does not need to try again but Python ignores all data sent by the second behavior because it does not recognize it.
This bug was fixed on master (commit) 1dd6cb6 because it caused another issue that we caught.
Thank you so much for your patience in this matter. ",finally got reproduce error elusive bug basically send information two python problem information incomplete yet python incomplete data received end sent data need try python data sent second behavior recognize bug fixed master commit another issue caught thank much patience matter,issue,negative,positive,neutral,neutral,positive,positive
620271295,I was going based off of semver semantics.  Behavioral/api change for the ones I've contributed. ,going based semantics change,issue,negative,neutral,neutral,neutral,neutral,neutral
620271156,"@vincentpierre 

![the bug](https://user-images.githubusercontent.com/24987395/80426704-9f88e700-88bc-11ea-885c-e1589c3c7728.gif)

The fact that today a decision interval of 5 is causing the bug but a decision interval of 6 does not is really confusing me... (I could swear that last night a decision interval of 6 was broken. This makes me have wild theories, like somehow the system calendar is related to this problem)....

Here is a gif from another ml-agents user showing the same issue 

 https://gyazo.com/e3986c887ee855d979dd3d11bc407ec5

The only code that I modified in the project settings override, but I have reverted those changes (and even removed the project settings override object entirely) when i was looking for the cause.

I would recommend trying all of the decision intervals from 1-10. The interval that causes the bug seems to be unreliable... At the moment (as seen in the gif, an interval of 5 with offset on the hard agent is causing the issue, and 6 works....

",bug fact today decision interval causing bug decision interval really could swear last night decision interval broken wild like somehow system calendar related problem gif another user showing issue code project override even removed project override object entirely looking cause would recommend trying decision interval bug unreliable moment seen gif interval offset hard agent causing issue work,issue,negative,negative,neutral,neutral,negative,negative
620264142,"I am unable to reproduce the bug. I followed the instructions you gave. Is there anything else you changed at all? Do you have errors in Unity? If you do in editor training, are there any errors? Maybe past the player logs? If you use an executable, can you check ""development build"" to see potential errors?

I tried changing the decision period to 7 and 8, but it still worked. I tried to set it to an insanely large number. The agent did not train then but I still saw the hyperparameters in the console : 
```
INFO:mlagents.trainers:Hyperparameters for the PPOTrainer of brain 3DBallHard: 
	trainer:	ppo
	batch_size:	1200
	beta:	0.001
	buffer_size:	12000
	epsilon:	0.2
	hidden_units:	128
	lambd:	0.95
	learning_rate:	0.0003
	learning_rate_schedule:	linear
	max_steps:	5.0e5
	memory_size:	256
	normalize:	True
	num_epoch:	3
	num_layers:	2
	time_horizon:	1000
	sequence_length:	64
	summary_freq:	12000
	use_recurrent:	False
	vis_encode_type:	simple
	reward_signals:	
	  extrinsic:	
	    strength:	1.0
	    gamma:	0.995
	summary_path:	ppo_3DBallHard
	model_path:	./models/ppo/3DBallHard
	keep_checkpoints:	5
```

So it connected to the trainer...
Did you modify any code besides the scene ? `git status` does not show any changed files ?",unable reproduce bug gave anything else unity editor training maybe past player use executable check development build see potential tried decision period still worked tried set insanely large number agent train still saw console brain trainer beta epsilon linear normalize true false simple extrinsic strength gamma connected trainer modify code besides scene git status show,issue,positive,negative,neutral,neutral,negative,negative
620254173,"Tracking this bug internally : MLA-961
In the mean time, we will now not set the resolution by default when doing training #3867 
",bug internally mean time set resolution default training,issue,negative,negative,negative,negative,negative,negative
620238251,"In my C# code
 if(args[1]==""NOWALL"")
        {
            walls.SetActive(false);
        }

When I run the standalone executable with args it works perfectly.
![image](https://user-images.githubusercontent.com/27775903/80421331-97d93a80-88dc-11ea-9b3b-ab20103b6bef.png)

When I run it with the mlagents-learn command it does not work
mlagents-learn config/myconfig.yaml --env=env/CurriculumRoller --run-id=argstest --inference --env-args=NOWALL

 Version information:
  ml-agents: 0.16.0.dev0,
  ml-agents-envs: 0.16.0.dev0,
  Communicator API: 0.16.0,
  TensorFlow: 2.0.1",code false run executable work perfectly image run command work inference version information dev dev communicator,issue,positive,positive,positive,positive,positive,positive
620234908,"@vincentpierre 

Can you try increasing the decision interval of the hard ball agent to 7? (this seems to be a way to trigger it)

When the bug occurs, it definitely does not train or load the second behavior at any point.",try increasing decision interval hard ball agent way trigger bug definitely train load second behavior point,issue,negative,negative,neutral,neutral,negative,negative
620234132,"Thanks @vincentpierre 
```
***ml-agents/README.md:
   64: | **master (unstable)** | -- | [source](https://github.com/Unity-Technologies/ml-agents/tree/master) | [docs](https://github.com/Unity-Technologies/ml-agents/tree/master/docs/Readme.md) | [download](https://github.com/Unity-Technologies/ml-agents/archive/master.zip) |
```
Leaving this one, it's an explicit link to master.

```
***ml-agents/ml-agents/README.md:
   13:   [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Python-API.md)
   19:   [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-ML-Agents.md)
   33: [ML-Agents Toolkit documentation.](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Readme.md)

***ml-agents/ml-agents-envs/README.md:
   14:   [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Python-API.md)
   28: [ML-Agents Toolkit documentation.](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Readme.md)
```
Updated these to be relative links.

```
***ml-agents/README.md:
    6: [![docs badge](https://img.shields.io/badge/docs-reference-blue.svg)](https://github.com/Unity-Technologies/ml-agents/tree/latest_release/docs/)
    9: ([latest release](https://github.com/Unity-Technologies/ml-agents/releases/tag/latest_release))
   48: [here](https://github.com/Unity-Technologies/ml-agents/tree/latest_release/docs/Readme.md) to
```
Will tackle these in another PR when I update the table.
```
 ***ml-agents/com.unity.ml-agents/Documentation~/com.unity.ml-agents.md:
  108: [installation instructions]: https://github.com/Unity-Technologies/ml-agents/blob/latest_release/docs/Installation.md
```
Updated to `release_1_docs`

```
***ml-agents/docs/Installation-Anaconda-Windows.md:
  134: [here](https://github.com/Unity-Technologies/ml-agents/archive/latest_release.zip).
```
Reworded to link to https://github.com/Unity-Technologies/ml-agents/releases

```
***ml-agents/ml-agents-envs/mlagents_envs/environment.py:
   80:             f""Please go to https://github.com/Unity-Technologies/ml-agents/releases/tag/latest_release ""
```
Reworded to link to https://github.com/Unity-Technologies/ml-agents/releases (same as other log links in that file).",thanks master unstable source leaving one explicit link master documentation documentation relative link badge latest release tackle another update table installation link please go link log link file,issue,negative,positive,positive,positive,positive,positive
620210492,"Hi,
We were able to reproduce this issue. Unfortunately, we plan to deprecate the CSV export of data in a later release so we will not be addressing this issue now. 
Like you said, it is possible to directly download the CSV from tensorboard if that is a suitable substitute. For other users looking for a substitute : Check the `Show data download links` on tensorboard and you will be able to download a CSV for each curve.",hi able reproduce issue unfortunately plan deprecate export data later release issue like said possible directly suitable substitute looking substitute check show data link able curve,issue,negative,positive,positive,positive,positive,positive
620190131," Thanks for the reply!  I understand that its the more general approach.

I realized I can also just write to a .csv within Unity, since I manually restart episodes there anyway, I can just call GetCumulativeReward and track that in a file.

I wanted to plot the data in matlab anyway, so its not much of an effort difference for me.",thanks reply understand general approach also write within unity since manually restart anyway call track file plot data anyway much effort difference,issue,negative,positive,positive,positive,positive,positive
620187903,"I think there are still some references to master and latest_release. do we want to remove all of them ?

```
***ml-agents/README.md:
   64: | **master (unstable)** | -- | [source](https://github.com/Unity-Technologies/ml-agents/tree/master) | [docs](https://github.com/Unity-Technologies/ml-agents/tree/master/docs/Readme.md) | [download](https://github.com/Unity-Technologies/ml-agents/archive/master.zip) |

***ml-agents/ml-agents/README.md:
   13:   [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Python-API.md)
   19:   [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-ML-Agents.md)
   33: [ML-Agents Toolkit documentation.](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Readme.md)

***ml-agents/ml-agents-envs/README.md:
   14:   [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Python-API.md)
   28: [ML-Agents Toolkit documentation.](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Readme.md)


***ml-agents/README.md:
    6: [![docs badge](https://img.shields.io/badge/docs-reference-blue.svg)](https://github.com/Unity-Technologies/ml-agents/tree/latest_release/docs/)
    9: ([latest release](https://github.com/Unity-Technologies/ml-agents/releases/tag/latest_release))
   48: [here](https://github.com/Unity-Technologies/ml-agents/tree/latest_release/docs/Readme.md) to

 ***ml-agents/com.unity.ml-agents/Documentation~/com.unity.ml-agents.md:
  108: [installation instructions]: https://github.com/Unity-Technologies/ml-agents/blob/latest_release/docs/Installation.md

***ml-agents/docs/Installation-Anaconda-Windows.md:
  134: [here](https://github.com/Unity-Technologies/ml-agents/archive/latest_release.zip).

***ml-agents/ml-agents-envs/mlagents_envs/environment.py:
   80:             f""Please go to https://github.com/Unity-Technologies/ml-agents/releases/tag/latest_release ""
```",think still master want remove master unstable source documentation documentation badge latest release installation please go,issue,negative,positive,positive,positive,positive,positive
620179549,"@harperj Yes, the plan for this release is:
Python version: `0.16.0`
C# version: `1.0.0`
Release tag: `release_1`",yes plan release python version version release tag,issue,negative,neutral,neutral,neutral,neutral,neutral
620174543,@mmattar Did you say you saw a missing link a few weeks ago? This should fix checking for it.,say saw missing link ago fix,issue,negative,negative,negative,negative,negative,negative
620173154,@ervteng If you want to have a look at the latest commit. We do okay on cartpole (not perfect) but  it seems montain-car is posing problems (maybe an issue with the wrapper and max_step),want look latest commit perfect posing maybe issue wrapper,issue,positive,positive,positive,positive,positive,positive
620162504,"@chriselion just to clarify, the separate release tag and version numbers are because the releases are decoupled with Python package versioning?  I'm assuming this is because of the Unity package version being different from Python in the future?",clarify separate release tag version python package assuming unity package version different python future,issue,negative,neutral,neutral,neutral,neutral,neutral
620162440,@vincentpierre I think that is OK. Would let us train gym envs using our trainer easily. ,think would let u train gym trainer easily,issue,negative,positive,positive,positive,positive,positive
620144168,"Hi, 
I think this would be informative, unfortunately, all of our plots have ""number of steps"" on the x-axis right now. The plots on tensorboard must work for all environments trained with ML-Agents, plotting the number of steps seems to be the solution that fits most use cases at this time.

I think it might be possible to generate this type of graph by modifying these lines of code.
https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/trainer/rl_trainer.py#L58
If you would like to make a PR or a draft of a PR, we can have a look at it. 
Internal tracking : MLA-958
",hi think would informative unfortunately number right must work trained plotting number solution use time think might possible generate type graph code would like make draft look internal,issue,negative,negative,neutral,neutral,negative,negative
620133272,"Well then I'm doing something wrong.
The x-axis corresponds to the number of time-steps that have been simulated in sum over all episodes. This is true for both cumulative reward and episode length...

Maybe I understood something completely wrong. I understand that the agent episode/step is something different from the environment/academy. But within the Unity environment with the runtime plugin, there is no way to set environment episodes, but only Agent episodes via agent.Done().

I only have a single agent, so I was assuming that the environment should count one episode as ended once all agents are set to done? How on earth would a single agent time-step be registered as a whole episode?",well something wrong number sum true cumulative reward episode length maybe understood something completely wrong understand agent something different within unity environment way set environment agent via single agent assuming environment count one episode ended set done earth would single agent registered whole episode,issue,negative,negative,neutral,neutral,negative,negative
620128027,"> Hi @mmattar
> 
> It looks like this PR deletes about half of the documentation that is being changed (at least from the lines changed metric and a quick glance at the actual changes). Am I perhaps missing something?

@awjuliani - It does refactor ~9 pages into Training-ML-Agents and ML-Agents-Overview, so those pages are intended to be deleted. It is part of re-structuring. Let me know if it makes things better or worse.",hi like half documentation least metric quick glance actual perhaps missing something intended part let know better worse,issue,negative,negative,neutral,neutral,negative,negative
620127584,"I tried to reproduce on 0.14.1 by following the described steps, but I worked for me. Since I only had one Hard agent, I had to wait longer to see the summary in the console, but it trained : 

```
ml-agents (release-0.14.1) $ mlagents-learn --train config/trainer_config.yaml 
WARNING:tensorflow:From /Users/vincentpierre/Documents/ml-agents/ml-agents/mlagents/tf_utils/tf.py:33: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.



                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓

        
 Version information:
  ml-agents: 0.14.1,
  ml-agents-envs: 0.14.1,
  Communicator API: API-14,
  TensorFlow: 1.15.0
INFO:mlagents_envs:Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
INFO:mlagents_envs:Connected new brain:
3DBallHard?team=0
INFO:mlagents_envs:Connected new brain:
3DBall?team=0
INFO:mlagents.trainers:Hyperparameters for the PPOTrainer of brain 3DBall: 
	trainer:	ppo
	batch_size:	64
	beta:	0.001
	buffer_size:	12000
	epsilon:	0.2
	hidden_units:	128
	lambd:	0.99
	learning_rate:	0.0003
	learning_rate_schedule:	linear
	max_steps:	5.0e5
	memory_size:	256
	normalize:	True
	num_epoch:	3
	num_layers:	2
	time_horizon:	1000
	sequence_length:	64
	summary_freq:	12000
	use_recurrent:	False
	vis_encode_type:	simple
	reward_signals:	
	  extrinsic:	
	    strength:	1.0
	    gamma:	0.99
	summary_path:	ppo_3DBall
	model_path:	./models/ppo/3DBall
	keep_checkpoints:	5
2020-04-27 10:09:48.988420: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-04-27 10:09:49.022876: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd8ca6174c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-04-27 10:09:49.022910: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
INFO:mlagents.trainers:Hyperparameters for the PPOTrainer of brain 3DBallHard: 
	trainer:	ppo
	batch_size:	1200
	beta:	0.001
	buffer_size:	12000
	epsilon:	0.2
	hidden_units:	128
	lambd:	0.95
	learning_rate:	0.0003
	learning_rate_schedule:	linear
	max_steps:	5.0e5
	memory_size:	256
	normalize:	True
	num_epoch:	3
	num_layers:	2
	time_horizon:	1000
	sequence_length:	64
	summary_freq:	12000
	use_recurrent:	False
	vis_encode_type:	simple
	reward_signals:	
	  extrinsic:	
	    strength:	1.0
	    gamma:	0.995
	summary_path:	ppo_3DBallHard
	model_path:	./models/ppo/3DBallHard
	keep_checkpoints:	5
INFO:mlagents.trainers: ppo: 3DBall: Step: 12000. Time Elapsed: 38.702 s Mean Reward: 1.135. Std of Reward: 0.748. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 24000. Time Elapsed: 73.358 s Mean Reward: 1.344. Std of Reward: 0.804. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 36000. Time Elapsed: 102.274 s Mean Reward: 1.712. Std of Reward: 1.060. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 48000. Time Elapsed: 132.748 s Mean Reward: 2.479. Std of Reward: 1.648. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 60000. Time Elapsed: 163.205 s Mean Reward: 3.836. Std of Reward: 3.055. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 72000. Time Elapsed: 195.402 s Mean Reward: 6.639. Std of Reward: 5.883. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 84000. Time Elapsed: 224.050 s Mean Reward: 14.521. Std of Reward: 12.335. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 96000. Time Elapsed: 251.838 s Mean Reward: 42.504. Std of Reward: 36.762. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 108000. Time Elapsed: 275.822 s Mean Reward: 47.755. Std of Reward: 50.392. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 120000. Time Elapsed: 301.871 s Mean Reward: 67.688. Std of Reward: 50.472. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 132000. Time Elapsed: 322.468 s Mean Reward: 107.900. Std of Reward: 0.000. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 144000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 156000. Time Elapsed: 378.165 s Mean Reward: 264.400. Std of Reward: 0.000. Training.
INFO:mlagents.trainers: ppo: 3DBallHard: Step: 12000. Time Elapsed: 401.258 s Mean Reward: 0.748. Std of Reward: 0.550. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 168000. Time Elapsed: 407.692 s Mean Reward: 678.100. Std of Reward: 0.000. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 180000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 192000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 204000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 216000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 228000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 240000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 252000. No episode was completed since last summary. Training.
```",tried reproduce following worked since one hard agent wait longer see summary console trained train warning name please use instead version information communicator listening port start training pressing play button unity editor connected new brain connected new brain brain trainer beta epsilon linear normalize true false simple extrinsic strength gamma binary use service platform host guarantee used device host default version brain trainer beta epsilon linear normalize true false simple extrinsic strength gamma step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step episode since last summary training step time mean reward reward training step time mean reward reward training step time mean reward reward training step episode since last summary training step episode since last summary training step episode since last summary training step episode since last summary training step episode since last summary training step episode since last summary training step episode since last summary training,issue,positive,negative,negative,negative,negative,negative
620125399,"Thank you for signaling this to us. (I think it does qualify as a bug)
We have made 0.15.1 the latest_release and we will be investigating why the latest_release was set to 0.13.0. 
For reference : MLA-956",thank u think qualify bug made investigating set reference,issue,negative,neutral,neutral,neutral,neutral,neutral
620123823,"Hi @mmattar 

It looks like this PR deletes about half of the documentation that is being changed (at least from the lines changed metric and a quick glance at the actual changes). Am I perhaps missing something? ",hi like half documentation least metric quick glance actual perhaps missing something,issue,negative,negative,neutral,neutral,negative,negative
620122668,"@awjuliani - I'm closing this PR, as I created a new one #3864 that targets the release_1_branch and includes the trainer configuration changes.",new one trainer configuration,issue,negative,positive,positive,positive,positive,positive
620122457,"Hi @Xiromtz 

Per-episode reward is exactly what is being displayed in the tensorboard statistics. ",hi reward exactly displayed statistic,issue,positive,positive,positive,positive,positive,positive
620115157,"Hi @automata  

Unfortunately this is still in the works. Unity has an internal working version, but it has yet to pass to the stage of being ready ready for release. ",hi automaton unfortunately still work unity internal working version yet pas stage ready ready release,issue,negative,negative,neutral,neutral,negative,negative
619902482,"> I just looked more closely at the tensorboard figures. Even though one agent is technically getting higher reward, it's overall ELO is decreasing because the final rewards of those trajectories are negative. Can you modify the reward function so that 'winning' has a positive final reward?

Sorry for the late reply on this one and thanks for taking the time to review!

1) Action is selected (a direction)
2) on Update() { HandleGridMovement() } function is called which updated the agents position, and performs a check called CheckSnakeBodyPosition()

3) Here is the reward structure during CheckSnakeBodyPosition()
if the new position hit food, self, or other snake:
-1 ```agent.SetReward(-1)``` point for hitting snake, other snakes get +1 ```agent.SetReward(1)```. End of episode. (Loop each snake and ```agent.EndEpisode()```)
-1 ```agent.SetReward(-1)``` point for hitting self, other snakes get +1 ```agent.SetReward(1)```. End of episode. (Loop each snake and ```agent.EndEpisode()```)
+1 point for hitting food, other snakes get -1. Continue episode.

Could the issue be with the third rule and the episode continuing after getting food? but other condition call '''agent.EndEpisode()'''?
",closely even though one agent technically getting higher reward overall decreasing final negative modify reward function positive final reward sorry late reply one thanks taking time review action selected direction update function position check reward structure new position hit food self snake point snake get end episode loop snake point self get end episode loop snake point food get continue episode could issue third rule episode getting food condition call,issue,positive,negative,neutral,neutral,negative,negative
619600200,"I was thinking on how to correctly formulate the question I originally intended to ask. Due to me being occupied with other work, I can only now get back to this question:

I know about the tensorboard statistics. I have been using them for a while now and they work nicely for most of use-cases. In my opinion, some information is still missing.

My original question was asking for a per-episode feedback on my agent. Using tensorboard, this does not really work as I would like. I believe the tensorboard feedback is based on a continuous environment, whereas my environment is episodic.

In short, the feedback I need is: Rewards per episode, regardless of time-steps. Apparently no statistics on a per-episode basis exist. Only on an average over all time-steps basis. Since my agent already receives a negative reward every time-step, episodic rewards suffice and are easier for me to understand.

Would it be possible for me to somehow edit the way information is output to the csv/tensorboard file?",thinking correctly formulate question originally intended ask due work get back question know statistic work nicely opinion information still missing original question feedback agent really work would like believe feedback based continuous environment whereas environment episodic short feedback need per episode regardless apparently statistic basis exist average basis since agent already negative reward every episodic suffice easier understand would possible somehow edit way information output file,issue,positive,positive,neutral,neutral,positive,positive
619594926,"Hi @FlimFlamm,
We have logged this issue internally as MLA-995 and will update this thread with any information pertaining to the issue you've reported. ",hi logged issue internally update thread information pertaining issue,issue,negative,neutral,neutral,neutral,neutral,neutral
619470768,"Hi @FlimFlamm,
We have removed the offset-step functionality on master and will be available in the next release.  If you'd like to have custom stepping timings for your agents, you will be able to subscribe to the `Academy.AgentPreStep` event and you can decide to call `RequestDecision/Action` from there.  Thank you for your feedback.  Please reopen this issue if you feel the problem was not addressed. Cheers. ",hi removed functionality master available next release like custom stepping able subscribe event decide call thank feedback please reopen issue feel problem,issue,positive,positive,positive,positive,positive,positive
619460858,Confirmed that `release_1_test2` tag worked on https://github.com/Unity-Technologies/ml-agents/pull/3857/files and uploaded to the test pypi: https://test.pypi.org/project/mlagents/#history,confirmed tag worked test,issue,negative,positive,positive,positive,positive,positive
619403954,"I still recommend 3.7 :) since 3.8 only supports a release candidate of tensorflow, not a final release.",still recommend since release candidate final release,issue,negative,neutral,neutral,neutral,neutral,neutral
619253600,"Hey @spaggi,
I now have Catalina installed on my machine and have not run into these issues.  If you can still reproduce this issue, please reopen this issue. Thanks for your input. ",hey catalina machine run still reproduce issue please reopen issue thanks input,issue,positive,positive,positive,positive,positive,positive
619175053,"Any ETA on ""native"" headless rendering using Vulkan?

We're trying to use `xvfb` without success on a Vulkan-based Unity standalone binary.",eta native headless rendering trying use without success unity binary,issue,negative,positive,positive,positive,positive,positive
619130716,"@surfnerd , We could do 
```csharp
# if not PLATFORM_LINUX and not EDITOR and VERSION_2019_3
Screen.SetResolution(width, height)
#endif
```
to make sure we do not set the resolution on this platform.
We could also not set the screen resolution by default in `learn.py`",could editor width height make sure set resolution platform could also set screen resolution default,issue,negative,positive,positive,positive,positive,positive
619119056,"Hi @automata,
This looks like a graphics bug within unity itself, not ML-Agents.  We are working with our graphics team to see if it's already fixed in a newer version, or if we can file a bug for this so they can fix it.  Thank you for finding this!",hi automaton like graphic bug within unity working graphic team see already fixed version file bug fix thank finding,issue,positive,positive,neutral,neutral,positive,positive
618966094,"Great catch Vincent! I commented that out and now it works perfectly:

![Screenshot from 2020-04-24 08-49-32](https://user-images.githubusercontent.com/49062/80209541-a5ef3880-8608-11ea-877d-3d43e9bb9b07.png)

The error message related with culling is still there but no more complains about missing textures.

Inspired by your suggestion I tried to change settings to allow resizable window like:

![Screenshot from 2020-04-24 08-45-52](https://user-images.githubusercontent.com/49062/80209705-ed75c480-8608-11ea-826a-97eff6378d76.png)

However that didn't work neither.

I put all the post processing back and your solution works.

Thank you!",great catch vincent work perfectly error message related culling still missing inspired suggestion tried change allow window like however work neither put post back solution work thank,issue,positive,positive,positive,positive,positive,positive
618724923,"Sorry for the delay on this. I don't think we should be adding a console message for this, but I added a better description for ProjectSettingsOverrides in https://github.com/Unity-Technologies/ml-agents/pull/3838",sorry delay think console message added better description,issue,negative,neutral,neutral,neutral,neutral,neutral
618695788,Maybe this is due to our use `Screen.SetResolution` in the code. Can you try commenting [this line out](https://github.com/Unity-Technologies/ml-agents/blob/0.15.1/com.unity.ml-agents/Runtime/SideChannels/EngineConfigurationChannel.cs#L32) and try again? ,maybe due use code try line try,issue,negative,negative,negative,negative,negative,negative
618692695,"OK I disabled ambient occlusion just in case and all post processing.

Run it one more time and got on logs:

```
Mono path[0] = '/media/data/tmp/unity/hdrp_cam5_Data/Managed'
Mono config path = '/media/data/tmp/unity/hdrp_cam5_Data/MonoBleedingEdge/etc'
PlayerConnection initialized from /media/data/tmp/unity/hdrp_cam5_Data (debug = 0)
PlayerConnection initialized network socket : 0.0.0.0 55044
Multi-casting ""[IP] 192.168.0.10 [Port] 55044 [Flags] 2 [Guid] 3739687994 [EditorId] 4223096421 [Version] 1048832 [Id] LinuxPlayer(192.168.0.10) [Debug] 0 [PackageName] LinuxPlayer [ProjectName] training-dots-hdrp"" to [225.0.0.222:54997]...
Started listening to [0.0.0.0:55044]
Preloaded 'lib_burst_generated.so'
Preloaded 'libgrpc_csharp_ext.x64.so'
PlayerConnection already initialized - listening to [0.0.0.0:55044]
Display 0 'LG ULTRAWIDE 29""': 2560x1080 (primary device).
Desktop is 2560 x 1080 @ 60 Hz
Invalid initial resolution 80 x 80 - forcing to 100 x 100
[Vulkan init] extensions: count=18
[Vulkan init] extensions: name=VK_KHR_device_group_creation, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_fence_capabilities, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_memory_capabilities, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_semaphore_capabilities, enabled=0
[Vulkan init] extensions: name=VK_KHR_get_display_properties2, enabled=0
[Vulkan init] extensions: name=VK_KHR_get_physical_device_properties2, enabled=0
[Vulkan init] extensions: name=VK_KHR_get_surface_capabilities2, enabled=0
[Vulkan init] extensions: name=VK_KHR_surface, enabled=1
[Vulkan init] extensions: name=VK_KHR_surface_protected_capabilities, enabled=0
[Vulkan init] extensions: name=VK_KHR_wayland_surface, enabled=1
[Vulkan init] extensions: name=VK_KHR_xcb_surface, enabled=0
[Vulkan init] extensions: name=VK_KHR_xlib_surface, enabled=1
[Vulkan init] extensions: name=VK_KHR_display, enabled=1
[Vulkan init] extensions: name=VK_EXT_direct_mode_display, enabled=0
[Vulkan init] extensions: name=VK_EXT_acquire_xlib_display, enabled=0
[Vulkan init] extensions: name=VK_EXT_display_surface_counter, enabled=0
[Vulkan init] extensions: name=VK_EXT_debug_report, enabled=1
[Vulkan init] extensions: name=VK_EXT_debug_utils, enabled=0
Vulkan detection: 2
Initialize engine version: 2019.3.10f1 (5968d7f82152)
[Subsystems] Discovering subsystems at path /media/data/tmp/unity/hdrp_cam5_Data/UnitySubsystems
GfxDevice: creating device client; threaded=1
[Vulkan init] extensions: count=18
[Vulkan init] extensions: name=VK_KHR_device_group_creation, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_fence_capabilities, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_memory_capabilities, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_semaphore_capabilities, enabled=0
[Vulkan init] extensions: name=VK_KHR_get_display_properties2, enabled=0
[Vulkan init] extensions: name=VK_KHR_get_physical_device_properties2, enabled=0
[Vulkan init] extensions: name=VK_KHR_get_surface_capabilities2, enabled=0
[Vulkan init] extensions: name=VK_KHR_surface, enabled=1
[Vulkan init] extensions: name=VK_KHR_surface_protected_capabilities, enabled=0
[Vulkan init] extensions: name=VK_KHR_wayland_surface, enabled=1
[Vulkan init] extensions: name=VK_KHR_xcb_surface, enabled=0
[Vulkan init] extensions: name=VK_KHR_xlib_surface, enabled=1
[Vulkan init] extensions: name=VK_KHR_display, enabled=1
[Vulkan init] extensions: name=VK_EXT_direct_mode_display, enabled=0
[Vulkan init] extensions: name=VK_EXT_acquire_xlib_display, enabled=0
[Vulkan init] extensions: name=VK_EXT_display_surface_counter, enabled=0
[Vulkan init] extensions: name=VK_EXT_debug_report, enabled=1
[Vulkan init] extensions: name=VK_EXT_debug_utils, enabled=0
[Vulkan init] extensions: count=89
[Vulkan init] extensions: name=VK_KHR_8bit_storage, enabled=0
[Vulkan init] extensions: name=VK_KHR_16bit_storage, enabled=0
[Vulkan init] extensions: name=VK_KHR_bind_memory2, enabled=0
[Vulkan init] extensions: name=VK_KHR_create_renderpass2, enabled=0
[Vulkan init] extensions: name=VK_KHR_dedicated_allocation, enabled=1
[Vulkan init] extensions: name=VK_KHR_depth_stencil_resolve, enabled=0
[Vulkan init] extensions: name=VK_KHR_descriptor_update_template, enabled=1
[Vulkan init] extensions: name=VK_KHR_device_group, enabled=0
[Vulkan init] extensions: name=VK_KHR_draw_indirect_count, enabled=0
[Vulkan init] extensions: name=VK_KHR_driver_properties, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_fence, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_fence_fd, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_memory, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_memory_fd, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_semaphore, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_semaphore_fd, enabled=0
[Vulkan init] extensions: name=VK_KHR_get_memory_requirements2, enabled=1
[Vulkan init] extensions: name=VK_KHR_image_format_list, enabled=1
[Vulkan init] extensions: name=VK_KHR_imageless_framebuffer, enabled=0
[Vulkan init] extensions: name=VK_KHR_maintenance1, enabled=1
[Vulkan init] extensions: name=VK_KHR_maintenance2, enabled=0
[Vulkan init] extensions: name=VK_KHR_maintenance3, enabled=0
[Vulkan init] extensions: name=VK_KHR_multiview, enabled=1
[Vulkan init] extensions: name=VK_KHR_pipeline_executable_properties, enabled=0
[Vulkan init] extensions: name=VK_KHR_push_descriptor, enabled=0
[Vulkan init] extensions: name=VK_KHR_relaxed_block_layout, enabled=0
[Vulkan init] extensions: name=VK_KHR_sampler_mirror_clamp_to_edge, enabled=1
[Vulkan init] extensions: name=VK_KHR_sampler_ycbcr_conversion, enabled=0
[Vulkan init] extensions: name=VK_KHR_shader_atomic_int64, enabled=0
[Vulkan init] extensions: name=VK_KHR_shader_draw_parameters, enabled=0
[Vulkan init] extensions: name=VK_KHR_shader_float16_int8, enabled=0
[Vulkan init] extensions: name=VK_KHR_shader_float_controls, enabled=0
[Vulkan init] extensions: name=VK_KHR_storage_buffer_storage_class, enabled=0
[Vulkan init] extensions: name=VK_KHR_swapchain, enabled=1
[Vulkan init] extensions: name=VK_KHR_swapchain_mutable_format, enabled=0
[Vulkan init] extensions: name=VK_KHR_timeline_semaphore, enabled=0
[Vulkan init] extensions: name=VK_KHR_uniform_buffer_standard_layout, enabled=0
[Vulkan init] extensions: name=VK_KHR_variable_pointers, enabled=0
[Vulkan init] extensions: name=VK_KHR_vulkan_memory_model, enabled=0
[Vulkan init] extensions: name=VK_EXT_blend_operation_advanced, enabled=0
[Vulkan init] extensions: name=VK_EXT_buffer_device_address, enabled=0
[Vulkan init] extensions: name=VK_EXT_calibrated_timestamps, enabled=0
[Vulkan init] extensions: name=VK_EXT_conditional_rendering, enabled=0
[Vulkan init] extensions: name=VK_EXT_conservative_rasterization, enabled=0
[Vulkan init] extensions: name=VK_EXT_depth_clip_enable, enabled=0
[Vulkan init] extensions: name=VK_EXT_depth_range_unrestricted, enabled=0
[Vulkan init] extensions: name=VK_EXT_descriptor_indexing, enabled=0
[Vulkan init] extensions: name=VK_EXT_discard_rectangles, enabled=0
[Vulkan init] extensions: name=VK_EXT_display_control, enabled=0
[Vulkan init] extensions: name=VK_EXT_fragment_shader_interlock, enabled=0
[Vulkan init] extensions: name=VK_EXT_global_priority, enabled=0
[Vulkan init] extensions: name=VK_EXT_host_query_reset, enabled=0
[Vulkan init] extensions: name=VK_EXT_index_type_uint8, enabled=0
[Vulkan init] extensions: name=VK_EXT_inline_uniform_block, enabled=0
[Vulkan init] extensions: name=VK_EXT_line_rasterization, enabled=0
[Vulkan init] extensions: name=VK_EXT_memory_budget, enabled=0
[Vulkan init] extensions: name=VK_EXT_pci_bus_info, enabled=0
[Vulkan init] extensions: name=VK_EXT_pipeline_creation_feedback, enabled=0
[Vulkan init] extensions: name=VK_EXT_post_depth_coverage, enabled=0
[Vulkan init] extensions: name=VK_EXT_sample_locations, enabled=0
[Vulkan init] extensions: name=VK_EXT_sampler_filter_minmax, enabled=0
[Vulkan init] extensions: name=VK_EXT_scalar_block_layout, enabled=0
[Vulkan init] extensions: name=VK_EXT_separate_stencil_usage, enabled=0
[Vulkan init] extensions: name=VK_EXT_shader_demote_to_helper_invocation, enabled=0
[Vulkan init] extensions: name=VK_EXT_shader_subgroup_ballot, enabled=0
[Vulkan init] extensions: name=VK_EXT_shader_subgroup_vote, enabled=0
[Vulkan init] extensions: name=VK_EXT_shader_viewport_index_layer, enabled=0
[Vulkan init] extensions: name=VK_EXT_subgroup_size_control, enabled=0
[Vulkan init] extensions: name=VK_EXT_texel_buffer_alignment, enabled=0
[Vulkan init] extensions: name=VK_EXT_transform_feedback, enabled=0
[Vulkan init] extensions: name=VK_EXT_vertex_attribute_divisor, enabled=0
[Vulkan init] extensions: name=VK_EXT_ycbcr_image_arrays, enabled=0
[Vulkan init] extensions: name=VK_NV_clip_space_w_scaling, enabled=0
[Vulkan init] extensions: name=VK_NV_dedicated_allocation, enabled=0
[Vulkan init] extensions: name=VK_NV_dedicated_allocation_image_aliasing, enabled=0
[Vulkan init] extensions: name=VK_NV_device_diagnostic_checkpoints, enabled=0
[Vulkan init] extensions: name=VK_NV_fill_rectangle, enabled=0
[Vulkan init] extensions: name=VK_NV_fragment_coverage_to_color, enabled=0
[Vulkan init] extensions: name=VK_NV_framebuffer_mixed_samples, enabled=0
[Vulkan init] extensions: name=VK_NV_coverage_reduction_mode, enabled=0
[Vulkan init] extensions: name=VK_NV_geometry_shader_passthrough, enabled=0
[Vulkan init] extensions: name=VK_NV_sample_mask_override_coverage, enabled=0
[Vulkan init] extensions: name=VK_NV_shader_sm_builtins, enabled=0
[Vulkan init] extensions: name=VK_NV_shader_subgroup_partitioned, enabled=0
[Vulkan init] extensions: name=VK_NV_viewport_array2, enabled=0
[Vulkan init] extensions: name=VK_NV_viewport_swizzle, enabled=0
[Vulkan init] extensions: name=VK_NVX_device_generated_commands, enabled=0
[Vulkan init] extensions: name=VK_NVX_multiview_per_view_attributes, enabled=0
[Vulkan init] extensions: name=VK_NV_ray_tracing, enabled=0
Vulkan API version 1.1.0 (1.1.119 supported by driver)
Vulkan vendor=[NVIDIA] id=[10de]
Vulkan renderer=[GeForce GTX 1060 6GB] id=[1c03]
Vulkan device type 2
Vulkan driverversion=[0x6e148000] uint=[1846837248]
Vulkan PSO: cache data not found [path=/tmp/DefaultCompany/training-dots-hdrp/vulkan_pso_cache.bin]
Begin MonoManager ReloadAssembly
- Completed reload, in  0.719 seconds
WARNING: Shader Unsupported: 'HDRP/Lit' - All passes removed
WARNING: Shader Did you use #pragma only_renderers and omit this platform?
requesting resize 80 x 80
resizing window to 80 x 80
Desktop is 2560 x 1080 @ 60 Hz
InitializeOrResetSwapChain 80x80 hdr=0 samples=1
UnloadTime: 0.484660 ms
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam5_Data/Mono/libcoreclr.so
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam5_Data/Mono/libcoreclr.so
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam5_Data/Mono/libcoreclr.so
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam5_Data/Mono/libSystem.dylib
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam5_Data/Mono/libSystem.dylib.so
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam5_Data/Mono/libSystem.dylib
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam5_Data/Mono/libcoreclr.so
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam5_Data/Mono/libcoreclr.so
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam5_Data/Mono/libcoreclr.so
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam5_Data/Mono/libSystem.dylib
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam5_Data/Mono/libSystem.dylib.so
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam5_Data/Mono/libSystem.dylib
requesting resize 80 x 80
resizing window to 80 x 80
Desktop is 2560 x 1080 @ 60 Hz
InitializeOrResetSwapChain 80x80 hdr=0 samples=1
Compute dispatch: missing UAV ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing UAV ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing UAV ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing UAV ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing UAV ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Compute dispatch: missing texture ID %d 
(Filename:  Line: 0)

Setting up 2 worker threads for Enlighten.
  Thread -> id: 7fc629979700 -> priority: 1 
  Thread -> id: 7fc629178700 -> priority: 1 
AssertionException: Assertion failure. Value was Null
Expected: Value was not Null
Culling was not prepared, please prepare cull before performing it.
  at UnityEngine.Assertions.Assert.Fail (System.String message, System.String userMessage) [0x0003c] in /home/builduser/buildslave/unity/build/Runtime/Export/Assertions/Assert/AssertBase.cs:29 
  at UnityEngine.Assertions.Assert.IsNotNull[T] (T value, System.String message) [0x00042] in /home/builduser/buildslave/unity/build/Runtime/Export/Assertions/Assert/AssertNull.cs:50 
  at UnityEngine.Rendering.HighDefinition.HDProbeSystemInternal.QueryCullResults (UnityEngine.Rendering.HighDefinition.HDProbeCullState state, UnityEngine.Rendering.HighDefinition.HDProbeCullingResults& results) [0x00001] in /media/data/vilson/src/extend/continuum/render/kb/expt/training-dots-hdrp/Library/PackageCache/com.unity.render-pipelines.high-definition@7.3.1/Runtime/Lighting/Reflection/HDProbeSystem.cs:263 
  at UnityEngine.Rendering.HighDefinition.HDProbeSystem.QueryCullResults (UnityEngine.Rendering.HighDefinition.HDProbeCullState state, UnityEngine.Rendering.HighDefinition.HDProbeCullingResults& results) [0x00000] in /media/data/vilson/src/extend/continuum/render/kb/expt/training-dots-hdrp/Library/PackageCache/com.unity.render-pipelines.high-definition@7.3.1/Runtime/Lighting/Reflection/HDProbeSystem.cs:80 
  at UnityEngine.Rendering.HighDefinition.HDRenderPipeline.TryCull (UnityEngine.Camera camera, UnityEngine.Rendering.HighDefinition.HDCamera hdCamera, UnityEngine.Rendering.ScriptableRenderContext renderContext, UnityEngine.Rendering.HighDefinition.SkyManager skyManager, UnityEngine.Rendering.ScriptableCullingParameters cullingParams, UnityEngine.Rendering.HighDefinition.HDRenderPipelineAsset hdrp, UnityEngine.Rendering.HighDefinition.HDRenderPipeline+HDCullingResults& cullingResults) [0x0013c] in /media/data/vilson/src/extend/continuum/render/kb/expt/training-dots-hdrp/Library/PackageCache/com.unity.render-pipelines.high-definition@7.3.1/Runtime/RenderPipeline/HDRenderPipeline.cs:2683 
  at UnityEngine.Rendering.HighDefinition.HDRenderPipeline.Render (UnityEngine.Rendering.ScriptableRenderContext renderContext, UnityEngine.Camera[] cameras) [0x00315] in /media/data/vilson/src/extend/continuum/render/kb/expt/training-dots-hdrp/Library/PackageCache/com.unity.render-pipelines.high-definition@7.3.1/Runtime/RenderPipeline/HDRenderPipeline.cs:1311 
  at UnityEngine.Rendering.RenderPipeline.InternalRender (UnityEngine.Rendering.ScriptableRenderContext context, UnityEngine.Camera[] cameras) [0x0001c] in /home/builduser/buildslave/unity/build/Runtime/Export/RenderPipeline/RenderPipeline.cs:34 
  at UnityEngine.Rendering.RenderPipelineManager.DoRenderLoop_Internal (UnityEngine.Rendering.RenderPipelineAsset pipe, System.IntPtr loopPtr) [0x00038] in /home/builduser/buildslave/unity/build/Runtime/Export/RenderPipeline/RenderPipelineManager.cs:86 
UnityEngine.Camera:Render()
MLAgents.Sensors.CameraSensor:ObservationToTexture(Camera, Int32, Int32) (at /media/data/vilson/src/extend/continuum/render/kb/expt/training-dots-hdrp/com.unity.ml-agents/Runtime/Sensors/CameraSensor.cs:145)
MLAgents.Sensors.CameraSensor:GetCompressedObservation() (at /media/data/vilson/src/extend/continuum/render/kb/expt/training-dots-hdrp/com.unity.ml-agents/Runtime/Sensors/CameraSensor.cs:85)
MLAgents.GrpcExtensions:GetObservationProto(ISensor, WriteAdapter) (at /media/data/vilson/src/extend/continuum/render/kb/expt/training-dots-hdrp/com.unity.ml-agents/Runtime/Communicator/GrpcExtensions.cs:232)
MLAgents.RpcCommunicator:PutObservations(String, AgentInfo, List`1) (at /media/data/vilson/src/extend/continuum/render/kb/expt/training-dots-hdrp/com.unity.ml-agents/Runtime/Communicator/RpcCommunicator.cs:287)
MLAgents.Policies.RemotePolicy:RequestDecision(AgentInfo, List`1) (at /media/data/vilson/src/extend/continuum/render/kb/expt/training-dots-hdrp/com.unity.ml-agents/Runtime/Policies/RemotePolicy.cs:33)
MLAgents.Agent:NotifyAgentDone(DoneReason) (at /media/data/vilson/src/extend/continuum/render/kb/expt/training-dots-hdrp/com.unity.ml-agents/Runtime/Agent.cs:324)
MLAgents.Agent:OnDisable() (at /media/data/vilson/src/extend/continuum/render/kb/expt/training-dots-hdrp/com.unity.ml-agents/Runtime/Agent.cs:311)
 
(Filename: /home/builduser/buildslave/unity/build/Runtime/Export/Assertions/Assert/AssertBase.cs Line: 29)

Vulkan PSO: Inconsistent pipeline cache header size[0], newDataSize[673224]
##utp:{""type"":""MemoryLeaks"",""version"":2,""phase"":""Immediate"",""time"":1587678800201,""processId"":25753,""allocatedMemory"":75157,""memoryLabels"":[{""Default"":40},{""Permanent"":40},{""NewDelete"":15487},{""Thread"":800},{""Manager"":1744},{""GfxDevice"":11536},{""Physics"":32},{""Serialization"":40},{""String"":554},{""DynamicArray"":30792},{""Sprites"":88},{""GI"":12644},{""VR"":2016},{""Subsystems"":-656}]}
```
While searching about that new error `Compute dispatch: missing texture ID %d (Filename:  Line: 0)` I got this https://forum.unity.com/threads/screen-setresolution-causing-materials-to-stop-rendering.804681/. Maybe related?
",disabled ambient occlusion case post run one time got mono path mono path network socket port version id listening already listening display primary device invalid initial resolution forcing detection initialize engine version path device client version driver de device type cache data found begin reload warning shader unsupported removed warning shader use omit platform resize window fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library resize window compute dispatch missing id line compute dispatch missing texture id line compute dispatch missing texture id line compute dispatch missing texture id line compute dispatch missing texture id line compute dispatch missing texture id line compute dispatch missing texture id line compute dispatch missing id line compute dispatch missing texture id line compute dispatch missing texture id line compute dispatch missing texture id line compute dispatch missing texture id line compute dispatch missing texture id line compute dispatch missing id line compute dispatch missing texture id line compute dispatch missing texture id line compute dispatch missing texture id line compute dispatch missing texture id line compute dispatch missing id line compute dispatch missing texture id line compute dispatch missing texture id line compute dispatch missing texture id line compute dispatch missing id line compute dispatch missing texture id line compute dispatch missing texture id line compute dispatch missing texture id line compute dispatch missing texture id line setting worker enlighten thread id priority thread id priority assertion failure value null value null culling prepared please prepare cull message value message state state camera context pipe render camera string list list line inconsistent pipeline cache header size type version phase immediate time default permanent thread manager physic serialization string searching new error compute dispatch missing texture id line got maybe related,issue,negative,negative,negative,negative,negative,negative
618688778,"@harperj I've actually already made the CI changes in this PR: https://github.com/Unity-Technologies/ml-agents-cloud/pull/105 but definitely it needs a looksee from one of you to make sure I didn't miss anything. Seems to work when I tested it. 

I think getting rid of the `run-log` (`summaries`) in the design doc could work, as there will only be two files in there (the timers.json and eventually the Player.Log). The only time it will make sense to have a separate `run-log` is when running many environments, as each will have their own Player.log (so Player-1.log, Player-2.log, etc.) and clutter up the {run-id} directory. ",actually already made definitely need one make sure miss anything work tested think getting rid design doc could work two eventually time make sense separate running many clutter directory,issue,positive,positive,positive,positive,positive,positive
618682600,"Hey!

Yes, I'm using Vulkan, on a GTX 1060 GPU.

Yes, I can run the environment with zero issues on Linux.

I tried to run it just by calling `./hdrp_cam.x86_64 -force-vulkan --mlagents-port 5004` and then running the Python code with ` UnityEnvironment(file_name=None, side_channels = [engine_configuration_channel], base_port=5004)`. Also tried running specifying the `file_name`. Both get me the same issue. Only runs flawlessly when inside Unity Editor ((2019.3.10f1).

The Player logs was a good call! Here you go:

```
Mono path[0] = '/media/data/tmp/unity/hdrp_cam_Data/Managed'
Mono config path = '/media/data/tmp/unity/hdrp_cam_Data/MonoBleedingEdge/etc'
PlayerConnection initialized from /media/data/tmp/unity/hdrp_cam_Data (debug = 0)
PlayerConnection initialized network socket : 0.0.0.0 55495
Multi-casting ""[IP] 192.168.0.10 [Port] 55495 [Flags] 2 [Guid] 1673654108 [EditorId] 0 [Version] 1048832 [Id] LinuxPlayer(192.168.0.10) [Debug] 0 [PackageName] LinuxPlayer [ProjectName] <no name>"" to [225.0.0.222:54997]...
Started listening to [0.0.0.0:55495]
Preloaded 'lib_burst_generated.so'
Preloaded 'libgrpc_csharp_ext.x64.so'
PlayerConnection already initialized - listening to [0.0.0.0:55495]
Display 0 'LG ULTRAWIDE 29""': 2560x1080 (primary device).
Desktop is 2560 x 1080 @ 60 Hz
Invalid initial resolution 80 x 80 - forcing to 100 x 100
[Vulkan init] extensions: count=18
[Vulkan init] extensions: name=VK_KHR_device_group_creation, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_fence_capabilities, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_memory_capabilities, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_semaphore_capabilities, enabled=0
[Vulkan init] extensions: name=VK_KHR_get_display_properties2, enabled=0
[Vulkan init] extensions: name=VK_KHR_get_physical_device_properties2, enabled=0
[Vulkan init] extensions: name=VK_KHR_get_surface_capabilities2, enabled=0
[Vulkan init] extensions: name=VK_KHR_surface, enabled=1
[Vulkan init] extensions: name=VK_KHR_surface_protected_capabilities, enabled=0
[Vulkan init] extensions: name=VK_KHR_wayland_surface, enabled=1
[Vulkan init] extensions: name=VK_KHR_xcb_surface, enabled=0
[Vulkan init] extensions: name=VK_KHR_xlib_surface, enabled=1
[Vulkan init] extensions: name=VK_KHR_display, enabled=1
[Vulkan init] extensions: name=VK_EXT_direct_mode_display, enabled=0
[Vulkan init] extensions: name=VK_EXT_acquire_xlib_display, enabled=0
[Vulkan init] extensions: name=VK_EXT_display_surface_counter, enabled=0
[Vulkan init] extensions: name=VK_EXT_debug_report, enabled=1
[Vulkan init] extensions: name=VK_EXT_debug_utils, enabled=0
Vulkan detection: 2
Initialize engine version: 2019.3.10f1 (5968d7f82152)
[Subsystems] Discovering subsystems at path /media/data/tmp/unity/hdrp_cam_Data/UnitySubsystems
Forcing GfxDevice: Vulkan
GfxDevice: creating device client; threaded=1
[Vulkan init] extensions: count=18
[Vulkan init] extensions: name=VK_KHR_device_group_creation, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_fence_capabilities, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_memory_capabilities, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_semaphore_capabilities, enabled=0
[Vulkan init] extensions: name=VK_KHR_get_display_properties2, enabled=0
[Vulkan init] extensions: name=VK_KHR_get_physical_device_properties2, enabled=0
[Vulkan init] extensions: name=VK_KHR_get_surface_capabilities2, enabled=0
[Vulkan init] extensions: name=VK_KHR_surface, enabled=1
[Vulkan init] extensions: name=VK_KHR_surface_protected_capabilities, enabled=0
[Vulkan init] extensions: name=VK_KHR_wayland_surface, enabled=1
[Vulkan init] extensions: name=VK_KHR_xcb_surface, enabled=0
[Vulkan init] extensions: name=VK_KHR_xlib_surface, enabled=1
[Vulkan init] extensions: name=VK_KHR_display, enabled=1
[Vulkan init] extensions: name=VK_EXT_direct_mode_display, enabled=0
[Vulkan init] extensions: name=VK_EXT_acquire_xlib_display, enabled=0
[Vulkan init] extensions: name=VK_EXT_display_surface_counter, enabled=0
[Vulkan init] extensions: name=VK_EXT_debug_report, enabled=1
[Vulkan init] extensions: name=VK_EXT_debug_utils, enabled=0
[Vulkan init] extensions: count=89
[Vulkan init] extensions: name=VK_KHR_8bit_storage, enabled=0
[Vulkan init] extensions: name=VK_KHR_16bit_storage, enabled=0
[Vulkan init] extensions: name=VK_KHR_bind_memory2, enabled=0
[Vulkan init] extensions: name=VK_KHR_create_renderpass2, enabled=0
[Vulkan init] extensions: name=VK_KHR_dedicated_allocation, enabled=1
[Vulkan init] extensions: name=VK_KHR_depth_stencil_resolve, enabled=0
[Vulkan init] extensions: name=VK_KHR_descriptor_update_template, enabled=1
[Vulkan init] extensions: name=VK_KHR_device_group, enabled=0
[Vulkan init] extensions: name=VK_KHR_draw_indirect_count, enabled=0
[Vulkan init] extensions: name=VK_KHR_driver_properties, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_fence, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_fence_fd, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_memory, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_memory_fd, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_semaphore, enabled=0
[Vulkan init] extensions: name=VK_KHR_external_semaphore_fd, enabled=0
[Vulkan init] extensions: name=VK_KHR_get_memory_requirements2, enabled=1
[Vulkan init] extensions: name=VK_KHR_image_format_list, enabled=1
[Vulkan init] extensions: name=VK_KHR_imageless_framebuffer, enabled=0
[Vulkan init] extensions: name=VK_KHR_maintenance1, enabled=1
[Vulkan init] extensions: name=VK_KHR_maintenance2, enabled=0
[Vulkan init] extensions: name=VK_KHR_maintenance3, enabled=0
[Vulkan init] extensions: name=VK_KHR_multiview, enabled=1
[Vulkan init] extensions: name=VK_KHR_pipeline_executable_properties, enabled=0
[Vulkan init] extensions: name=VK_KHR_push_descriptor, enabled=0
[Vulkan init] extensions: name=VK_KHR_relaxed_block_layout, enabled=0
[Vulkan init] extensions: name=VK_KHR_sampler_mirror_clamp_to_edge, enabled=1
[Vulkan init] extensions: name=VK_KHR_sampler_ycbcr_conversion, enabled=0
[Vulkan init] extensions: name=VK_KHR_shader_atomic_int64, enabled=0
[Vulkan init] extensions: name=VK_KHR_shader_draw_parameters, enabled=0
[Vulkan init] extensions: name=VK_KHR_shader_float16_int8, enabled=0
[Vulkan init] extensions: name=VK_KHR_shader_float_controls, enabled=0
[Vulkan init] extensions: name=VK_KHR_storage_buffer_storage_class, enabled=0
[Vulkan init] extensions: name=VK_KHR_swapchain, enabled=1
[Vulkan init] extensions: name=VK_KHR_swapchain_mutable_format, enabled=0
[Vulkan init] extensions: name=VK_KHR_timeline_semaphore, enabled=0
[Vulkan init] extensions: name=VK_KHR_uniform_buffer_standard_layout, enabled=0
[Vulkan init] extensions: name=VK_KHR_variable_pointers, enabled=0
[Vulkan init] extensions: name=VK_KHR_vulkan_memory_model, enabled=0
[Vulkan init] extensions: name=VK_EXT_blend_operation_advanced, enabled=0
[Vulkan init] extensions: name=VK_EXT_buffer_device_address, enabled=0
[Vulkan init] extensions: name=VK_EXT_calibrated_timestamps, enabled=0
[Vulkan init] extensions: name=VK_EXT_conditional_rendering, enabled=0
[Vulkan init] extensions: name=VK_EXT_conservative_rasterization, enabled=0
[Vulkan init] extensions: name=VK_EXT_depth_clip_enable, enabled=0
[Vulkan init] extensions: name=VK_EXT_depth_range_unrestricted, enabled=0
[Vulkan init] extensions: name=VK_EXT_descriptor_indexing, enabled=0
[Vulkan init] extensions: name=VK_EXT_discard_rectangles, enabled=0
[Vulkan init] extensions: name=VK_EXT_display_control, enabled=0
[Vulkan init] extensions: name=VK_EXT_fragment_shader_interlock, enabled=0
[Vulkan init] extensions: name=VK_EXT_global_priority, enabled=0
[Vulkan init] extensions: name=VK_EXT_host_query_reset, enabled=0
[Vulkan init] extensions: name=VK_EXT_index_type_uint8, enabled=0
[Vulkan init] extensions: name=VK_EXT_inline_uniform_block, enabled=0
[Vulkan init] extensions: name=VK_EXT_line_rasterization, enabled=0
[Vulkan init] extensions: name=VK_EXT_memory_budget, enabled=0
[Vulkan init] extensions: name=VK_EXT_pci_bus_info, enabled=0
[Vulkan init] extensions: name=VK_EXT_pipeline_creation_feedback, enabled=0
[Vulkan init] extensions: name=VK_EXT_post_depth_coverage, enabled=0
[Vulkan init] extensions: name=VK_EXT_sample_locations, enabled=0
[Vulkan init] extensions: name=VK_EXT_sampler_filter_minmax, enabled=0
[Vulkan init] extensions: name=VK_EXT_scalar_block_layout, enabled=0
[Vulkan init] extensions: name=VK_EXT_separate_stencil_usage, enabled=0
[Vulkan init] extensions: name=VK_EXT_shader_demote_to_helper_invocation, enabled=0
[Vulkan init] extensions: name=VK_EXT_shader_subgroup_ballot, enabled=0
[Vulkan init] extensions: name=VK_EXT_shader_subgroup_vote, enabled=0
[Vulkan init] extensions: name=VK_EXT_shader_viewport_index_layer, enabled=0
[Vulkan init] extensions: name=VK_EXT_subgroup_size_control, enabled=0
[Vulkan init] extensions: name=VK_EXT_texel_buffer_alignment, enabled=0
[Vulkan init] extensions: name=VK_EXT_transform_feedback, enabled=0
[Vulkan init] extensions: name=VK_EXT_vertex_attribute_divisor, enabled=0
[Vulkan init] extensions: name=VK_EXT_ycbcr_image_arrays, enabled=0
[Vulkan init] extensions: name=VK_NV_clip_space_w_scaling, enabled=0
[Vulkan init] extensions: name=VK_NV_dedicated_allocation, enabled=0
[Vulkan init] extensions: name=VK_NV_dedicated_allocation_image_aliasing, enabled=0
[Vulkan init] extensions: name=VK_NV_device_diagnostic_checkpoints, enabled=0
[Vulkan init] extensions: name=VK_NV_fill_rectangle, enabled=0
[Vulkan init] extensions: name=VK_NV_fragment_coverage_to_color, enabled=0
[Vulkan init] extensions: name=VK_NV_framebuffer_mixed_samples, enabled=0
[Vulkan init] extensions: name=VK_NV_coverage_reduction_mode, enabled=0
[Vulkan init] extensions: name=VK_NV_geometry_shader_passthrough, enabled=0
[Vulkan init] extensions: name=VK_NV_sample_mask_override_coverage, enabled=0
[Vulkan init] extensions: name=VK_NV_shader_sm_builtins, enabled=0
[Vulkan init] extensions: name=VK_NV_shader_subgroup_partitioned, enabled=0
[Vulkan init] extensions: name=VK_NV_viewport_array2, enabled=0
[Vulkan init] extensions: name=VK_NV_viewport_swizzle, enabled=0
[Vulkan init] extensions: name=VK_NVX_device_generated_commands, enabled=0
[Vulkan init] extensions: name=VK_NVX_multiview_per_view_attributes, enabled=0
[Vulkan init] extensions: name=VK_NV_ray_tracing, enabled=0
Vulkan API version 1.1.0 (1.1.119 supported by driver)
Vulkan vendor=[NVIDIA] id=[10de]
Vulkan renderer=[GeForce GTX 1060 6GB] id=[1c03]
Vulkan device type 2
Vulkan driverversion=[0x6e148000] uint=[1846837248]
Vulkan PSO: cache data not found [path=/tmp/DefaultCompany/training-dots-hdrp/vulkan_pso_cache.bin]
Begin MonoManager ReloadAssembly
- Completed reload, in  0.067 seconds
WARNING: Shader Unsupported: 'HDRP/Lit' - All passes removed
WARNING: Shader Did you use #pragma only_renderers and omit this platform?
requesting resize 80 x 80
resizing window to 80 x 80
Desktop is 2560 x 1080 @ 60 Hz
InitializeOrResetSwapChain 80x80 hdr=0 samples=1
UnloadTime: 0.455892 ms
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam_Data/Mono/libcoreclr.so
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam_Data/Mono/libcoreclr.so
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam_Data/Mono/libcoreclr.so
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam_Data/Mono/libSystem.dylib
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam_Data/Mono/libSystem.dylib.so
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam_Data/Mono/libSystem.dylib
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam_Data/Mono/libcoreclr.so
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam_Data/Mono/libcoreclr.so
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam_Data/Mono/libcoreclr.so
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam_Data/Mono/libSystem.dylib
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam_Data/Mono/libSystem.dylib.so
Fallback handler could not load library /media/data/tmp/unity/hdrp_cam_Data/Mono/libSystem.dylib
requesting resize 80 x 80
resizing window to 80 x 80
Desktop is 2560 x 1080 @ 60 Hz
InitializeOrResetSwapChain 80x80 hdr=0 samples=1
Setting up 2 worker threads for Enlighten.
  Thread -> id: 7f18935fe700 -> priority: 1 
  Thread -> id: 7f1892dfd700 -> priority: 1 
NullReferenceException: Object reference not set to an instance of an object
  at UnityEngine.Rendering.HighDefinition.HDProbeSystemInternal.QueryCullResults (UnityEngine.Rendering.HighDefinition.HDProbeCullState state, UnityEngine.Rendering.HighDefinition.HDProbeCullingResults& results) [0x00050] in <39762aba73064047936cd8a048e58147>:0 
  at UnityEngine.Rendering.HighDefinition.HDProbeSystem.QueryCullResults (UnityEngine.Rendering.HighDefinition.HDProbeCullState state, UnityEngine.Rendering.HighDefinition.HDProbeCullingResults& results) [0x00000] in <39762aba73064047936cd8a048e58147>:0 
  at UnityEngine.Rendering.HighDefinition.HDRenderPipeline.TryCull (UnityEngine.Camera camera, UnityEngine.Rendering.HighDefinition.HDCamera hdCamera, UnityEngine.Rendering.ScriptableRenderContext renderContext, UnityEngine.Rendering.HighDefinition.SkyManager skyManager, UnityEngine.Rendering.ScriptableCullingParameters cullingParams, UnityEngine.Rendering.HighDefinition.HDRenderPipelineAsset hdrp, UnityEngine.Rendering.HighDefinition.HDRenderPipeline+HDCullingResults& cullingResults) [0x0011a] in <39762aba73064047936cd8a048e58147>:0 
  at UnityEngine.Rendering.HighDefinition.HDRenderPipeline.Render (UnityEngine.Rendering.ScriptableRenderContext renderContext, UnityEngine.Camera[] cameras) [0x002a8] in <39762aba73064047936cd8a048e58147>:0 
  at UnityEngine.Rendering.RenderPipeline.InternalRender (UnityEngine.Rendering.ScriptableRenderContext context, UnityEngine.Camera[] cameras) [0x0001c] in <2e20a3f09314483f9ecf28d1adc29392>:0 
  at UnityEngine.Rendering.RenderPipelineManager.DoRenderLoop_Internal (UnityEngine.Rendering.RenderPipelineAsset pipe, System.IntPtr loopPtr) [0x0003d] in <2e20a3f09314483f9ecf28d1adc29392>:0 
UnityEngine.Camera:Render()
MLAgents.Sensors.CameraSensor:ObservationToTexture(Camera, Int32, Int32)
MLAgents.Sensors.CameraSensor:GetCompressedObservation()
MLAgents.GrpcExtensions:GetObservationProto(ISensor, WriteAdapter)
MLAgents.RpcCommunicator:PutObservations(String, AgentInfo, List`1)
MLAgents.Policies.RemotePolicy:RequestDecision(AgentInfo, List`1)
MLAgents.Agent:NotifyAgentDone(DoneReason)
MLAgents.Agent:OnDisable()
 
(Filename: <39762aba73064047936cd8a048e58147> Line: 0)

Vulkan PSO: Inconsistent pipeline cache header size[0], newDataSize[678036]
##utp:{""type"":""MemoryLeaks"",""version"":2,""phase"":""Immediate"",""time"":1587677058598,""processId"":17033,""allocatedMemory"":75156,""memoryLabels"":[{""Default"":40},{""Permanent"":40},{""NewDelete"":15487},{""Thread"":800},{""Manager"":1744},{""GfxDevice"":11536},{""Physics"":32},{""Serialization"":40},{""String"":553},{""DynamicArray"":30792},{""Sprites"":88},{""GI"":12644},{""VR"":2016},{""Subsystems"":-656}]}
```
Thanks in advance Vincent!

",hey yes yes run environment zero tried run calling running python code also tried running get issue flawlessly inside unity editor player good call go mono path mono path network socket port version id name listening already listening display primary device invalid initial resolution forcing detection initialize engine version path forcing device client version driver de device type cache data found begin reload warning shader unsupported removed warning shader use omit platform resize window fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library fallback handler could load library resize window setting worker enlighten thread id priority thread id priority object reference set instance object state state camera context pipe render camera string list list line inconsistent pipeline cache header size type version phase immediate time default permanent thread manager physic serialization string thanks advance vincent,issue,positive,positive,positive,positive,positive,positive
618677112,Closing this off since the equivalent functionality was added.,since equivalent functionality added,issue,negative,neutral,neutral,neutral,neutral,neutral
618670363,"Yeah, it's hard to do the changelog with the PR number before you create the PR :)",yeah hard number create,issue,negative,negative,negative,negative,negative,negative
618668411,"Hi,

What are the graphics settings you have on Linux ? (Are you using Vulkan ?)
Are you able to run the environment on Linux without issues ?
Are you running in batchmode or with the no-graphics command line ?
Do you have some Player logs that could help us debug this?
",hi graphic able run environment without running command line player could help u,issue,negative,positive,positive,positive,positive,positive
618640560,"@donamin - That should be possible, but it's important to have multiple agents, so that each agent can train with its own policy. The high-level agent's actions would then provide values for the low-level agent's obervations.",possible important multiple agent train policy agent would provide agent,issue,negative,positive,positive,positive,positive,positive
618629941,"> Don't forget to increase the communication protocol version!

@vincentpierre ",forget increase communication protocol version,issue,negative,neutral,neutral,neutral,neutral,neutral
618617097,"@ervteng you're right that this will break the daily CI runs.  I think there are 3 places in the CI we'll need to update:
- results reporter
- script for updating NN files
- the runner script's barracuda inference logic

One of us could take a stab if that would be useful.

Unrelatedly, I think the structure you listed in the PR description is a little off -- all files should be nested under the `run-id` folder.  I'd also suggest we might just get rid of the `run_logs` folder and move those files into the general `run-id` folder.",right break daily think need update reporter script runner script barracuda inference logic one u could take stab would useful think structure listed description little folder also suggest might get rid folder move general folder,issue,negative,positive,neutral,neutral,positive,positive
618591615,"```
ERROR: Could not find a version that satisfies the requirement tensorflow>=2.0.0 (from -c test_constraints_max_tf2_version.txt (line 5)) (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3)
ERROR: No matching distribution found for tensorflow>=2.0.0 (from -c test_constraints_max_tf2_version.txt (line 5))
```
Apparently 2.2.0rc1 is not > 2.0.0",error could find version requirement line error matching distribution found line apparently,issue,negative,positive,neutral,neutral,positive,positive
618500829,"@mmattar Nothing that I'm aware of. It sounds like something the [Coding Support Tools package] (https://docs.unity3d.com/Packages/com.unity.coding@0.1/manual/index.html) could potentially handle but not sure if it does (and we don't currently have a way to enforce anything else it does like code formatting anyway). 

More extreme, we could use that package to generate a dump of the API scraping and parse that.",nothing aware like something support package could potentially handle sure currently way enforce anything else like code anyway extreme could use package generate dump scraping parse,issue,negative,positive,positive,positive,positive,positive
618219203,"@mbaske Hi Mathias,

Do you know if it's possible to train multiple policies in parallel? I have a hiearchical policy structure in which it's not possible to train policies one after another. So I need to do all the training together. I have implemented this but now when I start the training, the ml-agents api only detects the high-level policy and does not train the rest.",hi know possible train multiple parallel policy structure possible train one another need training together start training policy train rest,issue,negative,neutral,neutral,neutral,neutral,neutral
618158867,@anupambhatnagar - Added you to the review for visibility as it will require a corresponding change to the CI pipeline. ,added review visibility require corresponding change pipeline,issue,negative,neutral,neutral,neutral,neutral,neutral
618105657,"@chriselion yes, we should be able to test this in a playmode test.",yes able test test,issue,negative,positive,positive,positive,positive,positive
618104782,This issue is fixed in master since we are on barracuda 0.6.3.  I will close this now and please reopen if the issue persists. ,issue fixed master since barracuda close please reopen issue,issue,negative,positive,neutral,neutral,positive,positive
618061102,"I skimmed through code blocks, and did a few spot checks with `git grep foo -- docs/.` and didn't find any others:
```
$ git grep maxStep -- docs/.
docs/Migrating.md:  instead of ""camelCase""; for example, `Agent.maxStep` was renamed to
docs/Migrating.md:    `maxStep` information)
docs/Migrating.md:  - `maxStep` is now a public field on the Agent. (Was moved from
```",skimmed code spot git foo find git instead example information public field agent,issue,negative,neutral,neutral,neutral,neutral,neutral
618053305,"There should be some changes to the documentation I think.
https://github.com/Unity-Technologies/ml-agents/blame/api-PascalCase/docs/Learning-Environment-Examples.md#L364

Otherwise looks good",documentation think otherwise good,issue,negative,positive,positive,positive,positive,positive
618044521,"Hi.

Please use the template for issues that is provided when creating a github issue. If none of the templates fit your issue, you should post on the Unity Forums. The error you are getting seems to come from parsing the yaml config file as suggested by 
```
File ""c:\users\thepr\appdata\local\programs\python\python36\lib\site-packages\yaml\__init__.py"", line 162, in safe_load
    return load(stream, SafeLoader)
```

Yaml is a rather capricious format, you need to be careful about indent for example. Make sure your yaml file is formatted exactly like the ones provided in our example environments :

```
Pyramids:
    summary_freq: 30000
    time_horizon: 128
    batch_size: 128
    buffer_size: 2048
    hidden_units: 512
    num_layers: 2
    beta: 1.0e-2
    max_steps: 1.0e7
    num_epoch: 3
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99
        curiosity:
            strength: 0.02
            gamma: 0.99
            encoding_size: 256

```",hi please use template provided issue none fit issue post unity error getting come file file line return load stream rather capricious format need careful indent example make sure file exactly like provided example beta extrinsic strength gamma curiosity strength gamma,issue,positive,positive,positive,positive,positive,positive
617947872,I merged this into another branch and ran on the cloud with 8 envs and threaded: false.  Everything seems to be running just fine.,another branch ran cloud threaded false everything running fine,issue,negative,positive,neutral,neutral,positive,positive
617488787,"I met this problem too, the unity console said 'SideChannelUtils' does not exist in the current context, have you solved this problem or get any other idea?",met problem unity console said exist current context problem get idea,issue,negative,neutral,neutral,neutral,neutral,neutral
617479861,Thank you for your reply. What do you mean by 'using the agent done'? I haven't found any information about agent done in the Python-API docs,thank reply mean agent done found information agent done,issue,negative,negative,negative,negative,negative,negative
617475398,"Coincidentally my project was v0.13.0, so I made that converter. I don't think it will support other versions",coincidentally project made converter think support,issue,negative,neutral,neutral,neutral,neutral,neutral
617450164,"@ervteng If we put this wrapper with the other gym wrapper, would we be okay having our trainers have a dependency on gym-unity? We might want to test our trainers with some gym environments latter. ",put wrapper gym wrapper would dependency might want test gym latter,issue,negative,neutral,neutral,neutral,neutral,neutral
617450093,"> > Is the opencv requirement dictated by Gym?
> 
> Only if we want to use Atari environments (removed it)

I think `gym` by itself is lightweight enough to require in `ml-agents-envs` if need be, so I'm ok with that. ",requirement gym want use removed think gym lightweight enough require need,issue,negative,neutral,neutral,neutral,neutral,neutral
617446942,"> Is the opencv requirement dictated by Gym?

Only if we want to use Atari environments (removed it)",requirement gym want use removed,issue,negative,neutral,neutral,neutral,neutral,neutral
617431428,"I can't tell if the blue agent behavior name is being caught. I can tell the red agent is because I see 'RedAgent?team=1'

Additionally, you are on release 0.15 but unfortunately this feature is only currently on our master branch.  It will however be in the next release! I encourage you to try it on master but be warned the master branch is not necessarily stable.",ca tell blue agent behavior name caught tell red agent see additionally release unfortunately feature currently master branch however next release encourage try master master branch necessarily stable,issue,negative,negative,neutral,neutral,negative,negative
617423847,"> > Sorry, after looking more at this, I think this should not go in ml-agents-envs. gym-unity is more appropriate and keeps the gym dependency simpler.
> 
> I disagree, gym has a very specific purpose and also a very specific directory structure (dictated by gym). All the gym code lives in the `__init__` file. Dependency matching does not sound like a good enough reason to me, this wrapper creates a UnityEnvironment, as such, I think it should be in ml-agents-envs. @ervteng @awjuliani any thoughts on this? Where do you think it should live?

Are we using the special directory structure that gym requires? It seems they do so b/c of the registry, but we don't register the environment. It also seems that it's OK to have additional files other than `__init__.py` as long as the __init__ is the entry point for the registry. If we don't plan on using the registry ever, I think adding it to gym-unity (even if it breaks the structure) is OK. 

Is the opencv requirement dictated by Gym?",sorry looking think go appropriate gym dependency simpler disagree gym specific purpose also specific directory structure gym gym code file dependency matching sound like good enough reason wrapper think think live special directory structure gym registry register environment also additional long entry point registry plan registry ever think even structure requirement gym,issue,positive,positive,positive,positive,positive,positive
617363017,"Hi @MashiroCl there have been changes in the low level API and hence this error. I recommend 
to not use done and instead consider the agent done if it is in the TerminalSteps.",hi low level hence error recommend use done instead consider agent done,issue,negative,neutral,neutral,neutral,neutral,neutral
617325998,"Here are two screenshots from the agents'  setup and a short video presenting the results
<img width=""625"" alt=""fig1"" src=""https://user-images.githubusercontent.com/35818737/79898285-b38f9d00-8413-11ea-9d98-b2b39d2299da.png"">
<img width=""656"" alt=""fig2"" src=""https://user-images.githubusercontent.com/35818737/79898292-b7232400-8413-11ea-8975-4e76936e63cd.png"">
![ezgif com-video-to-gif](https://user-images.githubusercontent.com/35818737/79898590-2bf65e00-8414-11ea-8210-27a708a3a4ae.gif)

",two setup short video fig fig,issue,negative,neutral,neutral,neutral,neutral,neutral
617320323,"In that case, I think either add another package, or add `gym` to the dependencies in setup.py",case think either add another package add gym,issue,negative,neutral,neutral,neutral,neutral,neutral
617316707,"> Sorry, after looking more at this, I think this should not go in ml-agents-envs. gym-unity is more appropriate and keeps the gym dependency simpler.

I disagree, gym has a very specific purpose and also a very specific directory structure (dictated by gym). All the gym code lives in the `__init__` file. Dependency matching does not sound like a good enough reason to me, this wrapper creates a UnityEnvironment, as such, I think it should be in ml-agents-envs. @ervteng  @awjuliani any thoughts on this? Where do you think it should live?",sorry looking think go appropriate gym dependency simpler disagree gym specific purpose also specific directory structure gym gym code file dependency matching sound like good enough reason wrapper think think live,issue,negative,positive,positive,positive,positive,positive
617315917,"> Each behavior name should have a corresponding entry in the trainer_config (both BlueAgent and RedAgent).

this is my yaml file 

default:
    trainer: ppo
    batch_size: 1024
    beta: 5.0e-3
    buffer_size: 10240
    epsilon: 0.2
    hidden_units: 128
    lambd: 0.95
    learning_rate: 3.0e-4
    learning_rate_schedule: linear
    max_steps: 5.0e5
    memory_size: 128
    normalize: false
    num_epoch: 3
    num_layers: 2
    time_horizon: 64
    sequence_length: 64
    summary_freq: 10000
    use_recurrent: false
    vis_encode_type: simple
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99

BlueAgent:
    max_steps: 6.0e4
    learning_rate_schedule: constant
    normalize: true
    batch_size: 2048
    buffer_size: 20480
    hidden_units: 128
    num_epoch: 2
    summary_freq: 1000
    time_horizon: 1000
    lambd: 0.99
    beta: 0.001
    self_play:
        window: 10
        play_against_latest_model_ratio: 0.5
        save_steps: 1000
        swap_steps: 25000
        team_change: 200000

RedAgent:
    max_steps: 6.0e4
    learning_rate_schedule: constant
    normalize: true
    batch_size: 2048
    buffer_size: 20480
    hidden_units: 128
    num_epoch: 2
    summary_freq: 1000
    time_horizon: 1000
    lambd: 0.99
    beta: 0.001
    self_play:
        play_against_latest_model_ratio: 0.5
        save_steps: 1000
        swap_steps: 25000
        team_change: 200000",behavior name corresponding entry file default trainer beta epsilon linear normalize false false simple extrinsic strength gamma constant normalize true beta window constant normalize true beta,issue,positive,negative,neutral,neutral,negative,negative
617315011,"Here is a more clear view of the result

MLAgentUnityEnv) chairis-mbp:CODE myuser$ mlagents-learn notebooks/trainer_config.yaml --run-id=myGame
WARNING:tensorflow:From /Users/myuser/Programming/MLAgentUnityEnv/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term


                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓

        
 Version information:
  ml-agents: 0.15.0,
  ml-agents-envs: 0.15.0,
  Communicator API: 0.15.0,
  TensorFlow: 2.0.1
2020-04-21 20:43:19 INFO [environment.py:131] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
2020-04-21 20:43:25 INFO [environment.py:159] Connected to Unity environment with package version 0.15.0-preview and communication version 0.15.0
2020-04-21 20:43:25 INFO [environment.py:304] Connected new brain:
RedAgent?team=1
2020-04-21 20:43:25 INFO [trainer_controller.py:167] Hyperparameters for the GhostTrainer of brain RedAgent: 
	trainer:	ppo
	batch_size:	2048
	beta:	0.001
	buffer_size:	20480
	epsilon:	0.2
	hidden_units:	128
	lambd:	0.99
	learning_rate:	0.0003
	learning_rate_schedule:	constant
	max_steps:	6.0e4
	memory_size:	128
	normalize:	True
	num_epoch:	2
	num_layers:	2
	time_horizon:	1000
	sequence_length:	64
	summary_freq:	1000
	use_recurrent:	False
	vis_encode_type:	simple
	reward_signals:	
	  extrinsic:	
	    strength:	1.0
	    gamma:	0.99
	summary_path:	myGame_RedAgent
	model_path:	./models/myGame/RedAgent
	keep_checkpoints:	5
	self_play:	
	  play_against_latest_model_ratio:	0.5
	  save_steps:	1000
	  swap_steps:	25000
	  team_change:	200000
2020-04-21 20:43:25.787774: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-04-21 20:43:25.800105: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb7da62a7c0 executing computations on platform Host. Devices:
2020-04-21 20:43:25.800122: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2020-04-21 20:43:39 INFO [subprocess_env_manager.py:149] UnityEnvironment worker 0: environment stopping.
",clear view result code warning removed future version long term version information communicator listening port start training pressing play button unity editor connected unity environment package version communication version connected new brain brain trainer beta epsilon constant normalize true false simple extrinsic strength gamma binary use service platform host device host default version worker environment stopping,issue,positive,positive,neutral,neutral,positive,positive
617314410,"Sorry, after looking more at this, I think this should not go in ml-agents-envs. gym-unity is more appropriate and keeps the gym dependency simpler.",sorry looking think go appropriate gym dependency simpler,issue,negative,neutral,neutral,neutral,neutral,neutral
617312415,Each behavior name should have a corresponding entry in the trainer_config (both BlueAgent and RedAgent).  ,behavior name corresponding entry,issue,negative,neutral,neutral,neutral,neutral,neutral
617308361,"Hi I am new in ml-agents and I think that I have a similar problem. I have developed a board game with two opponents.
Two agents with different behavior names (BlueAgent and RedAgent) and Team ID (0,1) 
I run the game with the following command (through the unity editor):
`mlagents-learn notebooks/trainer_config.yaml --run-id=myGame`

I use as Vector Observation Space Size:100, and Stacked Vector:1 (A 2D array transform to C# List). Vector Action Type Discrete with two branches.

The problem is that when I run the training I noticed that only the one agent is training and the second returns always 0 in both branches.
Also, I noticed that in the command line window the ml-agents system recognized only the one agent (BlueAgent).

Where is the problem ??

My goal is after the training to have two different NN each one for each agent.

2020-04-20 21:27:39 INFO [trainer_controller.py:167] Hyperparameters for the GhostTrainer of brain BlueAgent: 
    trainer:    ppo
   batch_size:    2048
    beta:    0.001
    buffer_size:    20480
    epsilon:    0.2
    hidden_units:    128
    lambd:    0.99
    learning_rate:    0.0003
    learning_rate_schedule:    constant
    max_steps:    6.0e4
    memory_size:    128
    normalize:    True
    num_epoch:    2
    num_layers:    2
    time_horizon:    1000
    sequence_length:    64
    summary_freq:    1000
    use_recurrent:    False
    vis_encode_type:    simple
    reward_signals:   
      extrinsic:   
        strength:    1.0
        gamma:    0.99
    summary_path:    RLGame_BlueAgent
    model_path:    ./models/RLGame/BlueAgent
    keep_checkpoints:    5
    self_play:   
      window:    10
      play_against_latest_model_ratio:    0.5
      save_steps:    1000
      swap_steps:    25000
      team_change:    200000
",hi new think similar problem board game two two different behavior team id run game following command unity editor use vector observation space size vector array transform list vector action type discrete two problem run training one agent training second always also command line window system one agent problem goal training two different one agent brain trainer beta epsilon constant normalize true false simple extrinsic strength gamma window,issue,negative,negative,neutral,neutral,negative,negative
617080687,"> Only v0.13.0
> ckpt to nn converter
> 
> https://github.com/ys-yamashita/cptk2barracuda

Seems interesting and could be really useful. Is there any reason it wouldn't work with other versions such as v0.15.1?

___

Also, @ervteng / @anupambhatnagar, could a utility like this be added to the project and kept up to date with releases if it is version specific?",converter interesting could really useful reason would work also could utility like added project kept date version specific,issue,positive,positive,positive,positive,positive,positive
616988141,"I converted a trained tensorflow model to Barracuda, and that went successfully. But the same error occurred when I tried to drag the .nn model from asset to the agent's component. I can't use my model in Unity. So is it also because of the names and constants?",converted trained model barracuda went successfully error tried drag model asset agent component ca use model unity also,issue,negative,positive,positive,positive,positive,positive
616880156,all changes made @awjuliani. i think it's ready for merge,made think ready merge,issue,negative,positive,positive,positive,positive,positive
616863853,"> Can we make make the constructor take a BaseEnv as input rather than create a UnityEnvironment in the __init__?

Works for me, any objection @awjuliani or @ervteng ?",make make constructor take input rather create work objection,issue,negative,neutral,neutral,neutral,neutral,neutral
616784808,"for further discussions about this issue, please reach out to us in the ML-Agent forum. https://forum.unity.com/forums/ml-agents.453/",issue please reach u forum,issue,negative,neutral,neutral,neutral,neutral,neutral
616758727,"@awjuliani I still want to split the YAML to trainer and model. With luck that will be done after the other CLI and output file changes are done. 

The files were generated using a script that loaded the old config's default and overrode them using the config, then outputted the new YAML. I'm also worried about the files diverging before we are able to create the default values in code. This PR is technically still compatible with the old config files (as long as you add `behaviors:`) to it. 

Would it be a good idea to have an intermediary step that uses the old `trainer_config` and only migrates generalization/curriculum? We can then migrate the others when the defaults are properly codified. ",still want split trainer model luck done output file done script loaded old default new also worried diverging able create default code technically still compatible old long add would good idea intermediary step old migrate properly,issue,positive,positive,positive,positive,positive,positive
616757139,I think the `python_3.7.3+tf2` failure is better on laster master (on not related to this change). Merging anyway.,think failure better laster master related change anyway,issue,negative,positive,neutral,neutral,positive,positive
616749943,"> @andrewcoh - can you take a look into why the CI tests are failing? My PR doesn't touch any training code.

",take look failing touch training code,issue,negative,neutral,neutral,neutral,neutral,neutral
616730270,"Hi @JohnBergago 
Thank you for your feedback, the done filed in the LLAPI doc is a mistake, I made a pull request to correct it.

We deprecated our multi-agent gym wrapper because it was very hard to maintain and did not work for a lot of environments. We encourage users to use the LLAPI directly or write their own wrappers for their specific needs.
If there is a functionality that you need in the LLAPI that does not exist, please let us know.",hi thank feedback done doc mistake made pull request correct gym wrapper hard maintain work lot encourage use directly write specific need functionality need exist please let u know,issue,positive,negative,neutral,neutral,negative,negative
616725712,Counter offer : Can we make make the constructor take a BaseEnv as input rather than create a UnityEnvironment in the __init__?,counter offer make make constructor take input rather create,issue,negative,neutral,neutral,neutral,neutral,neutral
616716250,I'll try to reproduce the issue you are having. We use github issues for bugs and feature requests only. I recommend you post the issue on the ML-Agents forum. https://forum.unity.com/forums/ml-agents.453/ We can continue the discussion there.,try reproduce issue use feature recommend post issue forum continue discussion,issue,negative,neutral,neutral,neutral,neutral,neutral
616715103,"As mentioned in the previous comment, I think it's not possible to pickle the Unity executable.",previous comment think possible pickle unity executable,issue,negative,negative,neutral,neutral,negative,negative
616708246,@starman12 your comment does not state the error message. moreover we reserve github for feature requests and bugs. could you please post your error message in the ML-Agents forum. https://forum.unity.com/forums/ml-agents.453/,comment state error message moreover reserve feature could please post error message forum,issue,negative,neutral,neutral,neutral,neutral,neutral
616707158,"The issue has not been resolved though. Every time I make a change to a project, I get the error and I have to re-clone the repo and re-install all dependencies. Maybe it is an installation issue, but I'm following all of the directions in the docs and still getting it. Do you have any insights into why exactly that could be?",issue resolved though every time make change project get error maybe installation issue following still getting exactly could,issue,negative,positive,positive,positive,positive,positive
616706741,that's great! I'll close the request as it has been resolved.,great close request resolved,issue,positive,positive,positive,positive,positive,positive
616705330,@atapley looks like you have been able to resolve the issue. It's not a bug but an installation issue. I'll be closing the issue.,like able resolve issue bug installation issue issue,issue,positive,positive,positive,positive,positive,positive
616576849,"I was able to get rid of the error once again by deleting and recloning the repo/creating a new python venv and installing the dependencies in ml-agents-envs and ml-agents. I was able to train my model with no issues on my custom environment. However, once I changed a line to the config.yaml file within my own directory (completely separate from ml-agents repo), I got the error again. Recloning the repo and creating a new venv solves the problem.

It seems like whenever I make a change within my project or the ml-agents project, this error shows up. ",able get rid error new python able train model custom environment however line file within directory completely separate got error new problem like whenever make change within project project error,issue,negative,positive,positive,positive,positive,positive
616455184,Referencing a folder with multiple demonstrations seems to be working. Thanks!,folder multiple working thanks,issue,negative,positive,neutral,neutral,positive,positive
616261712,"I asked on slack a few months ago about failing CI for doc xml warnings, but at the time there was nothing. If there are logs in yamato, we can parse them to do our own enforcement (similar to what I did for coverage %) but last I checked I couldn't find them.",slack ago failing doc time nothing parse enforcement similar coverage last checked could find,issue,negative,neutral,neutral,neutral,neutral,neutral
615896449,"Thanks for the answer!
I tried on 3DBall with only one agent  and  multiagent=False and still have the error so the problem should not come from the multiagent.",thanks answer tried one agent still error problem come,issue,negative,positive,positive,positive,positive,positive
615879008,"Hi @vincentpierre,
I tested your new implementation of the environment interfaces. At first it was a little bit confusing, that max_step is only part of the info dict, when a terminal step happened. However, that one was easy to fix. But since you are not supporting multiagent environment anymore (which I was using a lot) I will have to figure out how to write my own wrapper anyway.

By the way, I just recognized that the LLAPI docs say, that the `DecisionStep` and `TerminalStep` contain a `done` field, but this seems not to be the case.  

Thanks for your effort.",hi tested new implementation environment first little bit part terminal step however one easy fix since supporting environment lot figure write wrapper anyway way say contain done field case thanks effort,issue,positive,positive,positive,positive,positive,positive
615835971,"And one more question: 
Is it possible to combine the demonstrations with curriculum learning? I.e. I have a recording for each curriculum and can change the used demonstration depending on the current curriculum?
Using a recording of the first curriculum makes my agent act really nicely in the first curriculum, but not in later curricula. Conversely, if I use longer recordings of multiple curricula, the agent just does not perform in the first one.",one question possible combine curriculum learning recording curriculum change used demonstration depending current curriculum recording first curriculum agent act really nicely first curriculum later curriculum conversely use longer multiple curriculum agent perform first one,issue,positive,positive,positive,positive,positive,positive
615497046,"I'll give it a try tomorrow, thanks for the reply!
I do have a quick question:
The GAIL algorithm is gone into detail and the original paper is referenced. 
Is there any literature the behavioral cloning algorithm is based on?",give try tomorrow thanks reply quick question algorithm gone detail original paper literature behavioral algorithm based,issue,positive,positive,positive,positive,positive,positive
615494322,@anupambhatnagar - this will definitely break daily CI - what steps should I take to adapt the ml-agents-cloud utility to match?,definitely break daily take adapt utility match,issue,negative,neutral,neutral,neutral,neutral,neutral
615484203,"TensorFlowSharp is no longer supported by ML-Agents. If you're about to use it to run your model, you're welcome to try, but we can't help you with it. Or you can use a previous version of ML-Agents, such as 0.6.0 https://github.com/Unity-Technologies/ml-agents/tree/0.6.0",longer use run model welcome try ca help use previous version,issue,positive,positive,positive,positive,positive,positive
615480357,"This documentation page may be of help
https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Reward-Signals.md#gail-reward-signal",documentation page may help,issue,negative,neutral,neutral,neutral,neutral,neutral
615471973,"Hi @Fabien-Couthouis a couple of thoughts:
- the gym wrapper may not support more than one agent 
- it probably not possible to pickle the entire unity executable.",hi couple gym wrapper may support one agent probably possible pickle entire unity executable,issue,negative,neutral,neutral,neutral,neutral,neutral
615470142,"Seems odd to me that it says API version 0.15.0. Should it say 0.15.1 since I am cloning the latest?

EDIT: Also using python 3.6.10

After looking through the FAQ, I'm adding that I have no firewall on my Mac - so that is not an issue.",odd version say since latest edit also python looking mac issue,issue,negative,positive,positive,positive,positive,positive
615467940,"Hi @Xiromtz based on conversations with my colleagues we recommend trying out the following:
""Adding multiple recording files to the imitation learning is possible. I think you need to put them all into a folder and then pass the path to the folder in the yaml file"".

Let us know if this works. ",hi based recommend trying following multiple recording imitation learning possible think need put folder pas path folder file let u know work,issue,negative,negative,neutral,neutral,negative,negative
615463723,@alonshoa we use github issues for feature requests and bugs only. could you please ask your question in the ml-agents forum https://forum.unity.com/forums/ml-agents.453/?,use feature could please ask question forum,issue,negative,neutral,neutral,neutral,neutral,neutral
615463250,"> Should we put the wrapper in gym-unity vs. ml-agents-envs?

Not sure, if the only reason is dependencies, I would say no. 
I think gym-unity serves a specific purpose unity -> gym and not the other way around.

Make another package : ) ?",put wrapper sure reason would say think specific purpose unity gym way around make another package,issue,negative,positive,positive,positive,positive,positive
615463011,"@RLranger looking at your screenshot this may not be a bug. Have you tried to resolve the warning message ""the model expects 2 visual inputs, but only found 0 visual sensors."" I encourage you to post the issue on our forum. https://forum.unity.com/forums/ml-agents.453/",looking may bug tried resolve warning message model visual found visual encourage post issue forum,issue,positive,neutral,neutral,neutral,neutral,neutral
615459925,thanks for notifying about this issue @Alexander-Bakogeorge. I have added this to our bug tracker. ,thanks issue added bug tracker,issue,negative,positive,positive,positive,positive,positive
615458049,"Wrapper itself looks great!

Should we put the wrapper in gym-unity vs. ml-agents-envs? Not sure if we should have packages like `cmake`, `atari-py`, and `opencv` in our list of requirements for the `ml-agents-envs` package. ",wrapper great put wrapper sure like list package,issue,positive,positive,positive,positive,positive,positive
615342134,"My error has been solved when I tried to use virtualenv instead of Anaconda.
The cmd commands that I used:

`virtualenv --python=.<your Python 3.7 file> <newEnvironment>`

Mine was like this:
`virtualenv --python=..\AppData\Local\Programs\Python\Python37\python.exe vnv`

this creates a new virtual environment called vnv.

![image](https://user-images.githubusercontent.com/32769732/79591551-5cbb5800-80e1-11ea-85a7-ceabb766588d.png)


",error tried use instead anaconda used python file mine like new virtual environment image,issue,negative,positive,positive,positive,positive,positive
615282342,"Same.
And I tried by git clone & pip install -e too. Not working.",tried git clone pip install working,issue,negative,neutral,neutral,neutral,neutral,neutral
615182388,"This is really cool stuff, Is there a way to simulate robotic arm or a rover with arm (ex- Boston dynamics handle) which generating point cloud from the environment and train up.
  ",really cool stuff way simulate arm rover arm boston dynamic handle generating point cloud environment train,issue,positive,positive,positive,positive,positive,positive
614986398,"> Should I hold off on this change until closer to the release branch date? I'm worried we'll get a lot of support issues for people that are using master docs and release 0.15 code (on vice versa).

@chriselion - I think that may be better.",hold change closer release branch date worried get lot support people master release code vice think may better,issue,positive,positive,positive,positive,positive,positive
614972450,I did just drag and drop the newly learned models into my agent's behaviour parameters script post training. The 1st gif I shared is the behaviour I see when I press play in editor without the python process.,drag drop newly learned agent behaviour script post training st gif behaviour see press play editor without python process,issue,negative,positive,neutral,neutral,positive,positive
614954602,Should I hold off on this change until closer to the release branch date? I'm worried we'll get a lot of support issues for people that are using master docs and release 0.15 code (on vice versa).,hold change closer release branch date worried get lot support people master release code vice,issue,negative,neutral,neutral,neutral,neutral,neutral
614828065,"This is going to take a while :
```
System.Reflection.TargetInvocationException : Exception has been thrown by the target of an invocation.
  ----> System.InvalidOperationException : The following game object is invoking the DontDestroyOnLoad method: AcademyFixedUpdateStepper. Notice that DontDestroyOnLoad can only be used in play mode and, as such, cannot be part of an editor script.
```",going take exception thrown target invocation following game object method notice used play mode part editor script,issue,negative,negative,negative,negative,negative,negative
614753591,"Ok, I understand. Just to be sure, did you drag and drop the newly learned models into your agents?  Is this the same behavior you see when you just press play in the editor without launching the python process?",understand sure drag drop newly learned behavior see press play editor without python process,issue,negative,positive,positive,positive,positive,positive
614655735,"Clips were recorded at ~9 million steps.

The goal is for it to be a cooperative task, where the agents need to work together to keep themselves and the weight above the equator. If any of the agents, or the weight, drop past the equator Done is called on the agents. They are then re-positioned to the top of the planet along with the weight and the next episode starts. 

The agents get 0.1 reward per decision where they are still alive, with no penalties for environment resets or dying.",clip million goal task need work together keep weight equator weight drop past equator done top planet along weight next episode get reward per decision still alive environment dying,issue,positive,positive,neutral,neutral,positive,positive
614229619,"Good call out. This was a point I was looking for input on.

Options were meant to include aspects of ML-Agents that can be applied regardless of the algorithm choice (e.g. RL/IL,Hybrids). So it includes memory, curriculum environment randomization, and self-play. The Training Methods section focuses on the reward signals and whether you're using RL/IL or hybrids.

I don't feel strongly, it felt like a decent way to bucket the training capabilities. 
",good call point looking input meant include applied regardless algorithm choice memory curriculum environment randomization training section reward whether feel strongly felt like decent way bucket training,issue,positive,positive,positive,positive,positive,positive
614228496,"At what point in training did you record that python gif?

Can you describe the agent's objective and the reward function?",point training record python gif describe agent objective reward function,issue,positive,neutral,neutral,neutral,neutral,neutral
614192480,"@andrewcoh 

```
    trainer: ppo
    batch_size: 128
    beta: 5.0e-3
    buffer_size: 1024
    epsilon: 0.2
    hidden_units: 128
    lambd: 0.95
    learning_rate: 3.0e-4
    learning_rate_schedule: linear
    max_steps: 1.0e7
    memory_size: 256
    normalize: false
    num_epoch: 3
    num_layers: 2
    time_horizon: 64
    sequence_length: 64
    summary_freq: 10000
    use_recurrent: false
    vis_encode_type: simple
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99
        curiosity:
            strength: 0.02
            gamma: 0.99
            encoding_size: 256
```",trainer beta epsilon linear normalize false false simple extrinsic strength gamma curiosity strength gamma,issue,positive,negative,negative,negative,negative,negative
614179823,"Hi @Alexander-Bakogeorge 

Can you share your trainer configuration too?",hi share trainer configuration,issue,negative,neutral,neutral,neutral,neutral,neutral
613773428,"I'd say nothing to start.

I just pushed the first ""prettier"" PR #3782 as a follow-up to PR #3775",say nothing start first,issue,negative,positive,positive,positive,positive,positive
613762144,"So do you want me to merge this as is, or pick a different file to whitelist? Or nothing to start with?",want merge pick different file nothing start,issue,negative,neutral,neutral,neutral,neutral,neutral
613752654,"This looks great. As I'm touching most of the pages with my PRs, I can roll this out with each PR (on the specific pages). Then we can roll it out to the entire repo.",great touching roll specific roll entire,issue,positive,positive,positive,positive,positive,positive
613722807,"@mmattar Let me know if you like this and want me to roll it out more widely (I can also wait until your current PRs are landed, or pick some different guinea pigs).",let know like want roll widely also wait current landed pick different guinea,issue,negative,negative,neutral,neutral,negative,negative
613708186,"Hi @JohnBergago ,
Thank you for reporting this bug.
Our implementation of the `gym` wrapper changed on `master` since the 0.15.1 release and I think your issue has been resolved. Can you try to reproduce your error on `master` so I can make sure this is the case? 

",hi thank bug implementation gym wrapper master since release think issue resolved try reproduce error master make sure case,issue,negative,positive,positive,positive,positive,positive
613221559,"Hi ,

Heuristic() method is only sending Actions in non training mode to test actions (correct me if I am wrong ) , My request is  , in training mode from  OnActionReceived() , it would be great to have some hook to unity Input manager .

for Example instead of writing below in agent 

 public override void OnActionReceived(float[] vectorAction)
    {
case k_Right:
                targetPos = transform.position + new Vector3(1f, 0, 0f);
                break;
}

I would request something like 

 public override void OnActionReceived(float[] vectorAction)
    {
case k_Right:
                SetKey(A); // the logic corresponding to this action will be game but not in agent so from agent we only pass keystrokes , the game will receive this keystrokes and perform corresponding logic will be executed , this way we can keep ML as controller logic above game without modifying game files. 
                break;
}

",hi heuristic method sending non training mode test correct wrong request training mode would great hook unity input manager example instead writing agent public override void float case new vector break would request something like public override void float case logic corresponding action game agent agent pas game receive perform corresponding logic executed way keep controller logic game without game break,issue,positive,negative,negative,negative,negative,negative
613146281,"Old inspector for comparison: 
![image](https://user-images.githubusercontent.com/6877802/79170336-ef878a00-7da3-11ea-9b06-016dbe746f76.png)
",old inspector comparison image,issue,negative,positive,neutral,neutral,positive,positive
613059666,"@andrewcoh  Hi , 

The algorithm I'm using is a Rainbow DQN but it doesn't matter since any algorithm with Experience Replay would face with same problem.

I'm keeping all states in history to later sample and train from them(off -policy) . On my ""step"" function which basically stores environment results after taking calling environments own step function.  But since all  state shapes added before to batch/memory is (1,20) shaped when terminal state arrives code basically breaks down. I dont think this is correct behaviour from environment.

Just like you said It shouldn't new starting state because that literally breaks all existing implementations  since all shapes must be same. 

Problem may be solved if step doesn't return 2 states. I have hacked one of the baseline algorithms to work like this but it's not ideal.



```
def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:
        self.env.set_actions(group_name,np.atleast_2d(action))
        self.env.step()
        step_result = self.env.get_step_result(group_name)

        next_state = step_result.obs[0]
        reward = step_result.reward[0]
        done = step_result.done[0]
        if(next_state.shape[0] == 2):
            print(next_state.shape)
            next_state = np.d def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:
        self.env.set_actions(group_name,np.atleast_2d(action))
        self.env.step()
        step_result = self.env.get_step_result(group_name)

        next_state = step_result.obs[0]
        reward = step_result.reward[0]
        done = step_result.done[0]
        if(next_state.shape[0] == 2):
            print(next_state.shape)
            next_state = np.delete(next_state,1,0)
        if not self.is_test:
            self.transition += [reward, next_state, done]
            
            # N-step transition
            if self.use_n_step:
                one_step_transition = self.memory_n.store(*self.transition)
            # 1-step transition
            else:
                one_step_transition = self.transition

            # add a single step transition
            if one_step_transition:
                self.memory.store(*one_step_transition)
    
        return next_state, reward, doneelete(next_state,1,0)
        if not self.is_test:
            self.transition += [reward, next_state, done]
            
            # N-step transition
            if self.use_n_step:
                one_step_transition = self.memory_n.store(*self.transition)
            # 1-step transition
            else:
                one_step_transition = self.transition

            # add a single step transition
            if one_step_transition:
                self.memory.store(*one_step_transition)
    
        return next_state, reward, done
```",hi algorithm rainbow matter since algorithm experience replay would face problem keeping history later sample train step function basically environment taking calling step function since state added shaped terminal state code basically dont think correct behaviour environment like said new starting state literally since must problem may step return hacked one work like ideal step self action bool action reward done print step self action bool action reward done print reward done transition transition else add single step transition return reward reward done transition transition else add single step transition return reward done,issue,positive,positive,positive,positive,positive,positive
613049567,"Hi @ugurkanates 

The reason for this is that when an Agent reaches a terminal state, the environment returns the final state as well as a new starting state.

Can you explain your issue a bit more? I'm not sure if you mean there is an issue with the notebook example, or with something you're trying to do on your own.  Either way, let me know and I'll try to help.",hi reason agent terminal state environment final state well new starting state explain issue bit sure mean issue notebook example something trying either way let know try help,issue,positive,positive,neutral,neutral,positive,positive
613042655,"Thank you for pointing this out. We will clarify this parameter in the documentation.

In Unity, you can associate each object with a 'layer'.  The layer mask in the ray perception script enables you to use raycasts that ignore certain layers i.e. if you want to have ray casts that only detect a ball and you want to prevent the raycasts from being obstructed by other agents.",thank pointing clarify parameter documentation unity associate object layer mask ray perception script use ignore certain want ray detect ball want prevent,issue,negative,positive,positive,positive,positive,positive
613039982,"Hi @graybob 

It seems to me that what you are looking for is the Heuristic() method in the agent script. The Heuristic() function enables you to send actions to your agent via keyboard etc. Please see some of our example environments for examples. 

Let me know if you are looking for additional functionality.",hi looking heuristic method agent script heuristic function send agent via keyboard please see example let know looking additional functionality,issue,negative,neutral,neutral,neutral,neutral,neutral
613038899,"Hi @fedetask 

Buffer size should be a multiple of batch size but time horizon need not be.  One actor can contribute multiple trajectories of length `time_horizon` to a single buffer.",hi buffer size multiple batch size time horizon need one actor contribute multiple length single buffer,issue,negative,negative,neutral,neutral,negative,negative
613029243,"I just looked more closely at the tensorboard figures. Even though one agent is technically getting higher reward, it's overall ELO is decreasing because the final rewards of those trajectories are negative.  Can you modify the reward function so that 'winning' has a positive final reward?",closely even though one agent technically getting higher reward overall decreasing final negative modify reward function positive final reward,issue,positive,positive,neutral,neutral,positive,positive
613007610,Glad you were able to resolve the issue. ,glad able resolve issue,issue,positive,positive,positive,positive,positive,positive
612872200,"> Thanks for sharing these versions. Have you been able to make it all the way through this guide without Unity crashing: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started.md ?

Yes, I followed the instructions exactly. 
But now, after I clean up background programs in my computer, the problem was solved. 
I did some tests, and I think that is the reason causes Unity to crash on my computer. 
My CPU model is `Intel(R) Core(TM) i5-6300U CPU @ 2.40GHZ` and memory capacity is 8 GB.
Thank you for your help!",thanks able make way guide without unity yes exactly clean background computer problem think reason unity crash computer model core memory capacity thank help,issue,negative,positive,positive,positive,positive,positive
612657647,"Can you describe your reward function with more detail? Looking at the log, it seems the agent's are experiencing a negative final reward more often than not.  

Can you describe the behavior you'd like to see from the agents?",describe reward function detail looking log agent negative final reward often describe behavior like see,issue,positive,negative,negative,negative,negative,negative
612624464,"I am using PPO with self-play. My environment has fixed episode length, +1 for winning, -1 for losing, and 0 for a draw. The cumulative reward is correct, always zero, while the extrinsic reward shows some spikes.

![extr](https://user-images.githubusercontent.com/32360377/79071223-d8c93080-7cda-11ea-984b-856078c9d545.png)
![cum](https://user-images.githubusercontent.com/32360377/79071224-d961c700-7cda-11ea-9115-6557fc56fb9d.png)

I am not sure I understand the extrinsic reward. Does It show only the reward of the learning team?

",environment fixed episode length winning losing draw cumulative reward correct always zero extrinsic reward cum sure understand extrinsic reward show reward learning team,issue,positive,positive,positive,positive,positive,positive
612549997,"[Update] Similar results, the ELO of the agents steadily decreases.
Is this a bug or is there something I've not configured correctly? Open to suggestions

[twtich](https://www.twitch.tv/ellmwood) *videos of the process are on my Twitch Channel, also the live training is being streamed.

![reporting](https://user-images.githubusercontent.com/1829224/79058683-b8ca4a80-7ca3-11ea-88dc-1583946b0c46.png)

```



                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓


 Version information:
  ml-agents: 0.16.0.dev0,
  ml-agents-envs: 0.16.0.dev0,
  Communicator API: 0.16.0,
  TensorFlow: 2.0.1
WARNING:tensorflow:From c:\users\gigaf\source\repos\ml-agents\python-envs\mlagents\lib\site-packages\tensorflow_core\python\compat\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2020-04-11 17:27:35.337986: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-04-11 17:27:35 INFO [stats.py:130] Hyperparameters for behavior name snake-04_SnakeA:
        trainer:        ppo
        batch_size:     1024
        beta:   0.005
        buffer_size:    10240
        epsilon:        0.2
        hidden_units:   256
        lambd:  0.95
        learning_rate:  0.0003
        learning_rate_schedule: constant
        max_steps:      5.0e7
        memory_size:    128
        normalize:      True
        num_epoch:      3
        num_layers:     2
        time_horizon:   1000
        sequence_length:        64
        summary_freq:   10000
        use_recurrent:  False
        vis_encode_type:        simple
        reward_signals:
          extrinsic:
            strength:   1.0
            gamma:      0.99
        summary_path:   snake-04_SnakeA
        model_path:     ./models/snake-04/SnakeA
        keep_checkpoints:       5
        self_play:
          window:       10
          play_against_latest_model_ratio:      0.5
          save_steps:   50000
          swap_steps:   50000
          team_change:  100000
2020-04-11 17:27:36 INFO [tf_policy.py:118] Loading model for brain SnakeA?team=0 from ./models/snake-04/SnakeA.
2020-04-11 17:27:36 INFO [tf_policy.py:147] Resuming training from step 602741.
2020-04-11 17:27:36 INFO [stats.py:130] Hyperparameters for behavior name snake-04_SnakeB:
        trainer:        ppo
        batch_size:     1024
        beta:   0.005
        buffer_size:    10240
        epsilon:        0.2
        hidden_units:   256
        lambd:  0.95
        learning_rate:  0.0003
        learning_rate_schedule: constant
        max_steps:      5.0e7
        memory_size:    128
        normalize:      True
        num_epoch:      3
        num_layers:     2
        time_horizon:   1000
        sequence_length:        64
        summary_freq:   10000
        use_recurrent:  False
        vis_encode_type:        simple
        reward_signals:
          extrinsic:
            strength:   1.0
            gamma:      0.99
        summary_path:   snake-04_SnakeB
        model_path:     ./models/snake-04/SnakeB
        keep_checkpoints:       5
        self_play:
          window:       10
          play_against_latest_model_ratio:      0.5
          save_steps:   50000
          swap_steps:   50000
          team_change:  100000
2020-04-11 17:27:37 INFO [tf_policy.py:118] Loading model for brain SnakeB?team=1 from ./models/snake-04/SnakeB.
2020-04-11 17:27:37 INFO [tf_policy.py:147] Resuming training from step 516693.
2020-04-11 17:29:54 INFO [stats.py:111] snake-04_SnakeA: Step: 610000. Time Elapsed: 143.277 s Mean Reward: -0.500. Std of Reward: 1.118. Training.
2020-04-11 17:29:54 INFO [stats.py:116] snake-04_SnakeA ELO: 1199.501.
2020-04-11 17:32:38 INFO [stats.py:111] snake-04_SnakeA: Step: 620000. Time Elapsed: 307.693 s Mean Reward: -0.125. Std of Reward: 1.452. Training.
2020-04-11 17:32:38 INFO [stats.py:116] snake-04_SnakeA ELO: 1198.466.
2020-04-11 17:35:24 INFO [stats.py:111] snake-04_SnakeA: Step: 630000. Time Elapsed: 473.317 s Mean Reward: -0.357. Std of Reward: 1.288. Training.
2020-04-11 17:35:24 INFO [stats.py:116] snake-04_SnakeA ELO: 1197.231.
2020-04-11 17:38:14 INFO [stats.py:111] snake-04_SnakeA: Step: 640000. Time Elapsed: 643.950 s Mean Reward: -0.357. Std of Reward: 1.445. Training.
2020-04-11 17:38:14 INFO [stats.py:116] snake-04_SnakeA ELO: 1196.027.
2020-04-11 17:41:02 INFO [stats.py:111] snake-04_SnakeA: Step: 650000. Time Elapsed: 811.586 s Mean Reward: -0.500. Std of Reward: 1.258. Training.
2020-04-11 17:41:02 INFO [stats.py:116] snake-04_SnakeA ELO: 1194.594.
2020-04-11 17:41:48 INFO [trainer_controller.py:100] Saved Model
2020-04-11 17:44:02 INFO [stats.py:111] snake-04_SnakeA: Step: 660000. Time Elapsed: 991.859 s Mean Reward: 0.375. Std of Reward: 1.728. Training.
2020-04-11 17:44:02 INFO [stats.py:116] snake-04_SnakeA ELO: 1194.168.
2020-04-11 17:46:49 INFO [stats.py:111] snake-04_SnakeA: Step: 670000. Time Elapsed: 1158.775 s Mean Reward: 0.636. Std of Reward: 1.367. Training.
2020-04-11 17:46:49 INFO [stats.py:116] snake-04_SnakeA ELO: 1194.561.
2020-04-11 17:49:41 INFO [stats.py:111] snake-04_SnakeA: Step: 680000. Time Elapsed: 1330.236 s Mean Reward: 0.091. Std of Reward: 1.621. Training.
2020-04-11 17:49:41 INFO [stats.py:116] snake-04_SnakeA ELO: 1195.269.
2020-04-11 17:52:45 INFO [stats.py:111] snake-04_SnakeA: Step: 690000. Time Elapsed: 1514.293 s Mean Reward: -0.333. Std of Reward: 1.491. Training.
2020-04-11 17:52:45 INFO [stats.py:116] snake-04_SnakeA ELO: 1194.551.
2020-04-11 17:55:41 INFO [stats.py:111] snake-04_SnakeA: Step: 700000. Time Elapsed: 1690.137 s Mean Reward: -0.357. Std of Reward: 1.674. Training.
2020-04-11 17:55:41 INFO [stats.py:116] snake-04_SnakeA ELO: 1192.097.
2020-04-11 17:56:13 INFO [trainer_controller.py:100] Saved Model
2020-04-11 17:57:21 INFO [stats.py:111] snake-04_SnakeB: Step: 520000. Time Elapsed: 1790.171 s Mean Reward: -1.000. Std of Reward: 0.816. Training.
2020-04-11 17:57:21 INFO [stats.py:116] snake-04_SnakeB ELO: 1199.810.
2020-04-11 18:00:17 INFO [stats.py:111] snake-04_SnakeB: Step: 530000. Time Elapsed: 1966.579 s Mean Reward: -0.750. Std of Reward: 1.250. Training.
2020-04-11 18:00:17 INFO [stats.py:116] snake-04_SnakeB ELO: 1197.653.
2020-04-11 18:03:18 INFO [stats.py:111] snake-04_SnakeB: Step: 540000. Time Elapsed: 2147.085 s Mean Reward: 0.444. Std of Reward: 1.165. Training.
2020-04-11 18:03:18 INFO [stats.py:116] snake-04_SnakeB ELO: 1195.218.
2020-04-11 18:06:07 INFO [stats.py:111] snake-04_SnakeB: Step: 550000. Time Elapsed: 2316.644 s Mean Reward: -0.286. Std of Reward: 1.436. Training.
2020-04-11 18:06:07 INFO [stats.py:116] snake-04_SnakeB ELO: 1194.191.
2020-04-11 18:08:50 INFO [stats.py:111] snake-04_SnakeB: Step: 560000. Time Elapsed: 2479.585 s Mean Reward: -0.091. Std of Reward: 1.621. Training.
2020-04-11 18:08:50 INFO [stats.py:116] snake-04_SnakeB ELO: 1192.709.
2020-04-11 18:10:36 INFO [trainer_controller.py:100] Saved Model
2020-04-11 18:11:52 INFO [stats.py:111] snake-04_SnakeB: Step: 570000. Time Elapsed: 2661.293 s Mean Reward: -0.231. Std of Reward: 1.527. Training.
2020-04-11 18:11:52 INFO [stats.py:116] snake-04_SnakeB ELO: 1189.171.
2020-04-11 18:14:32 INFO [stats.py:111] snake-04_SnakeB: Step: 580000. Time Elapsed: 2821.262 s Mean Reward: -0.143. Std of Reward: 1.245. Training.
2020-04-11 18:14:32 INFO [stats.py:116] snake-04_SnakeB ELO: 1186.538.
2020-04-11 18:17:25 INFO [stats.py:111] snake-04_SnakeB: Step: 590000. Time Elapsed: 2994.530 s Mean Reward: -0.900. Std of Reward: 1.300. Training.
2020-04-11 18:17:25 INFO [stats.py:116] snake-04_SnakeB ELO: 1185.070.
2020-04-11 18:20:24 INFO [stats.py:111] snake-04_SnakeB: Step: 600000. Time Elapsed: 3173.927 s Mean Reward: 0.400. Std of Reward: 1.665. Training.
2020-04-11 18:20:24 INFO [stats.py:116] snake-04_SnakeB ELO: 1183.891.
2020-04-11 18:23:13 INFO [stats.py:111] snake-04_SnakeB: Step: 610000. Time Elapsed: 3342.301 s Mean Reward: -0.375. Std of Reward: 1.218. Training.
2020-04-11 18:23:13 INFO [stats.py:116] snake-04_SnakeB ELO: 1182.343.
2020-04-11 18:25:02 INFO [trainer_controller.py:100] Saved Model
2020-04-11 18:27:37 INFO [stats.py:111] snake-04_SnakeA: Step: 710000. Time Elapsed: 3606.930 s Mean Reward: -0.333. Std of Reward: 1.535. Training.
2020-04-11 18:27:37 INFO [stats.py:116] snake-04_SnakeA ELO: 1207.336.
2020-04-11 18:30:22 INFO [stats.py:111] snake-04_SnakeA: Step: 720000. Time Elapsed: 3771.369 s Mean Reward: -0.571. Std of Reward: 1.498. Training.
2020-04-11 18:30:22 INFO [stats.py:116] snake-04_SnakeA ELO: 1210.250.
2020-04-11 18:33:10 INFO [stats.py:111] snake-04_SnakeA: Step: 730000. Time Elapsed: 3939.134 s Mean Reward: -0.231. Std of Reward: 1.804. Training.
2020-04-11 18:33:10 INFO [stats.py:116] snake-04_SnakeA ELO: 1206.974.
2020-04-11 18:36:03 INFO [stats.py:111] snake-04_SnakeA: Step: 740000. Time Elapsed: 4112.371 s Mean Reward: -0.125. Std of Reward: 1.364. Training.
2020-04-11 18:36:03 INFO [stats.py:116] snake-04_SnakeA ELO: 1204.815.
2020-04-11 18:38:51 INFO [stats.py:111] snake-04_SnakeA: Step: 750000. Time Elapsed: 4280.429 s Mean Reward: 0.500. Std of Reward: 1.190. Training.
2020-04-11 18:38:51 INFO [stats.py:116] snake-04_SnakeA ELO: 1202.430.
2020-04-11 18:39:22 INFO [trainer_controller.py:100] Saved Model
2020-04-11 18:41:45 INFO [stats.py:111] snake-04_SnakeA: Step: 760000. Time Elapsed: 4454.462 s Mean Reward: -0.357. Std of Reward: 1.540. Training.
2020-04-11 18:41:45 INFO [stats.py:116] snake-04_SnakeA ELO: 1199.935.
2020-04-11 18:44:49 INFO [stats.py:111] snake-04_SnakeA: Step: 770000. Time Elapsed: 4638.846 s Mean Reward: -0.562. Std of Reward: 1.767. Training.
2020-04-11 18:44:49 INFO [stats.py:116] snake-04_SnakeA ELO: 1197.384.
2020-04-11 18:47:31 INFO [stats.py:111] snake-04_SnakeA: Step: 780000. Time Elapsed: 4800.156 s Mean Reward: 0.000. Std of Reward: 1.414. Training.
2020-04-11 18:47:31 INFO [stats.py:116] snake-04_SnakeA ELO: 1195.512.
2020-04-11 18:50:31 INFO [stats.py:111] snake-04_SnakeA: Step: 790000. Time Elapsed: 4980.861 s Mean Reward: -0.067. Std of Reward: 1.806. Training.
2020-04-11 18:50:31 INFO [stats.py:116] snake-04_SnakeA ELO: 1193.248.
2020-04-11 18:53:23 INFO [stats.py:111] snake-04_SnakeA: Step: 800000. Time Elapsed: 5152.071 s Mean Reward: -0.400. Std of Reward: 1.497. Training.
2020-04-11 18:53:23 INFO [stats.py:116] snake-04_SnakeA ELO: 1191.557.
2020-04-11 18:53:43 INFO [trainer_controller.py:100] Saved Model
2020-04-11 18:55:02 INFO [stats.py:111] snake-04_SnakeB: Step: 620000. Time Elapsed: 5251.974 s Mean Reward: -1.000. Std of Reward: 1.080. Training.
2020-04-11 18:55:02 INFO [stats.py:116] snake-04_SnakeB ELO: 1184.269.
2020-04-11 18:57:42 INFO [stats.py:111] snake-04_SnakeB: Step: 630000. Time Elapsed: 5411.535 s Mean Reward: -0.091. Std of Reward: 1.881. Training.
2020-04-11 18:57:42 INFO [stats.py:116] snake-04_SnakeB ELO: 1187.915.
2020-04-11 19:00:33 INFO [stats.py:111] snake-04_SnakeB: Step: 640000. Time Elapsed: 5582.488 s Mean Reward: -1.000. Std of Reward: 1.617. Training.
2020-04-11 19:00:33 INFO [stats.py:116] snake-04_SnakeB ELO: 1187.008.
2020-04-11 19:03:29 INFO [stats.py:111] snake-04_SnakeB: Step: 650000. Time Elapsed: 5758.259 s Mean Reward: -0.438. Std of Reward: 1.619. Training.
2020-04-11 19:03:29 INFO [stats.py:116] snake-04_SnakeB ELO: 1185.719.
2020-04-11 19:06:18 INFO [stats.py:111] snake-04_SnakeB: Step: 660000. Time Elapsed: 5927.364 s Mean Reward: 0.778. Std of Reward: 1.618. Training.
2020-04-11 19:06:18 INFO [stats.py:116] snake-04_SnakeB ELO: 1185.457.
2020-04-11 19:08:03 INFO [trainer_controller.py:100] Saved Model
2020-04-11 19:09:17 INFO [stats.py:111] snake-04_SnakeB: Step: 670000. Time Elapsed: 6106.813 s Mean Reward: -0.091. Std of Reward: 1.164. Training.
2020-04-11 19:09:17 INFO [stats.py:116] snake-04_SnakeB ELO: 1183.079.
2020-04-11 19:12:02 INFO [stats.py:111] snake-04_SnakeB: Step: 680000. Time Elapsed: 6271.915 s Mean Reward: -0.615. Std of Reward: 1.496. Training.
2020-04-11 19:12:02 INFO [stats.py:116] snake-04_SnakeB ELO: 1181.817.
2020-04-11 19:15:00 INFO [stats.py:111] snake-04_SnakeB: Step: 690000. Time Elapsed: 6449.091 s Mean Reward: -0.438. Std of Reward: 1.368. Training.
2020-04-11 19:15:00 INFO [stats.py:116] snake-04_SnakeB ELO: 1180.721.
2020-04-11 19:17:52 INFO [stats.py:111] snake-04_SnakeB: Step: 700000. Time Elapsed: 6621.489 s Mean Reward: -0.214. Std of Reward: 1.473. Training.
2020-04-11 19:17:52 INFO [stats.py:116] snake-04_SnakeB ELO: 1181.449.
2020-04-11 19:20:53 INFO [stats.py:111] snake-04_SnakeB: Step: 710000. Time Elapsed: 6802.054 s Mean Reward: 0.077. Std of Reward: 1.141. Training.
2020-04-11 19:20:53 INFO [stats.py:116] snake-04_SnakeB ELO: 1179.990.
2020-04-11 19:22:20 INFO [trainer_controller.py:100] Saved Model
2020-04-11 19:24:58 INFO [stats.py:111] snake-04_SnakeA: Step: 810000. Time Elapsed: 7047.450 s Mean Reward: 0.000. Std of Reward: 1.414. Training.
2020-04-11 19:24:58 INFO [stats.py:116] snake-04_SnakeA ELO: 1194.419.
2020-04-11 19:27:55 INFO [stats.py:111] snake-04_SnakeA: Step: 820000. Time Elapsed: 7224.915 s Mean Reward: -0.667. Std of Reward: 1.546. Training.
2020-04-11 19:27:55 INFO [stats.py:116] snake-04_SnakeA ELO: 1193.342.
2020-04-11 19:30:51 INFO [stats.py:111] snake-04_SnakeA: Step: 830000. Time Elapsed: 7400.112 s Mean Reward: -1.000. Std of Reward: 1.414. Training.
2020-04-11 19:30:51 INFO [stats.py:116] snake-04_SnakeA ELO: 1191.796.
2020-04-11 19:33:41 INFO [stats.py:111] snake-04_SnakeA: Step: 840000. Time Elapsed: 7570.229 s Mean Reward: 0.067. Std of Reward: 1.482. Training.
2020-04-11 19:33:41 INFO [stats.py:116] snake-04_SnakeA ELO: 1187.466.
2020-04-11 19:36:26 INFO [stats.py:111] snake-04_SnakeA: Step: 850000. Time Elapsed: 7735.707 s Mean Reward: -0.538. Std of Reward: 1.500. Training.
2020-04-11 19:36:26 INFO [stats.py:116] snake-04_SnakeA ELO: 1185.915.
2020-04-11 19:36:46 INFO [trainer_controller.py:100] Saved Model
2020-04-11 19:39:16 INFO [stats.py:111] snake-04_SnakeA: Step: 860000. Time Elapsed: 7905.441 s Mean Reward: 0.182. Std of Reward: 1.402. Training.
2020-04-11 19:39:16 INFO [stats.py:116] snake-04_SnakeA ELO: 1183.606.
2020-04-11 19:42:21 INFO [stats.py:111] snake-04_SnakeA: Step: 870000. Time Elapsed: 8090.905 s Mean Reward: 0.000. Std of Reward: 1.749. Training.
2020-04-11 19:42:21 INFO [stats.py:116] snake-04_SnakeA ELO: 1184.406.
2020-04-11 19:45:11 INFO [stats.py:111] snake-04_SnakeA: Step: 880000. Time Elapsed: 8260.113 s Mean Reward: -0.077. Std of Reward: 1.639. Training.
2020-04-11 19:45:11 INFO [stats.py:116] snake-04_SnakeA ELO: 1183.248.
2020-04-11 19:47:53 INFO [stats.py:111] snake-04_SnakeA: Step: 890000. Time Elapsed: 8422.684 s Mean Reward: -0.375. Std of Reward: 1.495. Training.
2020-04-11 19:47:53 INFO [stats.py:116] snake-04_SnakeA ELO: 1181.091.
2020-04-11 19:50:46 INFO [stats.py:111] snake-04_SnakeA: Step: 900000. Time Elapsed: 8595.280 s Mean Reward: 0.250. Std of Reward: 1.714. Training.
2020-04-11 19:50:46 INFO [stats.py:116] snake-04_SnakeA ELO: 1180.205.
2020-04-11 19:51:03 INFO [trainer_controller.py:100] Saved Model
2020-04-11 19:52:25 INFO [stats.py:111] snake-04_SnakeB: Step: 720000. Time Elapsed: 8694.513 s Mean Reward: -0.333. Std of Reward: 1.491. Training.
2020-04-11 19:52:25 INFO [stats.py:116] snake-04_SnakeB ELO: 1184.269.
2020-04-11 19:55:23 INFO [stats.py:111] snake-04_SnakeB: Step: 730000. Time Elapsed: 8872.134 s Mean Reward: 0.182. Std of Reward: 2.167. Training.
2020-04-11 19:55:23 INFO [stats.py:116] snake-04_SnakeB ELO: 1189.932.
2020-04-11 19:58:13 INFO [stats.py:111] snake-04_SnakeB: Step: 740000. Time Elapsed: 9042.987 s Mean Reward: -1.000. Std of Reward: 1.354. Training.
2020-04-11 19:58:13 INFO [stats.py:116] snake-04_SnakeB ELO: 1189.568.
2020-04-11 20:00:57 INFO [stats.py:111] snake-04_SnakeB: Step: 750000. Time Elapsed: 9206.850 s Mean Reward: 0.000. Std of Reward: 1.569. Training.
2020-04-11 20:00:57 INFO [stats.py:116] snake-04_SnakeB ELO: 1187.115.
2020-04-11 20:03:54 INFO [stats.py:111] snake-04_SnakeB: Step: 760000. Time Elapsed: 9383.820 s Mean Reward: -0.462. Std of Reward: 1.393. Training.
2020-04-11 20:03:54 INFO [stats.py:116] snake-04_SnakeB ELO: 1187.250.
2020-04-11 20:05:18 INFO [trainer_controller.py:100] Saved Model
2020-04-11 20:06:52 INFO [stats.py:111] snake-04_SnakeB: Step: 770000. Time Elapsed: 9561.236 s Mean Reward: -0.357. Std of Reward: 1.540. Training.
2020-04-11 20:06:52 INFO [stats.py:116] snake-04_SnakeB ELO: 1186.516.
2020-04-11 20:09:43 INFO [stats.py:111] snake-04_SnakeB: Step: 780000. Time Elapsed: 9732.210 s Mean Reward: -0.750. Std of Reward: 1.199. Training.
2020-04-11 20:09:43 INFO [stats.py:116] snake-04_SnakeB ELO: 1186.631.
2020-04-11 20:12:57 INFO [stats.py:111] snake-04_SnakeB: Step: 790000. Time Elapsed: 9926.597 s Mean Reward: -0.231. Std of Reward: 1.423. Training.
2020-04-11 20:12:57 INFO [stats.py:116] snake-04_SnakeB ELO: 1185.840.
2020-04-11 20:16:25 INFO [stats.py:111] snake-04_SnakeB: Step: 800000. Time Elapsed: 10134.928 s Mean Reward: 0.188. Std of Reward: 1.590. Training.
2020-04-11 20:16:25 INFO [stats.py:116] snake-04_SnakeB ELO: 1184.083.
2020-04-11 20:21:11 INFO [stats.py:111] snake-04_SnakeB: Step: 810000. Time Elapsed: 10420.211 s Mean Reward: -1.077. Std of Reward: 1.269. Training.
2020-04-11 20:21:11 INFO [stats.py:116] snake-04_SnakeB ELO: 1181.978.
2020-04-11 20:23:29 INFO [trainer_controller.py:100] Saved Model
2020-04-11 20:28:48 INFO [stats.py:111] snake-04_SnakeA: Step: 910000. Time Elapsed: 10877.291 s Mean Reward: -0.100. Std of Reward: 1.700. Training.
2020-04-11 20:28:48 INFO [stats.py:116] snake-04_SnakeA ELO: 1180.927.
2020-04-11 20:33:42 INFO [stats.py:111] snake-04_SnakeA: Step: 920000. Time Elapsed: 11171.210 s Mean Reward: 0.545. Std of Reward: 1.924. Training.
2020-04-11 20:33:42 INFO [stats.py:116] snake-04_SnakeA ELO: 1182.729.
2020-04-11 20:38:16 INFO [stats.py:111] snake-04_SnakeA: Step: 930000. Time Elapsed: 11445.918 s Mean Reward: -0.273. Std of Reward: 1.656. Training.
2020-04-11 20:38:16 INFO [stats.py:116] snake-04_SnakeA ELO: 1181.630.
2020-04-11 20:43:19 INFO [stats.py:111] snake-04_SnakeA: Step: 940000. Time Elapsed: 11748.095 s Mean Reward: -0.818. Std of Reward: 1.527. Training.
2020-04-11 20:43:19 INFO [stats.py:116] snake-04_SnakeA ELO: 1180.658.
2020-04-11 20:48:07 INFO [stats.py:111] snake-04_SnakeA: Step: 950000. Time Elapsed: 12036.358 s Mean Reward: 0.062. Std of Reward: 1.560. Training.
2020-04-11 20:48:07 INFO [stats.py:116] snake-04_SnakeA ELO: 1179.148.
2020-04-11 20:48:11 INFO [trainer_controller.py:100] Saved Model
2020-04-11 20:53:02 INFO [stats.py:111] snake-04_SnakeA: Step: 960000. Time Elapsed: 12331.939 s Mean Reward: -0.556. Std of Reward: 1.257. Training.
2020-04-11 20:53:02 INFO [stats.py:116] snake-04_SnakeA ELO: 1178.580.
2020-04-11 20:58:08 INFO [stats.py:111] snake-04_SnakeA: Step: 970000. Time Elapsed: 12637.124 s Mean Reward: 0.000. Std of Reward: 1.472. Training.
2020-04-11 20:58:08 INFO [stats.py:116] snake-04_SnakeA ELO: 1175.634.
2020-04-11 21:02:21 INFO [stats.py:111] snake-04_SnakeA: Step: 980000. Time Elapsed: 12890.494 s Mean Reward: 0.000. Std of Reward: 1.477. Training.
2020-04-11 21:02:21 INFO [stats.py:116] snake-04_SnakeA ELO: 1172.973.
2020-04-11 21:05:36 INFO [stats.py:111] snake-04_SnakeA: Step: 990000. Time Elapsed: 13085.959 s Mean Reward: -0.133. Std of Reward: 1.628. Training.
2020-04-11 21:05:36 INFO [stats.py:116] snake-04_SnakeA ELO: 1170.151.
2020-04-11 21:08:50 INFO [trainer_controller.py:100] Saved Model
2020-04-11 21:08:56 INFO [stats.py:111] snake-04_SnakeA: Step: 1000000. Time Elapsed: 13285.374 s Mean Reward: 0.643. Std of Reward: 1.493. Training.
2020-04-11 21:08:56 INFO [stats.py:116] snake-04_SnakeA ELO: 1170.362.
2020-04-11 21:10:32 INFO [stats.py:111] snake-04_SnakeB: Step: 820000. Time Elapsed: 13381.480 s Mean Reward: -0.556. Std of Reward: 1.165. Training.
2020-04-11 21:10:32 INFO [stats.py:116] snake-04_SnakeB ELO: 1184.831.
2020-04-11 21:13:43 INFO [stats.py:111] snake-04_SnakeB: Step: 830000. Time Elapsed: 13572.639 s Mean Reward: -0.500. Std of Reward: 1.500. Training.
2020-04-11 21:13:43 INFO [stats.py:116] snake-04_SnakeB ELO: 1190.957.
2020-04-11 21:17:06 INFO [stats.py:111] snake-04_SnakeB: Step: 840000. Time Elapsed: 13775.078 s Mean Reward: -1.000. Std of Reward: 1.265. Training.
2020-04-11 21:17:06 INFO [stats.py:116] snake-04_SnakeB ELO: 1187.480.
2020-04-11 21:20:18 INFO [stats.py:111] snake-04_SnakeB: Step: 850000. Time Elapsed: 13968.033 s Mean Reward: -1.000. Std of Reward: 1.247. Training.
2020-04-11 21:20:18 INFO [stats.py:116] snake-04_SnakeB ELO: 1185.761.
2020-04-11 21:23:22 INFO [stats.py:111] snake-04_SnakeB: Step: 860000. Time Elapsed: 14152.014 s Mean Reward: -0.615. Std of Reward: 1.496. Training.
2020-04-11 21:23:22 INFO [stats.py:116] snake-04_SnakeB ELO: 1182.368.
2020-04-11 21:24:49 INFO [trainer_controller.py:100] Saved Model
2020-04-11 21:26:37 INFO [stats.py:111] snake-04_SnakeB: Step: 870000. Time Elapsed: 14346.438 s Mean Reward: -0.250. Std of Reward: 1.090. Training.
2020-04-11 21:26:37 INFO [stats.py:116] snake-04_SnakeB ELO: 1183.259.
2020-04-11 21:29:48 INFO [stats.py:111] snake-04_SnakeB: Step: 880000. Time Elapsed: 14537.165 s Mean Reward: -0.214. Std of Reward: 1.655. Training.
2020-04-11 21:29:48 INFO [stats.py:116] snake-04_SnakeB ELO: 1181.874.
2020-04-11 21:32:58 INFO [stats.py:111] snake-04_SnakeB: Step: 890000. Time Elapsed: 14727.921 s Mean Reward: 0.143. Std of Reward: 1.457. Training.
2020-04-11 21:32:58 INFO [stats.py:116] snake-04_SnakeB ELO: 1180.243.
2020-04-11 21:36:21 INFO [stats.py:111] snake-04_SnakeB: Step: 900000. Time Elapsed: 14930.543 s Mean Reward: -1.083. Std of Reward: 1.187. Training.
2020-04-11 21:36:21 INFO [stats.py:116] snake-04_SnakeB ELO: 1178.691.
2020-04-11 21:39:24 INFO [stats.py:111] snake-04_SnakeB: Step: 910000. Time Elapsed: 15113.566 s Mean Reward: -0.333. Std of Reward: 1.333. Training.
2020-04-11 21:39:24 INFO [stats.py:116] snake-04_SnakeB ELO: 1176.351.
2020-04-11 21:40:53 INFO [trainer_controller.py:100] Saved Model
2020-04-11 21:44:06 INFO [stats.py:111] snake-04_SnakeA: Step: 1010000. Time Elapsed: 15395.179 s Mean Reward: -0.500. Std of Reward: 1.190. Training.
2020-04-11 21:44:06 INFO [stats.py:116] snake-04_SnakeA ELO: 1166.808.
2020-04-11 21:47:20 INFO [stats.py:111] snake-04_SnakeA: Step: 1020000. Time Elapsed: 15589.591 s Mean Reward: -0.750. Std of Reward: 1.534. Training.
2020-04-11 21:47:20 INFO [stats.py:116] snake-04_SnakeA ELO: 1164.072.
2020-04-11 21:50:46 INFO [stats.py:111] snake-04_SnakeA: Step: 1030000. Time Elapsed: 15795.345 s Mean Reward: -0.538. Std of Reward: 1.500. Training.
2020-04-11 21:50:46 INFO [stats.py:116] snake-04_SnakeA ELO: 1160.634.
2020-04-11 21:53:39 INFO [stats.py:111] snake-04_SnakeA: Step: 1040000. Time Elapsed: 15969.008 s Mean Reward: -0.333. Std of Reward: 1.633. Training.
2020-04-11 21:53:39 INFO [stats.py:116] snake-04_SnakeA ELO: 1159.198.
2020-04-11 21:56:54 INFO [trainer_controller.py:100] Saved Model
2020-04-11 21:57:07 INFO [stats.py:111] snake-04_SnakeA: Step: 1050000. Time Elapsed: 16176.361 s Mean Reward: -0.231. Std of Reward: 1.576. Training.
2020-04-11 21:57:07 INFO [stats.py:116] snake-04_SnakeA ELO: 1159.886.
2020-04-11 22:00:22 INFO [stats.py:111] snake-04_SnakeA: Step: 1060000. Time Elapsed: 16371.785 s Mean Reward: 0.417. Std of Reward: 1.847. Training.
2020-04-11 22:00:22 INFO [stats.py:116] snake-04_SnakeA ELO: 1158.896.
2020-04-11 22:03:32 INFO [stats.py:111] snake-04_SnakeA: Step: 1070000. Time Elapsed: 16561.604 s Mean Reward: 0.083. Std of Reward: 1.754. Training.
2020-04-11 22:03:32 INFO [stats.py:116] snake-04_SnakeA ELO: 1158.272.
2020-04-11 22:06:32 INFO [stats.py:111] snake-04_SnakeA: Step: 1080000. Time Elapsed: 16741.352 s Mean Reward: -0.647. Std of Reward: 1.747. Training.
2020-04-11 22:06:32 INFO [stats.py:116] snake-04_SnakeA ELO: 1157.351.
2020-04-11 22:10:01 INFO [stats.py:111] snake-04_SnakeA: Step: 1090000. Time Elapsed: 16950.146 s Mean Reward: -0.688. Std of Reward: 1.310. Training.
2020-04-11 22:10:01 INFO [stats.py:116] snake-04_SnakeA ELO: 1155.528.
2020-04-11 22:12:58 INFO [trainer_controller.py:100] Saved Model
2020-04-11 22:13:02 INFO [stats.py:111] snake-04_SnakeA: Step: 1100000. Time Elapsed: 17131.114 s Mean Reward: 0.300. Std of Reward: 1.100. Training.
2020-04-11 22:13:02 INFO [stats.py:116] snake-04_SnakeA ELO: 1154.695.
2020-04-11 22:14:56 INFO [stats.py:111] snake-04_SnakeB: Step: 920000. Time Elapsed: 17246.024 s Mean Reward: -0.333. Std of Reward: 1.563. Training.
2020-04-11 22:14:56 INFO [stats.py:116] snake-04_SnakeB ELO: 1175.569.
2020-04-11 22:18:04 INFO [stats.py:111] snake-04_SnakeB: Step: 930000. Time Elapsed: 17433.267 s Mean Reward: 0.375. Std of Reward: 1.409. Training.
2020-04-11 22:18:04 INFO [stats.py:116] snake-04_SnakeB ELO: 1187.395.
2020-04-11 22:21:11 INFO [stats.py:111] snake-04_SnakeB: Step: 940000. Time Elapsed: 17620.314 s Mean Reward: -0.467. Std of Reward: 1.543. Training.
2020-04-11 22:21:11 INFO [stats.py:116] snake-04_SnakeB ELO: 1186.636.
2020-04-11 22:24:36 INFO [stats.py:111] snake-04_SnakeB: Step: 950000. Time Elapsed: 17825.110 s Mean Reward: 0.077. Std of Reward: 1.859. Training.
2020-04-11 22:24:36 INFO [stats.py:116] snake-04_SnakeB ELO: 1184.472.
2020-04-11 22:27:50 INFO [stats.py:111] snake-04_SnakeB: Step: 960000. Time Elapsed: 18019.956 s Mean Reward: -0.583. Std of Reward: 1.441. Training.
2020-04-11 22:27:50 INFO [stats.py:116] snake-04_SnakeB ELO: 1182.798.
2020-04-11 22:28:58 INFO [trainer_controller.py:100] Saved Model
2020-04-11 22:31:06 INFO [stats.py:111] snake-04_SnakeB: Step: 970000. Time Elapsed: 18215.720 s Mean Reward: -1.000. Std of Reward: 1.713. Training.
2020-04-11 22:31:06 INFO [stats.py:116] snake-04_SnakeB ELO: 1180.118.
2020-04-11 22:34:21 INFO [stats.py:111] snake-04_SnakeB: Step: 980000. Time Elapsed: 18410.055 s Mean Reward: 0.083. Std of Reward: 1.382. Training.
2020-04-11 22:34:21 INFO [stats.py:116] snake-04_SnakeB ELO: 1177.228.
2020-04-11 22:37:27 INFO [stats.py:111] snake-04_SnakeB: Step: 990000. Time Elapsed: 18596.943 s Mean Reward: -0.556. Std of Reward: 1.771. Training.
2020-04-11 22:37:27 INFO [stats.py:116] snake-04_SnakeB ELO: 1176.074.
2020-04-11 22:40:41 INFO [stats.py:111] snake-04_SnakeB: Step: 1000000. Time Elapsed: 18790.494 s Mean Reward: -1.111. Std of Reward: 0.994. Training.
2020-04-11 22:40:41 INFO [stats.py:116] snake-04_SnakeB ELO: 1174.535.
2020-04-11 22:44:00 INFO [stats.py:111] snake-04_SnakeB: Step: 1010000. Time Elapsed: 18989.156 s Mean Reward: 0.077. Std of Reward: 1.439. Training.
2020-04-11 22:44:00 INFO [stats.py:116] snake-04_SnakeB ELO: 1172.697.
2020-04-11 22:45:00 INFO [trainer_controller.py:100] Saved Model
2020-04-11 22:48:43 INFO [stats.py:111] snake-04_SnakeA: Step: 1110000. Time Elapsed: 19272.428 s Mean Reward: -0.429. Std of Reward: 1.450. Training.
2020-04-11 22:48:43 INFO [stats.py:116] snake-04_SnakeA ELO: 1154.498.
2020-04-11 22:52:05 INFO [stats.py:111] snake-04_SnakeA: Step: 1120000. Time Elapsed: 19474.882 s Mean Reward: -0.154. Std of Reward: 1.610. Training.
2020-04-11 22:52:05 INFO [stats.py:116] snake-04_SnakeA ELO: 1154.134.
2020-04-11 22:55:11 INFO [stats.py:111] snake-04_SnakeA: Step: 1130000. Time Elapsed: 19660.831 s Mean Reward: -0.091. Std of Reward: 1.311. Training.
2020-04-11 22:55:11 INFO [stats.py:116] snake-04_SnakeA ELO: 1154.989.
2020-04-11 22:58:17 INFO [stats.py:111] snake-04_SnakeA: Step: 1140000. Time Elapsed: 19846.078 s Mean Reward: -0.667. Std of Reward: 1.054. Training.
2020-04-11 22:58:17 INFO [stats.py:116] snake-04_SnakeA ELO: 1153.679.
2020-04-11 23:01:04 INFO [trainer_controller.py:100] Saved Model
2020-04-11 23:01:30 INFO [stats.py:111] snake-04_SnakeA: Step: 1150000. Time Elapsed: 20039.368 s Mean Reward: -0.636. Std of Reward: 1.367. Training.
2020-04-11 23:01:30 INFO [stats.py:116] snake-04_SnakeA ELO: 1151.967.
2020-04-11 23:04:57 INFO [stats.py:111] snake-04_SnakeA: Step: 1160000. Time Elapsed: 20246.637 s Mean Reward: -0.091. Std of Reward: 1.379. Training.
2020-04-11 23:04:57 INFO [stats.py:116] snake-04_SnakeA ELO: 1150.728.
2020-04-11 23:07:56 INFO [stats.py:111] snake-04_SnakeA: Step: 1170000. Time Elapsed: 20425.704 s Mean Reward: -0.462. Std of Reward: 1.447. Training.
2020-04-11 23:07:56 INFO [stats.py:116] snake-04_SnakeA ELO: 1151.245.
2020-04-11 23:11:07 INFO [stats.py:111] snake-04_SnakeA: Step: 1180000. Time Elapsed: 20616.655 s Mean Reward: 0.083. Std of Reward: 1.320. Training.
2020-04-11 23:11:07 INFO [stats.py:116] snake-04_SnakeA ELO: 1150.758.
2020-04-11 23:14:28 INFO [stats.py:111] snake-04_SnakeA: Step: 1190000. Time Elapsed: 20817.958 s Mean Reward: -0.154. Std of Reward: 1.460. Training.
2020-04-11 23:14:28 INFO [stats.py:116] snake-04_SnakeA ELO: 1150.670.
2020-04-11 23:17:04 INFO [trainer_controller.py:100] Saved Model
2020-04-11 23:17:43 INFO [stats.py:111] snake-04_SnakeA: Step: 1200000. Time Elapsed: 21012.677 s Mean Reward: 0.000. Std of Reward: 1.651. Training.
2020-04-11 23:17:43 INFO [stats.py:116] snake-04_SnakeA ELO: 1149.904.
2020-04-11 23:19:11 INFO [stats.py:111] snake-04_SnakeB: Step: 1020000. Time Elapsed: 21100.697 s Mean Reward: -0.200. Std of Reward: 1.470. Training.
2020-04-11 23:19:11 INFO [stats.py:116] snake-04_SnakeB ELO: 1171.715.
2020-04-11 23:22:22 INFO [stats.py:111] snake-04_SnakeB: Step: 1030000. Time Elapsed: 21291.565 s Mean Reward: -1.000. Std of Reward: 0.471. Training.
2020-04-11 23:22:22 INFO [stats.py:116] snake-04_SnakeB ELO: 1170.145.
2020-04-11 23:25:38 INFO [stats.py:111] snake-04_SnakeB: Step: 1040000. Time Elapsed: 21487.653 s Mean Reward: -0.062. Std of Reward: 1.784. Training.
2020-04-11 23:25:38 INFO [stats.py:116] snake-04_SnakeB ELO: 1167.505.
2020-04-11 23:28:48 INFO [stats.py:111] snake-04_SnakeB: Step: 1050000. Time Elapsed: 21677.219 s Mean Reward: -0.556. Std of Reward: 1.499. Training.
2020-04-11 23:28:48 INFO [stats.py:116] snake-04_SnakeB ELO: 1166.734.
2020-04-11 23:32:02 INFO [stats.py:111] snake-04_SnakeB: Step: 1060000. Time Elapsed: 21871.503 s Mean Reward: -0.273. Std of Reward: 1.135. Training.
2020-04-11 23:32:02 INFO [stats.py:116] snake-04_SnakeB ELO: 1163.598.
2020-04-11 23:33:06 INFO [trainer_controller.py:100] Saved Model
2020-04-11 23:35:09 INFO [stats.py:111] snake-04_SnakeB: Step: 1070000. Time Elapsed: 22058.822 s Mean Reward: -0.667. Std of Reward: 1.700. Training.
2020-04-11 23:35:09 INFO [stats.py:116] snake-04_SnakeB ELO: 1160.459.
2020-04-11 23:38:35 INFO [stats.py:111] snake-04_SnakeB: Step: 1080000. Time Elapsed: 22264.543 s Mean Reward: -0.235. Std of Reward: 1.628. Training.
2020-04-11 23:38:35 INFO [stats.py:116] snake-04_SnakeB ELO: 1158.593.
2020-04-11 23:41:42 INFO [stats.py:111] snake-04_SnakeB: Step: 1090000. Time Elapsed: 22451.168 s Mean Reward: -0.167. Std of Reward: 1.344. Training.
2020-04-11 23:41:42 INFO [stats.py:116] snake-04_SnakeB ELO: 1155.800.
2020-04-11 23:44:56 INFO [stats.py:111] snake-04_SnakeB: Step: 1100000. Time Elapsed: 22645.917 s Mean Reward: 0.583. Std of Reward: 1.706. Training.
2020-04-11 23:44:56 INFO [stats.py:116] snake-04_SnakeB ELO: 1154.976.
2020-04-11 23:48:03 INFO [stats.py:111] snake-04_SnakeB: Step: 1110000. Time Elapsed: 22832.392 s Mean Reward: -0.200. Std of Reward: 1.641. Training.
2020-04-11 23:48:03 INFO [stats.py:116] snake-04_SnakeB ELO: 1157.000.
2020-04-11 23:49:07 INFO [trainer_controller.py:100] Saved Model
2020-04-11 23:53:01 INFO [stats.py:111] snake-04_SnakeA: Step: 1210000. Time Elapsed: 23130.514 s Mean Reward: 0.125. Std of Reward: 1.364. Training.
2020-04-11 23:53:01 INFO [stats.py:116] snake-04_SnakeA ELO: 1165.965.
2020-04-11 23:56:02 INFO [stats.py:111] snake-04_SnakeA: Step: 1220000. Time Elapsed: 23311.548 s Mean Reward: -0.231. Std of Reward: 1.761. Training.
2020-04-11 23:56:02 INFO [stats.py:116] snake-04_SnakeA ELO: 1166.748.
2020-04-11 23:59:25 INFO [stats.py:111] snake-04_SnakeA: Step: 1230000. Time Elapsed: 23514.342 s Mean Reward: -0.385. Std of Reward: 1.389. Training.
2020-04-11 23:59:25 INFO [stats.py:116] snake-04_SnakeA ELO: 1164.529.
2020-04-12 00:02:41 INFO [stats.py:111] snake-04_SnakeA: Step: 1240000. Time Elapsed: 23710.870 s Mean Reward: -0.667. Std of Reward: 1.075. Training.
2020-04-12 00:02:41 INFO [stats.py:116] snake-04_SnakeA ELO: 1161.919.
2020-04-12 00:05:08 INFO [trainer_controller.py:100] Saved Model
2020-04-12 00:05:41 INFO [stats.py:111] snake-04_SnakeA: Step: 1250000. Time Elapsed: 23890.075 s Mean Reward: -0.300. Std of Reward: 1.616. Training.
2020-04-12 00:05:41 INFO [stats.py:116] snake-04_SnakeA ELO: 1159.095.
2020-04-12 00:09:06 INFO [stats.py:111] snake-04_SnakeA: Step: 1260000. Time Elapsed: 24095.517 s Mean Reward: -0.083. Std of Reward: 1.801. Training.
2020-04-12 00:09:06 INFO [stats.py:116] snake-04_SnakeA ELO: 1160.573.
2020-04-12 00:12:39 INFO [stats.py:111] snake-04_SnakeA: Step: 1270000. Time Elapsed: 24308.527 s Mean Reward: 0.083. Std of Reward: 1.605. Training.
2020-04-12 00:12:39 INFO [stats.py:116] snake-04_SnakeA ELO: 1158.920.
2020-04-12 00:15:35 INFO [stats.py:111] snake-04_SnakeA: Step: 1280000. Time Elapsed: 24484.479 s Mean Reward: -0.923. Std of Reward: 1.492. Training.
2020-04-12 00:15:35 INFO [stats.py:116] snake-04_SnakeA ELO: 1157.934.
2020-04-12 00:18:56 INFO [stats.py:111] snake-04_SnakeA: Step: 1290000. Time Elapsed: 24685.957 s Mean Reward: 0.450. Std of Reward: 1.857. Training.
2020-04-12 00:18:56 INFO [stats.py:116] snake-04_SnakeA ELO: 1158.580.
2020-04-12 00:21:29 INFO [trainer_controller.py:100] Saved Model
2020-04-12 00:22:02 INFO [stats.py:111] snake-04_SnakeA: Step: 1300000. Time Elapsed: 24871.159 s Mean Reward: 0.333. Std of Reward: 1.247. Training.
2020-04-12 00:22:02 INFO [stats.py:116] snake-04_SnakeA ELO: 1156.593.
2020-04-12 00:23:51 INFO [stats.py:111] snake-04_SnakeB: Step: 1120000. Time Elapsed: 24980.555 s Mean Reward: -0.211. Std of Reward: 1.507. Training.
2020-04-12 00:23:51 INFO [stats.py:116] snake-04_SnakeB ELO: 1156.145.
2020-04-12 00:27:04 INFO [stats.py:111] snake-04_SnakeB: Step: 1130000. Time Elapsed: 25174.014 s Mean Reward: -0.286. Std of Reward: 1.868. Training.
2020-04-12 00:27:04 INFO [stats.py:116] snake-04_SnakeB ELO: 1159.499.
2020-04-12 00:30:11 INFO [stats.py:111] snake-04_SnakeB: Step: 1140000. Time Elapsed: 25360.208 s Mean Reward: -0.111. Std of Reward: 1.286. Training.
2020-04-12 00:30:11 INFO [stats.py:116] snake-04_SnakeB ELO: 1159.085.
2020-04-12 00:33:23 INFO [stats.py:111] snake-04_SnakeB: Step: 1150000. Time Elapsed: 25552.456 s Mean Reward: -0.462. Std of Reward: 1.393. Training.
2020-04-12 00:33:23 INFO [stats.py:116] snake-04_SnakeB ELO: 1159.264.
2020-04-12 00:36:45 INFO [stats.py:111] snake-04_SnakeB: Step: 1160000. Time Elapsed: 25754.860 s Mean Reward: -0.444. Std of Reward: 1.674. Training.
2020-04-12 00:36:45 INFO [stats.py:116] snake-04_SnakeB ELO: 1156.413.
2020-04-12 00:37:27 INFO [trainer_controller.py:100] Saved Model
2020-04-12 00:39:51 INFO [stats.py:111] snake-04_SnakeB: Step: 1170000. Time Elapsed: 25940.540 s Mean Reward: 0.615. Std of Reward: 1.211. Training.
2020-04-12 00:39:51 INFO [stats.py:116] snake-04_SnakeB ELO: 1155.084.
2020-04-12 00:43:05 INFO [stats.py:111] snake-04_SnakeB: Step: 1180000. Time Elapsed: 26134.058 s Mean Reward: -0.846. Std of Reward: 1.657. Training.
2020-04-12 00:43:05 INFO [stats.py:116] snake-04_SnakeB ELO: 1155.048.
2020-04-12 00:46:29 INFO [stats.py:111] snake-04_SnakeB: Step: 1190000. Time Elapsed: 26338.495 s Mean Reward: -0.067. Std of Reward: 1.526. Training.
2020-04-12 00:46:29 INFO [stats.py:116] snake-04_SnakeB ELO: 1153.108.
2020-04-12 00:49:43 INFO [stats.py:111] snake-04_SnakeB: Step: 1200000. Time Elapsed: 26533.013 s Mean Reward: -0.357. Std of Reward: 1.394. Training.
2020-04-12 00:49:43 INFO [stats.py:116] snake-04_SnakeB ELO: 1152.770.
2020-04-12 00:52:43 INFO [stats.py:111] snake-04_SnakeB: Step: 1210000. Time Elapsed: 26712.894 s Mean Reward: -0.250. Std of Reward: 1.785. Training.
2020-04-12 00:52:43 INFO [stats.py:116] snake-04_SnakeB ELO: 1151.588.
2020-04-12 00:53:28 INFO [trainer_controller.py:100] Saved Model
2020-04-12 00:55:52 INFO [stats.py:111] snake-04_SnakeB: Step: 1220000. Time Elapsed: 26901.646 s Mean Reward: 0.000. Std of Reward: 1.592. Training.
2020-04-12 00:55:52 INFO [stats.py:116] snake-04_SnakeB ELO: 1149.928.
2020-04-12 00:57:43 INFO [stats.py:111] snake-04_SnakeA: Step: 1310000. Time Elapsed: 27012.907 s Mean Reward: -0.222. Std of Reward: 1.685. Training.
2020-04-12 00:57:43 INFO [stats.py:116] snake-04_SnakeA ELO: 1157.049.
2020-04-12 01:00:50 INFO [stats.py:111] snake-04_SnakeA: Step: 1320000. Time Elapsed: 27199.294 s Mean Reward: -0.667. Std of Reward: 1.491. Training.
2020-04-12 01:00:50 INFO [stats.py:116] snake-04_SnakeA ELO: 1159.557.
2020-04-12 01:04:08 INFO [stats.py:111] snake-04_SnakeA: Step: 1330000. Time Elapsed: 27397.509 s Mean Reward: -0.053. Std of Reward: 1.317. Training.
2020-04-12 01:04:08 INFO [stats.py:116] snake-04_SnakeA ELO: 1158.290.
2020-04-12 01:07:14 INFO [stats.py:111] snake-04_SnakeA: Step: 1340000. Time Elapsed: 27583.939 s Mean Reward: -0.889. Std of Reward: 1.370. Training.
2020-04-12 01:07:14 INFO [stats.py:116] snake-04_SnakeA ELO: 1155.745.
2020-04-12 01:09:26 INFO [trainer_controller.py:100] Saved Model
2020-04-12 01:10:21 INFO [stats.py:111] snake-04_SnakeA: Step: 1350000. Time Elapsed: 27770.227 s Mean Reward: -0.533. Std of Reward: 1.668. Training.
2020-04-12 01:10:21 INFO [stats.py:116] snake-04_SnakeA ELO: 1152.737.
2020-04-12 01:13:37 INFO [stats.py:111] snake-04_SnakeA: Step: 1360000. Time Elapsed: 27966.708 s Mean Reward: 0.000. Std of Reward: 1.519. Training.
2020-04-12 01:13:37 INFO [stats.py:116] snake-04_SnakeA ELO: 1154.140.
2020-04-12 01:16:44 INFO [stats.py:111] snake-04_SnakeA: Step: 1370000. Time Elapsed: 28153.638 s Mean Reward: 0.000. Std of Reward: 1.342. Training.
2020-04-12 01:16:44 INFO [stats.py:116] snake-04_SnakeA ELO: 1154.694.
2020-04-12 01:19:53 INFO [stats.py:111] snake-04_SnakeA: Step: 1380000. Time Elapsed: 28342.753 s Mean Reward: -0.200. Std of Reward: 1.990. Training.
2020-04-12 01:19:53 INFO [stats.py:116] snake-04_SnakeA ELO: 1154.708.
2020-04-12 01:23:17 INFO [stats.py:111] snake-04_SnakeA: Step: 1390000. Time Elapsed: 28546.455 s Mean Reward: -0.500. Std of Reward: 1.740. Training.
2020-04-12 01:23:17 INFO [stats.py:116] snake-04_SnakeA ELO: 1153.353.
2020-04-12 01:25:28 INFO [trainer_controller.py:100] Saved Model
2020-04-12 01:26:27 INFO [stats.py:111] snake-04_SnakeA: Step: 1400000. Time Elapsed: 28736.979 s Mean Reward: -0.353. Std of Reward: 1.678. Training.
2020-04-12 01:26:27 INFO [stats.py:116] snake-04_SnakeA ELO: 1153.930.
2020-04-12 01:31:16 INFO [stats.py:111] snake-04_SnakeB: Step: 1230000. Time Elapsed: 29025.535 s Mean Reward: -0.053. Std of Reward: 1.356. Training.
2020-04-12 01:31:16 INFO [stats.py:116] snake-04_SnakeB ELO: 1154.351.
2020-04-12 01:34:41 INFO [stats.py:111] snake-04_SnakeB: Step: 1240000. Time Elapsed: 29230.166 s Mean Reward: -0.600. Std of Reward: 1.583. Training.
2020-04-12 01:34:41 INFO [stats.py:116] snake-04_SnakeB ELO: 1153.625.
2020-04-12 01:37:41 INFO [stats.py:111] snake-04_SnakeB: Step: 1250000. Time Elapsed: 29410.466 s Mean Reward: -0.800. Std of Reward: 1.327. Training.
2020-04-12 01:37:41 INFO [stats.py:116] snake-04_SnakeB ELO: 1150.649.
2020-04-12 01:41:00 INFO [stats.py:111] snake-04_SnakeB: Step: 1260000. Time Elapsed: 29609.695 s Mean Reward: -0.600. Std of Reward: 1.665. Training.
2020-04-12 01:41:00 INFO [stats.py:116] snake-04_SnakeB ELO: 1147.895.
2020-04-12 01:41:28 INFO [trainer_controller.py:100] Saved Model
2020-04-12 01:44:09 INFO [stats.py:111] snake-04_SnakeB: Step: 1270000. Time Elapsed: 29798.619 s Mean Reward: -0.667. Std of Reward: 1.563. Training.
2020-04-12 01:44:09 INFO [stats.py:116] snake-04_SnakeB ELO: 1147.175.
2020-04-12 01:47:23 INFO [stats.py:111] snake-04_SnakeB: Step: 1280000. Time Elapsed: 29992.961 s Mean Reward: -0.357. Std of Reward: 1.586. Training.
2020-04-12 01:47:23 INFO [stats.py:116] snake-04_SnakeB ELO: 1147.803.
2020-04-12 01:50:27 INFO [stats.py:111] snake-04_SnakeB: Step: 1290000. Time Elapsed: 30176.823 s Mean Reward: -1.091. Std of Reward: 1.311. Training.
2020-04-12 01:50:27 INFO [stats.py:116] snake-04_SnakeB ELO: 1147.055.
2020-04-12 01:53:54 INFO [stats.py:111] snake-04_SnakeB: Step: 1300000. Time Elapsed: 30383.445 s Mean Reward: -0.846. Std of Reward: 1.099. Training.
2020-04-12 01:53:54 INFO [stats.py:116] snake-04_SnakeB ELO: 1143.994.
2020-04-12 01:56:52 INFO [stats.py:111] snake-04_SnakeB: Step: 1310000. Time Elapsed: 30561.179 s Mean Reward: -0.455. Std of Reward: 1.437. Training.
2020-04-12 01:56:52 INFO [stats.py:116] snake-04_SnakeB ELO: 1142.274.
2020-04-12 01:57:28 INFO [trainer_controller.py:100] Saved Model
2020-04-12 02:00:15 INFO [stats.py:111] snake-04_SnakeB: Step: 1320000. Time Elapsed: 30764.200 s Mean Reward: -0.333. Std of Reward: 1.300. Training.
2020-04-12 02:00:15 INFO [stats.py:116] snake-04_SnakeB ELO: 1139.901.
2020-04-12 02:01:54 INFO [stats.py:111] snake-04_SnakeA: Step: 1410000. Time Elapsed: 30863.609 s Mean Reward: 0.167. Std of Reward: 1.280. Training.
2020-04-12 02:01:54 INFO [stats.py:116] snake-04_SnakeA ELO: 1155.790.
2020-04-12 02:05:07 INFO [stats.py:111] snake-04_SnakeA: Step: 1420000. Time Elapsed: 31056.080 s Mean Reward: 0.000. Std of Reward: 1.633. Training.
2020-04-12 02:05:07 INFO [stats.py:116] snake-04_SnakeA ELO: 1160.270.
2020-04-12 02:08:21 INFO [stats.py:111] snake-04_SnakeA: Step: 1430000. Time Elapsed: 31250.652 s Mean Reward: 0.000. Std of Reward: 1.683. Training.
2020-04-12 02:08:21 INFO [stats.py:116] snake-04_SnakeA ELO: 1160.316.
2020-04-12 02:11:20 INFO [stats.py:111] snake-04_SnakeA: Step: 1440000. Time Elapsed: 31429.261 s Mean Reward: 0.100. Std of Reward: 1.578. Training.
2020-04-12 02:11:20 INFO [stats.py:116] snake-04_SnakeA ELO: 1159.025.
2020-04-12 02:13:31 INFO [trainer_controller.py:100] Saved Model
2020-04-12 02:14:35 INFO [stats.py:111] snake-04_SnakeA: Step: 1450000. Time Elapsed: 31624.282 s Mean Reward: 0.273. Std of Reward: 1.286. Training.
2020-04-12 02:14:35 INFO [stats.py:116] snake-04_SnakeA ELO: 1158.857.
2020-04-12 02:17:44 INFO [stats.py:111] snake-04_SnakeA: Step: 1460000. Time Elapsed: 31813.890 s Mean Reward: -0.100. Std of Reward: 1.700. Training.
2020-04-12 02:17:44 INFO [stats.py:116] snake-04_SnakeA ELO: 1157.633.
2020-04-12 02:21:05 INFO [stats.py:111] snake-04_SnakeA: Step: 1470000. Time Elapsed: 32014.860 s Mean Reward: -0.214. Std of Reward: 1.423. Training.
2020-04-12 02:21:05 INFO [stats.py:116] snake-04_SnakeA ELO: 1157.586.
2020-04-12 02:24:30 INFO [stats.py:111] snake-04_SnakeA: Step: 1480000. Time Elapsed: 32219.718 s Mean Reward: 0.222. Std of Reward: 1.872. Training.
2020-04-12 02:24:30 INFO [stats.py:116] snake-04_SnakeA ELO: 1155.864.
2020-04-12 02:27:37 INFO [stats.py:111] snake-04_SnakeA: Step: 1490000. Time Elapsed: 32406.337 s Mean Reward: 0.222. Std of Reward: 1.750. Training.
2020-04-12 02:27:37 INFO [stats.py:116] snake-04_SnakeA ELO: 1157.937.
2020-04-12 02:29:37 INFO [trainer_controller.py:100] Saved Model
2020-04-12 02:30:47 INFO [stats.py:111] snake-04_SnakeA: Step: 1500000. Time Elapsed: 32596.529 s Mean Reward: -0.727. Std of Reward: 1.213. Training.
2020-04-12 02:30:47 INFO [stats.py:116] snake-04_SnakeA ELO: 1156.351.
2020-04-12 02:35:55 INFO [stats.py:111] snake-04_SnakeB: Step: 1330000. Time Elapsed: 32904.088 s Mean Reward: 0.500. Std of Reward: 1.258. Training.
2020-04-12 02:35:55 INFO [stats.py:116] snake-04_SnakeB ELO: 1141.081.
2020-04-12 02:39:13 INFO [stats.py:111] snake-04_SnakeB: Step: 1340000. Time Elapsed: 33102.586 s Mean Reward: -0.583. Std of Reward: 1.656. Training.
2020-04-12 02:39:13 INFO [stats.py:116] snake-04_SnakeB ELO: 1139.342.
2020-04-12 02:42:19 INFO [stats.py:111] snake-04_SnakeB: Step: 1350000. Time Elapsed: 33288.111 s Mean Reward: 0.000. Std of Reward: 1.183. Training.
2020-04-12 02:42:19 INFO [stats.py:116] snake-04_SnakeB ELO: 1134.991.
2020-04-12 02:45:44 INFO [stats.py:111] snake-04_SnakeB: Step: 1360000. Time Elapsed: 33493.774 s Mean Reward: -0.250. Std of Reward: 1.436. Training.
2020-04-12 02:45:44 INFO [stats.py:116] snake-04_SnakeB ELO: 1132.011.
2020-04-12 02:45:48 INFO [trainer_controller.py:100] Saved Model
2020-04-12 02:48:52 INFO [stats.py:111] snake-04_SnakeB: Step: 1370000. Time Elapsed: 33681.319 s Mean Reward: -0.600. Std of Reward: 1.356. Training.
2020-04-12 02:48:52 INFO [stats.py:116] snake-04_SnakeB ELO: 1129.378.
2020-04-12 02:52:06 INFO [stats.py:111] snake-04_SnakeB: Step: 1380000. Time Elapsed: 33875.392 s Mean Reward: -0.667. Std of Reward: 1.776. Training.
2020-04-12 02:52:06 INFO [stats.py:116] snake-04_SnakeB ELO: 1127.144.
2020-04-12 02:55:13 INFO [stats.py:111] snake-04_SnakeB: Step: 1390000. Time Elapsed: 34062.104 s Mean Reward: 0.231. Std of Reward: 2.154. Training.
2020-04-12 02:55:13 INFO [stats.py:116] snake-04_SnakeB ELO: 1126.916.
2020-04-12 02:58:26 INFO [stats.py:111] snake-04_SnakeB: Step: 1400000. Time Elapsed: 34255.340 s Mean Reward: -0.600. Std of Reward: 1.428. Training.
2020-04-12 02:58:26 INFO [stats.py:116] snake-04_SnakeB ELO: 1127.256.
2020-04-12 03:01:46 INFO [stats.py:111] snake-04_SnakeB: Step: 1410000. Time Elapsed: 34455.941 s Mean Reward: 0.308. Std of Reward: 1.856. Training.
2020-04-12 03:01:46 INFO [stats.py:116] snake-04_SnakeB ELO: 1127.327.
2020-04-12 03:01:50 INFO [trainer_controller.py:100] Saved Model
2020-04-12 03:05:01 INFO [stats.py:111] snake-04_SnakeB: Step: 1420000. Time Elapsed: 34650.378 s Mean Reward: -0.889. Std of Reward: 0.994. Training.
2020-04-12 03:05:01 INFO [stats.py:116] snake-04_SnakeB ELO: 1126.332.
2020-04-12 03:06:11 INFO [stats.py:111] snake-04_SnakeA: Step: 1510000. Time Elapsed: 34720.205 s Mean Reward: -0.333. Std of Reward: 1.333. Training.
2020-04-12 03:06:11 INFO [stats.py:116] snake-04_SnakeA ELO: 1153.556.
2020-04-12 03:09:33 INFO [stats.py:111] snake-04_SnakeA: Step: 1520000. Time Elapsed: 34922.186 s Mean Reward: -0.353. Std of Reward: 1.570. Training.
2020-04-12 03:09:33 INFO [stats.py:116] snake-04_SnakeA ELO: 1151.561.
2020-04-12 03:12:43 INFO [stats.py:111] snake-04_SnakeA: Step: 1530000. Time Elapsed: 35112.729 s Mean Reward: 0.316. Std of Reward: 1.893. Training.
2020-04-12 03:12:43 INFO [stats.py:116] snake-04_SnakeA ELO: 1154.236.
2020-04-12 03:15:57 INFO [stats.py:111] snake-04_SnakeA: Step: 1540000. Time Elapsed: 35306.703 s Mean Reward: -0.500. Std of Reward: 1.360. Training.
2020-04-12 03:15:57 INFO [stats.py:116] snake-04_SnakeA ELO: 1153.129.
2020-04-12 03:17:47 INFO [trainer_controller.py:100] Saved Model
2020-04-12 03:19:16 INFO [stats.py:111] snake-04_SnakeA: Step: 1550000. Time Elapsed: 35505.269 s Mean Reward: -0.231. Std of Reward: 1.310. Training.
2020-04-12 03:19:16 INFO [stats.py:116] snake-04_SnakeA ELO: 1151.528.
2020-04-12 03:22:23 INFO [stats.py:111] snake-04_SnakeA: Step: 1560000. Time Elapsed: 35692.225 s Mean Reward: -0.500. Std of Reward: 1.384. Training.
2020-04-12 03:22:23 INFO [stats.py:116] snake-04_SnakeA ELO: 1151.601.
2020-04-12 03:25:28 INFO [stats.py:111] snake-04_SnakeA: Step: 1570000. Time Elapsed: 35877.962 s Mean Reward: 0.000. Std of Reward: 1.758. Training.
2020-04-12 03:25:28 INFO [stats.py:116] snake-04_SnakeA ELO: 1150.880.
2020-04-12 03:28:50 INFO [stats.py:111] snake-04_SnakeA: Step: 1580000. Time Elapsed: 36079.955 s Mean Reward: 0.100. Std of Reward: 1.044. Training.
2020-04-12 03:28:50 INFO [stats.py:116] snake-04_SnakeA ELO: 1149.522.
2020-04-12 03:31:56 INFO [stats.py:111] snake-04_SnakeA: Step: 1590000. Time Elapsed: 36265.047 s Mean Reward: -0.250. Std of Reward: 1.689. Training.
2020-04-12 03:31:56 INFO [stats.py:116] snake-04_SnakeA ELO: 1149.447.
2020-04-12 03:33:48 INFO [trainer_controller.py:100] Saved Model
2020-04-12 03:35:00 INFO [stats.py:111] snake-04_SnakeA: Step: 1600000. Time Elapsed: 36449.448 s Mean Reward: 0.400. Std of Reward: 0.917. Training.
2020-04-12 03:35:00 INFO [stats.py:116] snake-04_SnakeA ELO: 1145.790.
2020-04-12 03:40:24 INFO [stats.py:111] snake-04_SnakeB: Step: 1430000. Time Elapsed: 36773.770 s Mean Reward: -0.615. Std of Reward: 1.389. Training.
2020-04-12 03:40:24 INFO [stats.py:116] snake-04_SnakeB ELO: 1131.809.
2020-04-12 03:43:50 INFO [stats.py:111] snake-04_SnakeB: Step: 1440000. Time Elapsed: 36980.028 s Mean Reward: -0.533. Std of Reward: 1.204. Training.
2020-04-12 03:43:50 INFO [stats.py:116] snake-04_SnakeB ELO: 1129.131.
2020-04-12 03:46:56 INFO [stats.py:111] snake-04_SnakeB: Step: 1450000. Time Elapsed: 37165.895 s Mean Reward: -1.000. Std of Reward: 1.155. Training.
2020-04-12 03:46:56 INFO [stats.py:116] snake-04_SnakeB ELO: 1127.143.
2020-04-12 03:49:58 INFO [trainer_controller.py:100] Saved Model
2020-04-12 03:50:07 INFO [stats.py:111] snake-04_SnakeB: Step: 1460000. Time Elapsed: 37356.800 s Mean Reward: 0.300. Std of Reward: 1.616. Training.
2020-04-12 03:50:07 INFO [stats.py:116] snake-04_SnakeB ELO: 1127.676.
2020-04-12 03:53:21 INFO [stats.py:111] snake-04_SnakeB: Step: 1470000. Time Elapsed: 37551.006 s Mean Reward: 0.214. Std of Reward: 1.567. Training.
2020-04-12 03:53:21 INFO [stats.py:116] snake-04_SnakeB ELO: 1125.781.
2020-04-12 03:56:38 INFO [stats.py:111] snake-04_SnakeB: Step: 1480000. Time Elapsed: 37747.352 s Mean Reward: -0.500. Std of Reward: 1.722. Training.
2020-04-12 03:56:38 INFO [stats.py:116] snake-04_SnakeB ELO: 1124.364.
2020-04-12 03:59:41 INFO [stats.py:111] snake-04_SnakeB: Step: 1490000. Time Elapsed: 37930.231 s Mean Reward: -1.111. Std of Reward: 1.197. Training.
2020-04-12 03:59:41 INFO [stats.py:116] snake-04_SnakeB ELO: 1120.950.
2020-04-12 04:03:01 INFO [stats.py:111] snake-04_SnakeB: Step: 1500000. Time Elapsed: 38130.410 s Mean Reward: 0.154. Std of Reward: 1.703. Training.
2020-04-12 04:03:01 INFO [stats.py:116] snake-04_SnakeB ELO: 1119.407.
2020-04-12 04:05:56 INFO [trainer_controller.py:100] Saved Model
2020-04-12 04:06:13 INFO [stats.py:111] snake-04_SnakeB: Step: 1510000. Time Elapsed: 38322.469 s Mean Reward: -0.818. Std of Reward: 1.113. Training.
2020-04-12 04:06:13 INFO [stats.py:116] snake-04_SnakeB ELO: 1116.038.
2020-04-12 04:09:34 INFO [stats.py:111] snake-04_SnakeB: Step: 1520000. Time Elapsed: 38523.468 s Mean Reward: -0.200. Std of Reward: 1.327. Training.
2020-04-12 04:09:34 INFO [stats.py:116] snake-04_SnakeB ELO: 1116.631.
2020-04-12 04:10:48 INFO [stats.py:111] snake-04_SnakeA: Step: 1610000. Time Elapsed: 38597.727 s Mean Reward: -0.533. Std of Reward: 1.628. Training.
2020-04-12 04:10:48 INFO [stats.py:116] snake-04_SnakeA ELO: 1145.695.
2020-04-12 04:14:09 INFO [stats.py:111] snake-04_SnakeA: Step: 1620000. Time Elapsed: 38798.908 s Mean Reward: 0.100. Std of Reward: 1.446. Training.
2020-04-12 04:14:09 INFO [stats.py:116] snake-04_SnakeA ELO: 1150.387.
2020-04-12 04:17:36 INFO [stats.py:111] snake-04_SnakeA: Step: 1630000. Time Elapsed: 39005.673 s Mean Reward: -0.364. Std of Reward: 1.666. Training.
2020-04-12 04:17:36 INFO [stats.py:116] snake-04_SnakeA ELO: 1149.909.
2020-04-12 04:20:49 INFO [stats.py:111] snake-04_SnakeA: Step: 1640000. Time Elapsed: 39198.362 s Mean Reward: 0.000. Std of Reward: 1.633. Training.
2020-04-12 04:20:49 INFO [stats.py:116] snake-04_SnakeA ELO: 1148.314.
2020-04-12 04:22:20 INFO [trainer_controller.py:100] Saved Model
2020-04-12 04:23:46 INFO [stats.py:111] snake-04_SnakeA: Step: 1650000. Time Elapsed: 39375.496 s Mean Reward: -0.100. Std of Reward: 1.972. Training.
2020-04-12 04:23:46 INFO [stats.py:116] snake-04_SnakeA ELO: 1146.984.
2020-04-12 04:27:08 INFO [stats.py:111] snake-04_SnakeA: Step: 1660000. Time Elapsed: 39577.675 s Mean Reward: 0.000. Std of Reward: 1.852. Training.
2020-04-12 04:27:08 INFO [stats.py:116] snake-04_SnakeA ELO: 1145.823.
2020-04-12 04:30:13 INFO [stats.py:111] snake-04_SnakeA: Step: 1670000. Time Elapsed: 39762.695 s Mean Reward: 0.000. Std of Reward: 1.275. Training.
2020-04-12 04:30:13 INFO [stats.py:116] snake-04_SnakeA ELO: 1144.808.
2020-04-12 04:33:41 INFO [stats.py:111] snake-04_SnakeA: Step: 1680000. Time Elapsed: 39970.160 s Mean Reward: -0.846. Std of Reward: 1.747. Training.
2020-04-12 04:33:41 INFO [stats.py:116] snake-04_SnakeA ELO: 1141.466.
2020-04-12 04:36:41 INFO [stats.py:111] snake-04_SnakeA: Step: 1690000. Time Elapsed: 40150.071 s Mean Reward: 0.111. Std of Reward: 1.370. Training.
2020-04-12 04:36:41 INFO [stats.py:116] snake-04_SnakeA ELO: 1140.195.
2020-04-12 04:38:23 INFO [trainer_controller.py:100] Saved Model
2020-04-12 04:39:58 INFO [stats.py:111] snake-04_SnakeA: Step: 1700000. Time Elapsed: 40347.428 s Mean Reward: -0.250. Std of Reward: 1.601. Training.
2020-04-12 04:39:58 INFO [stats.py:116] snake-04_SnakeA ELO: 1139.269.
2020-04-12 04:45:13 INFO [stats.py:111] snake-04_SnakeB: Step: 1530000. Time Elapsed: 40662.481 s Mean Reward: 0.133. Std of Reward: 1.586. Training.
2020-04-12 04:45:13 INFO [stats.py:116] snake-04_SnakeB ELO: 1127.101.
2020-04-12 04:48:29 INFO [stats.py:111] snake-04_SnakeB: Step: 1540000. Time Elapsed: 40858.889 s Mean Reward: -0.273. Std of Reward: 1.601. Training.
2020-04-12 04:48:29 INFO [stats.py:116] snake-04_SnakeB ELO: 1126.854.
2020-04-12 04:51:24 INFO [stats.py:111] snake-04_SnakeB: Step: 1550000. Time Elapsed: 41033.830 s Mean Reward: -0.538. Std of Reward: 1.550. Training.
2020-04-12 04:51:24 INFO [stats.py:116] snake-04_SnakeB ELO: 1126.889.
2020-04-12 04:54:19 INFO [trainer_controller.py:100] Saved Model
2020-04-12 04:54:35 INFO [stats.py:111] snake-04_SnakeB: Step: 1560000. Time Elapsed: 41224.517 s Mean Reward: -0.600. Std of Reward: 1.800. Training.
2020-04-12 04:54:35 INFO [stats.py:116] snake-04_SnakeB ELO: 1125.519.
2020-04-12 04:57:49 INFO [stats.py:111] snake-04_SnakeB: Step: 1570000. Time Elapsed: 41418.427 s Mean Reward: -0.286. Std of Reward: 1.943. Training.
2020-04-12 04:57:49 INFO [stats.py:116] snake-04_SnakeB ELO: 1124.214.
2020-04-12 05:00:59 INFO [stats.py:111] snake-04_SnakeB: Step: 1580000. Time Elapsed: 41608.310 s Mean Reward: 0.000. Std of Reward: 1.541. Training.
2020-04-12 05:00:59 INFO [stats.py:116] snake-04_SnakeB ELO: 1125.151.
2020-04-12 05:04:09 INFO [stats.py:111] snake-04_SnakeB: Step: 1590000. Time Elapsed: 41798.193 s Mean Reward: 1.000. Std of Reward: 1.291. Training.
2020-04-12 05:04:09 INFO [stats.py:116] snake-04_SnakeB ELO: 1123.388.
2020-04-12 05:07:21 INFO [stats.py:111] snake-04_SnakeB: Step: 1600000. Time Elapsed: 41990.951 s Mean Reward: -0.400. Std of Reward: 1.562. Training.
2020-04-12 05:07:21 INFO [stats.py:116] snake-04_SnakeB ELO: 1122.976.
2020-04-12 05:10:16 INFO [trainer_controller.py:100] Saved Model
2020-04-12 05:10:38 INFO [stats.py:111] snake-04_SnakeB: Step: 1610000. Time Elapsed: 42187.461 s Mean Reward: -0.267. Std of Reward: 1.340. Training.
2020-04-12 05:10:38 INFO [stats.py:116] snake-04_SnakeB ELO: 1120.821.
2020-04-12 05:13:43 INFO [stats.py:111] snake-04_SnakeB: Step: 1620000. Time Elapsed: 42372.960 s Mean Reward: -0.750. Std of Reward: 1.738. Training.
2020-04-12 05:13:43 INFO [stats.py:116] snake-04_SnakeB ELO: 1117.986.
2020-04-12 05:15:10 INFO [stats.py:111] snake-04_SnakeA: Step: 1710000. Time Elapsed: 42459.101 s Mean Reward: 0.111. Std of Reward: 2.079. Training.
2020-04-12 05:15:10 INFO [stats.py:116] snake-04_SnakeA ELO: 1138.475.
2020-04-12 05:18:13 INFO [stats.py:111] snake-04_SnakeA: Step: 1720000. Time Elapsed: 42643.032 s Mean Reward: 0.308. Std of Reward: 1.136. Training.
2020-04-12 05:18:13 INFO [stats.py:116] snake-04_SnakeA ELO: 1138.359.
2020-04-12 05:21:41 INFO [stats.py:111] snake-04_SnakeA: Step: 1730000. Time Elapsed: 42850.160 s Mean Reward: 0.333. Std of Reward: 1.660. Training.
2020-04-12 05:21:41 INFO [stats.py:116] snake-04_SnakeA ELO: 1136.779.
2020-04-12 05:24:50 INFO [stats.py:111] snake-04_SnakeA: Step: 1740000. Time Elapsed: 43039.579 s Mean Reward: -0.111. Std of Reward: 1.286. Training.
2020-04-12 05:24:50 INFO [stats.py:116] snake-04_SnakeA ELO: 1133.651.
2020-04-12 05:26:11 INFO [trainer_controller.py:100] Saved Model
2020-04-12 05:27:52 INFO [stats.py:111] snake-04_SnakeA: Step: 1750000. Time Elapsed: 43221.244 s Mean Reward: 0.308. Std of Reward: 1.635. Training.
2020-04-12 05:27:52 INFO [stats.py:116] snake-04_SnakeA ELO: 1132.621.
2020-04-12 05:31:02 INFO [stats.py:111] snake-04_SnakeA: Step: 1760000. Time Elapsed: 43411.871 s Mean Reward: 0.077. Std of Reward: 1.439. Training.
2020-04-12 05:31:02 INFO [stats.py:116] snake-04_SnakeA ELO: 1132.136.
2020-04-12 05:34:14 INFO [stats.py:111] snake-04_SnakeA: Step: 1770000. Time Elapsed: 43603.327 s Mean Reward: -0.900. Std of Reward: 1.300. Training.
2020-04-12 05:34:14 INFO [stats.py:116] snake-04_SnakeA ELO: 1131.740.
2020-04-12 05:37:24 INFO [stats.py:111] snake-04_SnakeA: Step: 1780000. Time Elapsed: 43793.586 s Mean Reward: -0.900. Std of Reward: 1.578. Training.
2020-04-12 05:37:24 INFO [stats.py:116] snake-04_SnakeA ELO: 1130.553.
2020-04-12 05:40:35 INFO [stats.py:111] snake-04_SnakeA: Step: 1790000. Time Elapsed: 43984.048 s Mean Reward: -0.133. Std of Reward: 1.784. Training.
2020-04-12 05:40:35 INFO [stats.py:116] snake-04_SnakeA ELO: 1129.343.
2020-04-12 05:42:04 INFO [trainer_controller.py:100] Saved Model
2020-04-12 05:43:48 INFO [stats.py:111] snake-04_SnakeA: Step: 1800000. Time Elapsed: 44177.412 s Mean Reward: 0.000. Std of Reward: 1.572. Training.
2020-04-12 05:43:48 INFO [stats.py:116] snake-04_SnakeA ELO: 1129.174.
2020-04-12 05:48:54 INFO [stats.py:111] snake-04_SnakeB: Step: 1630000. Time Elapsed: 44483.068 s Mean Reward: -1.000. Std of Reward: 1.069. Training.
2020-04-12 05:48:54 INFO [stats.py:116] snake-04_SnakeB ELO: 1117.618.
2020-04-12 05:52:05 INFO [stats.py:111] snake-04_SnakeB: Step: 1640000. Time Elapsed: 44674.869 s Mean Reward: -0.600. Std of Reward: 1.583. Training.
2020-04-12 05:52:05 INFO [stats.py:116] snake-04_SnakeB ELO: 1116.484.
2020-04-12 05:55:11 INFO [stats.py:111] snake-04_SnakeB: Step: 1650000. Time Elapsed: 44860.186 s Mean Reward: -0.462. Std of Reward: 1.737. Training.
2020-04-12 05:55:11 INFO [stats.py:116] snake-04_SnakeB ELO: 1115.036.
2020-04-12 05:57:59 INFO [trainer_controller.py:100] Saved Model
2020-04-12 05:58:29 INFO [stats.py:111] snake-04_SnakeB: Step: 1660000. Time Elapsed: 45058.630 s Mean Reward: -1.308. Std of Reward: 1.136. Training.
2020-04-12 05:58:29 INFO [stats.py:116] snake-04_SnakeB ELO: 1113.030.
2020-04-12 06:01:38 INFO [stats.py:111] snake-04_SnakeB: Step: 1670000. Time Elapsed: 45247.355 s Mean Reward: 0.077. Std of Reward: 1.328. Training.
2020-04-12 06:01:38 INFO [stats.py:116] snake-04_SnakeB ELO: 1111.287.
2020-04-12 06:04:54 INFO [stats.py:111] snake-04_SnakeB: Step: 1680000. Time Elapsed: 45443.128 s Mean Reward: -0.091. Std of Reward: 1.621. Training.
2020-04-12 06:04:54 INFO [stats.py:116] snake-04_SnakeB ELO: 1110.589.
2020-04-12 06:08:03 INFO [stats.py:111] snake-04_SnakeB: Step: 1690000. Time Elapsed: 45632.391 s Mean Reward: -1.091. Std of Reward: 1.164. Training.
2020-04-12 06:08:03 INFO [stats.py:116] snake-04_SnakeB ELO: 1109.980.
2020-04-12 06:11:12 INFO [stats.py:111] snake-04_SnakeB: Step: 1700000. Time Elapsed: 45821.215 s Mean Reward: -1.000. Std of Reward: 1.472. Training.
2020-04-12 06:11:12 INFO [stats.py:116] snake-04_SnakeB ELO: 1109.146.
2020-04-12 06:13:56 INFO [trainer_controller.py:100] Saved Model
2020-04-12 06:14:26 INFO [stats.py:111] snake-04_SnakeB: Step: 1710000. Time Elapsed: 46015.110 s Mean Reward: 0.083. Std of Reward: 1.656. Training.
2020-04-12 06:14:26 INFO [stats.py:116] snake-04_SnakeB ELO: 1109.244.
2020-04-12 06:17:42 INFO [stats.py:111] snake-04_SnakeB: Step: 1720000. Time Elapsed: 46211.128 s Mean Reward: -1.600. Std of Reward: 1.200. Training.
2020-04-12 06:17:42 INFO [stats.py:116] snake-04_SnakeB ELO: 1108.712.
2020-04-12 06:18:58 INFO [stats.py:111] snake-04_SnakeA: Step: 1810000. Time Elapsed: 46287.225 s Mean Reward: -0.933. Std of Reward: 1.652. Training.
2020-04-12 06:18:58 INFO [stats.py:116] snake-04_SnakeA ELO: 1126.790.
2020-04-12 06:22:07 INFO [stats.py:111] snake-04_SnakeA: Step: 1820000. Time Elapsed: 46476.931 s Mean Reward: 0.154. Std of Reward: 1.460. Training.
2020-04-12 06:22:07 INFO [stats.py:116] snake-04_SnakeA ELO: 1132.483.
2020-04-12 06:25:35 INFO [stats.py:111] snake-04_SnakeA: Step: 1830000. Time Elapsed: 46684.615 s Mean Reward: -0.133. Std of Reward: 1.258. Training.
2020-04-12 06:25:35 INFO [stats.py:116] snake-04_SnakeA ELO: 1131.841.
2020-04-12 06:28:34 INFO [stats.py:111] snake-04_SnakeA: Step: 1840000. Time Elapsed: 46863.234 s Mean Reward: 0.167. Std of Reward: 1.462. Training.
2020-04-12 06:28:34 INFO [stats.py:116] snake-04_SnakeA ELO: 1129.398.
2020-04-12 06:29:52 INFO [trainer_controller.py:100] Saved Model
2020-04-12 06:31:54 INFO [stats.py:111] snake-04_SnakeA: Step: 1850000. Time Elapsed: 47063.585 s Mean Reward: -0.294. Std of Reward: 1.774. Training.
2020-04-12 06:31:54 INFO [stats.py:116] snake-04_SnakeA ELO: 1127.901.
2020-04-12 06:35:07 INFO [stats.py:111] snake-04_SnakeA: Step: 1860000. Time Elapsed: 47256.382 s Mean Reward: -0.333. Std of Reward: 1.434. Training.
2020-04-12 06:35:07 INFO [stats.py:116] snake-04_SnakeA ELO: 1125.924.
2020-04-12 06:38:28 INFO [stats.py:111] snake-04_SnakeA: Step: 1870000. Time Elapsed: 47457.190 s Mean Reward: -0.529. Std of Reward: 1.913. Training.
2020-04-12 06:38:28 INFO [stats.py:116] snake-04_SnakeA ELO: 1124.543.
2020-04-12 06:41:22 INFO [stats.py:111] snake-04_SnakeA: Step: 1880000. Time Elapsed: 47631.939 s Mean Reward: 0.182. Std of Reward: 1.402. Training.
2020-04-12 06:41:22 INFO [stats.py:116] snake-04_SnakeA ELO: 1125.390.
2020-04-12 06:44:46 INFO [stats.py:111] snake-04_SnakeA: Step: 1890000. Time Elapsed: 47835.480 s Mean Reward: -1.091. Std of Reward: 1.443. Training.
2020-04-12 06:44:46 INFO [stats.py:116] snake-04_SnakeA ELO: 1123.697.
2020-04-12 06:45:51 INFO [trainer_controller.py:100] Saved Model
2020-04-12 06:47:47 INFO [stats.py:111] snake-04_SnakeA: Step: 1900000. Time Elapsed: 48016.379 s Mean Reward: -0.286. Std of Reward: 1.750. Training.
2020-04-12 06:47:47 INFO [stats.py:116] snake-04_SnakeA ELO: 1122.461.
2020-04-12 06:52:49 INFO [stats.py:111] snake-04_SnakeB: Step: 1730000. Time Elapsed: 48318.070 s Mean Reward: -1.000. Std of Reward: 1.758. Training.
2020-04-12 06:52:49 INFO [stats.py:116] snake-04_SnakeB ELO: 1106.498.
2020-04-12 06:55:52 INFO [stats.py:111] snake-04_SnakeB: Step: 1740000. Time Elapsed: 48501.093 s Mean Reward: -0.429. Std of Reward: 1.450. Training.
2020-04-12 06:55:52 INFO [stats.py:116] snake-04_SnakeB ELO: 1103.889.
2020-04-12 06:59:02 INFO [stats.py:111] snake-04_SnakeB: Step: 1750000. Time Elapsed: 48691.449 s Mean Reward: -0.077. Std of Reward: 1.639. Training.
2020-04-12 06:59:02 INFO [stats.py:116] snake-04_SnakeB ELO: 1102.653.
2020-04-12 07:01:44 INFO [trainer_controller.py:100] Saved Model
2020-04-12 07:02:16 INFO [stats.py:111] snake-04_SnakeB: Step: 1760000. Time Elapsed: 48885.364 s Mean Reward: 0.000. Std of Reward: 1.710. Training.
2020-04-12 07:02:16 INFO [stats.py:116] snake-04_SnakeB ELO: 1104.321.
2020-04-12 07:05:26 INFO [stats.py:111] snake-04_SnakeB: Step: 1770000. Time Elapsed: 49075.830 s Mean Reward: 0.154. Std of Reward: 1.511. Training.
2020-04-12 07:05:26 INFO [stats.py:116] snake-04_SnakeB ELO: 1106.664.
2020-04-12 07:08:43 INFO [stats.py:111] snake-04_SnakeB: Step: 1780000. Time Elapsed: 49272.394 s Mean Reward: 0.556. Std of Reward: 1.257. Training.
2020-04-12 07:08:43 INFO [stats.py:116] snake-04_SnakeB ELO: 1107.944.
2020-04-12 07:11:46 INFO [stats.py:111] snake-04_SnakeB: Step: 1790000. Time Elapsed: 49455.644 s Mean Reward: 0.000. Std of Reward: 1.633. Training.
2020-04-12 07:11:46 INFO [stats.py:116] snake-04_SnakeB ELO: 1107.476.
2020-04-12 07:15:01 INFO [stats.py:111] snake-04_SnakeB: Step: 1800000. Time Elapsed: 49650.318 s Mean Reward: 0.625. Std of Reward: 1.317. Training.
2020-04-12 07:15:01 INFO [stats.py:116] snake-04_SnakeB ELO: 1106.004.
2020-04-12 07:17:37 INFO [trainer_controller.py:100] Saved Model
2020-04-12 07:18:14 INFO [stats.py:111] snake-04_SnakeB: Step: 1810000. Time Elapsed: 49843.819 s Mean Reward: -0.400. Std of Reward: 1.306. Training.
2020-04-12 07:18:14 INFO [stats.py:116] snake-04_SnakeB ELO: 1105.080.
2020-04-12 07:21:22 INFO [stats.py:111] snake-04_SnakeB: Step: 1820000. Time Elapsed: 50031.788 s Mean Reward: -0.400. Std of Reward: 1.685. Training.
2020-04-12 07:21:22 INFO [stats.py:116] snake-04_SnakeB ELO: 1104.044.
2020-04-12 07:22:46 INFO [stats.py:111] snake-04_SnakeA: Step: 1910000. Time Elapsed: 50115.730 s Mean Reward: 0.167. Std of Reward: 1.344. Training.
2020-04-12 07:22:46 INFO [stats.py:116] snake-04_SnakeA ELO: 1121.775.
2020-04-12 07:26:01 INFO [stats.py:111] snake-04_SnakeA: Step: 1920000. Time Elapsed: 50310.951 s Mean Reward: -0.538. Std of Reward: 1.337. Training.
2020-04-12 07:26:01 INFO [stats.py:116] snake-04_SnakeA ELO: 1122.719.
2020-04-12 07:29:01 INFO [stats.py:111] snake-04_SnakeA: Step: 1930000. Time Elapsed: 50490.235 s Mean Reward: -0.500. Std of Reward: 1.000. Training.
2020-04-12 07:29:01 INFO [stats.py:116] snake-04_SnakeA ELO: 1119.866.
2020-04-12 07:32:05 INFO [stats.py:111] snake-04_SnakeA: Step: 1940000. Time Elapsed: 50674.584 s Mean Reward: -0.333. Std of Reward: 1.599. Training.
2020-04-12 07:32:05 INFO [stats.py:116] snake-04_SnakeA ELO: 1117.711.
2020-04-12 07:32:59 INFO [trainer_controller.py:100] Saved Model
2020-04-12 07:35:08 INFO [stats.py:111] snake-04_SnakeA: Step: 1950000. Time Elapsed: 50857.870 s Mean Reward: -0.533. Std of Reward: 1.668. Training.
2020-04-12 07:35:08 INFO [stats.py:116] snake-04_SnakeA ELO: 1115.634.
2020-04-12 07:38:09 INFO [stats.py:111] snake-04_SnakeA: Step: 1960000. Time Elapsed: 51039.011 s Mean Reward: -0.500. Std of Reward: 1.118. Training.
2020-04-12 07:38:09 INFO [stats.py:116] snake-04_SnakeA ELO: 1114.630.
2020-04-12 07:41:09 INFO [stats.py:111] snake-04_SnakeA: Step: 1970000. Time Elapsed: 51218.834 s Mean Reward: 0.500. Std of Reward: 1.555. Training.
2020-04-12 07:41:09 INFO [stats.py:116] snake-04_SnakeA ELO: 1112.744.
2020-04-12 07:44:07 INFO [stats.py:111] snake-04_SnakeA: Step: 1980000. Time Elapsed: 51396.812 s Mean Reward: -0.667. Std of Reward: 1.650. Training.
2020-04-12 07:44:07 INFO [stats.py:116] snake-04_SnakeA ELO: 1111.370.
2020-04-12 07:47:26 INFO [stats.py:111] snake-04_SnakeA: Step: 1990000. Time Elapsed: 51595.108 s Mean Reward: -0.273. Std of Reward: 1.420. Training.
2020-04-12 07:47:26 INFO [stats.py:116] snake-04_SnakeA ELO: 1109.909.
2020-04-12 07:48:17 INFO [trainer_controller.py:100] Saved Model
2020-04-12 07:50:22 INFO [stats.py:111] snake-04_SnakeA: Step: 2000000. Time Elapsed: 51771.295 s Mean Reward: -0.385. Std of Reward: 1.689. Training.
2020-04-12 07:50:22 INFO [stats.py:116] snake-04_SnakeA ELO: 1111.763.
2020-04-12 07:55:14 INFO [stats.py:111] snake-04_SnakeB: Step: 1830000. Time Elapsed: 52063.055 s Mean Reward: 0.700. Std of Reward: 2.147. Training.
2020-04-12 07:55:14 INFO [stats.py:116] snake-04_SnakeB ELO: 1103.514.
2020-04-12 07:58:21 INFO [stats.py:111] snake-04_SnakeB: Step: 1840000. Time Elapsed: 52250.042 s Mean Reward: -0.727. Std of Reward: 1.656. Training.
2020-04-12 07:58:21 INFO [stats.py:116] snake-04_SnakeB ELO: 1104.661.
2020-04-12 08:01:24 INFO [stats.py:111] snake-04_SnakeB: Step: 1850000. Time Elapsed: 52433.310 s Mean Reward: 0.000. Std of Reward: 1.852. Training.
2020-04-12 08:01:24 INFO [stats.py:116] snake-04_SnakeB ELO: 1104.948.
2020-04-12 08:03:33 INFO [trainer_controller.py:100] Saved Model
2020-04-12 08:04:24 INFO [stats.py:111] snake-04_SnakeB: Step: 1860000. Time Elapsed: 52613.655 s Mean Reward: -0.462. Std of Reward: 1.946. Training.
2020-04-12 08:04:24 INFO [stats.py:116] snake-04_SnakeB ELO: 1104.490.
2020-04-12 08:07:30 INFO [stats.py:111] snake-04_SnakeB: Step: 1870000. Time Elapsed: 52799.952 s Mean Reward: -0.100. Std of Reward: 1.640. Training.
2020-04-12 08:07:30 INFO [stats.py:116] snake-04_SnakeB ELO: 1105.467.
2020-04-12 08:10:37 INFO [stats.py:111] snake-04_SnakeB: Step: 1880000. Time Elapsed: 52986.652 s Mean Reward: -0.083. Std of Reward: 1.754. Training.
2020-04-12 08:10:37 INFO [stats.py:116] snake-04_SnakeB ELO: 1104.828.
2020-04-12 08:13:33 INFO [stats.py:111] snake-04_SnakeB: Step: 1890000. Time Elapsed: 53162.314 s Mean Reward: -0.231. Std of Reward: 1.367. Training.
2020-04-12 08:13:33 INFO [stats.py:116] snake-04_SnakeB ELO: 1103.475.
2020-04-12 08:16:31 INFO [stats.py:111] snake-04_SnakeB: Step: 1900000. Time Elapsed: 53341.016 s Mean Reward: 0.167. Std of Reward: 1.462. Training.
2020-04-12 08:16:31 INFO [stats.py:116] snake-04_SnakeB ELO: 1101.804.
2020-04-12 08:18:50 INFO [trainer_controller.py:100] Saved Model
2020-04-12 08:19:41 INFO [stats.py:111] snake-04_SnakeB: Step: 1910000. Time Elapsed: 53530.696 s Mean Reward: -0.500. Std of Reward: 1.658. Training.
2020-04-12 08:19:41 INFO [stats.py:116] snake-04_SnakeB ELO: 1098.734.
2020-04-12 08:22:41 INFO [stats.py:111] snake-04_SnakeB: Step: 1920000. Time Elapsed: 53710.603 s Mean Reward: -0.467. Std of Reward: 1.628. Training.
2020-04-12 08:22:41 INFO [stats.py:116] snake-04_SnakeB ELO: 1098.544.
2020-04-12 08:23:55 INFO [stats.py:111] snake-04_SnakeA: Step: 2010000. Time Elapsed: 53784.774 s Mean Reward: -0.250. Std of Reward: 1.588. Training.
2020-04-12 08:23:55 INFO [stats.py:116] snake-04_SnakeA ELO: 1111.800.
2020-04-12 08:27:07 INFO [stats.py:111] snake-04_SnakeA: Step: 2020000. Time Elapsed: 53976.180 s Mean Reward: 0.062. Std of Reward: 1.478. Training.
2020-04-12 08:27:07 INFO [stats.py:116] snake-04_SnakeA ELO: 1118.972.
2020-04-12 08:30:03 INFO [stats.py:111] snake-04_SnakeA: Step: 2030000. Time Elapsed: 54152.360 s Mean Reward: -0.846. Std of Reward: 1.610. Training.
2020-04-12 08:30:03 INFO [stats.py:116] snake-04_SnakeA ELO: 1116.563.
2020-04-12 08:33:06 INFO [stats.py:111] snake-04_SnakeA: Step: 2040000. Time Elapsed: 54335.065 s Mean Reward: -0.333. Std of Reward: 1.619. Training.
2020-04-12 08:33:06 INFO [stats.py:116] snake-04_SnakeA ELO: 1113.444.
2020-04-12 08:34:04 INFO [trainer_controller.py:100] Saved Model
2020-04-12 08:36:09 INFO [stats.py:111] snake-04_SnakeA: Step: 2050000. Time Elapsed: 54518.565 s Mean Reward: -0.300. Std of Reward: 1.345. Training.
2020-04-12 08:36:09 INFO [stats.py:116] snake-04_SnakeA ELO: 1111.212.
2020-04-12 08:39:25 INFO [stats.py:111] snake-04_SnakeA: Step: 2060000. Time Elapsed: 54714.204 s Mean Reward: -0.071. Std of Reward: 1.791. Training.
2020-04-12 08:39:25 INFO [stats.py:116] snake-04_SnakeA ELO: 1110.664.
2020-04-12 08:42:27 INFO [stats.py:111] snake-04_SnakeA: Step: 2070000. Time Elapsed: 54896.907 s Mean Reward: -0.636. Std of Reward: 1.720. Training.
2020-04-12 08:42:27 INFO [stats.py:116] snake-04_SnakeA ELO: 1108.967.
2020-04-12 08:45:19 INFO [stats.py:111] snake-04_SnakeA: Step: 2080000. Time Elapsed: 55068.875 s Mean Reward: -0.154. Std of Reward: 1.231. Training.
2020-04-12 08:45:19 INFO [stats.py:116] snake-04_SnakeA ELO: 1107.998.
2020-04-12 08:48:21 INFO [stats.py:111] snake-04_SnakeA: Step: 2090000. Time Elapsed: 55250.184 s Mean Reward: -0.182. Std of Reward: 1.336. Training.
2020-04-12 08:48:21 INFO [stats.py:116] snake-04_SnakeA ELO: 1106.409.
2020-04-12 08:49:16 INFO [trainer_controller.py:100] Saved Model
2020-04-12 08:51:30 INFO [stats.py:111] snake-04_SnakeA: Step: 2100000. Time Elapsed: 55439.516 s Mean Reward: -1.000. Std of Reward: 1.414. Training.
2020-04-12 08:51:30 INFO [stats.py:116] snake-04_SnakeA ELO: 1103.227.
2020-04-12 08:56:19 INFO [stats.py:111] snake-04_SnakeB: Step: 1930000. Time Elapsed: 55728.944 s Mean Reward: -0.267. Std of Reward: 1.914. Training.
2020-04-12 08:56:19 INFO [stats.py:116] snake-04_SnakeB ELO: 1103.162.
2020-04-12 08:59:24 INFO [stats.py:111] snake-04_SnakeB: Step: 1940000. Time Elapsed: 55913.330 s Mean Reward: -0.500. Std of Reward: 1.323. Training.
2020-04-12 08:59:24 INFO [stats.py:116] snake-04_SnakeB ELO: 1102.347.
2020-04-12 09:02:29 INFO [stats.py:111] snake-04_SnakeB: Step: 1950000. Time Elapsed: 56098.267 s Mean Reward: 0.647. Std of Reward: 1.412. Training.
2020-04-12 09:02:29 INFO [stats.py:116] snake-04_SnakeB ELO: 1102.033.
2020-04-12 09:04:22 INFO [trainer_controller.py:100] Saved Model
2020-04-12 09:05:24 INFO [stats.py:111] snake-04_SnakeB: Step: 1960000. Time Elapsed: 56273.606 s Mean Reward: 0.462. Std of Reward: 1.781. Training.
2020-04-12 09:05:24 INFO [stats.py:116] snake-04_SnakeB ELO: 1102.939.
2020-04-12 09:08:25 INFO [stats.py:111] snake-04_SnakeB: Step: 1970000. Time Elapsed: 56454.081 s Mean Reward: 0.000. Std of Reward: 1.581. Training.
2020-04-12 09:08:25 INFO [stats.py:116] snake-04_SnakeB ELO: 1102.594.
2020-04-12 09:11:25 INFO [stats.py:111] snake-04_SnakeB: Step: 1980000. Time Elapsed: 56634.302 s Mean Reward: 0.500. Std of Reward: 1.360. Training.
2020-04-12 09:11:25 INFO [stats.py:116] snake-04_SnakeB ELO: 1103.909.
2020-04-12 09:14:27 INFO [stats.py:111] snake-04_SnakeB: Step: 1990000. Time Elapsed: 56816.098 s Mean Reward: -0.364. Std of Reward: 1.494. Training.
2020-04-12 09:14:27 INFO [stats.py:116] snake-04_SnakeB ELO: 1104.221.
2020-04-12 09:17:24 INFO [stats.py:111] snake-04_SnakeB: Step: 2000000. Time Elapsed: 56993.343 s Mean Reward: -0.312. Std of Reward: 1.685. Training.
2020-04-12 09:17:24 INFO [stats.py:116] snake-04_SnakeB ELO: 1103.104.
2020-04-12 09:19:22 INFO [trainer_controller.py:100] Saved Model
2020-04-12 09:20:33 INFO [stats.py:111] snake-04_SnakeB: Step: 2010000. Time Elapsed: 57182.781 s Mean Reward: -0.077. Std of Reward: 1.774. Training.
2020-04-12 09:20:33 INFO [stats.py:116] snake-04_SnakeB ELO: 1102.549.
2020-04-12 09:23:41 INFO [stats.py:111] snake-04_SnakeB: Step: 2020000. Time Elapsed: 57370.395 s Mean Reward: -0.444. Std of Reward: 1.499. Training.
2020-04-12 09:23:41 INFO [stats.py:116] snake-04_SnakeB ELO: 1099.453.
2020-04-12 09:24:47 INFO [stats.py:111] snake-04_SnakeA: Step: 2110000. Time Elapsed: 57436.846 s Mean Reward: -0.917. Std of Reward: 1.605. Training.
2020-04-12 09:24:47 INFO [stats.py:116] snake-04_SnakeA ELO: 1102.797.
2020-04-12 09:28:08 INFO [stats.py:111] snake-04_SnakeA: Step: 2120000. Time Elapsed: 57637.415 s Mean Reward: 0.154. Std of Reward: 1.231. Training.
2020-04-12 09:28:08 INFO [stats.py:116] snake-04_SnakeA ELO: 1106.255.
2020-04-12 09:31:12 INFO [stats.py:111] snake-04_SnakeA: Step: 2130000. Time Elapsed: 57821.259 s Mean Reward: -0.500. Std of Reward: 1.658. Training.
2020-04-12 09:31:12 INFO [stats.py:116] snake-04_SnakeA ELO: 1105.827.
2020-04-12 09:34:23 INFO [stats.py:111] snake-04_SnakeA: Step: 2140000. Time Elapsed: 58012.427 s Mean Reward: -0.500. Std of Reward: 1.384. Training.
2020-04-12 09:34:23 INFO [stats.py:116] snake-04_SnakeA ELO: 1104.615.
2020-04-12 09:34:59 INFO [trainer_controller.py:100] Saved Model
2020-04-12 09:37:15 INFO [stats.py:111] snake-04_SnakeA: Step: 2150000. Time Elapsed: 58184.471 s Mean Reward: 0.750. Std of Reward: 0.722. Training.
2020-04-12 09:37:15 INFO [stats.py:116] snake-04_SnakeA ELO: 1104.233.
2020-04-12 09:40:09 INFO [stats.py:111] snake-04_SnakeA: Step: 2160000. Time Elapsed: 58358.912 s Mean Reward: -0.067. Std of Reward: 1.652. Training.
2020-04-12 09:40:09 INFO [stats.py:116] snake-04_SnakeA ELO: 1102.780.
2020-04-12 09:43:18 INFO [stats.py:111] snake-04_SnakeA: Step: 2170000. Time Elapsed: 58547.052 s Mean Reward: -0.375. Std of Reward: 1.536. Training.
2020-04-12 09:43:18 INFO [stats.py:116] snake-04_SnakeA ELO: 1100.922.
2020-04-12 09:45:59 INFO [stats.py:111] snake-04_SnakeA: Step: 2180000. Time Elapsed: 58708.637 s Mean Reward: -0.250. Std of Reward: 1.392. Training.
2020-04-12 09:45:59 INFO [stats.py:116] snake-04_SnakeA ELO: 1098.513.
2020-04-12 09:49:00 INFO [stats.py:111] snake-04_SnakeA: Step: 2190000. Time Elapsed: 58889.128 s Mean Reward: -0.250. Std of Reward: 1.984. Training.
2020-04-12 09:49:00 INFO [stats.py:116] snake-04_SnakeA ELO: 1098.010.
2020-04-12 09:49:46 INFO [trainer_controller.py:100] Saved Model
```",update similar steadily bug something correctly open process twitch channel also live training version information dev dev communicator warning removed future version long term binary use behavior name trainer beta epsilon constant normalize true false simple extrinsic strength gamma window loading model brain training step behavior name trainer beta epsilon constant normalize true false simple extrinsic strength gamma window loading model brain training step step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model,issue,positive,negative,negative,negative,negative,negative
612460900,@surfnerd Thanks God in version 0.14.1... It works...,thanks god version work,issue,positive,positive,positive,positive,positive,positive
612446092,"> Hi @alex7e98,
> Did you run your Unity Environment after running the Pyton command in the terminal prompt? We had some users on the forums run into the same issue because the Unity Logo didn't pop up telling them to press play in the Editor. Please let us know if this is your issue.

Same issue here, the Unity Logo did show up but it seems broken in Windows CMD.:sweat_smile:
![image](https://user-images.githubusercontent.com/33959089/79047817-aa504480-7c4b-11ea-8c69-bf3cb4f737bd.png)

And the message “press play in the Editor” that supposed to be poped up didn't show up. 

But as you suggested, I tried press ***play*** after I ran command `mlagents-learn config/trainer_config.yaml --run-id=firstRun` (actually wait a second). Then the training process starts..:satisfied:

So it seems like it's version 0.15.1's bug. I will continue testing, thanks anyway!

---
(Updated)

Hummmmmm.. Actually it's not training at all!:dizzy_face: Compared to the output log in the [document](https://github.com/Unity-Technologies/ml-agents/blob/0.15.0/docs/Basic-Guide.md) 
```console
INFO:mlagents_envs:
'Ball3DAcademy' started successfully!
Unity Academy name: Ball3DAcademy

INFO:mlagents_envs:Connected new brain:
Unity brain name: 3DBallLearning
        Number of Visual Observations (per agent): 0
        Vector Observation space size (per agent): 8
        Number of stacked Vector Observation: 1
        Vector Action space type: continuous
        Vector Action space size (per agent): [2]
        Vector Action descriptions: ,
INFO:mlagents_envs:Hyperparameters for the PPO Trainer of brain 3DBallLearning:
        batch_size:          64
        beta:                0.001
        buffer_size:         12000
        epsilon:             0.2
        gamma:               0.995
        hidden_units:        128
        lambd:               0.99
        learning_rate:       0.0003
        max_steps:           5.0e4
        normalize:           True
        num_epoch:           3
        num_layers:          2
        time_horizon:        1000
        sequence_length:     64
        summary_freq:        1000
        use_recurrent:       False
        summary_path:        ./summaries/first-run-0
        memory_size:         256
        use_curiosity:       False
        curiosity_strength:  0.01
        curiosity_enc_size:  128
        model_path:	./models/first-run-0/3DBallLearning
INFO:mlagents.trainers: first-run-0: 3DBallLearning: Step: 1000. Mean Reward: 1.242. Std of Reward: 0.746. Training.
INFO:mlagents.trainers: first-run-0: 3DBallLearning: Step: 2000. Mean Reward: 1.319. Std of Reward: 0.693. Training.
INFO:mlagents.trainers: first-run-0: 3DBallLearning: Step: 3000. Mean Reward: 1.804. Std of Reward: 1.056. Training.
INFO:mlagents.trainers: first-run-0: 3DBallLearning: Step: 4000. Mean Reward: 2.151. Std of Reward: 1.432. Training.
INFO:mlagents.trainers: first-run-0: 3DBallLearning: Step: 5000. Mean Reward: 3.175. Std of Reward: 2.250. Training.
INFO:mlagents.trainers: first-run-0: 3DBallLearning: Step: 6000. Mean Reward: 4.898. Std of Reward: 4.019. Training.
INFO:mlagents.trainers: first-run-0: 3DBallLearning: Step: 7000. Mean Reward: 6.716. Std of Reward: 5.125. Training.
INFO:mlagents.trainers: first-run-0: 3DBallLearning: Step: 8000. Mean Reward: 12.124. Std of Reward: 11.929. Training.
INFO:mlagents.trainers: first-run-0: 3DBallLearning: Step: 9000. Mean Reward: 18.151. Std of Reward: 16.871. Training.
INFO:mlagents.trainers: first-run-0: 3DBallLearning: Step: 10000. Mean Reward: 27.284. Std of Reward: 28.667. Training.
```
What I got is
```console
 Version information:
  ml-agents: 0.15.1,
  ml-agents-envs: 0.15.1,
  Communicator API: 0.15.0,
  TensorFlow: 2.0.1
WARNING:tensorflow:From d:\unityprojects\python-envs\sample-env\lib\site-packages\tensorflow_core\python\compat\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2020-04-11 23:24:03 INFO [trainer_controller.py:167] Hyperparameters for the PPOTrainer of brain 3DBall:
        trainer:        ppo
        batch_size:     64
        beta:   0.001
        buffer_size:    12000
        epsilon:        0.2
        hidden_units:   128
        lambd:  0.99
        learning_rate:  0.0003
        learning_rate_schedule: linear
        max_steps:      5.0e5
        memory_size:    128
        normalize:      True
        num_epoch:      3
        num_layers:     2
        time_horizon:   1000
        sequence_length:        64
        summary_freq:   12000
        use_recurrent:  False
        vis_encode_type:        simple
        reward_signals:
          extrinsic:
            strength:   1.0
            gamma:      0.99
        summary_path:   firstRun_3DBall
        model_path:     ./models/firstRun/3DBall
        keep_checkpoints:       5
2020-04-11 23:24:03.537975: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-04-11 23:25:27 INFO [trainer.py:214] firstRun: 3DBall: Step: 12000. Time Elapsed: 84.394 s Mean Reward: 1.196. Std of Reward: 0.699. Not Training.
2020-04-11 23:26:48 INFO [trainer.py:214] firstRun: 3DBall: Step: 24000. Time Elapsed: 165.058 s Mean Reward: 1.146. Std of Reward: 0.695. Not Training.
2020-04-11 23:28:08 INFO [trainer.py:214] firstRun: 3DBall: Step: 36000. Time Elapsed: 244.525 s Mean Reward: 1.237. Std of Reward: 0.754. Not Training.
2020-04-11 23:29:28 INFO [trainer.py:214] firstRun: 3DBall: Step: 48000. Time Elapsed: 324.852 s Mean Reward: 1.214. Std of Reward: 0.742. Not Training.
```
I will try download 0.14! :cry:",hi run unity environment running command terminal prompt run issue unity pop telling press play editor please let u know issue issue unity show broken image message press play editor supposed show tried press play ran command actually wait second training process satisfied like version bug continue testing thanks anyway actually training output log document console successfully unity academy name connected new brain unity brain name number visual per agent vector observation space size per agent number vector observation vector action space type continuous vector action space size per agent vector action trainer brain beta epsilon gamma normalize true false false step mean reward reward training step mean reward reward training step mean reward reward training step mean reward reward training step mean reward reward training step mean reward reward training step mean reward reward training step mean reward reward training step mean reward reward training step mean reward reward training got console version information communicator warning removed future version long term brain trainer beta epsilon linear normalize true false simple extrinsic strength gamma binary use step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training try cry,issue,positive,negative,negative,negative,negative,negative
612429950,Thank you very much. I upgraded and it successfully builds when I disable Burst. ,thank much successfully disable burst,issue,positive,positive,positive,positive,positive,positive
612261019,"Hey @AsadJeewa,
According to the Burst team, this is a knew issue with Burst 1.1.  We just updated barracuda on our master branch which updates the Burst dependency.  This will allow you to disable it and not see the compile errors.  You can try it out now by checking out our master branch and trying to build for Linux with Burst disabled.  Unfortunately, the fix for cross-compilation with Burst won't be available until they release the 1.3 version of their package.  

You could also install the 0.6.3-preview Barracuda package into your project until you update ml-agents after the next release to get the Burst plugin update. 

Let me know if you run into any issues with these workarounds. ",hey according burst team knew issue burst barracuda master branch burst dependency allow disable see compile try master branch trying build burst disabled unfortunately fix burst wo available release version package could also install barracuda package project update next release get burst update let know run,issue,negative,negative,neutral,neutral,negative,negative
612065932,"@andrewcoh That's great to hear. However, I think there has to be another issue there, because the graphs are not the same even when self-play is turned off and all agents have the same brain. 

Here's an example graph: 
![rewards3](https://user-images.githubusercontent.com/1988272/78999918-9dcbcf00-7b19-11ea-80a4-7103e9e2502b.png)
In this scenario there are rewards and penalties but the rewards are always higher in magnitude. 
It looks like the cumulative environment rewards graph is summing the rewards while the extrinsic rewards graph is displaying each one as a separate data point. Again this behaviour is different from what it used to be. Could that be it?",great hear however think another issue even turned brain example graph scenario always higher magnitude like cumulative environment graph extrinsic graph one separate data point behaviour different used could,issue,positive,positive,positive,positive,positive,positive
612049777,Thank you so much. Really appreciate it. ,thank much really appreciate,issue,positive,positive,positive,positive,positive,positive
611953334,"@surfnerd , sorry for my late response but haven't been working with Unity for a while. For now the issues seems to have been resolved by doing the following:

![image](https://user-images.githubusercontent.com/2350015/78979673-a07bf380-7b1c-11ea-9115-131be76cddef.png)

Once I'll get back to working with Unity I'll run the training algorithm again and see if it remains working. Thanks for the quick responses and help you offered!",sorry late response working unity resolved following image get back working unity run training algorithm see remains working thanks quick help,issue,positive,negative,neutral,neutral,negative,negative
611931016,"> You might want to also try using [tensorflow2onnx](https://github.com/onnx/tensorflow-onnx) to convert the model to ONNX format, since the barracuda package can load ONNX files.

@chriselion  Hi, I find that Unity used to have the support of TensorFlowSharp to run tf models, but it was removed in Feb 2019. I have troubles in converting my tf model to onnx or barracuda, since some operations are not supported by these scripts. I hope to use TensorFlowSharp to solve my problem, is it possible?",might want also try convert model format since barracuda package load hi find unity used support run removed converting model barracuda since hope use solve problem possible,issue,positive,neutral,neutral,neutral,neutral,neutral
611852050,"Hi @AsadJeewa,
I'm reaching out to the Burst team now.  I'll get back to you when I get a response.  ",hi reaching burst team get back get response,issue,negative,neutral,neutral,neutral,neutral,neutral
611845532,"overall looks good.  I would rather we re-use the array.  If you feel strongly about no re-using it, let's talk. ",overall good would rather array feel strongly let talk,issue,positive,positive,positive,positive,positive,positive
611827716,"[Update] after 2,000,000 steps The ELO of the agents steadily decreased. I believe this was because the reward wasn't zero sub. +1 point for food (without other snakes loosing a point).
 
Retraining new brains with new rules:
-1 point for hitting snake, other snakes get +1. End of episode.
+1 point for hitting food, other snakes get -1. Continue episode.

Hopefully this will create an environment where the ELO of the agents steadily increase.",update steadily believe reward zero sub point food without loosing point new brain new point snake get end episode point food get continue episode hopefully create environment steadily increase,issue,positive,positive,positive,positive,positive,positive
611820582,"Hi ,

Yes this Workaround works good , the above warning messages are gone and Bird continues to train. :):)

Thanks.",hi yes work good warning gone bird train thanks,issue,positive,positive,positive,positive,positive,positive
611771415,"![image](https://user-images.githubusercontent.com/27775903/78944263-c7efa380-7abd-11ea-919e-abc3afe16696.png)

You mean here? I did this and it still gives exactly the same BuildFailedException",image mean still exactly,issue,negative,negative,neutral,neutral,negative,negative
611687740,"Hi @AsadJeewa,
Could you try to open up the Project settings by going to the menu -> Edit -> Project Settings... and disable burst for linux from there?",hi could try open project going menu edit project disable burst,issue,negative,neutral,neutral,neutral,neutral,neutral
611683899,What do `Cumulative Rewards` and `Extrinsic Rewards` represent when we have multiple agents in the same instance? ,cumulative extrinsic represent multiple instance,issue,negative,neutral,neutral,neutral,neutral,neutral
611647642,"**edit:** I had the wrong code in the `FixedUpdate` method. 

Hi @graybob,
We've logged this issue internally as MLA-874.  We will work on this and update this thread when it's fixed.  

In the time being, you can turn off the Academy Automatic Stepping and call RequestDecision in your Agent's FixedUpdate method for now. 

In your Agent:
```
protected override void Initialize()
{
    Academy.Instance.AutomaticSteppingEnabled = false;
}

void FixedUpdate()
{
    Academy.Instance.EnvironmentStep();
}
```

Let me know if this workaround helps you move forward.",edit wrong code method hi logged issue internally work update thread fixed time turn academy automatic stepping call agent method agent override void initialize false void let know move forward,issue,negative,negative,negative,negative,negative,negative
611619426,"Hi ,
Yes , The log is as below.. (Using ML-Agents Beta 0.15.1)

Fewer observations (0) made than vector observation size (3). The observations will be padded.
UnityEngine.Debug:LogWarningFormat(String, Object[])
MLAgents.Sensors.VectorSensor:Write(WriteAdapter) (at Assets/com.unity.ml-agents/Runtime/Sensors/VectorSensor.cs:50)
MLAgents.GrpcExtensions:GetObservationProto(ISensor, WriteAdapter) (at Assets/com.unity.ml-agents/Runtime/Communicator/GrpcExtensions.cs:222)
MLAgents.RpcCommunicator:PutObservations(String, AgentInfo, List`1) (at Assets/com.unity.ml-agents/Runtime/Communicator/RpcCommunicator.cs:284)
MLAgents.Policies.RemotePolicy:RequestDecision(AgentInfo, List`1) (at Assets/com.unity.ml-agents/Runtime/Policies/RemotePolicy.cs:33)
MLAgents.Agent:NotifyAgentDone(DoneReason) (at Assets/com.unity.ml-agents/Runtime/Agent.cs:324)
MLAgents.Agent:OnDisable() (at Assets/com.unity.ml-agents/Runtime/Agent.cs:311)

Thanks

",hi yes log beta made vector observation size string object write string list list thanks,issue,positive,positive,positive,positive,positive,positive
611618520,"Hi @surfnerd 
Thank you for your response.
I have tried to run python command with Unity Enviroment and also without Unity Enviroment. The same issue popped up. Anyway, the Unity Logo did show up in the terminal prompt and even if 
I ignore the message and press Play in the Unity Editor, nothing happened.

FYI, I don't know if it is relevant, in Unity Console, the message below keeps showing up.

`Couldn't connect to trainer on port 5004 using API version 0.15.0. Will perform inference instead.`

However, I can still run ML-Agents v 0.14.1 without any problem but it is not the same case for ML-agent v 0.15.1 & v 0.15.0

I also tried to update Unity version from 2019.3.7f1 to version 2019.3.9f1 but still no luck.",hi thank response tried run python command unity also without unity issue anyway unity show terminal prompt even ignore message press play unity editor nothing know relevant unity console message showing could connect trainer port version perform inference however still run without problem case also tried update unity version version still luck,issue,positive,positive,positive,positive,positive,positive
611609716,"Hi @alex7e98,
Did you run your Unity Environment after running the Pyton command in the terminal prompt?  We had some users on the forums run into the same issue because the Unity Logo didn't pop up telling them to press play in the Editor.  Please let us know if this is your issue.",hi run unity environment running command terminal prompt run issue unity pop telling press play editor please let u know issue,issue,positive,neutral,neutral,neutral,neutral,neutral
611608030,"Hi @graybob,
Do you see anything in the Unity Console or python output when the scene is reloaded after the bird game object is destroyed?",hi see anything unity console python output scene bird game object,issue,negative,negative,negative,negative,negative,negative
611567837,Please make sure your C# code is up to date.  The Sensor namespace was introduced in 0.15.0,please make sure code date sensor,issue,positive,positive,positive,positive,positive,positive
611472001,"Hi,
What happened was that I was recording a demo for imitation learning and I was controlling the character myself using the heuristic function. In my heuristic function i was creating a float array of length 5 (this was an accident and i meant to have length of 4) and returning it. My agent had a discrete Vector Action Space of size 4 with branch sizes of 2,3,3 and 1 respectively. The recording went fine. but when i started training I got the error

""ValueError: setting an array element with a sequence"".

I fixed it by fixing my heuristic function which now returned an array of length 4. This solved the issue. Hope this cleared up any ambiguities",hi recording imitation learning character heuristic function heuristic function float array length accident meant length agent discrete vector action space size branch size respectively recording went fine training got error setting array element sequence fixed fixing heuristic function returned array length issue hope,issue,negative,positive,neutral,neutral,positive,positive
611322473,"[Update] After 5000 steps they now perform different actions ^_^ Cant wait to see them train! 

![image](https://github.com/ElliotWood/SnakeAI/raw/master/Snake3.gif)",update perform different cant wait see train image,issue,negative,neutral,neutral,neutral,neutral,neutral
611321123,"Ok upgraded to lastest master, the agents appear to still be mirroring actions. I'd expect them to use different actions, is that assumption correct?

![image](https://github.com/ElliotWood/SnakeAI/raw/master/Snake2.gif)

```
(mlagents) (base) \repos\ml-agents>mlagents-learn config/trainer_config.yaml --run-id=snake-02
WARNING:tensorflow:From \repos\ml-agents\python-envs\mlagents\lib\site-packages\tensorflow_core\python\compat\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term


                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓


 Version information:
  ml-agents: 0.16.0.dev0,
  ml-agents-envs: 0.16.0.dev0,
  Communicator API: 0.16.0,
  TensorFlow: 2.0.1
WARNING:tensorflow:From \repos\ml-agents\python-envs\mlagents\lib\site-packages\tensorflow_core\python\compat\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2020-04-09 12:17:42.015424: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-04-09 12:17:42 INFO [stats.py:130] Hyperparameters for behavior name snake-02_SnakeB:
        trainer:        ppo
        batch_size:     1024
        beta:   0.005
        buffer_size:    10240
        epsilon:        0.2
        hidden_units:   256
        lambd:  0.95
        learning_rate:  0.0003
        learning_rate_schedule: constant
        max_steps:      5.0e7
        memory_size:    128
        normalize:      True
        num_epoch:      3
        num_layers:     2
        time_horizon:   1000
        sequence_length:        64
        summary_freq:   10000
        use_recurrent:  False
        vis_encode_type:        simple
        reward_signals:
          extrinsic:
            strength:   1.0
            gamma:      0.99
        summary_path:   snake-02_SnakeB
        model_path:     ./models/snake-02/SnakeB
        keep_checkpoints:       5
        self_play:
          window:       10
          play_against_latest_model_ratio:      0.5
          save_steps:   50000
          swap_steps:   50000
          team_change:  100000
2020-04-09 12:17:43 INFO [stats.py:130] Hyperparameters for behavior name snake-02_SnakeA:
        trainer:        ppo
        batch_size:     1024
        beta:   0.005
        buffer_size:    10240
        epsilon:        0.2
        hidden_units:   256
        lambd:  0.95
        learning_rate:  0.0003
        learning_rate_schedule: constant
        max_steps:      5.0e7
        memory_size:    128
        normalize:      True
        num_epoch:      3
        num_layers:     2
        time_horizon:   1000
        sequence_length:        64
        summary_freq:   10000
        use_recurrent:  False
        vis_encode_type:        simple
        reward_signals:
          extrinsic:
            strength:   1.0
            gamma:      0.99
        summary_path:   snake-02_SnakeA
        model_path:     ./models/snake-02/SnakeA
        keep_checkpoints:       5
        self_play:
          window:       10
          play_against_latest_model_ratio:      0.5
          save_steps:   50000
          swap_steps:   50000
          team_change:  100000
2020-04-09 12:20:09 INFO [stats.py:111] snake-02_SnakeB: Step: 10000. Time Elapsed: 151.135 s Mean Reward: 0.667. Std of Reward: 0.471. Training.
2020-04-09 12:20:09 INFO [stats.py:116] snake-02_SnakeB ELO: 1199.500.
2020-04-09 12:22:19 INFO [stats.py:111] snake-02_SnakeB: Step: 20000. Time Elapsed: 280.779 s Mean Reward: 1.000. Std of Reward: 0.000. Training.
2020-04-09 12:22:19 INFO [stats.py:116] snake-02_SnakeB ELO: 1198.176.
```
```
SnakeA:
    normalize: true
    max_steps: 5.0e7
    learning_rate_schedule: constant
    batch_size: 1024
    buffer_size: 10240
    hidden_units: 256
    time_horizon: 1000
    self_play:
        window: 10
        play_against_latest_model_ratio: 0.5
        save_steps: 50000
        swap_steps: 50000
        team_change: 100000

SnakeB:
    normalize: true
    max_steps: 5.0e7
    learning_rate_schedule: constant
    batch_size: 1024
    buffer_size: 10240
    hidden_units: 256
    time_horizon: 1000
    self_play:
        window: 10
        play_against_latest_model_ratio: 0.5
        save_steps: 50000
        swap_steps: 50000
        team_change: 100000
```",master appear still expect use different assumption correct image base warning removed future version long term version information dev dev communicator warning removed future version long term binary use behavior name trainer beta epsilon constant normalize true false simple extrinsic strength gamma window behavior name trainer beta epsilon constant normalize true false simple extrinsic strength gamma window step time mean reward reward training step time mean reward reward training normalize true constant window normalize true constant window,issue,positive,negative,neutral,neutral,negative,negative
611283239,"Ok I pulled master (.dev4) before testing, bit will double check now.
Yes using different teams also.",master testing bit double check yes different also,issue,negative,neutral,neutral,neutral,neutral,neutral
611259258,"> > We've deleted more images than we added - what am I missing?
> 
> A lot of images were not referenced in the markdowns. So I removed them, I don't think they are useful if they are not referenced.

Ok, as long as they're just orphan images, it makes sense to delete them!",added missing lot removed think useful long orphan sense delete,issue,negative,positive,neutral,neutral,positive,positive
611257843,"I have a project that I have been working on (Windows 10). Now that I have good results, I want to scale it and perform multiple runs on a high performance computer remotely i.e a Linux system, So I need to cross-build. Your assistance is greatly appreciated.",project working good want scale perform multiple high performance computer remotely system need assistance greatly,issue,negative,positive,positive,positive,positive,positive
611257452,"I am using Unity 2019.2.5f1 and I still get burst error even when I have disabled burst. 

![burst2](https://user-images.githubusercontent.com/27775903/78844975-9c11e680-7a07-11ea-94a3-2a161de2c21c.png)
![burst1](https://user-images.githubusercontent.com/27775903/78844976-9caa7d00-7a07-11ea-8f91-f81a331bfb7d.PNG)

",unity still get burst error even disabled burst burst burst,issue,negative,negative,negative,negative,negative,negative
611256842,"> We've deleted more images than we added - what am I missing?

A lot of images were not referenced in the markdowns. So I removed them, I don't think they are useful if they are not referenced.
",added missing lot removed think useful,issue,negative,positive,neutral,neutral,positive,positive
611236572,"Hi @lukemadera 

I believe the 'inference only' behavior type in the behavior parameters script is what you are looking for. Agents with this set should run as inference only and not be trained. 

Let me know if this solves the issue.",hi believe behavior type behavior script looking set run inference trained let know issue,issue,negative,neutral,neutral,neutral,neutral,neutral
611234607,"Hi @zain-noman 

Can you elaborate a little more on exactly the problem and your solution? I just want to make sure I understand it fully before addressing it.",hi elaborate little exactly problem solution want make sure understand fully,issue,negative,positive,positive,positive,positive,positive
611234098,"Hi @niskander 

The reason for this is that in release v0.15, rewards are averaged over all agents which share a behavior name. So, in zero sum games, this results in the figure you see.  Your concern (raised in the issue [3563](https://github.com/Unity-Technologies/ml-agents/issues/3563)) led us to modify this so that only trajectories collected by the learning agent will be logged in tensorboard.  This is currently on master and will be part of the next release.  

Thank you very much for using this feature and helping us refine it!",hi reason release share behavior name zero sum figure see concern raised issue led u modify collected learning agent logged currently master part next release thank much feature helping u refine,issue,positive,positive,neutral,neutral,positive,positive
611230967,"Hi @ElliotWood 

Have you set different team_ids for both agents?

Also, the asymmetric self-play feature is currently on the master branch and is not yet part of an official release.  It seems you are using the latest release 0.15 which only supports symmetric games. Are you also looking at the documentation on the master branch?

If you are comfortable doing so, I encourage you to try this out on master and let me know how it goes.
",hi set different also asymmetric feature currently master branch yet part official release latest release symmetric also looking documentation master branch comfortable encourage try master let know go,issue,positive,positive,positive,positive,positive,positive
611166356,If you have Unity 2019.1+ you can [disable burst per platform](https://docs.unity3d.com/Packages/com.unity.burst@1.1/manual/index.html#standalone-player-support) in the Unity Project settings. ,unity disable burst per platform unity project,issue,negative,neutral,neutral,neutral,neutral,neutral
611164859,what do you mean?  Do you still get a burst error? Or do you get another error?,mean still get burst error get another error,issue,negative,negative,negative,negative,negative,negative
611074329,"Can you open a new issue using the new template? If this is a bug, we will need to be able to reproduce the bug to help you.",open new issue new template bug need able reproduce bug help,issue,negative,positive,positive,positive,positive,positive
610957308,disabling compilation does not have any effect. So it is impossible to build to Linux from Windows or are there any other workarounds?,compilation effect impossible build,issue,negative,negative,negative,negative,negative,negative
610873183,"@vincentpierre, I tried to change the port on my second call to the environment like this
`env = UnityEnvironment(file_name=""./Banana.app"", worker_id=2)`
But it did not work. Any idea? I'm using Mac OS, not Linux by the way. Thx",tried change port second call environment like work idea mac o way,issue,negative,neutral,neutral,neutral,neutral,neutral
610687536,"Hi @carlos-aguayo, you're right that it won't train without the `--train` parameter on the latest release (0.15.1). We've removed this need in the latest `master` build so if you're using that, no need to use `--train`. 

For the 0.15.1 docs check out this link: https://github.com/Unity-Technologies/ml-agents/blob/0.15.1/docs/Readme.md

Thanks!",hi right wo train without train parameter latest release removed need latest master build need use train check link thanks,issue,negative,positive,positive,positive,positive,positive
610510815,"Correct. But I don't think support for it would be a major burden, we'd just need to check the version and switch which loading code we call.",correct think support would major burden need check version switch loading code call,issue,negative,positive,neutral,neutral,positive,positive
610506466,"Hello @smartcharith 

It looks like you are on an older version of ML-Agents, as we no longer set controls for the `Time Scale` in this window. That being said, it is possible to overwrite these values by setting the time scale yourself in the code https://docs.unity3d.com/ScriptReference/Time-timeScale.html. 

The reason we didn't include values less than one in this slider was because it was only for use in speeding up the simulation for training purposes, not slowing it down. ",hello like older version longer set time scale window said possible overwrite setting time scale code reason include le one slider use speeding simulation training,issue,negative,positive,neutral,neutral,positive,positive
610499475,"> or drop support for version 1

If we ship v1.0 of the SDK with the Demo version set to 1, doesn't this mean that we can't drop support for it? Otherwise, that would be a major breaking change. ",drop support version ship version set mean ca drop support otherwise would major breaking change,issue,negative,negative,negative,negative,negative,negative
610060031,"Good point @ervteng 

I removed the graph and the accompanying text associated with it. ",good point removed graph text associated,issue,negative,positive,positive,positive,positive,positive
610039933,"@awjuliani Got it, I'm OK with using the old WallJump as an example then. But maybe we should remove the training plot altogether, as the type of effect the curriculum would have on the plot is very much dependent on how it's used in the env.  

I think with Soccer we're using curriculum to give a small bonus reward for touching the ball, then remove this bonus later once the Agent gets used to that and can kick the ball in the goal. 
We could do something very similar with Pyramids (or a similarly sparse env), where we have a small incremental reward for a while (e.g. towards the target) but remove it later. It's not as illustrative as changing the wall height in WallJump, for sure. ",got old example maybe remove training plot altogether type effect curriculum would plot much dependent used think soccer curriculum give small bonus reward touching ball remove bonus later agent used kick ball goal could something similar similarly sparse small incremental reward towards target remove later illustrative wall height sure,issue,positive,positive,neutral,neutral,positive,positive
610039382,code changes look ok to me.  Going to run the samples locally before approving,code look going run locally,issue,negative,neutral,neutral,neutral,neutral,neutral
610004870,"Hi @DemyCode 
We modified the Python API and the documentation associated with it. Your PR was taken into account when changing the docs. 
Thank you for your contribution. ",hi python documentation associated taken account thank contribution,issue,negative,neutral,neutral,neutral,neutral,neutral
609947560,"I am up for a brainstorm session on this, though ""a simple environment that cannot be solved (or barely be solved) without curriculum"" may be impossible by definition. ",session though simple environment barely without curriculum may impossible definition,issue,negative,negative,negative,negative,negative,negative
609941213,Maybe we should create an environment that only demonstrates curriculum (it might be confusing if the same environment is used to demonstrate multi-agent and curriculum. Maybe we can create a simple environment that cannot be solved (or barely be solved) without curriculum. ,maybe create environment curriculum might environment used demonstrate curriculum maybe create simple environment barely without curriculum,issue,negative,positive,neutral,neutral,positive,positive
609933143,"Hi @StarWardo 

When it comes to learning via RL, it can often be the case that the agent simply learns to memorize the environment, unless enough variation is included. 

To better discuss this issue, I recommend we move the discussion to the ML-Agents forums, as we are reserving the github issues just to bugs and feature requests: https://forum.unity.com/forums/ml-agents.453/. On the forums we have a large community of users who can provide additional help. ",hi come learning via often case agent simply memorize environment unless enough variation included better discus issue recommend move discussion feature large community provide additional help,issue,positive,positive,positive,positive,positive,positive
609930854,"Hi @ljarasunas2021 

The issue with the tensorboard graphs being rendered strangely is due to you likely not using a unique `run-id` for each training session. If you don't use a novel `run-id` then tensorboard will combine the data collected from every run into a single graph. Try calling `mlagents-learn` with a novel `run-id=something`. Let me know if that resolves your issue.",hi issue strangely due likely unique training session use novel combine data collected every run single graph try calling novel let know issue,issue,negative,positive,neutral,neutral,positive,positive
609928558,"Hello @lostandsupine 

Thank you for making this request. We have actually had a number of people ask for this recently, and we are looking into a way of making this possible. Currently there is some difficulty based on how TensorFlow deals with ""freezing"" models (which is required to produce a `.nn` file), but we know how useful it would be for people, so we want to make it happen. 

We are internally tracking this request with MLA-315. ",hello thank making request actually number people ask recently looking way making possible currently difficulty based freezing produce file know useful would people want make happen internally request,issue,negative,positive,neutral,neutral,positive,positive
609927518,"Hi @Xiromtz 

There are many additional statistics about the training process which are recorded and presented in the TensorBoard interface. See here: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Using-Tensorboard.md for how to use it, and the list of statistics it presents. I believe it should meet your needs. If not, please feel free to reopen this issue.",hi many additional statistic training process interface see use list statistic believe meet need please feel free reopen issue,issue,positive,positive,positive,positive,positive,positive
609925596,Thanks for sharing these versions. Have you been able to make it all the way through this guide without Unity crashing: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started.md  ?,thanks able make way guide without unity,issue,negative,positive,positive,positive,positive,positive
609922957,"Hi @MarcoMeter 

I know it is somewhat disappointing. We were hoping to have an alternative by this time, but things move somewhat slowly when they are part of the Unity Engine itself, rather than a package like ML-Agents.",hi know somewhat disappointing alternative time move somewhat slowly part unity engine rather package like,issue,negative,negative,negative,negative,negative,negative
609921930,"@andrewcoh 

I think that would definitely be helpful. Do you think the curricula you use are demonstrative enough that we can just fully replace the Wall Jump example in the docs with it? ",think would definitely helpful think curriculum use demonstrative enough fully replace wall jump example,issue,positive,neutral,neutral,neutral,neutral,neutral
609097535,I figured it out. You basically have to kill the previous port if you are using tensorboard.,figured basically kill previous port,issue,negative,negative,negative,negative,negative,negative
609036078,"Hi @awjuliani 
I hope that we will be able to omit X in the future, because the clusters that I'm working with do not want to provide X support.

Did you guys take EGL into account and have made any experiences with that as an X alternative?",hi hope able omit future working want provide support take account made alternative,issue,positive,positive,positive,positive,positive,positive
609021362,"> @OneFiveChen
> 
> I just realized that you likely forgot the `--train` argument when launching `mlagents-learn`. That is why `Not Training` is appearing.

this solved my problem",likely forgot train argument training problem,issue,negative,neutral,neutral,neutral,neutral,neutral
609013572,"It turned out that the models/ folder generated during training was outside Assets/, and when I was selecting the model on the Unity Editor I was actually selecting a different model inside Assets/ (that had the same name).

Sorry",turned folder training outside model unity editor actually different model inside name sorry,issue,negative,negative,negative,negative,negative,negative
609010469,"The weird thing is that the training works without raising any error. But if I stop the training, set the generated model in the agent and run, then I start getting the exceptions.",weird thing training work without raising error stop training set model agent run start getting,issue,negative,negative,negative,negative,negative,negative
608952538,"> Hello @canyon
> 
> Can you share what version of Unity you are running?

Sorry, I forgot. It's `Unity 2018.4.19f1` and `Ml-agents 0.15.1`.",hello canyon share version unity running sorry forgot unity,issue,negative,negative,negative,negative,negative,negative
608887028,Closing this for another solution. Please see [#3735 ](https://github.com/Unity-Technologies/ml-agents/pull/3735),another solution please see,issue,positive,neutral,neutral,neutral,neutral,neutral
608883001,I'm using curriculum for both Soccer and CubeWars. Let me know if you want me to update the curriculum doc using Soccer/CubeWars @awjuliani ,curriculum soccer let know want update curriculum doc,issue,negative,neutral,neutral,neutral,neutral,neutral
608652513,"> How did you test the GhostTrainer?

Ran it for half an hour and the ELO went up a bit. Am running a full cloud run of all envs now. ",test ran half hour went bit running full cloud run,issue,negative,positive,neutral,neutral,positive,positive
608626693,@Phong13 apologies for the delay in replying. this feature has been removed from our code base and there is no equivalent of this anymore. I recommend upgrading to the latest version of ML-Agents as we have introduced several new features and many bugs have been fixed.,delay feature removed code base equivalent recommend latest version several new many fixed,issue,negative,positive,neutral,neutral,positive,positive
608621000,">Left a bit of feedback, but didn't get into the details of the Agent processing.

Thanks, I incorporated some of these changes. I want this to be informally approved before I made another PR targeting this one with the docs changes.

> If you want, feel free to comment out the gym yamato test for now, until you get it working with single-agent in the other PR.

I think I will wait on #3725 to be merged and then merge master into this one to fix the missing test.",left bit feedback get agent thanks incorporated want informally made another one want feel free comment gym test get working think wait merge master one fix missing test,issue,positive,positive,positive,positive,positive,positive
608597010,"> This sounds like a bug in the walljump environment to me. This means that we have no way of testing curriculum learning works then do we? Should we fix the environment instead? Or add to the ""known issues""?

It isn't that there is something wrong with the environment. It is just that because we improved the environment and our PPO implementation, it is no longer necessary or beneficial to use a curriculum for this problem.

I know that @andrewcoh is using curriculum for Soccer. I think perhaps we can use that environment to validate/demonstrate curriculum going forward.

In the mean time, the ""old"" WallJump example we use in the documentation is still quite valuable from an educational perspective, as it demonstrates pretty concretely the concept. ",like bug environment way testing curriculum learning work fix environment instead add known something wrong environment environment implementation longer necessary beneficial use curriculum problem know curriculum soccer think perhaps use environment curriculum going forward mean time old example use documentation still quite valuable educational perspective pretty concretely concept,issue,positive,negative,neutral,neutral,negative,negative
608595328,"This sounds like a bug in the walljump environment to me. This means that we have no way of testing curriculum learning works then do we? Should we fix the environment instead? Or add to the ""known issues""?",like bug environment way testing curriculum learning work fix environment instead add known,issue,negative,neutral,neutral,neutral,neutral,neutral
608594576,"> Not sure if that's a typo, or a tennis term I never heard of. Looks good otherwise.

Definitely the former... 😄",sure typo tennis term never good otherwise definitely former,issue,negative,positive,neutral,neutral,positive,positive
608589673,"Unfortunately headless rendering is still not a native part of the Unity linux build process. If you are running on a remote machine, I recommend setting up a virtual x-server on the machine, if possible: https://github.com/Unity-Technologies/obstacle-tower-env/blob/master/examples/gcp_training.md#set-up-xserver.",unfortunately headless rendering still native part unity build process running remote machine recommend setting virtual machine possible,issue,negative,negative,negative,negative,negative,negative
608581383,"Hello @canyon 

Can you share what version of Unity you are running? ",hello canyon share version unity running,issue,negative,neutral,neutral,neutral,neutral,neutral
608580408,"Hi @FinnickWang 

We recommend using a python virtual environment when installing and using ML-Agents because of conflicting dependency issues such as this. If you are new to virtual envs, we have a guide in our documentation here: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Using-Virtual-Environment.md",hi recommend python virtual environment conflicting dependency new virtual guide documentation,issue,negative,positive,positive,positive,positive,positive
608579657,No worries. I can definitely take care of it. Thanks again for bringing it to our attention! ,definitely take care thanks attention,issue,positive,positive,neutral,neutral,positive,positive
608579358,"@ervteng 

The issue is that a number of intermediate improvements to our trainers and to the physics/movement of the agent in WallJump resulted in no curriculum being needed to learn. In fact, it actually learns faster/better without one. ",issue number intermediate agent curriculum learn fact actually without one,issue,negative,neutral,neutral,neutral,neutral,neutral
608400765,Thank you for your reply but I can't find out how to pull request. Could you please do that?,thank reply ca find pull request could please,issue,positive,neutral,neutral,neutral,neutral,neutral
608185910,"Hmm, running the current wall-jump will still result in a similar curve right? I'd be more in favor of just grabbing the last CI run of walljump-ppo-curri and saving the plot :P But otherwise the text looks good",running current still result similar curve right favor last run saving plot otherwise text good,issue,positive,positive,positive,positive,positive,positive
608182259,This has been fixed since release 0.14.0. Now when running inference it will continue to log from where you last started. Feel free to reopen the issue if you're still having issues. Thanks for reporting!,fixed since release running inference continue log last feel free reopen issue still thanks,issue,positive,positive,positive,positive,positive,positive
608181764,"Hi @Dastyn, this issue has been fixed in the latest release 0.15.1; thanks for reporting! Feel free to re-open the issue if you're still having problems. ",hi issue fixed latest release thanks feel free issue still,issue,positive,positive,positive,positive,positive,positive
608113945,"I have created a PR to let users know about the non-replicability of the documentation: https://github.com/Unity-Technologies/ml-agents/pull/3723. As such, I will close this PR, but please add an additional comment or re-open it if you feel it is necessary.",let know documentation close please add additional comment feel necessary,issue,negative,neutral,neutral,neutral,neutral,neutral
608108610,"Hi @davidsonic 

This is a good point you've raised. The truth is that the example we provide with the curriculum learning documentation is based on a very early version of the Wall Jump environment, where it was indeed necessary for a curriculum. Due to improvements in our learning algorithms and changes in the environment itself, the current WallJump actually no longer needs a curriculum to learn, as you have pointed out here. Instead we use WallJump as a means of demonstrating the ability to switch behaviors within a single agent. 

We are currently working on more complex environments that do actually need curriculum, and will share those examples in the future. For now, I will amend the curriculum learning documentation to let folks know that the example we provide there is for instructional purposes, and doesn't correspond to the current WallJump. ",hi good point raised truth example provide curriculum learning documentation based early version wall jump environment indeed necessary curriculum due learning environment current actually longer need curriculum learn pointed instead use ability switch within single agent currently working complex actually need curriculum share future amend curriculum learning documentation let know example provide instructional correspond current,issue,positive,positive,neutral,neutral,positive,positive
608107000,"Hi @ugurkanates 

This is an interesting idea. Given what you've shared though, it sounds more like a general question about Unity's capabilities, rather than an ML-Agents specific feature. It may be best to post this on our forum, where other users can discuss and share their ideas.

https://forum.unity.com/forums/ml-agents.453/

 Just from what you've described, I would imagine that sending image data over TCP to another device would likely reduce many of the gains you would expect from the extra processing power on that device.

I am going to close this issue here, but please feel free to re-open it on our forums.  ",hi interesting idea given though like general question unity rather specific feature may best post forum discus share would imagine sending image data another device would likely reduce many gain would expect extra power device going close issue please feel free,issue,positive,positive,positive,positive,positive,positive
608087420,"Hi @devedse,
We have not been able to find any memory leaks on our end after a few months of testing.  I am going to close this now.  I hope you have resolved your issue.
Cheers.",hi able find memory end testing going close hope resolved issue,issue,positive,positive,positive,positive,positive,positive
608047227,"Hi @Porigon45 

Thank you for bringing this to our attention. Would you be willing to submit a pull request with this change in it? If not, I can take a look at making the change myself.",hi thank attention would willing submit pull request change take look making change,issue,negative,positive,positive,positive,positive,positive
607585618,"Hi @jaros3, this issue has been fixed in the latest release 0.15.1. Thanks for reporting the bug! Closing the issue for now. ",hi issue fixed latest release thanks bug issue,issue,negative,positive,positive,positive,positive,positive
607441685,"Hey @RaStall,
This was fixed in #3703 and was put into the [0.15.0 release](https://github.com/Unity-Technologies/ml-agents/releases/tag/0.15.1).  I will close this for now.  Please re-open this issue if the problem persists.  ",hey fixed put release close please issue problem,issue,negative,positive,neutral,neutral,positive,positive
607432511,"@Parktaeryang 

Can you share a little more about what is still not working for you?",share little still working,issue,negative,negative,negative,negative,negative,negative
607389595,"> Hi @Parktaeryang
> 
> Unless you want to use custom protobuf definitions, it is not necessary to build them yourself. Can you try a fresh installation of the ML-Agents package in a new Unity project?
> 
> From your original error, it is also possible that there is a grpc/protobuf version mismatch between what is expected by TensorFlow and what you installed to rebuild the definitions.

I  tried everything . So I reinstalled everything and changed from 0.15.1 into 0.13.1 it worked. But still there is little problem .",hi unless want use custom necessary build try fresh installation package new unity project original error also possible version mismatch rebuild tried everything everything worked still little problem,issue,negative,positive,positive,positive,positive,positive
607343856,"Hi @Parktaeryang 

Unless you want to use custom protobuf definitions, it is not necessary to build them yourself. Can you try a fresh installation of the ML-Agents package in a new Unity project? 

From your original error, it is also possible that there is a grpc/protobuf version mismatch between what is expected by TensorFlow and what you installed to rebuild the definitions. ",hi unless want use custom necessary build try fresh installation package new unity project original error also possible version mismatch rebuild,issue,negative,positive,positive,positive,positive,positive
606919982,"
And another problem is below
I've done  protobuf  https://github.com/Unity-Technologies/ml-agents/blob/master/protobuf-definitions/README.md 

And i add local package in unity
 
![두번](https://user-images.githubusercontent.com/62952298/78080911-ee278c00-73b7-11ea-8185-9224f577d717.png)
![세번](https://user-images.githubusercontent.com/62952298/78080915-eff14f80-73b7-11ea-8f98-fa577ed67694.jpg)
![google](https://user-images.githubusercontent.com/62952298/78080918-f1227c80-73b7-11ea-96cc-befea4a53696.jpg)


I add reference google.protobuf.dll in that folder but that bug doesn't disappear at all",another problem done add local package unity add reference folder bug disappear,issue,negative,neutral,neutral,neutral,neutral,neutral
606894955,"Hi @Parktaeryang 

Judging from that error, these seems to be an issue with your tensorflow installation. Can you share the version of TensorFlow you installed? ",hi error issue installation share version,issue,negative,neutral,neutral,neutral,neutral,neutral
606864804,"**AND THEN IF I ADD --help Command** 
### output below
**(ml-agents) C:\ml-agents>mlagents-learn --help**
```WARNING:tensorflow:From C:\Users\skygv\anaconda3\envs\ml-agents\lib\site-packages\tensorflow_core\python\compat\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
usage: mlagents-learn [-h] [--env ENV_PATH]
                      [--curriculum CURRICULUM_CONFIG_PATH]
                      [--sampler SAMPLER_FILE_PATH]
                      [--keep-checkpoints KEEP_CHECKPOINTS] [--lesson LESSON]
                      [--load] [--run-id RUN_ID] [--save-freq SAVE_FREQ]
                      [--seed SEED] [--train] [--base-port BASE_PORT]
                      [--num-envs NUM_ENVS]
                      [--docker-target-name DOCKER_TARGET_NAME]
                      [--no-graphics] [--debug] [--env-args ...] [--cpu]
                      [--version] [--width WIDTH] [--height HEIGHT]
                      [--quality-level QUALITY_LEVEL]
                      [--time-scale TIME_SCALE]
                      [--target-frame-rate TARGET_FRAME_RATE]
                      trainer_config_path

positional arguments:
  trainer_config_path

optional arguments:
  -h, --help            show this help message and exit
  --env ENV_PATH        Name of the Unity executable (default: None)
  --curriculum CURRICULUM_CONFIG_PATH
                        Curriculum config yaml file for environment (default:
                        None)
  --sampler SAMPLER_FILE_PATH
                        Reset parameter yaml file for environment (default:
                        None)
  --keep-checkpoints KEEP_CHECKPOINTS
                        How many model checkpoints to keep (default: 5)
  --lesson LESSON       Start learning from this lesson (default: 0)
  --load                Whether to load the model or randomly initialize
                        (default: False)
  --run-id RUN_ID       The directory name for model and summary statistics
                        (default: ppo)
  --save-freq SAVE_FREQ
                        Frequency at which to save model (default: 50000)
  --seed SEED           Random seed used for training (default: -1)
  --train               Whether to train model, or only run inference
                        (default: False)
  --base-port BASE_PORT
                        Base port for environment communication (default:
                        5005)
  --num-envs NUM_ENVS   Number of parallel environments to use for training
                        (default: 1)
  --docker-target-name DOCKER_TARGET_NAME
                        Docker volume to store training-specific files
                        (default: None)
  --no-graphics         Whether to run the environment in no-graphics mode
                        (default: False)
  --debug               Whether to run ML-Agents in debug mode with detailed
                        logging (default: False)
  --env-args ...        Arguments passed to the Unity executable. (default:
                        None)
  --cpu                 Run with CPU only (default: False)
  --version             show program's version number and exit

Engine Configuration:
  --width WIDTH         The width of the executable window of the
                        environment(s) (default: 84)
  --height HEIGHT       The height of the executable window of the
                        environment(s) (default: 84)
  --quality-level QUALITY_LEVEL
                        The quality level of the environment(s) (default: 5)
  --time-scale TIME_SCALE
                        The time scale of the Unity environment(s) (default:
                        20)
  --target-frame-rate TARGET_FRAME_RATE
                        The target frame rate of the Unity environment(s)
                        (default: -1)```",add help command output help warning removed future version long term usage curriculum sampler lesson lesson load seed seed train version width width height height positional optional help show help message exit name unity executable default none curriculum curriculum file environment default none sampler reset parameter file environment default none many model keep default lesson lesson start learning lesson default load whether load model randomly initialize default false directory name model summary statistic default frequency save model default seed seed random seed used training default train whether train model run inference default false base port environment communication default number parallel use training default docker volume store default none whether run environment mode default false whether run mode detailed logging default false unity executable default none run default false version show program version number exit engine configuration width width width executable window environment default height height height executable window environment default quality level environment default time scale unity environment default target frame rate unity environment default,issue,positive,negative,negative,negative,negative,negative
606764798,"Hi @MarkTension,
Thanks for making a formal request.  This feature request has been brought up internally before.  Hopefully, we can get to it sooner rather than later.  We have logged this request internally as MLA-812.  All updates on this request will be posted on this thread.  ",hi thanks making formal request feature request brought internally hopefully get sooner rather later logged request internally request posted thread,issue,positive,positive,neutral,neutral,positive,positive
606755484,"Hi @fredleefarr,
We are only changing the version based on changes in our communication API now.  The current version is correct.  They will no longer follow the release version.  If the API version has caused an issue for you, please open another GitHub issue with the details of the bug.  I am closing this issue for now.  Thank you for your feedback. ",hi version based communication current version correct longer follow release version version issue please open another issue bug issue thank feedback,issue,positive,neutral,neutral,neutral,neutral,neutral
606743984,"How to install Grpc.Tools-Version1.14.1 into specific directory( $MLAGENTS_ROOT\protobuf-definitions)using Anaconda3?? 
I don't know the command Nuget and pip. What should I use and do???
 That is only things I have to do. Please help me 

Navigate to your installation of nuget and run the following:

nuget install Grpc.Tools -Version 1.14.1 -OutputDirectory $MLAGENTS_ROOT\protobuf-definitions",install specific directory anaconda know command pip use please help navigate installation run following install,issue,positive,neutral,neutral,neutral,neutral,neutral
606138268,@vincentpierre Okay. But we should make a Jira ticket for setting up a Colab notebook.,make ticket setting notebook,issue,negative,neutral,neutral,neutral,neutral,neutral
606135259,"Isn't that what https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Python-API.md and https://github.com/Unity-Technologies/ml-agents/blob/master/gym-unity/README.md are for. If we just move this code around, we will be facing the same problem : we cannot test this code.",move code around facing problem test code,issue,negative,neutral,neutral,neutral,neutral,neutral
606129404,"My preference would be that we have something to replace these with. At the very least, ensure that the code is present within the docs. ",preference would something replace least ensure code present within,issue,negative,negative,negative,negative,negative,negative
605417391,"https://issuetracker.unity3d.com/issues/task-dot-rasterdata-dot-vertexbuffer-equals-equals-null-errors-when-using-a-particle-system?_ga=2.13234228.269379034.1585150652-1092474247.1569082073

please vote for it, it should help bring it on the top of the pile",please vote help bring top pile,issue,positive,positive,positive,positive,positive,positive
605365612,"By test, you mean testing or letting our users try it out? If it is testing, I think we should have a better way to do it. For letting our users try it out, I think we should have something but I don't know what yet. ",test mean testing try testing think better way try think something know yet,issue,negative,positive,neutral,neutral,positive,positive
605360074,I don't love notebooks in general. But I worry that this doesn't give us any way (that's under source control) to test the LL API or gym interface.,love general worry give u way source control test gym interface,issue,negative,positive,positive,positive,positive,positive
605141728,Thanks for the feedback.  This is being looked into today.  We’ll update you when it’s fixed.,thanks feedback today update fixed,issue,negative,positive,positive,positive,positive,positive
605097160,@surfnerd Aah Daim. You're right. I'm foolish and accidentally added a watch for this. Thank you for pointing out,right foolish accidentally added watch thank pointing,issue,negative,positive,positive,positive,positive,positive
605081031,"Hi @marktension,
We don’t instantiate your agents for you.  I’m not sure why there are two `this` variables in your debugger.  Did you add a watch for `this`?  Could you add breakpoints in your initialize method in your Agent?  

Are you using the gym wrapper in python?",hi sure two add watch could add initialize method agent gym wrapper python,issue,negative,positive,positive,positive,positive,positive
604748015,"Yeah, I was able to get it running by pressing play after launching mlagents-learn, but it's really confusing without the print, as most tutorials state that I should wait for ""Start training by pressing the Play button in the Unity Editor."" print in the console.",yeah able get running pressing play really without print state wait start training pressing play button unity editor print console,issue,positive,positive,positive,positive,positive,positive
604743319,"For this error:
```
File ""\workspace\ml-agents\gym-unity\gym_unity\envs\__init__.py"", line 531, in mark_agent_done
    gym_index = self._agent_id_to_gym_index.pop(agent_id)
KeyError: 7
```

We found a bug in the gym wrapper which would appear if an Agent called EndEpisode multiple times in the same step. The fix for this is in https://github.com/Unity-Technologies/ml-agents/pull/3700, and we'll be doing a patch release next week that contains this fix (and a few others).",error file line found bug gym wrapper would appear agent multiple time step fix patch release next week fix,issue,negative,neutral,neutral,neutral,neutral,neutral
604730384,Good. Thanks for the quick answer @surfnerd !  Looking forward for an update.  We could be available for testing that if needed in the future. ,good thanks quick answer looking forward update could available testing future,issue,positive,positive,positive,positive,positive,positive
604690894,"I see the print out is still there 
https://github.com/Unity-Technologies/ml-agents/blob/7507a5d3f5515ae7877eb3a3e9abaa2e2a270930/ml-agents-envs/mlagents_envs/environment.py#L132

If you press play in the editor after you launch mlagents-learn, you should see it work.",see print still press play editor launch see work,issue,negative,neutral,neutral,neutral,neutral,neutral
604688725,"It used to, maybe it was removed.  Did you get it to work?",used maybe removed get work,issue,negative,neutral,neutral,neutral,neutral,neutral
604687959,"Hi @GProulx, 
We have an internal repo that we aren't ready to share yet.  The timeline is still up in the air since we are getting ready to release the current package through Unity's Package Manager.   We will definitely update this thread when we have more information to share. ",hi internal ready share yet still air since getting ready release current package unity package manager definitely update thread information share,issue,positive,positive,neutral,neutral,positive,positive
604678014,"> Not sure, if we want to demonstrate the use of GetSideChannels we would need to have a way to distinguish the FloatPropertiesChannel obtained and we do not have way to do this...

If we don't have one, neither do our users.",sure want demonstrate use would need way distinguish way one neither,issue,negative,positive,positive,positive,positive,positive
604659033,"@chriselion , just a very very! late update on that topic to mention you that the new Side Channel replacement of the CustomObservation works even better that the latter for us!  Thanks!",late update topic mention new side channel replacement work even better latter u thanks,issue,positive,positive,positive,positive,positive,positive
604651440,"Hi @unityjeffrey and @surfnerd !

We are also looking to move to DOTS while using ml-agents. Can you give us an update on where your are now with that topic on your side? Have you seen any hybrid DOTS/non-DOTS implementations that used ml-agents?",hi also looking move give u update topic side seen hybrid used,issue,negative,neutral,neutral,neutral,neutral,neutral
604624394,">General comment: since it's possible to have multiple FloatPropertiesChannels, should we at least one example of using Academy.Instance.GetSideChannels<FloatPropertiesChannel>()[0]?

Not sure, if we want to demonstrate the use of `GetSideChannels` we would need to have a way to distinguish the FloatPropertiesChannel obtained and we do not have way to do this...",general comment since possible multiple least one example sure want demonstrate use would need way distinguish way,issue,negative,positive,neutral,neutral,positive,positive
604609170,"`DecisionRequester` is on our list of things to review too, but we missed the cutoff for the last release.

`DecisionRequester` just calls `Agent.RequestDecision()` at a regular frequency; this is public and you can call it on the steps you want your Agent to observe and act.",list review cutoff last release regular frequency public call want agent observe act,issue,negative,neutral,neutral,neutral,neutral,neutral
604600907,"General comment: since it's possible to have multiple FloatPropertiesChannels, should we at least one example of using `Academy.Instance.GetSideChannels<FloatPropertiesChannel>()[0]`?

",general comment since possible multiple least one example,issue,negative,negative,neutral,neutral,negative,negative
604599110,"Finally got back to this (external events are intervening), and I got a lot further, but now it seems I need to add a ""**DecisionRequester**"" to my agent, and that is an **internal** type. So I am not quite able to automate my agent creation yet. Without that it just sits there...",finally got back external got lot need add agent internal type quite able agent creation yet without,issue,negative,positive,neutral,neutral,positive,positive
604594497,Shouldn't mlagents display a message in the console with the prompt to Press play? ,display message console prompt press play,issue,negative,neutral,neutral,neutral,neutral,neutral
604568290,I tried just pressing play in the Unity editor when I was waiting for the timeout and it began training.,tried pressing play unity editor waiting training,issue,negative,neutral,neutral,neutral,neutral,neutral
604552977,"@vincentpierre followup from the meeting - 0.14 used the old format for the communication protocol versioning, so there's no expectation that it'll work with master (or 0.15):
https://github.com/Unity-Technologies/ml-agents/blob/0.14.0/ml-agents-envs/mlagents_envs/environment.py#L54",meeting used old format communication protocol expectation work master,issue,negative,positive,neutral,neutral,positive,positive
604528017,"Hi,
Burst cross-compilation (in your case from Windows to Linux) does not work with the version that you have installed.  This is installed through our dependency on the Barracuda package.
This inability to cross-compile is a known limitation of the Burst package at the version installed.  

You can disable burst compilation from the `Jobs -> Burst` menu to work around this issue.

![Screen Shot 2020-03-26 at 9 22 18 AM](https://user-images.githubusercontent.com/1356616/77670362-56531980-6f43-11ea-9c56-6af8638f0994.png)

I am going to close this for now as this is not an issue with ML-Agents. 
",hi burst case work version dependency barracuda package inability known limitation burst package version disable burst compilation burst menu work around issue screen shot going close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
604161816,"Confirmed that 0.14.0 versions of C# and python failed (although not quite the way I would have expected):
https://github.com/Unity-Technologies/ml-agents/pull/3693
https://github.com/Unity-Technologies/ml-agents/pull/3694",confirmed python although quite way would,issue,negative,positive,positive,positive,positive,positive
604137155,"Hey @sniffle6, 
Could you try this on master?  It may be fixed there.  There will be a hotfix release coming soon.",hey sniffle could try master may fixed release coming soon,issue,negative,positive,neutral,neutral,positive,positive
604130499,"Hi @RaStall, 
I'll try to reproduce your error.",hi try reproduce error,issue,negative,neutral,neutral,neutral,neutral,neutral
604034745,"I'm looking forward to it! You guys have done an outstanding job. I hope we'll get a lot of updated examples and maybe video tutorials for v1 once you have a stable release out! I know that you guys have been constantly working on this, so maintaining up-to-date tutorials would be quite challenging and wasteful when you're modifying it all the time. Super excited. You guys have helped developers of all ages comes closer to understanding the fundamentals of machine learning- if they didn't already. Thanks for all your guys's hard work.",looking forward done outstanding job hope get lot maybe video stable release know constantly working would quite wasteful time super excited come closer understanding machine already thanks hard work,issue,positive,positive,positive,positive,positive,positive
604031802,"> Hi @Blueeyesjt1,
> Am I still supposed to have some type of SDK in my assets folder?
> No, you shouldn't need anything except for the package.
> Couple of questions:
> Can you confirm that you're on the 0.15.0 release of ml-agents? That was tagged with `0.15.0` in github, and the SHA (from running `git log`) should be `7507a5d3f5515ae7877eb3a3e9abaa2e2a270930`
> ...

Thanks for the response. I apologize for my own late reply. I, unfortunately, switched back to v13 prior to your response because of just my knowledge in using it. Once another version comes out- which I believe is version 1, I'll try to update again. Thank you.",hi still supposed type asset folder need anything except package couple confirm release tagged sha running git log thanks response apologize late reply unfortunately switched back prior response knowledge another version come believe version try update thank,issue,positive,negative,negative,negative,negative,negative
603980574,"This was implemented in https://github.com/Unity-Technologies/ml-agents/pull/3660/files - it's available on the master branch now, and will be in the next release.",available master branch next release,issue,negative,positive,positive,positive,positive,positive
603624877,"I have fixed it, the problem was with the path
**config/trainer_config.yaml**

instead, I manually searched for the file and dragged it to the terminal for a correct path, being as follows:
`mlagents-learn /Users/macos/ml-agents/config/trainer_config.yaml --run-id=firstRun --train`

3DBall was trained! (I struggled for days with this problem)

thanks",fixed problem path instead manually file dragged terminal correct path train trained day problem thanks,issue,negative,positive,positive,positive,positive,positive
603604226,"I'm having this issue but I have only installed ml-agents via Pypi in the instructions. I haven't tried the ones included in the repo.

Edit: I couldn't get it to work with the alternate installation method: pip3 install -e ./ml-agents and ml-agents-env

I will try to make a fresh python env and retry all that.

Edit: a fresh python environment didn't work via PyPi",issue via tried included edit could get work alternate installation method pip install try make fresh python retry edit fresh python environment work via,issue,positive,positive,positive,positive,positive,positive
603579559,"Hi @MikeWise2718,
This was implemented in https://github.com/Unity-Technologies/ml-agents/pull/3660 and I think it should be equivalent to your PR in terms of interface and end results.
",hi think equivalent interface end,issue,negative,neutral,neutral,neutral,neutral,neutral
603574264,"FWIW here's FoodCollector with the `contribute` flag set to true everywhere: 
![image](https://user-images.githubusercontent.com/6877802/77489318-0c5c1d80-6df5-11ea-910b-3717d9611fae.png)
",contribute flag set true everywhere image,issue,negative,positive,positive,positive,positive,positive
603561775,"Sorry for the delay - no-meeting Monday -> all-meeting Tuesday.

We haven't started working on this, and to be honest it's not at the top of our list right now (especially since you already have a test case set up). If you want to try it out and submit a PR, that would be great.

I think the parts of the code I linked to before should be enough for you to get started; you basically should be able to 
* pick the TextureFormat for Texture2D based on compression type and the Camera's RenderTexture (or add a new flag to toggle between the current behavior and full float precision)
* extend (or duplicate) TextureToTensorProxy as needed to get the values as floats",sorry delay working honest top list right especially since already test case set want try submit would great think code linked enough get basically able pick based compression type camera add new flag toggle current behavior full float precision extend duplicate get,issue,positive,positive,positive,positive,positive,positive
603559358,"@ervteng Good timing :-)

Just pushed a test showing we raise the exception from SubprocessEnvManager.",good timing test showing raise exception,issue,negative,positive,positive,positive,positive,positive
603558031,Looks good to me - should we add a test that makes sure the exception is passed correctly up?,good add test sure exception correctly,issue,positive,positive,positive,positive,positive,positive
603557854,"Hi @ho4040,

I recently merged the Basic and Getting Started Guides. I can make the suggested change in the new document. Thanks for pointing it out here. ",hi ho recently basic getting make change new document thanks pointing,issue,negative,positive,positive,positive,positive,positive
603532674,I like that solution @chriselion.  I've made the change.,like solution made change,issue,positive,neutral,neutral,neutral,neutral,neutral
603526228,"This looks good as it is, but just wanted to give one other possible approach:
* make _close() take a timeout parameter. This can default to None and use self.timeout_wait if the actual value is None
* pass 0 timeout in the cases we want to close immediately (or make _close_now() which calls _close() with 0 timeout). I think this should happen everywhere except for `close()` (so mismatched API, connection timeout, and invalid launch_string all close immediately)
* Keep `close()` using self.timeout_wait",good give one possible approach make take parameter default none use actual value none pas want close immediately make think happen everywhere except close connection invalid close immediately keep close,issue,negative,positive,positive,positive,positive,positive
603492609,"Coming back to this, Unity 2019.3.6f1 is out. I added the line below to my `manifest.json` and it seems to work like any other package :)

    ""com.unity.ml-agents"": ""https://github.com/Unity-Technologies/ml-agents.git?path=/com.unity.ml-agents#7507a5d3f5515ae7877eb3a3e9abaa2e2a270930"",",coming back unity added line work like package,issue,negative,neutral,neutral,neutral,neutral,neutral
603403679,"Hi,
In theory, this should be possible using a [`StackingSensor`](https://github.com/Unity-Technologies/ml-agents/blob/0.15.0/com.unity.ml-agents/Runtime/Sensors/StackingSensor.cs) wrapped around a [`VectorSensor`](https://github.com/Unity-Technologies/ml-agents/blob/0.15.0/com.unity.ml-agents/Runtime/Sensors/VectorSensor.cs). Unfortunately, I think I went overboard cleaning up the API and made `StackingSensor` `internal`, so you can't do this at the moment.

I've got this logged as MLA-792 in our internal tracker, and will try to get it cleaned up soon.",hi theory possible wrapped around unfortunately think went overboard cleaning made internal ca moment got logged internal tracker try get soon,issue,negative,negative,negative,negative,negative,negative
603359942,"Hi @jaros3, this is indeed an inconsistency between our docs and the code. I've logged it with internal tracking number MLA-790, and will fix it shortly. 

In the meantime, the default type is `simple`, so if you put `vis_encode_type: simple` it should have the same behavior as not putting anything. Thanks!",hi indeed inconsistency code logged internal number fix shortly default type simple put simple behavior anything thanks,issue,negative,positive,neutral,neutral,positive,positive
603325826,"Interestingly, I have the exact same use case! Would love to see this implementation as well. Counting down the days when it is ready.",interestingly exact use case would love see implementation well counting day ready,issue,positive,positive,positive,positive,positive,positive
603146949,"Sorry for late reply. I had a couple of fires to put out.

I am no unity developer either. Have used Unity just as much as ml-agents :) But I think that your suggestions would be enough. 

For rendering depth buffer I refer to a shader in [ML-ImageSynthesis repo](https://bitbucket.org/Unity-Technologies/ml-imagesynthesis/src/master/Assets/ImageSynthesis/Shaders/UberReplacement.shader).  With that shader it is possible to render depth buffer, normals and or semantic segmented images. 

The shader uses the macro `COMPUTE_DEPTH_01`. More similar macros exists here https://docs.unity3d.com/Manual/SL-DepthTextures.html .

Have you started work on this?/ Do you have time for it? I am wondering if I should try to make a contribution.",sorry late reply couple put unity developer either used unity much think would enough rendering depth buffer refer shader shader possible render depth buffer semantic segmented shader macro similar work time wondering try make contribution,issue,negative,negative,negative,negative,negative,negative
603012427,"Much better now, old one would have hit 8GB by now: 

![image](https://user-images.githubusercontent.com/5085265/77389205-b97c5a80-6d4f-11ea-8565-f6d2e1257646.png)
",much better old one would hit image,issue,negative,positive,positive,positive,positive,positive
602956319,"Unfortunately no. But I have something that is good enough.

In my case, I believe it happened when my agent got out of bounds and didn't reset properly. It was going out of bound because my implementation was acting weird when I push the simulation speed very high.

Currently, it happens only every 10-100 episodes, so when it does I just catch it and... pass.",unfortunately something good enough case believe agent got reset properly going bound implementation acting weird push simulation speed high currently every catch pas,issue,negative,negative,neutral,neutral,negative,negative
602938644,"Mind taking another look at this? It changed a bit, mostly around the multiple aggregation types.",mind taking another look bit mostly around multiple aggregation,issue,negative,positive,positive,positive,positive,positive
602938409,"> I wonder how you got the graph for the number of times the ball got dropped. Are you keeping trak of the total number in C#?

I hacked that as a static for testing :)",wonder got graph number time ball got keeping total number hacked static testing,issue,negative,positive,positive,positive,positive,positive
602937840,"Downsides - not that I know of. I suggested a wider range since some common tools such as tensorboard use a default of 6006 and some other services like jupyter/spark/yarn/airflow use ports in the 8000 range.

5004-5050 sounds reasonable too.",know range since common use default like use range reasonable,issue,negative,negative,neutral,neutral,negative,negative
602915654,Is there any downside to opening up such a wide range? How about 5004-5050 to allow for the editor or several environments?,downside opening wide range allow editor several,issue,negative,negative,neutral,neutral,negative,negative
602912622,@chriselion My recommendation would be to open a broader range of ports. In my use case I often run multiple mlagents-learn with the same executable in a docker container so have a bigger range of ports to be open is useful.,recommendation would open range use case often run multiple executable docker container bigger range open useful,issue,negative,positive,neutral,neutral,positive,positive
602882129,"Hi,
I believe you're right; we currently default the base_port to 5005, regardless of whether an environment path is set or not. A better approach would be
*default base_port to None
*if base_port is None, use 5005 if file_name is specified, else DEFAULT_EDITOR_PORT (5004)

We have this logged in our internal tracker as MLA-709; I'll try to get a fix in for it soon...",hi believe right currently default regardless whether environment path set better approach would default none none use else logged internal tracker try get fix soon,issue,negative,positive,positive,positive,positive,positive
602862824,"@ankur-00007 We haven't used `.bytes` files in a long time; I believe that was for using tensorflow sharp. You should refer to the documentation for the version of ML-Agents that you're using, or upgrade to the latest version and follow its documentation.",used long time believe sharp refer documentation version upgrade latest version follow documentation,issue,negative,positive,positive,positive,positive,positive
602855254,Going to run another cloud run with the Python fix #3671. If it works we can merge this and it would likely need to be in a hotfix 0.15.1. ,going run another cloud run python fix work merge would likely need,issue,negative,neutral,neutral,neutral,neutral,neutral
602850550,"Hi @Blueeyesjt1,

> Am I still supposed to have some type of SDK in my assets folder? 

No, you shouldn't need anything except for the package.

Couple of questions:
Can you confirm that you're on the 0.15.0 release of ml-agents? That was tagged with `0.15.0` in github, and the SHA (from running `git log`) should be `7507a5d3f5515ae7877eb3a3e9abaa2e2a270930`

Does your project's manifest.json have something like `""com.unity.ml-agents"": ""file:../../com.unity.ml-agents"",`  in it? (the exact path might obviously be different).

Are there any other compiler errors? From the screenshot you posted, it looks like the ML-Agents assemblies might not have been compiled yet.

Can you try right clicking on the `ML Agents` package and click ""reimport"" (or right click anywhere in the explorer and try ""Reimport All"")?

You mentioned installing Barracuda, but this shouldn't be necessary anymore since it's a dependency of the ML-Agents package.

One other possible solution is to delete the Library folder from your project, just in case something is stale there.",hi still supposed type asset folder need anything except package couple confirm release tagged sha running git log project something like file exact path might obviously different compiler posted like might yet try right package click reimport right click anywhere explorer try reimport barracuda necessary since dependency package one possible solution delete library folder project case something stale,issue,positive,positive,neutral,neutral,positive,positive
602724736,"This touches on the release 0.15 migration docs. Should this be a hotfix onto 0.15 ? Does this mean it will only be fixed in 0.15.1 ?
",release migration onto mean fixed,issue,negative,negative,negative,negative,negative,negative
602558187,"Hi @batu ,
Did you fix it? I got the same error when I use gym-unity-0.15.0. Looking forward to your reply. Thank you.",hi fix got error use looking forward reply thank,issue,negative,neutral,neutral,neutral,neutral,neutral
602179920,"Maybe take that to the [mlagents unity forum](https://forum.unity.com/forums/ml-agents.453/) and specify what command, and kind of model. Custom model, or ml-agent generated etc. G'luck",maybe take unity forum specify command kind model custom model,issue,positive,positive,positive,positive,positive,positive
602160746,"I am not able to convert my .pb file to bytes using any of the above suggestions. My tf version is 1.15,,, request you to please guide which git branch i should take,,, have tried with master, 0.15 already,,

I am getting the same ""Assertion Error "" as detailed above",able convert file version request please guide git branch take tried master already getting assertion error detailed,issue,negative,positive,positive,positive,positive,positive
602096069,"Thanks for catching this. It's in the [changelog](https://github.com/Unity-Technologies/ml-agents/blob/master/com.unity.ml-agents/CHANGELOG.md) (""Automatic stepping for Academy is now controlled from the AutomaticSteppingEnabled property"") but I forgot to put it in the Migration Guide.",thanks catching automatic stepping academy property forgot put migration guide,issue,negative,positive,positive,positive,positive,positive
601977235,"It's still leaking, thought for sure this would fix the issue. Gonna look into the Python side and see what's going on there
",still thought sure would fix issue gon na look python side see going,issue,negative,positive,positive,positive,positive,positive
601671301,"@ervteng  I'll try with example as soon as i can. Meanwhile, here is the demo file :
[ScrollerJim4.zip](https://github.com/Unity-Technologies/ml-agents/files/4359425/ScrollerJim4.zip)
",try example soon meanwhile file,issue,negative,neutral,neutral,neutral,neutral,neutral
601425493,"Hi @JimZeeKing, can you send us the .demo file that is causing the issue? If you don't feel comfortable attaching it here you can send it to ml-agents at unity3d.com.

Does the same issue happen when recording and using demos from the example environments? I'd try 3d ball as that seems the most similar. ",hi send u file causing issue feel comfortable send issue happen recording demo example try ball similar,issue,positive,positive,positive,positive,positive,positive
601420085,"Hi @MikeWise2718,
Sorry for the delay, as you can imagine things are a bit crazy right now, and it took a while to get the group together to discuss how to handle this.

I'm going to start working on a sidechannel approach for this; hopefully I'll have something to review today or tomorrow.",hi sorry delay imagine bit crazy right took get group together discus handle going start working approach hopefully something review today tomorrow,issue,negative,negative,negative,negative,negative,negative
601325320,"> You have added uncompressed visual observations. But as I understand the data are still uint8?.

It depends on how you're using it. If you create your own ISensor that uses SensorCompressionType.None and produces float ""visual"" (len(shape) == 3) observations, no additional quantization will be performed. We have a simple example of this here https://github.com/Unity-Technologies/ml-agents/blob/7507a5d3f5515ae7877eb3a3e9abaa2e2a270930/com.unity.ml-agents/Tests/Editor/Sensor/FloatVisualSensorTests.cs#L6
and it should be straightforward to extend to more channels. But you're correct that setting the CompressionType on a CameraSensor to None will still force things to uint8 first.

I'll admit that I know very little about rendering in Unity (and I don't think anyone else on the team is an expert either), so you might need to walk me through this a bit :) It looks like we'd need to change the texture format here:
https://github.com/Unity-Technologies/ml-agents/blob/7507a5d3f5515ae7877eb3a3e9abaa2e2a270930/com.unity.ml-agents/Runtime/Sensors/CameraSensor.cs#L128
to either `RFloat` or `RGBAFloat` depending on the `grayscale` flag.

Then when converting the texture to an observation, we'd need to change this part of the code
https://github.com/Unity-Technologies/ml-agents/blob/7507a5d3f5515ae7877eb3a3e9abaa2e2a270930/com.unity.ml-agents/Runtime/Utilities.cs#L31
to use `GetPixels()` (and obviously not divide by 255). Does that cover it? Anything I'm missing?

BTW, do you know of any examples of how to set up rendering using the depth buffer? It's come up a few times before so it might be good for us to have an example of it.",added uncompressed visual understand data still create float visual shape additional quantization simple example straightforward extend correct setting none still force first admit know little rendering unity think anyone else team expert either might need walk bit like need change texture format either depending flag converting texture observation need change part code use obviously divide cover anything missing know set rendering depth buffer come time might good u example,issue,positive,positive,positive,positive,positive,positive
601303518,You should be able to add `--env-args -batchmode` to the commandline to pass the argument to the executable (and any other arguments to want).,able add pas argument executable want,issue,negative,positive,positive,positive,positive,positive
600887889,"> I didn't see domain randomization :'(

Oops. Just added it. ",see domain randomization added,issue,negative,neutral,neutral,neutral,neutral,neutral
600748084,"Hi @dlindmark, I did see similar problems when my machine was close to running out of RAM, so that might be it. I think what was happening was memory was running out, the Unity environment quits, and then the Python code tries to save the replay buffer - causing the error. If 20x20 is too small, I'd recommend reducing the replay buffer size to around 50-100k - that should help as well and usually isn't too detrimental to training. ",hi see similar machine close running ram might think happening memory running unity environment quits python code save replay buffer causing error small recommend reducing replay buffer size around help well usually detrimental training,issue,positive,negative,negative,negative,negative,negative
600728426,"No, you're not missing something -  the documentation was out of date. This was fixed on master a few days ago: https://github.com/Unity-Technologies/ml-agents/pull/3568  but is not in a release yet (soon though).

Anywhere it says to check the ""Use Heuristic"" checkbox, you should set the ""Behavior Type"" to ""Heuristic Only"".",missing something documentation date fixed master day ago release yet soon though anywhere check use heuristic set behavior type heuristic,issue,negative,negative,neutral,neutral,negative,negative
600551255,Have not really had time to look into this. But I have completed trainings while saving `replay_buffer` now. It have not been a problem since decreasing the resolution from 64x64 to the minimum 20x20. So seems likely that the process just crashed because my desktop ran out of memory. Or somehow related anyway.,really time look saving problem since decreasing resolution minimum likely process ran memory somehow related anyway,issue,negative,positive,neutral,neutral,positive,positive
600480242,"> 
> 
> @jdP19 sorry, can you clarify. In your first message, it seemed the communication from unity to python was not working. Now, it's python to unity?
> 
> Is the debug log in the editor completely empty?

I wrote something wrong, the way from python to unity is working, but not from untiy to python !!! 
It only show the error messages: `Plugins: Failed to load 'libcoreclr' because one or more of its dependencies could not be loaded.`",sorry clarify first message communication unity python working python unity log editor completely empty wrote something wrong way python unity working python show error load one could,issue,negative,negative,negative,negative,negative,negative
600151650,"@jdP19 sorry, can you clarify.  In your first message, it seemed the communication from unity to python was not working.  Now, it's python to unity?

Is the debug log in the editor completely empty?",sorry clarify first message communication unity python working python unity log editor completely empty,issue,negative,negative,negative,negative,negative,negative
600091117,"I know it's almost been two months since the last comment, but I wanted to pitch in. I'm having a similar issue with my agents and I'm still trying to figure out exactly what is causing them to not learn properly. Anyways I've had to go back checkpoints and the way I'm doing it is by going into where the checkpoints are saved deep in the models folder. I open the checkpoint file with notepad, delete the checkpoints until the one I want and change the pointer to the one I want. Then I delete the checkpoint files in the folder that I don't need. Then I resume training with the --load parameter and once the Unity window opens up I end the training to build the new nn model. Of course before all this I backup the models and summaries folder just to be safe.",know almost two since last comment pitch similar issue still trying figure exactly causing learn properly anyways go back way going saved deep folder open file delete one want change pointer one want delete folder need resume training load parameter unity window end training build new model course backup folder safe,issue,positive,positive,neutral,neutral,positive,positive
599915826,"> 
> 
> Hi @jdP19 @NaiveRed
> 
> Are you all actually experiencing issues/crashes or is it just popping up in the logs? Is the training proceeding as expected?

It is popping up in the logs, the training proceeds expectingly. But the problem is the side channels are not working when you get this issues popping up. I used the custom side channel example provided in the repo, where the unity logs are piped to python and the python logs are piped to unity. The exchange of messages from python to unity is not working. 


> 
> 
> Also, @jdP19 are you on the latest release (v0.14.1) of C# and python? If not, does this issue persist when you switch?

Yes the issue persist when i use the v0.14.1 and the newest devoplement of the ml-agents repo.",hi actually training proceeding training proceeds expectingly problem side working get used custom side channel example provided unity piped python python piped unity exchange python unity working also latest release python issue persist switch yes issue persist use,issue,negative,positive,positive,positive,positive,positive
599912786,@andrewcoh It just pops up in the log and I think it works fine in the training process. (no other errors),log think work fine training process,issue,negative,positive,positive,positive,positive,positive
599909148,"> Hello~~ yes. there's no something wrong as I know. I don't know about this problem until now. but I just change the way.. and I solved the problem. In stead of importing the ML-Agents asset in my project, I started my project from ML-Agents already imported. 2019년 2월 22일 (금) 오후 8:01, asdf2154s <notifications@github.com>님이 작성:
> […](#)
> Hi, I have same problem. I am trying to add MLAgents to this <https://github.com/Armour/Multiplayer-FPS> project. I tried this steps: 1. Selected Scripting Runtime Version to .NET 4.x Equivalent 2. Added ENABLE_TENSORFLOW to the Scripting Define Symbols 3. Drag the ML-Agents and Gizmos folders from UnitySDK/Assets to the Unity Editor Project window. I also tried: 1. Selected Scripting Runtime Version to .NET 4.x Equivalent 2. Added ENABLE_TENSORFLOW to the Scripting Define Symbols 3. Imported TensorFlowSharp Plugin Is there something that I did wrong? — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#1121 (comment)](https://github.com/Unity-Technologies/ml-agents/issues/1121#issuecomment-466359528)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AkMMFFAxnn7-uCqImMICUNvx5KDI22w_ks5vP84hgaJpZM4WKslL> .
Can you tell how you imported the ML agents beforehand ?
",yes something wrong know know problem change way problem stead asset project project already hi problem trying add project tried selected version equivalent added define drag unity editor project window also tried selected version equivalent added define something wrong reply directly view comment mute thread tell beforehand,issue,negative,negative,negative,negative,negative,negative
599768488,I'm currently expecting the most recent commit to fail - I set the coverage % to 80 to make sure that the exit(1) actually causes yamato to fail,currently recent commit fail set coverage make sure exit actually fail,issue,negative,negative,negative,negative,negative,negative
599766350,"```
################################### Running next command ###################################
 python ml-agents/tests/yamato/check_coverage_percent.py upm-ci~/test-results/ 71 
############################################################################################
[21:18:51.073 INF] Coverage 73.9 is above the min percentage of 71.0.
```",running next command python coverage min percentage,issue,negative,neutral,neutral,neutral,neutral,neutral
599696142,"Also, @jdP19 are you on the latest release (v0.14.1) of C# and python?  If not, does this issue persist when you switch?",also latest release python issue persist switch,issue,negative,positive,positive,positive,positive,positive
599670821,"Hi @jdP19 @NaiveRed 

Are you all actually experiencing issues/crashes or is it just popping up in the logs? Is the training proceeding as expected?",hi actually training proceeding,issue,negative,neutral,neutral,neutral,neutral,neutral
599664273,"Hello @andrewcoh 

Thanks for the comment!

I will stick to traditional navigation agents as hunters for now. ",hello thanks comment stick traditional navigation,issue,negative,positive,neutral,neutral,positive,positive
599648874,"Hi @guillefix 

Implementing a robust command line option to support this feature has been in consideration for a while and is currently underway.",hi robust command line option support feature consideration currently underway,issue,positive,neutral,neutral,neutral,neutral,neutral
599638957,"Hi @StarWardo 

The game you describe is an example of an asymmetric game which unfortunately the current implementation of self-play does not support.  It is high priority to extend the implementation to support this case. Keep an eye out over the next few weeks!",hi game describe example asymmetric game unfortunately current implementation support high priority extend implementation support case keep eye next,issue,positive,negative,negative,negative,negative,negative
599522753,"Technically you could access Unity graphics context from the native rendering plugin and pass it to the other app, but for the starters it would be easier just to grab compute buffer data to the CPU memory and share that memory region with other application.",technically could access unity graphic context native rendering pas would easier grab compute buffer data memory share memory region application,issue,positive,neutral,neutral,neutral,neutral,neutral
599287263,"Please use the 0.14.1 version of the documentation: https://github.com/Unity-Technologies/ml-agents/tree/0.14.1/docs. The version on the master branch is more recent, and not compatible with the 0.14.1 code.",please use version documentation version master branch recent compatible code,issue,negative,neutral,neutral,neutral,neutral,neutral
599286992,"@muafakul This issue has been fixed for several releases; I would recommend upgrading to the latest release (0.14.1).

In general, this problem happened after another exception was raised; you should look at the first error in the logs and try to resolve that first.",issue fixed several would recommend latest release general problem another exception raised look first error try resolve first,issue,negative,positive,positive,positive,positive,positive
599286739,"Hi @Ranxi,
For models that weren't created by ML Agents, please report any problems on the barracuda repo's issue page: https://github.com/Unity-Technologies/barracuda-release/issues

You might want to also try using [tensorflow2onnx](https://github.com/onnx/tensorflow-onnx) to convert the model to ONNX format, since the barracuda package can load ONNX files.",hi please report barracuda issue page might want also try convert model format since barracuda package load,issue,negative,neutral,neutral,neutral,neutral,neutral
599282507,"You can resume training with the --load flag.  As it's implemented, --load will set the step count to whatever it was when the previous training session was cancelled.  So, to continue training after you've reached a max step, you can increase the max step in the trainer config, and also run with --load and the same run id.",resume training load flag load set step count whatever previous training session continue training step increase step trainer also run load run id,issue,negative,negative,negative,negative,negative,negative
599182197,I think this method still works: https://github.com/Unity-Technologies/ml-agents/issues/1047,think method still work,issue,negative,neutral,neutral,neutral,neutral,neutral
599079012,"Thanks for taking a look.

Is it about using the `bytes side_channel` instead of a custom `map` in the protobuf definitions? I could give it a shot of using that. I thought about using that, but I didn't see how I could  `side_channel` in a way that would accommodate usage by other features - there needs to be some kind of id I would think.

I could also clean up the submission if that would help. I didn't do a great job on getting rid of extraneous changes before submitting, which is kind of stupid I guess.",thanks taking look instead custom map could give shot thought see could way would accommodate usage need kind id would think could also clean submission would help great job getting rid extraneous kind stupid guess,issue,positive,positive,positive,positive,positive,positive
599029122,"I did have the script, although I don't understand why it wasn't doing what it was supposed to? I just downloaded the repo and copied over the ""Project"" folder. It fixed it. Thanks!",script although understand supposed copied project folder fixed thanks,issue,negative,positive,positive,positive,positive,positive
599028773,"I also have this problem. I did not do about the custom side channels but only follow the Learning-Environment-Executable tutorial with ml-agents.
Not sure it is a bug in my case, because the training and game seem to work properly. It only appears the message without other errors.
![image](https://user-images.githubusercontent.com/11832387/76678323-cb545400-6611-11ea-988e-642d234c309d.png)
Environment:
* PushBlock
* Unity version: 2019.3.5
* OS: Windows 10
* ml-agents: 0.14.1
",also problem custom side follow tutorial sure bug case training game seem work properly message without image environment unity version o,issue,negative,positive,neutral,neutral,positive,positive
598953812,I've got this logged in our backlog as MLA-675; I'm going to write up a small proposal for it today.,got logged backlog going write small proposal today,issue,negative,negative,negative,negative,negative,negative
598837694,"Got it, so it's crashing in the middle of the run w/o intervention. Thanks, that'll help us find the issue.",got middle run intervention thanks help u find issue,issue,positive,positive,neutral,neutral,positive,positive
598645777,"@ervteng Ok, did not have much time yesterday. But started a training with same environment but with only one. Crashed at the same point. The max steps are ` max_steps:      5.0e6   ` And I am quite certain I have not unintentionally Ctrl-c the training three times at the row at the same spot. (It have crashed after approx. 190 000 steps every time)

But for some reason the environment stops. Noticed that the memory usage for the python process seemed to be much higher than usual. Will try to investigate during the weekend.",much time yesterday training environment one point quite certain unintentionally training three time row spot every time reason environment memory usage python process much higher usual try investigate weekend,issue,negative,positive,positive,positive,positive,positive
598492098,"@andrewcoh I don't have a decision requester component (is this in 0.13.1?).  But, I am using Agent's On Demand Decision.  And now that I thought about it, I realize that I'm calling the `RequestDecision()` in a state machine which is running in a Monobehaviour Update (not FixedUpdate)... This makes sense now.  I am fairly certain this is my bug -- thank you for helping me figure it out!",decision requester component agent demand decision thought realize calling state machine running update sense fairly certain bug thank helping figure,issue,positive,positive,positive,positive,positive,positive
598481473,"Cool, as long as they're going to get more use in the future, sounds good.",cool long going get use future good,issue,positive,positive,positive,positive,positive,positive
598478662,"> Do you anticipate other parameter types being added in the future? Right now it seems a little weird to have a general proprerty interface, but only use it for hyperparameters.

I actually took this idea from the progress bar branch, since there we need to write `max_step` to the StatsWriters as well (but only certain writers will need it). I'd also like to use it to differentiate between a multi-agent trainer and a regular trainer (and have the ConsoleWriter output ELO instead of reward). I'd imagine there are other one-time parameters that we'll need eventually. ",anticipate parameter added future right little weird general interface use actually took idea progress bar branch since need write well certain need also like use differentiate trainer regular trainer output instead reward imagine need eventually,issue,positive,negative,neutral,neutral,negative,negative
598475680,"Do you anticipate other parameter types being added in the future? Right now it seems a little weird to have a general proprerty interface, but only use it for hyperparameters.",anticipate parameter added future right little weird general interface use,issue,negative,negative,neutral,neutral,negative,negative
598474377,"> We shouldn't have the `1DTestContinuous.demo` and `1DTestDiscrete.demo` files right?

I was including them so that one test did not depend on another.",right one test depend another,issue,negative,positive,positive,positive,positive,positive
598471626,Just a heads up that this was fixed in https://github.com/Unity-Technologies/ml-agents/pull/3605 and will be in the 0.15.0 release (tentatively next week).,fixed release tentatively next week,issue,negative,positive,neutral,neutral,positive,positive
598435308,"Hi @zako42,

Is your decision period set to 5 in the decision requester component on your agent?  AgentAction is called after a decision period elapses.  A decision period corresponds to the number of fixed updates that occur between decisions.  You can think of AgentAction as setting the action that the agent will execute for the following decision period.  

Setting the decision period to 1 might give you the behavior you want.",hi decision period set decision requester component agent decision period decision period number fixed occur think setting action agent execute following decision period setting decision period might give behavior want,issue,negative,positive,neutral,neutral,positive,positive
598389930,"I realize now, this is for release, not master. 
Please merge as is and maybe add comments in another PR.",realize release master please merge maybe add another,issue,negative,neutral,neutral,neutral,neutral,neutral
598352865,"@niskander,
please create a new, issue for the burst problem you are having.  It is hard to keep track of issues on merged PRs. ",please create new issue burst problem hard keep track,issue,negative,negative,neutral,neutral,negative,negative
598352729,It breaks as expected since the release tag for 0.15.0 is not out yet.,since release tag yet,issue,negative,neutral,neutral,neutral,neutral,neutral
598352548,"@FlimFlamm,
Please create a new issue if you are having trouble with gRPC using a version of ML-Agents where this was fixed.  i.e. the master branch, or the 0.15.0 release branch.  This fix will go into the 0.15.0 release.  If you are not on master, or using our release branch, you will still see this error on unity 2020.1.x",please create new issue trouble version fixed master branch release branch fix go release master release branch still see error unity,issue,negative,positive,neutral,neutral,positive,positive
598351019,not sure why all of the `latest_release` links were removed.  Please change those back to `latest_release` for the 0.15.0 links.  And add back the `master (unstable)` doc links.,sure link removed please change back link add back master unstable doc link,issue,negative,positive,positive,positive,positive,positive
598299820,"@vincentpierre it was a pain to even install 2018.4.14 for consistency -- and the patch versions should only be bug and security fixes, so no expected changes.  This is totally unrelated to the idea of moving past 2019.x or 2020.x.",pain even install consistency patch bug security totally unrelated idea moving past,issue,negative,negative,negative,negative,negative,negative
598298809,@dlindmark thanks for checking; so it happens when training hits `max_step`? About how many steps did it go for before quitting? ,thanks training many go,issue,negative,positive,positive,positive,positive,positive
598289277,"I think your change works but I wonder why updating now? Is there a feature we are after in 18f1?
Should this go to the release instead of master?
@mmattar Should we do the jump all the way to 2020.1 ?",think change work wonder feature go release instead master jump way,issue,negative,neutral,neutral,neutral,neutral,neutral
598069751,"@chriselion I have reverted to training without saving replay buffer. It is not vital for me, but would be convinient sometimes. 

@ervteng I have been using 4 environments. I have not intentionally stopped the training with CTRL+C. I will try reproducing with one environment and experiment with the observations etc. To see if I can narrow it down in some way.

Aand look at h5py issue template today.",training without saving replay buffer vital would sometimes intentionally stopped training try one environment experiment see narrow way look issue template today,issue,negative,negative,neutral,neutral,negative,negative
598005462,"Same error with @sekne18 
In 0.14.1, everything is installed well, but still cannot connect with python terminal.
The editor of Unity just throw 'Couldn't connect to trainer on port 5004 using API version API-14. Will perform inference instead.', And this is no response in terminal, it always show 'INFO:mlagents_envs:Listening on port 5004. Start training by pressing the Play button in the Unity Editor.'

OS: MacOS 10.15.3
Unity: 2018.4.17f1 or 2019.3.2f1",error everything well still connect python terminal editor unity throw connect trainer port version perform inference instead response terminal always show listening port start training pressing play button unity editor o unity,issue,negative,neutral,neutral,neutral,neutral,neutral
597931834,"note: merged after Mac package tests finished, but before Win/Linux tests passed",note mac package finished,issue,negative,neutral,neutral,neutral,neutral,neutral
597904924,"@chriselion @dlindmark If it helps with repro when I was getting this error it was usually with a `--load`, not on the first run. Also, my buffers were in the order of GBs so much bigger than 27 MB. Ever since reducing the buffer size to 0.99GB it hasn't occurred for me.",getting error usually load first run also order much bigger ever since reducing buffer size,issue,negative,neutral,neutral,neutral,neutral,neutral
597900365,"@dlindmark We're stumped. @ervteng tried to reproduce this on Windows yesterday and wasn't able to get it to happen. I looked a bit through the h5py code and it don't see where that error would be coming from (we're writing, so I don't understand where the complaints about read access are coming from)

Could you either open an issue in h5py, or provide the details here that they ask for in their [issue template](https://github.com/h5py/h5py/issues/new), and I'll open it?",tried reproduce yesterday able get happen bit code see error would coming writing understand read access coming could either open issue provide ask issue template open,issue,negative,positive,positive,positive,positive,positive
597899174,"Hi @dlindmark, I noticed this line in your output: `INFO:mlagents_envs:Environment shut down with return code 0 (CTRL_C_EVENT).`
How many environments are you using? Also, when quitting ML-Agents, after pressing CTRL+C, the program _will_ hang on ""Saving Experience Replay Buffer"" for a bit before quitting. Hitting CTRL+C again would cause similar errors to what you have there. Thanks!",hi line output environment shut return code many also pressing program saving experience replay buffer bit would cause similar thanks,issue,negative,positive,positive,positive,positive,positive
597890488,"OK, in that case I don't think we'll plan on adding that as a feature; in general, I think we should provide debug visualization via gizmos, but shouldn't draw anything in the ""game'.",case think plan feature general think provide visualization via draw anything game,issue,negative,negative,negative,negative,negative,negative
597853744,"This issue was fixed in #3471, referenced here.  You can try it out in the latest [v0.14.1 release](https://github.com/Unity-Technologies/ml-agents/releases/tag/0.14.1).  I'm going to close this issue report, but please feel free to reopen if you continue to have problems.",issue fixed try latest release going close issue report please feel free reopen continue,issue,positive,positive,positive,positive,positive,positive
597851397,"This issue was fixed in the [v0.14.1 release](https://github.com/Unity-Technologies/ml-agents/releases/tag/0.14.1).  Feel free to give it a try and see if it works for you.  I'm going to close this issue, but feel free to reopen if you continue to have trouble.",issue fixed release feel free give try see work going close issue feel free reopen continue trouble,issue,positive,positive,positive,positive,positive,positive
597849329,"This problem is fixed in recent releases of ML-Agents, in which we updated `AgentOnDone` and logic around resetting agents.  I'm going to close this issue, but please feel free to re-open if you have any more trouble.",problem fixed recent logic around going close issue please feel free trouble,issue,negative,positive,neutral,neutral,positive,positive
597842896,"Hi @afewvowels -- I've checked and double checked on the latest release of ML-Agents (v0.14) and it seems that if this was an issue at some point it has been resolved.  I'm going to close this issue, but please feel free to re-open and we can try to resolve it together if you are still having trouble with this.",hi checked double checked latest release issue point resolved going close issue please feel free try resolve together still trouble,issue,positive,positive,positive,positive,positive,positive
597838671,"Hi @FlimFlamm -- I was never able to reproduce this issue at the time.  I've tried again in the latest releases (including v0.14, the most recent release) and found that training works correctly.  We've also managed to combine various other agent types together with no issue.

Since it has been a long time and this is resolved in recent releases I'm going to go ahead and close the issue.",hi never able reproduce issue time tried latest recent release found training work correctly also combine various agent together issue since long time resolved recent going go ahead close issue,issue,negative,positive,neutral,neutral,positive,positive
597813051,"Revisiting this issue, it seems that your problem may be related to the writing of the checkpoint file on your virtual machine, rather than a specific issue with ML-Agents, as we have been unable to reproduce the problem. If it is still causing problems for you, perhaps you can share the memory and storage specifications of the VM you were using? And also the size of the network? ",issue problem may related writing file virtual machine rather specific issue unable reproduce problem still causing perhaps share memory storage also size network,issue,negative,negative,negative,negative,negative,negative
597793414,"@alphonsocrawford Hi, I've done more investigating regarding these issues.

For the Policy Extrinsic Reward/Environment Cumulative Reward no longer being the same, I've encountered the issue even with self-play turned off. (I only have extrinsic rewards.) This behavior is definitely different from before and I believe it is a bug.

For the freezes, I believe these were caused by very frequent training updates using large batch sizes. I increased the batch sizes according to the [migration guide](https://github.com/Unity-Technologies/ml-agents/blob/0.14.0/docs/Migrating.md), which said that steps are now counted per agent (0.13 to 0.14)
I'm still unsure about the right number to use btw. To get one episode, I'm using `num_agents * num_envs * episode_length`
In any case, not related to Self-play either.

Should I be creating a different bug for the reward graphs issue?
",hi done investigating regarding policy extrinsic cumulative reward longer issue even turned extrinsic behavior definitely different believe bug believe frequent training large batch size batch size according migration guide said per agent still unsure right number use get one episode case related either different bug reward issue,issue,positive,positive,neutral,neutral,positive,positive
597522585,"Getting this GRPC.core version error on 2020.a25...

Any timeline on a fix, or instructions for manual fix?

Edit: possibly i am causing this issue myself as i have encountered it before, and it seems to still be happening with the current mlagents version and 2020a17 (fresh env)...

EDIT 2:

In the end we got this working only with 20201b (which was not available last night somehow). Some additional fuss with mlagents ensued regarding API versions, but I think the following managed to solve the problem
```
conda create -n mlagents141 python=3.7
activate mlagents141
conda install pip
pip install mlagents==0.14.1```",getting version error fix manual fix edit possibly causing issue still happening current version fresh edit end got working available last night somehow additional fuss regarding think following solve problem create activate install pip pip install,issue,negative,positive,positive,positive,positive,positive
597421046,"Hi @chriselion, Thank you so much.
As for ""in-game"" visualization, I am so sorry that I'm new to Unity. All I know is the simple LineRenderer. Sorry that I can't give you a good recommendation.",hi thank much visualization sorry new unity know simple sorry ca give good recommendation,issue,positive,positive,neutral,neutral,positive,positive
597415847,"Sorry, I forgot to update:
There's now a `get` property for the RayPerceptionSensor:
https://github.com/Unity-Technologies/ml-agents/blob/97e4feae106813c0fcbeb2e160a52043fe682b11/com.unity.ml-agents/Runtime/Sensors/RayPerceptionSensorComponentBase.cs#L147-L150

and I made the method for getting the RayPerceptionInput public:
https://github.com/Unity-Technologies/ml-agents/blob/97e4feae106813c0fcbeb2e160a52043fe682b11/com.unity.ml-agents/Runtime/Sensors/RayPerceptionSensorComponentBase.cs#L238-L254

Hopefully that should have you covered.

I'm not sure what other packages like ours do for visualization; do you have any other good examples of ""in-game"" visualization?",sorry forgot update get property made method getting public hopefully covered sure like visualization good visualization,issue,positive,positive,positive,positive,positive,positive
597407433,"Since we've deprecated --num-runs option, this bug no longer exist, so I'm going to close it for now. Feel free to reopen if you find more issues with it. ",since option bug longer exist going close feel free reopen find,issue,positive,positive,positive,positive,positive,positive
597406904,"This issue has been fixed in the latest version, so I'm going to close it for now. If you find this continues to be an issue, feel free to reopen it. ",issue fixed latest version going close find issue feel free reopen,issue,positive,positive,positive,positive,positive,positive
597405523,"I tried to set up something similar to reproduce the bug, but I wasn't able to on the latest version, so I guess this is no longer an issue. Feel free to reopen if this still occurs. ",tried set something similar reproduce bug able latest version guess longer issue feel free reopen still,issue,positive,positive,positive,positive,positive,positive
597359071,"Hi @MikeWise2718,
Thanks for the submission. The team needs to talk internally about this a bit more - it's definitely a useful feature (there have been several requests for it), but I don't think think this is the way we want it implemented. 

I'm trying to wrap up a few other loose ends, but I'll return to this later this week...",hi thanks submission team need talk internally bit definitely useful feature several think think way want trying wrap loose return later week,issue,positive,positive,neutral,neutral,positive,positive
597290358,"> > I'm assuming with this change - the first call to env.step() will have the initial observation seen by the agents, right? And we shouldn't call env.reset() to get the first observation? If so there are some changes we will need to make to the env_manager and trainer_controller.
> 
> The first call can be reset or set and it does not change anything. This is because the Academy initializes before the Agents do (it is now guaranteed since Academy is a singleton).
> I modified the code so there is no need to edit the env_manager.

I think introducing ambiguity about where the initial observation is presented would be an issue. Right now it is clear that it is on `env.reset()`.",assuming change first call initial observation seen right call get first observation need make first call reset set change anything academy since academy singleton code need edit think ambiguity initial observation would issue right clear,issue,negative,positive,positive,positive,positive,positive
597279500,"friendly ping @devedse,
I was wondering if you were still having this issue.  From the debugging I did, I was unable to find a leak in our code.  Where you able to find any more in yours?",friendly ping wondering still issue unable find leak code able find,issue,negative,positive,positive,positive,positive,positive
597251320,"Sorry for the delayed response. Thanks for checking on this. We will try to see how we might resolve it so that we can avoid people seeing the deformed logo :) 

We are tracking this internally with MLA-757",sorry response thanks try see might resolve avoid people seeing deformed internally,issue,negative,negative,neutral,neutral,negative,negative
597233511,"Hello @Cpache1, I've tried to reproduce this issue with no luck, the necessary `DemonstrationImporter` script is present after a fresh clone of the repository. The script your missing is located here `ml-agents/com.unity.ml-agents/Editor/`, but I would suggest re-cloning the repo as a whole to make sure you're not actually missing the file",hello tried reproduce issue luck necessary script present fresh clone repository script missing would suggest whole make sure actually missing file,issue,negative,positive,neutral,neutral,positive,positive
597224651,"> I'm assuming with this change - the first call to env.step() will have the initial observation seen by the agents, right? And we shouldn't call env.reset() to get the first observation? If so there are some changes we will need to make to the env_manager and trainer_controller.

The first call can be reset or set and it does not change anything. This is because the Academy initializes before the Agents do (it is now guaranteed since Academy is a singleton).
I modified the code so there is no need to edit the env_manager.",assuming change first call initial observation seen right call get first observation need make first call reset set change anything academy since academy singleton code need edit,issue,negative,positive,positive,positive,positive,positive
597213992,"Hi all,

We have depreciated the online and offline BC trainers in favor of a new approach which can work concurrently with RL training. It should also resolve the issues that have been described here: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Imitation-Learning.md. ",hi favor new approach work concurrently training also resolve,issue,positive,positive,positive,positive,positive,positive
597212486,"Hi All,

We have deprecated the custom protobuf feature in favor of a new method of communication between Unity and Python called side channels. You can read more about them here: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Custom-SideChannels.md. The main benefit is that building custom protobuf objects is no longer necessary, which should provide faster turnaround time during experimentation. 

Feel free to open a new issue if there are any issues or suggestions related to the new side channels.",hi custom feature favor new method communication unity python side read main benefit building custom longer necessary provide faster turnaround time experimentation feel free open new issue related new side,issue,positive,positive,positive,positive,positive,positive
597209786,"Hi all,

In the most recent versions of ML-Agents, we have made significant changes to simplify the order of operations with respect to agent resets, steps, and observations. Please let us know if these changes have resolved the difficulties you may have run into. ",hi recent made significant simplify order respect agent please let u know resolved may run,issue,positive,positive,positive,positive,positive,positive
596943172,"I am on Windows. With python 3.6.7.

The error occurs while training. When it saves. I guess it tries to read the buffer after saving as a sanity check or something? I never get a printout as `Experience replay buffer has X experiences`

If I try to restart the failed training using `--load` it crashes while loading the replay buffer. It looks like it does not find the file `./models/sac1/MuckingBehavior\last_replay_buffer.hdf5`. Even though it exists.

```
INFO:mlagents.trainers:Loading Model for brain MuckingBehavior?team=0
INFO:mlagents.trainers:Loading Experience Replay Buffer from ./models/sac1/MuckingBehavior\last_replay_buffer.hdf5
INFO:mlagents_envs:Environment shut down with return code 0 (CTRL_C_EVENT).
Traceback (most recent call last):
  File ""E:\xxx\xxx\venv\Scripts\mlagents-learn-script.py"", line 11, in <module>
    load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')()
  File ""e:\xxx\xxx\ml-agents-fork\ml-agents\mlagents\trainers\learn.py"", line 479, in main
    run_cli(parse_command_line())
  File ""e:\xxx\xxx\ml-agents-fork\ml-agents\mlagents\trainers\learn.py"", line 475, in run_cli
    run_training(run_seed, options)
  File ""e:\xxx\xxx\ml-agents-fork\ml-agents\mlagents\trainers\learn.py"", line 320, in run_training
    tc.start_learning(env_manager)
  File ""e:\xxx\xxx\ml-agents-fork\ml-agents\mlagents\trainers\trainer_controller.py"", line 213, in start_learning
    self._create_trainers_and_managers(env_manager, new_behavior_ids)
  File ""e:\xxx\xxx\ml-agents-fork\ml-agents\mlagents\trainers\trainer_controller.py"", line 200, in _create_trainers_and_managers
    self._create_trainer_and_manager(env_manager, behavior_id)
  File ""e:\xxx\xxx\ml-agents-fork\ml-agents\mlagents\trainers\trainer_controller.py"", line 180, in _create_trainer_and_manager
    policy = trainer.create_policy(env_manager.external_brains[name_behavior_id])
  File ""e:\xxx\xxx\ml-agents-fork\ml-agents\mlagents\trainers\sac\trainer.py"", line 229, in create_policy
    self.load_replay_buffer()
  File ""e:\xxx\xxx\ml-agents-fork\ml-agents\mlagents\trainers\sac\trainer.py"", line 129, in load_replay_buffer
    self.update_buffer.load_from_file(file_object)
  File ""e:\xxx\xxx\ml-agents-fork\ml-agents\mlagents\trainers\buffer.py"", line 237, in load_from_file
    with h5py.File(file_object, ""r"") as read_file:
  File ""e:\xxx\xxx\venv\lib\site-packages\h5py\_hl\files.py"", line 408, in __init__
    swmr=swmr)
  File ""e:\xxx\xxx\venv\lib\site-packages\h5py\_hl\files.py"", line 173, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File ""h5py\_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper
  File ""h5py\_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper
  File ""h5py\h5f.pyx"", line 88, in h5py.h5f.open
OSError: Unable to open file (file signature not found)

(venv) E:\xxx\xxx>dir models\sac1\MuckingBehavior\
 Volume in drive E is Seagate3TB
 Volume Serial Number is 4869-2B9D

 Directory of E:\xxx\xxx\models\sac1\MuckingBehavior

2020-03-09  11:46    <DIR>          .
2020-03-09  11:46    <DIR>          ..
2020-03-09  11:46               233 checkpoint
2020-03-09  11:47        28 115 676 last_replay_buffer.hdf5
2020-03-09  10:54        47 272 540 model-146749.cptk.data-00000-of-00001
2020-03-09  10:54             6 963 model-146749.cptk.index
2020-03-09  10:54           777 645 model-146749.cptk.meta                                                               
```",python error training guess read buffer saving sanity check something never get experience replay buffer try restart training load loading replay buffer like find file even though loading model brain loading experience replay buffer environment shut return code recent call last file line module file line main file line file line file line file line file line policy file line file line file line file line file line fid name file line file line file line unable open file file signature found volume drive volume serial number directory,issue,negative,negative,neutral,neutral,negative,negative
596840171,"I'm assuming with this change - the first call to `env.step()` will have the _initial_ observation seen by the agents, right? And we shouldn't call `env.reset()` to get the first observation? If so there are some changes we will need to make to the `env_manager` and `trainer_controller`. ",assuming change first call observation seen right call get first observation need make,issue,negative,positive,positive,positive,positive,positive
596794851,"Hi @batu, 
Sorry I no longer have access to the code. You can ask [@ReinforcedMan](https://github.com/ReinforcedMan) for more info.",hi sorry longer access code ask,issue,negative,negative,negative,negative,negative,negative
596767312,"Hi @hanseoup, unfortunately this is an issue with how `freeze_graph` works in Tensorflow. My suggestion is to keep the checkpoints and load/call `freeze_graph` on just the `ckpt` that you want to convert. We're looking for a way to do it at every save without leaking memory, and will update this issue if there are any updates. ",hi unfortunately issue work suggestion keep want convert looking way every save without memory update issue,issue,negative,negative,negative,negative,negative,negative
596764421,This bug has been fixed in the latest `master` and will be released shortly. Closing this issue for now. ,bug fixed latest master shortly issue,issue,negative,positive,positive,positive,positive,positive
596698187,"Hi @dlindmark, we're trying to reproduce your issue on our end. So far we've been able to load replay buffers >400MB with no issue. Had a couple of questions:

- Which platform are you on (Mac/Windows/Linux)?
- Which ver of Python are you using?
- On the last successful load, the console should have printed a line similar to this: `Experience replay buffer has X experiences.` How many experiences was it able to load successfully?

Thanks!",hi trying reproduce issue end far able load replay issue couple platform python last successful load console printed line similar experience replay buffer many able load successfully thanks,issue,positive,positive,positive,positive,positive,positive
596695875,Thanks for sharing the steps for reproducing this issue and the bug report. I'll share with the team and address the issue accordingly! ,thanks issue bug report share team address issue accordingly,issue,positive,positive,positive,positive,positive,positive
596676306,"This would be a helpful feature for me as the model was at one point much better than it was at the final point, like Devedse's example

> I totally agree on the need of having to solve this for @Tobbse 's issue as I'm also running into that problem:
> 
> ![image](https://user-images.githubusercontent.com/2350015/71351723-b1f0be00-2574-11ea-9996-8ed79030e730.png)
> 
> In the example above I'd want to get the .nn model of one of these peaks as apparently the model performed best there.
> 
> Now it's almost like following the stock trade market 😆

",would helpful feature model one point much better final point like example totally agree need solve issue also running problem image example want get model one apparently model best almost like following stock trade market,issue,positive,positive,positive,positive,positive,positive
596659224,"```
pip show h5py
Name: h5py
Version: 2.10.0
```
```
pip show mlagents
Name: mlagents
Version: 0.14.1
```",pip show name version pip show name version,issue,negative,neutral,neutral,neutral,neutral,neutral
596644548,"Hi @JohnBergago,
Thanks for the feedback. Converting the internal setters to public is on my list of things to do this week.",hi thanks feedback converting internal public list week,issue,negative,positive,neutral,neutral,positive,positive
596577455,"Hi @chriselion,
I came across this issue and would like to mention, that I have the same problem for several sensor components such as RayPerception and CameraSensor. I really like to create and configure them on start-up. I'm using side-channels to specify the settings and configuration from python code. That way it is easy to have multiple experiments running in parallel without having a slightly different environment for each experiment. 
Until recently I could simply add the sensors I needed in InitializeAgent() before the sensors were created. However, this is not possible anymore as properties such as height and width in camera sensors or raysPerDirection, detectableTags in RayPerceptionSensors are only internally settable. 

I get your point, that it gets complicated with the setters **after** simulation started, but what about some kind of **initialization method**, that will only have effect before the Sensor component was created. Otherwise it would fire a warning. That way nothing will be changed during simulation, but preserve the freedom to configure everything from code.",hi came across issue would like mention problem several sensor really like create configure specify configuration python code way easy multiple running parallel without slightly different environment experiment recently could simply add however possible height width camera internally settable get point complicated simulation kind method effect sensor component otherwise would fire warning way nothing simulation preserve freedom configure everything code,issue,negative,positive,neutral,neutral,positive,positive
596536169,"Yes, it seems to crash again. After approx.  the same amount of time. Are there any tests with visual observations and `save_replay_buffer = true`? 
```
INFO:mlagents.trainers:Saving Experience Replay Buffer to ./models/sac1/MuckingBehavior\last_replay_buffer.hdf5
INFO:mlagents_envs:Environment shut down with return code 0 (CTRL_C_EVENT).
INFO:mlagents_envs:Environment shut down with return code 0 (CTRL_C_EVENT).
INFO:mlagents_envs:Environment shut down with return code 0 (CTRL_C_EVENT).
Traceback (most recent call last):
  File ""C:\Python\Python36\Lib\multiprocessing\queues.py"", line 236, in _feed
    send_bytes(obj)
  File ""C:\Python\Python36\Lib\multiprocessing\connection.py"", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File ""C:\Python\Python36\Lib\multiprocessing\connection.py"", line 290, in _send_bytes
    nwritten, err = ov.GetOverlappedResult(True)
BrokenPipeError: [WinError 109] The pipe has been ended
INFO:mlagents_envs:Environment shut down with return code 0 (CTRL_C_EVENT).
Traceback (most recent call last):
  File ""e:\xxxx\mlagents\trainers\buffer.py"", line 231, in save_to_file
    write_file.create_dataset(key, data=data, dtype=""f"", compression=""gzip"")
  File ""e:\xxxx\lib\site-packages\h5py\_hl\group.py"", line 137, in create_dataset
    dset = dataset.Dataset(dsid)
  File ""h5py\_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper
  File ""h5py\_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper
  File ""e:\xxx\xxx\venv\lib\site-packages\h5py\_hl\dataset.py"", line 403, in __init__
    self._dcpl = self.id.get_create_plist()
  File ""h5py\_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper
  File ""h5py\_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper
  File ""h5py\h5d.pyx"", line 324, in h5py.h5d.DatasetID.get_create_plist
  File ""h5py\defs.pyx"", line 440, in h5py.defs.H5Dget_create_plist
  File ""h5py\h5fd.pyx"", line 158, in h5py.h5fd.H5FD_fileobj_read
io.UnsupportedOperation: read
```",yes crash amount time visual true saving experience replay buffer environment shut return code environment shut return code environment shut return code recent call last file line file line offset offset size file line err true pipe ended environment shut return code recent call last file line key file line file line file line file line file line file line file line file line file line read,issue,positive,positive,neutral,neutral,positive,positive
596379777,"I recently got this error aswell. With `buffer_size = 750000`, but using one visual observation. Grayscale with size 128x64. So the buffer size is well below 2 million but with visual observation the amount of data might be in the same range?

It was able to save the buffer 3 times before crashing. The saved file size is about 27 mb. I have started the run again to investigate if it is reproducible.",recently got error aswell one visual observation size buffer size well million visual observation amount data might range able save buffer time saved file size run investigate reproducible,issue,positive,positive,positive,positive,positive,positive
596266073,"Hello @sahandrez ,
Can you tell me how you fixed the problem? I am facing the same KeyError issue in `mark_agent_done`
",hello tell fixed problem facing issue,issue,negative,positive,neutral,neutral,positive,positive
596134714,"Hi, 
The fix for this was merged into master from #3590. Please open this issue again if a problem persists. ",hi fix master please open issue problem,issue,negative,neutral,neutral,neutral,neutral,neutral
596125342,"Hi,
This is a duplicate of https://github.com/Unity-Technologies/ml-agents/issues/3336

You should either:
* downgrade barracuda to 0.4.0
* upgrade ML-Agents to 0.14.1
* use the branch from https://github.com/Unity-Technologies/ml-agents/pull/3339

I'm going to close this; please re-open if you have more problems.",hi duplicate either downgrade barracuda upgrade use branch going close please,issue,negative,neutral,neutral,neutral,neutral,neutral
596112039,"So if I have something I need to do every time there is an Academy Step, should I be using `Settings.Update` or `Settings.FixedUpdate`? The design seems to indicate both of them are okay, but they are not really equivalent, so I am not sure what to do. 

Hmm, looking at the old code, `AcademyStep` was called by `EnvironmentStep` which was called by `FixedUpdate`, so I am going with that.",something need every time academy step design indicate really equivalent sure looking old code going,issue,negative,positive,positive,positive,positive,positive
596058221,"@chriselion Hi! I'm closing this issue as I understand some of the issues mentioned, like it not showing in Edit mode, have been fixed in newer version. If you need an illustration for the specific issue of rays not properly conveying information to their 3Dimensionality, please let me know here or in [this issue](https://github.com/Unity-Technologies/ml-agents/issues/3429) I raised and I'll post an image to clarify that issue, and potential solutions, specifically.",hi issue understand like showing edit mode fixed version need illustration specific issue properly information dimensionality please let know issue raised post image clarify issue potential specifically,issue,positive,positive,neutral,neutral,positive,positive
596049702,"Thank you for the update. To be honest, I have stopped using ML agents right after this problem; because:
- ML Agents has been going under major changes and it did not seem like a good idea to train the model and use it in production while the traffic on the changes are so high.
- I did not want to introduce ""any"" random data ""manually"" to the whole decision process and in the version I have worked, it was not that much possible to re-request the decision in the automated process where agent had already made a decision 1 time and needs the consequence, no turning back. I am not aware of the ""actual"" results of the latest changes in the codebase right at this moment; I only track changelogs announced by you to keep an eye on what is going on in general.

I have not given up using ML Agents at all, but I postponed the use of it (in my current project)  until its the first major release or the release candidate version. I want to start using it again in non-production environments and test projects, for adapting to the latest changes, as soon as I have time.

In the mean time, I had to go back and make the AI in my project the old-school way; hopefully to be replaced or updated with ML Agents.

Thank you for your concern, time, sharing ML Agents with us and letting us be a part of it too.",thank update honest stopped right problem going major seem like good idea train model use production traffic high want introduce random data manually whole decision process version worked much possible decision process agent already made decision time need consequence turning back aware actual latest right moment track keep eye going general given use current project first major release release candidate version want start test latest soon time mean time go back make ai project way hopefully thank concern time u u part,issue,positive,positive,positive,positive,positive,positive
596034559,"> Can we record a smaller version? 30MB is pretty big

Done.",record smaller version pretty big done,issue,negative,positive,neutral,neutral,positive,positive
596028615,"Custom observations have been removed in the latest ML-Agents - if you'd like similar behavior, we suggest using Side Channels (https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Python-API.md). Closing this issue for now. ",custom removed latest like similar behavior suggest side issue,issue,negative,positive,positive,positive,positive,positive
596026535,"thanks, i meant to take care of this a while ago 😬 ",thanks meant take care ago,issue,positive,positive,positive,positive,positive,positive
596025780,"🤦‍♂ Sorry, that was so that we test non-square observations (to make sure we don't mix up height and width).",sorry test make sure mix height width,issue,negative,neutral,neutral,neutral,neutral,neutral
596018780,"Hi @hanseoup, to follow up - are you still seeing this issue on the latest ML-Agents release?",hi follow still seeing issue latest release,issue,negative,positive,positive,positive,positive,positive
596017834,This bug has been fixed - closing the issue for now. Feel free to post a new issue if you're still having problems.,bug fixed issue feel free post new issue still,issue,positive,positive,positive,positive,positive,positive
596013364,"Hi @chechulinYuri,
This was fixed in barracuda version 0.6.0-preview.  You can find their changelong [here](https://docs.unity3d.com/Packages/com.unity.barracuda@0.6/changelog/CHANGELOG.html) and will be in the 0.15.0 release of ML-Agents.  ",hi fixed barracuda version find release,issue,negative,positive,neutral,neutral,positive,positive
596013237,This issue has been fixed in version 14.1 and in the latest Master. Closing this issue for now - feel free to reopen if you're still having problems.,issue fixed version latest master issue feel free reopen still,issue,positive,positive,positive,positive,positive,positive
596012386,"Hi @noocin, there currently isn't a supported path to get PyTorch models into ML-Agents - we only support the models created with our toolkit, so we don't have any working examples.

With that said, it is possible - you're right in that the names and constants need to match. If you're able to create a model with the tensor names specified, it *should* work. You may be able to omit the `recurrent_in` ones if you're not using an LSTM, and the ""value_estimate"" one if you don't need that. ",hi currently path get support working said possible right need match able create model tensor work may able omit one need,issue,positive,positive,positive,positive,positive,positive
596012107,"I'm still having trouble understanding some of the problems that you're describing without a screenshot. Would it be possible to get some screenshots of what you're seeing in the editor, along with what you'd expect them to look like (even if it's just hackily drawn in)?",still trouble understanding without would possible get seeing editor along expect look like even drawn,issue,negative,negative,neutral,neutral,negative,negative
596010257,"Hi @CharlieReece,
Sorry for the long delay on this. This bug is still open but we hope to get it fixed soon.

As for what to do when an invalid action, I would recommend:
* Randomly selecting one of the remaining actions during training.
* Either re-requesting or randomly selecting during inference.",hi sorry long delay bug still open hope get fixed soon invalid action would recommend randomly one training either randomly inference,issue,negative,negative,negative,negative,negative,negative
596006346,"HI @Phong13 - sorry that this one slipped through the cracks.

Some of this code has changed recently, but I think the core issue (Agents that are added after the Academy starts don’t get reset) is still a problem. 

I have this logged in our internal tracker as MLA-743 - hopefully we'll have a resolution for it soon.",hi sorry one slipped code recently think core issue added academy get reset still problem logged internal tracker hopefully resolution soon,issue,negative,negative,negative,negative,negative,negative
595994718,"Yeah, I don't think they care that much, but the downside is they might waste time ""optimizing"" the sequence length when it's not because it's getting overridden. I'm fine with the exception approach.",yeah think care much downside might waste time sequence length getting fine exception approach,issue,negative,positive,positive,positive,positive,positive
595994059,"> > In PPO, the code sneakily set the sequence length equal to the batch size
> 
> How about being less sneaky (log a warning) but still doing this?

Open to this. In general my feeling is that warnings tend to get lost (and thus ignored) in the stream of prints that occur when the trainer starts, and the user will never know what their actual batch size is - perhaps this isn't a real issue and most users won't care if their batch size is bigger than it was set",code set sequence length equal batch size le sneaky log warning still open general feeling tend get lost thus stream occur trainer user never know actual batch size perhaps real issue wo care batch size bigger set,issue,negative,positive,neutral,neutral,positive,positive
595993353,"> In PPO, the code sneakily set the sequence length equal to the batch size

How about being less sneaky (log a warning) but still doing this?",code set sequence length equal batch size le sneaky log warning still,issue,negative,neutral,neutral,neutral,neutral,neutral
595990062,"Great, I look forward to running my example against it and seeing if I can get it to work again. Let me know if I can help test it in anyway.",great look forward running example seeing get work let know help test anyway,issue,positive,positive,positive,positive,positive,positive
595980142,"Hi @MikeWise2718,
Thanks for raising this issue.

Just a little background: we've been trying to reduce the surface area of our public API as we get ready to become a preview package. Once we hit that milestone, we have to follow strict semantic versioning on the API, and any breaking change (like renaming a variable or method) requires a major version increase. Unfortunately, there's no way to mark some parts of the API as ""stable"" and others ""experimental"", so we tend to be defensive and don't expose things we're not 100% sure about.

That being said, I agree with you that we went too far on this by making `BehaviorParameters` `internal` (and probably `BrainParameters` too). We'll work on revising this in the next few days (probably in the form of private members and public properties). 

 
> How do you write your own test cases against it?

All of unit tests have access to internal classes/members due to the `InternalsVisibleTo` statements in our `AssemblyInfo.cs`: https://github.com/Unity-Technologies/ml-agents/blob/2c2f9304a4e16430c07e9815d4ee8cc0d666c712/com.unity.ml-agents/Runtime/AssemblyInfo.cs#L3-L4 But this is definitely discouraging us from eating our own dogfood, so we're going to add some additional test cases without this access, to make sure that operations like creating Agents and attaching custom sensors are possible for users to do via code.

 
> I would think that anything exposed in a Unity Inspector should be public, since someone might want to modify it or inspect it programmatically. I think most everything in this inspector is now inaccessable.

I agree in general, but there are a few details that need more thought/work on our side:
 * Fields that affect the model (e.g. observation and action sizes in `BehaviorParameters`) can't be changed once simulation starts. Therefore we should probably disable them in the Inspector when in Play mode.
 * Adding a `get` property for the fields is straightforward, but having a public `set` is complicated - if you try to change the observation size after simulation has started, we'd probably have to ignore the new value and fire a warning. 
 * Some fields are only read at initialization time (e.g. Behavior Type in `BehaviorParameters`), so it takes a bit of extra work to make sure these changed propagate when they're changed in the Inspecter or from code.

I'm not saying the extra effort is a reason not to do it :)  Just that it might take a few more days to get it fixed on master. 

I'm open to other suggestions on how to keep this user-friendly without letting you shoot yourself in the foot...

",hi thanks raising issue little background trying reduce surface area public get ready become preview package hit milestone follow strict semantic breaking change like variable method major version increase unfortunately way mark stable experimental tend defensive expose sure said agree went far making internal probably work next day probably form private public write test unit access internal due definitely discouraging u eating going add additional test without access make sure like custom possible via code would think anything exposed unity inspector public since someone might want modify inspect programmatically think everything inspector agree general need side affect model observation action size ca simulation therefore probably disable inspector play mode get property straightforward public set complicated try change observation size simulation probably ignore new value fire warning read time behavior type bit extra work make sure propagate code saying extra effort reason might take day get fixed master open keep without shoot foot,issue,positive,positive,neutral,neutral,positive,positive
595965234,"Sounds good, I'll merge this and let's just keep an eye out to make sure no issues come up in the retraining.",good merge let keep eye make sure come,issue,positive,positive,positive,positive,positive,positive
595907267,"HI @scscgit,
This is great feedback.  We have discussed this internally and will address the issue ASAP.  We have logged the issue internally as  MLA-739, and will update this thread when a fix is merged into master.",hi great feedback internally address issue logged issue internally update thread fix master,issue,positive,positive,positive,positive,positive,positive
595780029,"Making the BehaviorParameter internal, and all its fields internal breaks my code, since I like to build agents programmatically. This means I add the components with script and set the parameters that way too. This feels to me like a more scalable and testable approach. Unless I am overseeing something (which is entirely possible), this means I now have to base everything on hand constructed prefabs.  

Isn't this a pretty severe limitation? How do you write your own test cases against it?

-----

After thinking about this, I think I will open an issue around it.",making internal internal code since like build programmatically add script set way like scalable testable approach unless something entirely possible base everything hand pretty severe limitation write test thinking think open issue around,issue,positive,negative,neutral,neutral,negative,negative
595583025,"> @xiaomaogy It explicitly was not supposed to deprecate using the environment variable, as I described in the PR description. Did you do somehow test it and find that it didn't work? because I'd like to fix it if so.

Oh I missed the code you added below to load the environment variable if no command line argument is passed. ",explicitly supposed deprecate environment variable description somehow test find work like fix oh code added load environment variable command line argument,issue,negative,neutral,neutral,neutral,neutral,neutral
595515107,"> I found a typo, but otherwise looks good.

All right, I will wait for another approval since this is a rather controversial PR.
",found typo otherwise good right wait another approval since rather controversial,issue,positive,positive,positive,positive,positive,positive
595483576,"@alphonsocrawford Thanks for the response.
For issue 1, thanks for the explanation. I looked through the code and understand how it's intended to work better now!

If the rewards from one team are ignored for the ""Policy Extrinsic Reward"" graph but both teams are considered for ""Environment Cumulative Reward"" it would explain the second issue (which wouldn't be an issue).

For the freeze, it's very random. I will try to see if I can repro it with one of the example environments. 

Still unsure about  the summary frequency, but it's not a big deal anyways. Thanks!",thanks response issue thanks explanation code understand intended work better one team policy extrinsic reward graph considered environment cumulative reward would explain second issue would issue freeze random try see one example still unsure summary frequency big deal anyways thanks,issue,positive,positive,neutral,neutral,positive,positive
595451769,"@niskander 
For the first issue, seeing only Team 2 logs is actually intended behavior. When training with Self-Play, the first team is actually a fixed model, and second team is logged since that is the model that is being trained

For issue 2 and 4, we will look into these issues and follow-up as they could be bugs

And for issue 3, it is hard for us to reproduce an issue for an environment we did not create. Could you possibly reproduce this issue with one of our sample environments like Tennis?

",first issue seeing team actually intended behavior training first team actually fixed model second team logged since model trained issue look could issue hard u reproduce issue environment create could possibly reproduce issue one sample like tennis,issue,negative,positive,neutral,neutral,positive,positive
595439700,"@xiaomaogy It explicitly was not supposed to deprecate using the environment variable, as I described in the PR description.  Did you do somehow test it and find that it didn't work?  because I'd like to fix it if so.",explicitly supposed deprecate environment variable description somehow test find work like fix,issue,negative,neutral,neutral,neutral,neutral,neutral
595365805,"> I would like to second this request. It would be even better if ML-Agents was available as a package through Unity's new package manager.

Looking at the roadmap, I think this may be the plan??

But if you can't wait for that, with the new refactoring of the project, in Unity 2020.1.a21+, and Unity 2019.3.4f1+ (not released yet :/) you will be able to add the package from this git repo using the sub-directory in the repo. See [this forum post](https://forum.unity.com/threads/some-feedback-on-package-manager-git-support.743345/) for more info :)
",would like second request would even better available package unity new package manager looking think may plan ca wait new project unity unity yet able add package git see forum post,issue,positive,positive,positive,positive,positive,positive
594985516,This seems to deprecate the old way of using the environment variable to control the loading of the scene. I think it would be helpful to enable both for a while before directly deprecate the existing one. ,deprecate old way environment variable control loading scene think would helpful enable directly deprecate one,issue,negative,positive,neutral,neutral,positive,positive
594930419,"Might also be worth mentioning that even though only 1 team appears in the logs, both teams are receiving decisions correctly. ",might also worth even though team correctly,issue,negative,positive,positive,positive,positive,positive
594924963,"@surfnerd sorry, didn't mean to hijack the issue. It's a warning about the same dll (System.Interactive.Async.dll). It doesn't seem to cause any actual issues so just ignoring it for the time being is fine, just thought I'd mention it in case it's relevant.

This is the warning:
```
Could not load assembly System.Interactive.Async, Version=3.0.1000.0, Culture=neutral, PublicKeyToken=94bc3704cddfc263
UnityEngine.Debug:LogWarning(Object)
Unity.Burst.Editor.BurstReflection:CollectAssembly(Assembly, HashSet`1) (at Library/PackageCache/com.unity.burst@1.2.3/Editor/BurstReflection.cs:260)
Unity.Burst.Editor.BurstReflection:CollectAssembly(Assembly, HashSet`1) (at Library/PackageCache/com.unity.burst@1.2.3/Editor/BurstReflection.cs:256)
Unity.Burst.Editor.BurstReflection:CollectAssembly(Assembly, HashSet`1) (at Library/PackageCache/com.unity.burst@1.2.3/Editor/BurstReflection.cs:256)
Unity.Burst.Editor.BurstReflection:CollectAssembly(Assembly, HashSet`1) (at Library/PackageCache/com.unity.burst@1.2.3/Editor/BurstReflection.cs:256)
Unity.Burst.Editor.BurstReflection:GetAssemblyList(AssembliesType) (at Library/PackageCache/com.unity.burst@1.2.3/Editor/BurstReflection.cs:239)
Unity.Burst.Editor.BurstLoader:.cctor() (at Library/PackageCache/com.unity.burst@1.2.3/Editor/BurstLoader.cs:69)
UnityEditor.EditorAssemblies:ProcessInitializeOnLoadAttributes(Type[])

```
",sorry mean hijack issue warning seem cause actual time fine thought mention case relevant warning could load assembly object assembly assembly assembly assembly type,issue,negative,positive,neutral,neutral,positive,positive
594902216,"@niskander,
What burst issues are you seeing?  Could you create a separate issue for that?
Thanks,
Chris",burst seeing could create separate issue thanks,issue,positive,positive,positive,positive,positive,positive
594849495,FYI a similar issue exists with Burst. (I have 0.14 so I haven't tried the fix yet; hopefully it fixes both issues.),similar issue burst tried fix yet hopefully,issue,negative,neutral,neutral,neutral,neutral,neutral
594690011,is it possible to leverage [this](https://docs.unity3d.com/Manual/APIUpdater.html). I think it would make our users' lives easier if we could.,possible leverage think would make easier could,issue,negative,neutral,neutral,neutral,neutral,neutral
594657930,Deleting my `Library` folder then re-opening the project fixed it!,library folder project fixed,issue,negative,positive,neutral,neutral,positive,positive
594574270,"
[ml-agents_0.13.1_log_2.txt](https://github.com/Unity-Technologies/ml-agents/files/4287940/ml-agents_0.13.1_log_2.txt)
[uav_ml_013-0_timers.json.txt](https://github.com/Unity-Technologies/ml-agents/files/4287941/uav_ml_013-0_timers.json.txt)

Hi Vincent,

Please find attached the timer.json file for ml-agents 0.13.1 (there is no timer.json for ml-agents 0.8?) and the associated python console log.

Best regards,

David



",hi vincent please find attached file associated python console log best,issue,positive,positive,positive,positive,positive,positive
594259462,"Hi @chriselion ,
Thanks for your reply. The Gizmos only shows in the ""scene"" window of Unity editor but not in the ""game"" window or in executables. I sometimes need to record the inference process and want to visualize all the rays. So LineRenderer might help me achieve this.

Actually, it would be great if you guys decide to integrate this kind of visualization into `RayPerceptionSensorComponent`. Otherwise, I can implement a simple visualization by myself if I can access to the some properties of `RayPerceptionSensorComponentBase` like `m_RaySensor`.",hi thanks reply scene window unity editor game window sometimes need record inference process want visualize might help achieve actually would great decide integrate kind visualization otherwise implement simple visualization access like,issue,positive,positive,positive,positive,positive,positive
594210882,"Hi @AdhamAlHarazi thanks for posting this! I raised your issue within the team, and it seems that for now we don't have an easy way to help you debug this problem. We are thinking about adding more metrics to understand the differences between two demo files to get more insight into it, but for now you've tried everything we have. 

(Also I realized that you can't play back an export demo file since there is randomness in the pyramids environment). ",hi thanks posting raised issue within team easy way help problem thinking metric understand two get insight tried everything also ca play back export file since randomness environment,issue,positive,positive,positive,positive,positive,positive
594208871,"Hi @TouraisDavid, could you please give us your timers.json file in the summaries folder? It would help us debug and find out why this upgrade caused it to go slower. ",hi could please give u file folder would help u find upgrade go,issue,positive,neutral,neutral,neutral,neutral,neutral
594147920,"@xXZetserXx Just realized that you are trying to connect to a Unity Editor on Ubuntu OS. Is this Unity Editor running under Ubuntu or Windows, and how do you ensure the port gets forwarded?",trying connect unity editor o unity editor running ensure port,issue,negative,neutral,neutral,neutral,neutral,neutral
594145343,"HI @ReinforcedMan, currently our docker feature has been deprecated. As you could imagine with various version of OS and tensorflow version and training configuration and gym versions, it would be super hard to maintain. 

The error you get means the unity environment didn't get connected to the python process within a certain amount of time, but the reason behind it is unclear without further digging into it. ",hi currently docker feature could imagine various version o version training configuration gym would super hard maintain error get unity environment get connected python process within certain amount time reason behind unclear without digging,issue,negative,negative,neutral,neutral,negative,negative
594132471,"> > Is it worth testing the other visual encoders? Maybe a smaller number of iterations just to cover the code?
> 
> Definitely. Let me add it and see how long they would take to train properly

I added some tests for resnet and nature. The resnet-2 encoder takes especially long, so I cut the steps to a very small number and don't actually check the reward.",worth testing visual maybe smaller number cover code definitely let add see long would take train properly added nature especially long cut small number actually check reward,issue,positive,negative,neutral,neutral,negative,negative
594118806,Training and standalone tests are passing - going to merge this to get unit tests on master passing again.,training passing going merge get unit master passing,issue,negative,neutral,neutral,neutral,neutral,neutral
594102396,"> Is it worth testing the other visual encoders? Maybe a smaller number of iterations just to cover the code?

Definitely. Let me add it and see how long they would take to train properly",worth testing visual maybe smaller number cover code definitely let add see long would take train properly,issue,positive,positive,neutral,neutral,positive,positive
594096563,"Since the user is supposed to write their own GtCompressedObservations, maybe we should check for null and raise?",since user supposed write maybe check null raise,issue,negative,neutral,neutral,neutral,neutral,neutral
594076333,"BTW, I'm curious what you're doing for the line rendering. Are you using https://docs.unity3d.com/Manual/class-LineRenderer.html or something else? ",curious line rendering something else,issue,negative,negative,neutral,neutral,negative,negative
594075442,"Hi @BlueFisher,
The gizmo problem has already been reported (https://github.com/Unity-Technologies/ml-agents/issues/3412), plus there were some other requests to draw the gizmos outside of play mode. This was resolved in https://github.com/Unity-Technologies/ml-agents/pull/3484. If you want a ""smaller"" workaround, https://github.com/Unity-Technologies/ml-agents/pull/3542 (just merged) will step the sensors in Heuristic mode, which should also be enough to get the gizmos drawing.

I'll look into making the sensor more accessible - would a public `get` property be enough instead of making it `protected`?",hi problem already plus draw outside play mode resolved want smaller step heuristic mode also enough get drawing look making sensor accessible would public get property enough instead making,issue,negative,positive,neutral,neutral,positive,positive
593989597,"@xiaomaogy I don't currently have Unity and ML-Agents setup on another operating system, but I do know of other people who have used the library (although not necessarily the gym-unity portion) on Windows without issue. Is it possible that I need some OS level environment variable needs to be set or something like that?",currently unity setup another operating system know people used library although necessarily portion without issue possible need o level environment variable need set something like,issue,negative,neutral,neutral,neutral,neutral,neutral
593985013,"> Not sure about that warning. I think adding
> `self.summary_writer.add_graph(self.policy.graph)`
> in Trainer.save_model() might be a better place instead; that gets called if you stop training early.

Any updates doing this with ml agents 0.14?",sure warning think might better place instead stop training early,issue,negative,positive,positive,positive,positive,positive
593925387,"Thank you for your answer, I didn't take into account that. Now I understand ;)",thank answer take account understand,issue,negative,neutral,neutral,neutral,neutral,neutral
593812290,"HI @xiaomaogy  , thank you for answering

I followed these steps to record the demonstration: 
- added the Demonstration Recorder component
- checked the record checkbox
- removed the NN model 
- Switched to Heuristic only  
- Hit play and started playing with WASD keys.

The resulting .demo file has:
Number Experiences: 4430
Number Episodes: 23
Mean Reward: 1.722

Which is higher than the ExpertPyramid which has:
Number Experiences: 1984
Number Episodes: 13
Mean Reward: 1.694


I do not know how to playback the demo file.  ",hi thank record demonstration added demonstration recorder component checked record removed model switched heuristic hit play resulting file number number mean reward higher number number mean reward know playback file,issue,positive,negative,negative,negative,negative,negative
593769490,Is it worth testing the other visual encoders? Maybe a smaller number of iterations just to cover the code?,worth testing visual maybe smaller number cover code,issue,negative,positive,neutral,neutral,positive,positive
593674360,Hi @xXZetserXx Does this only happen on Ubuntu? I tried on mac and it seems to use 5004 by default. ,hi happen tried mac use default,issue,negative,neutral,neutral,neutral,neutral,neutral
593669707,"Hi @AdamPalmarUnity , this really depends on how you recorded the demonstration. How did you record your own demonstration? Have you tried to play back the expert demo file and compare it against yours? ",hi really demonstration record demonstration tried play back expert file compare,issue,positive,positive,neutral,neutral,positive,positive
593669128,"Hi @Procuste34, this is not a bug. As noted in https://github.com/Unity-Technologies/ml-agents/blob/93a419715022336bc1af06cf3f067f7f575fd842/com.unity.ml-agents/Runtime/DecisionRequester.cs. The decision period means: The frequency with which the agent requests a decision. A DecisionPeriod of 5 means that the Agent will request a decision every 5 Academy steps.""

Here Academy step is different from the step on the ""step"" on the python side (and what you see in the command line). On the python side,  every time the agent request a decision, we call that as one step. So longer the decision period, longer one step will take (because for each step, we need to wait for decision period). ",hi bug noted decision period frequency agent decision agent request decision every academy academy step different step step python side see command line python side every time agent request decision call one step longer decision period longer one step take step need wait decision period,issue,negative,neutral,neutral,neutral,neutral,neutral
593646591,"Hi @ervteng, 
Thanks. We solved our problem. There was a bug in our environment which was not flagged in V0.13 but with the new multiagent system in V0.14 we figured it out. ",hi thanks problem bug environment new system figured,issue,negative,positive,positive,positive,positive,positive
593605910,"Hi @sahandrez, are the agents done at different times/do they use on demand decisions? 

Could you post the Inspector for your agents? Would be really helpful for us to debug. Thanks!",hi done different use demand could post inspector would really helpful u thanks,issue,positive,positive,positive,positive,positive,positive
593562684,"Hi @niskander,
gRPC is not used for inference, sorry for the confusion.  There is a build issue with gRPC and IL2CPP.  With the way our code is currently organized, we include gRPC with builds no matter if you are training or not.  For reasons unknown to us at this time, training does not work when IL2CPP is enabled.  

The file you see in the repo fixes gRPC link errors on windows for standalone builds with IL2CPP enabled, yet training still does not work.  So you can technically build a standalone player with IL2CPP enabled, but you cannot train with it.  You can only run inference. 

Does that make sense?",hi used inference sorry confusion build issue way code currently organized include matter training unknown u time training work file see link yet training still work technically build player train run inference make sense,issue,negative,negative,negative,negative,negative,negative
593552626,"@surfnerd Thanks for the response. It's good to see progress on that front, but just so I fully understand, is gRPC actually used for inference? I was under the impression that it was only used for training. ",thanks response good see progress front fully understand actually used inference impression used training,issue,positive,positive,positive,positive,positive,positive
593521626,"Hi @niskander,
There were some stubs added that fixed compilation on windows and only inference works.  I spent some time last week trying to get training to work with IL2CPP and a newer version of gRPC, but ran into a bunch of deadlocks with the interaction between gRPC and Mono in the Unity Editor.  

Long story short: IL2CPP still isn't working with ML-Agents yet, but it may compile and be able to run inference on some platforms. ",hi added fixed compilation inference work spent time last week trying get training work version ran bunch interaction mono unity editor long story short still working yet may compile able run inference,issue,negative,positive,neutral,neutral,positive,positive
593484392,"@chechulinYuri @surfnerd I saw an IL2CPP file in the latest release (0.14) with a comment that says the issue should be fixed for inference, so maybe. I kept my old workaround so can't confirm. @surfnerd can you confirm?",saw file latest release comment issue fixed inference maybe kept old ca confirm confirm,issue,negative,positive,positive,positive,positive,positive
592663967,"Hi @ervteng, 
I see. Thanks. I'm having another problem after fixing what you mentioned. Now in our custom environment, we are getting this error as soon as an agent is done in the environment: 
```
File ""\workspace\ml-agents\gym-unity\gym_unity\envs\__init__.py"", line 531, in mark_agent_done
    gym_index = self._agent_id_to_gym_index.pop(agent_id)
KeyError: 7
```
Any idea what might have caused it? Thanks.",hi see thanks another problem fixing custom environment getting error soon agent done environment file line idea might thanks,issue,negative,positive,positive,positive,positive,positive
592652893,"Hi @sahandrez, the multi-agent gym currently doesn't support environments with multiple types of behaviors (WallJump has two), or on-demand decisions (e.g. Bouncer). This is due to how gym expects all data per step to be from the same observation/action space. ",hi gym currently support multiple two bouncer due gym data per step space,issue,negative,negative,neutral,neutral,negative,negative
592268605,Re-requested review because I added a bunch of new tests.,review added bunch new,issue,negative,positive,positive,positive,positive,positive
592248985,"Yes, the signature of this method has changed in master.",yes signature method master,issue,negative,neutral,neutral,neutral,neutral,neutral
592248708,"Hi @surfnerd - I did download latest_release (and Master) originally. Have cleaned packages and installed unity package from disk ""latest-release"" again. C# compiler is now complaining about CollectObservations signature in my agent so guessing parent class has regressed to 0.14 signatures? ",hi master originally unity package disk compiler signature agent guessing parent class,issue,negative,positive,positive,positive,positive,positive
592246835,"> Looks good, not sure we need ""discrete"" everywhere though.

I will leave this decision to the discretion of the reviewers",good sure need discrete everywhere though leave decision discretion,issue,positive,positive,positive,positive,positive,positive
592244785,"Hi, the soccer example now has a trained model. ",hi soccer example trained model,issue,negative,neutral,neutral,neutral,neutral,neutral
592242969,"Hi @AndyGisby,
Please checkout the [latest release](https://github.com/Unity-Technologies/ml-agents/blob/latest_release/docs/Installation.md#clone-the-ml-agents-toolkit-repository) of ML-Agents.  It looks like you have checked out master.",hi please latest release like checked master,issue,positive,positive,positive,positive,positive,positive
592200420,"Hi @sahandrez,
In the build dialog you need to add the open scene and remove this one.  I'll fix this for the next release.",hi build need add open scene remove one fix next release,issue,negative,neutral,neutral,neutral,neutral,neutral
592175196,I will try to first reinstall my python environment for ml-agents and update to the version that 0.14.0 uses and try again with one of the sample learning environments to see if the problem happens again.,try first reinstall python environment update version try one sample learning see problem,issue,negative,positive,positive,positive,positive,positive
592156976,"Standalone tests are passing, so I'm going to assume this is fixed and not wait for the package tests.",passing going assume fixed wait package,issue,negative,positive,neutral,neutral,positive,positive
592130472,TimerStack change done in https://github.com/Unity-Technologies/ml-agents/pull/3517/files instead (needs some other Agent change),change done instead need agent change,issue,negative,neutral,neutral,neutral,neutral,neutral
592118101,"Hrm, we currently need a TimerStack method for recording inference rewards. Let me try something else there.",currently need method recording inference let try something else,issue,negative,neutral,neutral,neutral,neutral,neutral
592115716,We've updated the System.Interactive.Async assembly to resolve this issue.  The fix was merged to master in #3512 and will be in the next release.,assembly resolve issue fix master next release,issue,negative,neutral,neutral,neutral,neutral,neutral
592076215,hi @wightwhale  - can you open your request on our forum instead? https://forum.unity.com/forums/ml-agents.453/,hi open request forum instead,issue,negative,neutral,neutral,neutral,neutral,neutral
592075627,"There's actually two errors. 

NullReferenceException: Object reference not set to an instance of an object
MLAgents.InferenceBrain.ModelRunner.DecideBatch () (at Assets/ML-Agents/Scripts/InferenceBrain/ModelRunner.cs:129)
MLAgents.BarracudaPolicy.DecideAction () (at Assets/ML-Agents/Scripts/Policy/BarracudaPolicy.cs:53)
MLAgents.Agent.DecideAction () (at Assets/ML-Agents/Scripts/Agent.cs:1006)
MLAgents.Academy.EnvironmentStep () (at Assets/ML-Agents/Scripts/Academy.cs:535)
MLAgents.Academy.FixedUpdate () (at Assets/ML-Agents/Scripts/Academy.cs:567)
UnityEngine.GUIUtility:ProcessEvent(Int32, IntPtr) (at C:/buildslave/unity/build/Modules/IMGUI/GUIUtility.cs:179)
",actually two object reference set instance object,issue,negative,neutral,neutral,neutral,neutral,neutral
591808271,"@chriselion Thank you for the fix. 
I already checked with the patch given at #3464 on ml-agents/mlagents/trainers/demo_loader.py
that it works fine by now.
Thanks!",thank fix already checked patch given work fine thanks,issue,positive,positive,positive,positive,positive,positive
591786410,"@JoRouss Good point, that was something that Barracuda addressed in the 0.6.0 release:
> Fix: enabled Google Protobuf for all platforms as ML Agents rely on this package. Fixes building ML Agents on non-desktop platforms.

(from https://github.com/Unity-Technologies/barracuda-release/blob/release/0.6.0/CHANGELOG.md#060---2020-02-14)",good point something barracuda release fix rely package building,issue,negative,positive,positive,positive,positive,positive
591781832,"Just a heads up that this should be fixed in the 0.14.1 release, which came out today. Your demonstration files from 0.14.0 will load, but you probably need to re-record them because some information wasn't captured during the initial recording.",fixed release came today demonstration load probably need information initial recording,issue,negative,positive,neutral,neutral,positive,positive
591765580,"In unity 2019.3.3f1, ml-agent 0.14.1, I just had to update the Barracuda package from version 0.5.0 to version 0.6.0 and I was able to build without the error.",unity update barracuda package version version able build without error,issue,negative,positive,positive,positive,positive,positive
591728875,@vincentpierre - let me take a stab at the testing for side channels and then I'll circle back to comment on IFloatProperties.,let take stab testing side circle back comment,issue,negative,neutral,neutral,neutral,neutral,neutral
591683768,"> looks good once the tests pass. Thanks for this!

No idea why that one breaks. Wondering if I should just relaunch the test...
",good pas thanks idea one wondering relaunch test,issue,positive,positive,positive,positive,positive,positive
591123513,"Just a followup on this - I just merged a https://github.com/Unity-Technologies/ml-agents/pull/3484 which (amongst other things) will draw the RayPerception gizmos whenever the Agent is selected (including non-play mode and using Heuristic behavior). It's on the master branch now and will be in the next release (in a few weeks).

This still doesn't run the sensors when using Heuristic behaviors (which is a larger discussion) but it should be enough to solve your original problem.",amongst draw whenever agent selected mode heuristic behavior master branch next release still run heuristic discussion enough solve original problem,issue,negative,positive,positive,positive,positive,positive
591044174,">  I wonder if these would be helpful in a player build as well.

Do you mean the gizmo drawing? What's the typical setup?",wonder would helpful player build well mean drawing typical setup,issue,positive,negative,negative,negative,negative,negative
590758903,"@chriselion For what it's worth, I was using 0.13 instead of 0.14, so I understand some ray length issues may have alreay been fixed. Maybe my issue post here, which was more of a general wrapup, is (after having been read) of better use as Closed? (It's also very hard for me to see the proper length to begin with, as due to the other issues I've mentioned which makes the ray non-3d.) And thanks!",worth instead understand ray length may fixed maybe issue post general read better use closed also hard see proper length begin due ray thanks,issue,positive,positive,neutral,neutral,positive,positive
590695528,"Sorry, just getting back to the feedback on this. Can you provide a screenshot of the original issue you mentioned (""The Ray sensor gizmos almost never show their true ray length"")? Or do you mean you want to visual the full ray instead of the hit location? Do you have any non-1.0 scale on the agent?",sorry getting back feedback provide original issue ray sensor almost never show true ray length mean want visual full ray instead hit location scale agent,issue,negative,positive,neutral,neutral,positive,positive
590599831,"Hi @Xiromtz - thanks for calling out this issue. We just updated the doxygen config file to point to the correct directory. This is currently fixed on the `master` (unreleased branch) and will be pushed out in our next release. You can find the change in this PR: https://github.com/Unity-Technologies/ml-agents/pull/3490/
",hi thanks calling issue file point correct directory currently fixed master unreleased branch next release find change,issue,negative,positive,positive,positive,positive,positive
590597472,"I commented out the unnecessary flags in the dox file to manage the initial warnings.

I noticed the comment-block errors and they're similar to the package validation ones, so I suspect these will go away as we fix the package validation errors. But good to keep those in mind in CircleCI if we keep doxygen around.",unnecessary file manage initial similar package validation suspect go away fix package validation good keep mind keep around,issue,negative,positive,neutral,neutral,positive,positive
590595331,"Note: if I run locally, I get a few config warnings, and a few things noted as missing. Don't expect them to be fixed in this PR, but this might make a good first pass for sanity-checking the docs locally or in CircleCI.

```
warning: Tag 'CLANG_ASSISTED_PARSING' at line 1034 of file 'dox-ml-agents.conf' belongs to an option that was not enabled at compile time.
         To avoid this warning please remove this line from your configuration file or upgrade it using ""doxygen -u"", or recompile doxygen with this feature enabled.
warning: Tag 'CLANG_OPTIONS' at line 1042 of file 'dox-ml-agents.conf' belongs to an option that was not enabled at compile time.
         To avoid this warning please remove this line from your configuration file or upgrade it using ""doxygen -u"", or recompile doxygen with this feature enabled.
warning: Tag 'PERL_PATH' at line 2125 of file 'dox-ml-agents.conf' has become obsolete.
         To avoid this warning please remove this line from your configuration file or upgrade it using ""doxygen -u""
warning: Tag 'MSCGEN_PATH' at line 2147 of file 'dox-ml-agents.conf' has become obsolete.
         To avoid this warning please remove this line from your configuration file or upgrade it using ""doxygen -u""
warning: failed to open layout file 'doxygen/doxlayout.xml' for reading!
/Users/chris.elion/code/ml-agents/com.unity.ml-agents/Runtime/Academy.cs:189: warning: argument 'sideChannel' of command @param is not found in the argument list of Academy::RegisterSideChannel(SideChannel channel)
/Users/chris.elion/code/ml-agents/com.unity.ml-agents/Runtime/Academy.cs:193: warning: The following parameter of MLAgents.Academy.RegisterSideChannel(SideChannel channel) is not documented:
  parameter 'channel'
/Users/chris.elion/code/ml-agents/com.unity.ml-agents/Runtime/Academy.cs:199: warning: argument 'sideChannel' of command @param is not found in the argument list of Academy::UnregisterSideChannel(SideChannel channel)
/Users/chris.elion/code/ml-agents/com.unity.ml-agents/Runtime/Academy.cs:204: warning: The following parameter of MLAgents.Academy.UnregisterSideChannel(SideChannel channel) is not documented:
  parameter 'channel'
/Users/chris.elion/code/ml-agents/com.unity.ml-agents/Runtime/Sensor/RayPerceptionSensor.cs:309: warning: argument 'output' of command @param is not found in the argument list of RayPerceptionSensor::PerceiveStatic(RayPerceptionInput input)```",note run locally get noted missing expect fixed might make good first pas locally warning tag line file option compile time avoid warning please remove line configuration file upgrade recompile feature warning tag line file option compile time avoid warning please remove line configuration file upgrade recompile feature warning tag line file become obsolete avoid warning please remove line configuration file upgrade warning tag line file become obsolete avoid warning please remove line configuration file upgrade warning open layout file reading warning argument command param found argument list academy channel warning following parameter channel parameter warning argument command param found argument list academy channel warning following parameter channel parameter warning argument command param found argument list input,issue,negative,positive,neutral,neutral,positive,positive
590589648,"Looks like this just failed on formatting.  You can have the pre-commit hooks automatically run locally by running:
```
pip install pre-commit
pre-commit install
```
 from your ML-Agents repo root.",like automatically run locally running pip install install root,issue,negative,neutral,neutral,neutral,neutral,neutral
590469999,"Hi @xtc002007 -- glad you were able to figure out the problem!  I'll close this issue now, but feel free to open a new issue if you run into any further problems.",hi glad able figure problem close issue feel free open new issue run,issue,positive,positive,positive,positive,positive,positive
590146177,"hi @rjstange - please provide more details to reproduce the behavior, including which environment and any other relevant information.",hi please provide reproduce behavior environment relevant information,issue,negative,positive,positive,positive,positive,positive
589783577,"I will certainly do it. I think several people are having this issue, so I think I will wait to the next release.. ",certainly think several people issue think wait next release,issue,negative,positive,neutral,neutral,positive,positive
589776345,"No unfortunately :(

I am taking a break from ML Agents, if you find anything about WebGL build I would appreciate it if you could message me. Thanks Guido",unfortunately taking break find anything build would appreciate could message thanks,issue,negative,negative,negative,negative,negative,negative
589473562,"@harperj 
I have solved this problem, mainly because the input values are not normalized. Thank you",problem mainly input thank,issue,negative,positive,positive,positive,positive,positive
589409492,hi @albertoxamin - can you set the option to save the replay buffer for SAC (save_replay_buffer)?,hi set option save replay buffer sac,issue,negative,neutral,neutral,neutral,neutral,neutral
589281515,Ran cloud training on this branch and as far as I can tell there are no regressions on the example environments. ,ran cloud training branch far tell example,issue,negative,positive,neutral,neutral,positive,positive
588558494,"```
markdown-link-check -c markdown-link-check.fast.json com.unity.*********/Third 

ERROR: File not found! Please provide a vaild filename as an argument.
markdown-link-check -c markdown-link-check.fast.json Party 

ERROR: File not found! Please provide a vaild filename as an argument.
markdown-link-check -c markdown-link-check.fast.json Notices.md 

ERROR: File not found! Please provide a vaild filename as an argument.
```
Will try to escape the file names properly, if I can ever figure out what that bash magic is doing.",error file found please provide argument party error file found please provide argument error file found please provide argument try escape file properly ever figure bash magic,issue,negative,positive,positive,positive,positive,positive
588520216,"Seems like an issue that you send variable numbers of banana observations @muffinmiffin.   Thanks for pointing this out though, helps me find a simple example of my game to test out similar theories.
",like issue send variable banana thanks pointing though find simple example game test similar,issue,positive,negative,neutral,neutral,negative,negative
588476207,"(going to merge as soon as the ""Fast Training"" tests pass)",going merge soon fast training pas,issue,negative,positive,positive,positive,positive,positive
588341414,"Yeah at the time of writing I came up with the idea of using multiple raycast sensors and thought raycasting for multiple tags would be the optimal solution. 
The fixed size observations didn't occur to me since I don't know the raycast implementation. I guess every Ray has N - number of tags - observations, where each observation depicts the normalized distance to the tag found?
Using N number of possible hits per ray definetely sounds like a solution. After implementing multiple raycast sensors, it works really well and seems like the more sensible and flexible solution though. With the sensor detection on child objects feature, it's pretty easy to have multiple ray sensor components more clearly defined.

@surfnerd regarding youur question: My agent should be able to detect enemy sightcones á la stealth gameplay, but when already detected and wihin a sightcone, should still be able to detect objects in front of him. Hooking up the sightcones with its own layer and using a single ray perception sensor for that single layer works fine.

I am a bit worried about the overhead and in some usecases, we might want to detect objects behind others within the same layer. For these cases, the current solutions won't work.

In short: For me, using multiple ray cast sensors works perfectly fine, I might have prematurely written this request before thinking it more into detail. I do believe in some cases, having N collision per ray may be useful, albeit niche.

Thanks everyone for the time taken looking into this!",yeah time writing came idea multiple thought multiple would optimal solution fixed size occur since know implementation guess every ray number observation distance tag found number possible per ray like solution multiple work really well like sensible flexible solution though sensor detection child feature pretty easy multiple ray sensor clearly defined regarding question agent able detect enemy la stealth already still able detect front layer single ray perception sensor single layer work fine bit worried overhead might want detect behind within layer current wo work short multiple ray cast work perfectly fine might prematurely written request thinking detail believe collision per ray may useful albeit niche thanks everyone time taken looking,issue,positive,positive,positive,positive,positive,positive
588245010,"I see. Then I think it makes more sense to have pre-built binaries, especially for those who just want to test out their algorithms on a gym environment instead of building one. I personally work on a gym environment that uses Java as backend engine. To make it convenient, I use continuous integration service to pre-build binaries like http://microrts.s3-website-us-east-1.amazonaws.com/microrts/artifacts/.

As a result, the instructions for hello world is very simple(without downloading something like eclipse to build everything):

```
$ rm ~/microrts -fR && mkdir ~/microrts && \
wget -O ~/microrts/microrts.zip http://microrts.s3.amazonaws.com/microrts/artifacts/202002051504.microrts.zip && \
unzip ~/microrts/microrts.zip -d ~/microrts/ && \
rm ~/microrts/microrts.zip
$ git clone https://github.com/vwxyzjn/gym-microrts.git && \
cd gym-microrts && \
pip install dacite && \
pip install -e .
$ python3 hello_world.py
```
Perhaps this is something that you guys should consider as well. :)",see think sense especially want test gym environment instead building one personally work gym environment engine make convenient use continuous integration service like result hello world simple without something like eclipse build everything git clone pip install dacite pip install python perhaps something consider well,issue,positive,neutral,neutral,neutral,neutral,neutral
588088984,"@harperj 
I currently have two questions:
1  How to turn on debug flag in python training program?
2  In the current training process, the continuous output of vectorAction values will be very concentrated, for example, vectorAction [0] will be the same value continuously for a period of time, and other vectorAction [n] will remain the same, even an episode is such. I learned earlier that at the beginning of training, the ratio of each vectorAction [n] was equal. Could this be caused by a configuration file? My configuration file is as follows:
default:
    trainer: ppo
    batch_size: 1024
    beta: 5.0e-3
    buffer_size: 10240
    epsilon: 0.2
    hidden_units: 128
    lambd: 0.95
    learning_rate: 3.0e-4
    learning_rate_schedule: linear
    max_steps: 5.0e5
    memory_size: 256
    normalize: false
    num_epoch: 3
    num_layers: 2
    time_horizon: 64
    sequence_length: 64
    summary_freq: 10000
    use_recurrent: false
    vis_encode_type: simple
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99
            
War03:
    batch_size: 128
    buffer_size: 4096
    epsilon: 0.3
    beta: 5.0e-2",currently two turn flag python training program current training process continuous output concentrated example value continuously period time remain even episode learned beginning training ratio equal could configuration file configuration file default trainer beta epsilon linear normalize false false simple extrinsic strength gamma war epsilon beta,issue,negative,negative,negative,negative,negative,negative
588066960," @xtc002007 I'm doubtful you'll get much more that is useful from the debug flag.  I'd recommend making sure your observations cover all of the important information the agents need for decision making and that you give reward for any incremental progress.

Stopping training and loading up the model into your game to look at how the agent behaves might also be useful.",doubtful get much useful flag recommend making sure cover important information need decision making give reward incremental progress stopping training loading model game look agent might also useful,issue,positive,positive,neutral,neutral,positive,positive
588061617,@harperj    Thank you for your answer. Can I turn on debug mode during training and output more information? In order to observe some details in the process,thank answer turn mode training output information order observe process,issue,negative,neutral,neutral,neutral,neutral,neutral
588018933,"Since this has broken twice now, I definitely needs some tests. It looks like none of the logic that was added previously has any coverage, and neither do most of the changes here.",since broken twice definitely need like none logic added previously coverage neither,issue,negative,negative,negative,negative,negative,negative
587982240,"Thanks for the report @yosider.  I've made the fix in #3470.  Going to close this issue, but please let us know if you run into any more problems.",thanks report made fix going close issue please let u know run,issue,positive,positive,positive,positive,positive,positive
587955231,"> If we could remove the demo store stuff from Agent, it feels like that would be ideal. 

Open to suggestions on how to do this. Maybe an IPolicy that wraps the ""main"" IPolicy, and forwards everything to the wrapped IPolicy and also handles the writing?

Anyway, you're happy revisiting this later, right?",could remove store stuff agent like would ideal open maybe main forward everything wrapped also writing anyway happy later right,issue,positive,positive,positive,positive,positive,positive
587954657,"Hi @Xiromtz,
I'm curious about your comment 
```
For example, when the agent is inside of an enemy sightcone, it won't be able to perceive ANYTHING outside of the sightcone itself. This is obviously very problematic.
```

To me, it seems like your agent's raycasts should be ignoring your enemy's sight cone.  This can be set up in the layer mask of the agent's ray cast sensor. 

If you'd like you know the first N things a ray cast hit, then that is something we would need to work on.  Could you clarify what you are trying to achieve?",hi curious comment example agent inside enemy wo able perceive anything outside obviously problematic like agent enemy sight cone set layer mask agent ray cast sensor like know first ray cast hit something would need work could clarify trying achieve,issue,negative,positive,positive,positive,positive,positive
587952272,"I'm not too familiar with `RaycastAll()`, but it looks like it can return an arbitrary number of results (depending on how long the ray is and what's in the way). On the other hand, observations need to be a fixed size. How do you propose deciding which results from RaycastAll get used in the observation? Use the closest N hits per ray, where N is a parameter on the sensor?",familiar like return arbitrary number depending long ray way hand need fixed size propose get used observation use per ray parameter sensor,issue,positive,positive,neutral,neutral,positive,positive
587950126,I re-ran your linux yamato job since it failed with an error that had nothing to do with your changes. ,job since error nothing,issue,negative,neutral,neutral,neutral,neutral,neutral
587747050,"Hi @AidanNelson -- yes, this issue seems to be related to the other issues we're having with demo files in 0.14.0.  We're working on a fix for this and will update when this is fixed.",hi yes issue related working fix update fixed,issue,negative,positive,neutral,neutral,positive,positive
587744713,Hi @Procuste34 -- thanks for the bug report and repro steps.  I'll share this issue with the team.,hi thanks bug report share issue team,issue,positive,positive,positive,positive,positive,positive
587736992,"Hi @xtc002007 -- unfortunately our team doesn't have the resources to help debug training issues with custom training environments.  Anecdotally, many of us have had certain reward functions and environments regress during the training process.

You might find more support from the community on our forums: https://forum.unity.com/forums/ml-agents.453/",hi unfortunately team help training custom training many u certain reward regress training process might find support community,issue,positive,positive,neutral,neutral,positive,positive
587736273,"@vwxyzjn Yes, the editor does requires a desktop environment. You do not need to install unity if you have a pre-built binary.",yes editor environment need install unity binary,issue,negative,neutral,neutral,neutral,neutral,neutral
587732729,Hi @Xiromtz -- thanks for the suggestion.  I agree that the best alternative at this time is to use multiple ray perception components.  I'll share your feedback with the team so that we can consider how to improve the ray perception feature.,hi thanks suggestion agree best alternative time use multiple ray perception share feedback team consider improve ray perception feature,issue,positive,positive,positive,positive,positive,positive
587692539,BTW can you re-record `test.demo` (it's just 3DBall using inference) in the `test` directory? Just so we can make sure the Python is compatible with the new demos,inference test directory make sure python compatible new demo,issue,negative,positive,positive,positive,positive,positive
587664523,Hi @yosider -- I think you're right that the checkpoints should be `.ckpt`.  I'll bring this issue up with the team.,hi think right bring issue team,issue,negative,positive,positive,positive,positive,positive
587662393,Thanks for the information @Dastyn / @Xiromtz.  We've confirmed that this is a bug with v0.14.0 and are working on a fix.,thanks information confirmed bug working fix,issue,negative,positive,positive,positive,positive,positive
587631882,"Hi @vwxyzjn.

I do not maintain that notebook anymore as there was little to no feedback from the community. I have a version of this notebook that works with my own fork of ``mlagents``. You are more than welcome to copy my notebook and fix the issue you mentioned. The master branch is changing rapidly; I strongly suggest to switch to one of the stable release branches. 

**Some notes for you:**
* Upgrade the version of Unity to 2019.3, Here is the [URL](https://beta.unity3d.com/download/27ab2135bccf/UnitySetup-2019.3.0f6) for latest Linux version of Unity

* You'll probably run into more issues when compiling a project, I strongly suggest to take a look at [this repository](https://gitlab.com/gableroux/unity3d/). For example, [this issue](https://gitlab.com/gableroux/unity3d/issues/35) will help you to solve a problem related to the sound card.

* Instead of TensorBoard in Colab, Try [Tensorboard.dev](https://tensorboard.dev/). (Or you can use [WandB](www.wandb.com))

Cheers,
M
",hi maintain notebook little feedback community version notebook work fork welcome copy notebook fix issue master branch rapidly strongly suggest switch one stable release upgrade version unity latest version unity probably run project strongly suggest take look repository example issue help solve problem related sound card instead try use,issue,positive,positive,positive,positive,positive,positive
587617182,I'm going to close this issue since it doesn't provide any information about the bug.  Feel free to open a new issue with the filled in template if you have found a bug.,going close issue since provide information bug feel free open new issue filled template found bug,issue,positive,positive,positive,positive,positive,positive
587564407,"Hi @jli98dan, 
The version of Unity you are using is an alpha version of a tech release.  An alpha version means it's unstable, and having bugs is an expectation.  I recommend you upgrade to the actual release of 2019.3.  Please let me know if that helps you.",hi version unity alpha version tech release alpha version unstable expectation recommend upgrade actual release please let know,issue,negative,neutral,neutral,neutral,neutral,neutral
587297524,"Thanks, I'll keep an eye out for it, but feel free to link to the forum thread here too. ",thanks keep eye feel free link forum thread,issue,positive,positive,positive,positive,positive,positive
587118904,"Hi,
I have ML-Agents 0.14.0 version installed. I retrieved the library folder
and now I am not even able to add the package.JSON file when I select the
""Add package from disk"" button under package management.
Thank you so much for your help and time!
Best,
Danielle Li

On Sun, Feb 16, 2020 at 11:03 PM Chris Goy <notifications@github.com> wrote:

> Hi @jli98dan <https://github.com/jli98dan>,
> Could you provide the version of ML-Agents you are using? Please give us
> all the information you can when making a request.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/3397?email_source=notifications&email_token=AOOBRJB2IKXWOSXCUXIZURTRDIECJA5CNFSM4KR2W7KKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEL47XKY#issuecomment-586808235>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AOOBRJEL2QTF5EHHPKYFWN3RDIECJANCNFSM4KR2W7KA>
> .
>
",hi version library folder even able add file select add package disk button package management thank much help time best li sun goy wrote hi could provide version please give u information making request reply directly view,issue,positive,positive,positive,positive,positive,positive
587094706,"It seems to be a specific problem of gym_unity, as the reward is not 0 when interacting with the environement with mlagents_envs.
Code : 

```
import matplotlib.pyplot as plt
import numpy as np
import sys

from mlagents_envs.environment import UnityEnvironment
from mlagents_envs.side_channel.engine_configuration_channel import EngineConfig, EngineConfigurationChannel

engine_configuration_channel = EngineConfigurationChannel()
env = UnityEnvironment(base_port = UnityEnvironment.DEFAULT_EDITOR_PORT, worker_id=7, file_name=""../../../ml-agents/envs/buildbasic/Basic"", side_channels = [engine_configuration_channel])

#Reset the environment
env.reset()

# Set the default brain to work with
group_name = env.get_agent_groups()[0]
group_spec = env.get_agent_group_spec(group_name)

# Set the time scale of the engine
engine_configuration_channel.set_configuration_parameters(time_scale = 1.0)

for episode in range(2):
    env.reset()
    step_result = env.get_step_result(group_name)
    done = False
    episode_rewards = 0
    while not done:
        action = np.array([2]).reshape(1, 1)
        
        env.set_actions(group_name, action)
        env.step()
        step_result = env.get_step_result(group_name)
        
        episode_rewards += step_result.reward[0]
        done = step_result.done[0]
        print(done, step_result.reward[0])
        
    print(""Total reward this episode: {}"".format(episode_rewards))
env.close()
```
Output : 
```
False -0.01
False -0.01
False -0.01
False -0.01
False -0.01
False -0.01
True 0.99
Total reward this episode: 0.9300000108778477
False -0.01
False -0.01
False -0.01
False -0.01
False -0.01
False -0.01
True 0.99
Total reward this episode: 0.9300000108778477
```

As you can see, we observe a reward of +1 at the last timestep (0.99 exactly because we receive -0.1 per timestep)",specific problem reward code import import import import import reset environment set default brain work set time scale engine episode range done false done action action done print done print total reward episode output false false false false false false true total reward episode false false false false false false true total reward episode see observe reward last exactly receive per,issue,positive,negative,negative,negative,negative,negative
587089184,Could you please post this on the Unity forums? We're trying to keep questions related to installation (like this one) there instead of on Github.,could please post unity trying keep related installation like one instead,issue,positive,neutral,neutral,neutral,neutral,neutral
586808235,"Hi @jli98dan,
Could you provide the version of ML-Agents you are using?  Please give us all the information you can when making a request.  ",hi could provide version please give u information making request,issue,negative,neutral,neutral,neutral,neutral,neutral
586778328,"Hi Chris,
I am using Unity 2019 3.0a5 version. For the project that I am working on
right now, we are told to use this specific version of unity.
This error occurred while I was following the tutorial for making a new
learning environment. I tried deleting the library folder like you said,
however I am still unable to run my project.
Thanks a lot for your help!
Best,
Danielle Li

Get Outlook for iOS <https://aka.ms/o0ukef>
------------------------------
*From:* Chris Goy <notifications@github.com>
*Sent:* Monday, February 10, 2020 1:02:15 PM
*To:* Unity-Technologies/ml-agents <ml-agents@noreply.github.com>
*Cc:* jli98dan <jli98dan@terpmail.umd.edu>; Comment <
comment@noreply.github.com>
*Subject:* Re: [Unity-Technologies/ml-agents] Burst Package Error w/ Unity
2019_3 + (#3397)


Hi @LewisSim <https://github.com/LewisSim>,
Could you give a specific version of Unity that you are using and other
details about your setup? We are testing against Unity 2019.3 on every
commit with no issues.

Could you try to delete your Library folder under your Unity project
directory to see if that helps. It's not ideal, but sometimes that helps
fix things like this.

—
You are receiving this because you commented.
Reply to this email directly, view it on GitHub
<https://github.com/Unity-Technologies/ml-agents/issues/3397?email_source=notifications&email_token=AOOBRJAMB3LIDZTUCOL5MF3RCGJCPA5CNFSM4KR2W7KKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELJP26Q#issuecomment-584252794>,
or unsubscribe
<https://github.com/notifications/unsubscribe-auth/AOOBRJALLQGBYANXBNZCBKDRCGJCPANCNFSM4KR2W7KA>
.
",hi unity version project working right told use specific version unity error following tutorial making new learning environment tried library folder like said however still unable run project thanks lot help best li get outlook goy sent comment comment subject burst package error unity hi could give specific version unity setup testing unity every commit could try delete library folder unity project directory see ideal sometimes fix like reply directly view,issue,positive,positive,positive,positive,positive,positive
586687061,"Thanks @chriselion and @Xiromtz for your help and reactivity.

The same for me: this issue concerns new demo recordings with the 0.14.0 version.
(my .demo files were 0.14.0-compliant)
 ",thanks help reactivity issue new version,issue,positive,positive,positive,positive,positive,positive
586680666,"I have exactly the same issue, it happens with both GAIL and Behavioral Cloning, though the error message differs when using Behavioral Cloning. I made a thread in the Unity forums for it.
I tested the Unity examples folder with their GAIL implementation and interestingly enough, no errors occur there.
Did you do use old recordings or did you make new ones in version 0.14.0? This might be a bug with the recordings themselves, since I recorded mine in the new version..",exactly issue behavioral though error message behavioral made thread unity tested unity folder implementation interestingly enough occur use old make new version might bug since mine new version,issue,negative,positive,positive,positive,positive,positive
586653768,"Hope you don't mind, I edited the formatting on your comment to make the stacktrace easier to read.

Are you able to reproduce this with any of the example scenes?

We'll look into it first thing on Tuesday (Monday is a holiday for our office).",hope mind comment make easier read able reproduce example look first thing holiday office,issue,positive,positive,positive,positive,positive,positive
586548267,"@anupambhatnagar Thanks for the prompt reply. I will look forward to that. 

When you said “within the editor”, the editor requires a desktop environment, right?

Another question: if we have the pre-built binaries, do we even still need to install unity?",thanks prompt reply look forward said within editor editor environment right another question even still need install unity,issue,negative,positive,positive,positive,positive,positive
586539037,"@vwxyzjn we expect to have something ready by the end of the month. If it's not a docker image then there would be a dockerfile with instructions on how to build it locally. 

Regarding the binaries - at present we don't have any plans to make the pre-built binaries of the example envs available to the user. One way to generate the executable would be to delete all but one scene and run the build from within the editor. It's a bit of work but certainly doable.",expect something ready end month docker image would build locally regarding present make example available user one way generate executable would delete one scene run build within editor bit work certainly doable,issue,positive,positive,positive,positive,positive,positive
586528073,"> Are we considering this for a hotfix release?

Yes; probably would want to bundle it with other fixes though. ",considering release yes probably would want bundle though,issue,negative,neutral,neutral,neutral,neutral,neutral
586491280,"> Is my understanding correct?

Yes, this is what I am aiming at",understanding correct yes aiming,issue,negative,neutral,neutral,neutral,neutral,neutral
586491086,"@smartcharith no, unfortunately at this time none of our environments combine raycast and visual observations.",unfortunately time none combine visual,issue,negative,negative,negative,negative,negative,negative
586486009,"what does the step count look like for an episode?

It looks like if `maxStep = 5` the step count will look like:
```
AgentStep = 1
  Agent Observe; Agent Act;
AgentStep = 2
  Agent Observe; Agent Act;
AgentStep = 3
  Agent Observe; Agent Act;
AgentStep = 4
  Agent Observe; Agent Act;
AgentStep = 5
  Agent Observe; Agent Act; Agent Reset;
```

Is my understanding correct?",step count look like episode like step count look like agent observe agent act agent observe agent act agent observe agent act agent observe agent act agent observe agent act agent reset understanding correct,issue,positive,neutral,neutral,neutral,neutral,neutral
586480688,"> This is a valid concern, I think we need to make sure that there is a standard way to share environments and side channels to avoid such collisions. I do not know how to work around this, maybe using reflection to get the name of the class and use that as Id ?

Well, I think it comes back to the discussion we had.  Do we want to hard-code UUIDs on both sides?

For example, on mac you can do this:
```
(ml-agents) ➜  ml-agents git:(master) uuidgen
18F20E35-1258-4BA0-926C-E5F6DE5F6ACD
```

Would this be too much for users who want to create side channels?  This is basically the workflow of using the [Bluetooth LE protocol for characteristics](https://www.oreilly.com/library/view/getting-started-with/9781491900550/ch04.html).

It feels ripe for change, especially if we want to promote the sharing of environments and avoid conflicts of side-channel IDs.",valid concern think need make sure standard way share side avoid know work around maybe reflection get name class use id well think come back discussion want side example mac git master fe would much want create side basically protocol ripe change especially want promote avoid,issue,positive,positive,positive,positive,positive,positive
586473405,"I was receiving the same error messages , then started receiving no definition for ML-Agents.

- Unity 2019.3.0.f6
- Barracuda 0.6.0
- ML-Agents 0.13.1
- Burst 1.2.3

Issue resolved after manually installing the new required package.

window->package manager -> + -> (package located in ml-agents folder) com.unity.ml-agents

Instructions found in installation section.

Hopefully this also resolves the issue for you as well.

",error definition unity barracuda burst issue resolved manually new package package manager package folder found installation section hopefully also issue well,issue,negative,positive,positive,positive,positive,positive
586469626,"@ervteng Barracuda importer will handle everything, we just need to export the ONNX.",barracuda importer handle everything need export,issue,negative,neutral,neutral,neutral,neutral,neutral
586468019,This doesn't yet do ONNX -> Barracuda right? The export part LGTM,yet barracuda right export part,issue,negative,positive,positive,positive,positive,positive
586460318,@chriselion the PR is not generated even after this change. Based on [this](https://github.com/ionic-team/ionic/blob/master/.github/PULL_REQUEST_TEMPLATE.md) example I'm changing the filename to uppercase and putting it in the `.github` folder.,even change based example folder,issue,negative,neutral,neutral,neutral,neutral,neutral
586450976,you are correct @chriselion. I'll change the folder name to PULL_REQUEST_TEMPLATE,correct change folder name,issue,negative,neutral,neutral,neutral,neutral,neutral
586450246,The last paragraph on this [page](https://help.github.com/en/enterprise/2.19/user/github/building-a-strong-community/about-issue-and-pull-request-templates#pull-request-templates) mentions that the name is not case sensitive but it needs to be in the `.github` folder.,last paragraph page name case sensitive need folder,issue,negative,positive,neutral,neutral,positive,positive
586450221,"> The only thing that worries me about the way SideChannel Ids are created is that if people start sharing environments (big if) channel Ids may clash since everyone is probably just going to go from the UserIStartId + 1

This is a valid concern, I think we need to make sure that there is a standard way to share environments and side channels to avoid such collisions. I do not know how to work around this, maybe using reflection to get the name of the class and use that as Id ?",thing way people start big channel may clash since everyone probably going go valid concern think need make sure standard way share side avoid know work around maybe reflection get name class use id,issue,negative,positive,positive,positive,positive,positive
586446622,"Not sure this is right either (upper case file name).

[This](https://help.github.com/en/github/building-a-strong-community/creating-a-pull-request-template-for-your-repository) makes it sound like either of these would work
* `.github/pull_request_template.md`
* `.github/PULL_REQUEST_TEMPLATE/pull_request_template.md`",sure right either upper case file name sound like either would work,issue,positive,positive,positive,positive,positive,positive
586411063,is there any example project available for study??,example project available study,issue,negative,positive,positive,positive,positive,positive
586396012,The only thing that worries me about the way SideChannel Ids are created is that if people start sharing environments (big if) channel Ids may clash since everyone is probably just going to go from the `UserIStartId + 1`,thing way people start big channel may clash since everyone probably going go,issue,negative,neutral,neutral,neutral,neutral,neutral
586368191,"@anupambhatnagar Hello, that sounds great. Thanks for the great work. Any ETA on this? The current installation process without a GUI seems very convoluted and under-documented. 

In addition, it will be great if those docker containers come with pre-built binaries of all the example envs. I would like to run a simple benchmark using my own RL algorithms, but following the instructions from https://github.com/Unity-Technologies/ml-agents/issues/2684 to build every env seems like an over-complicated process. 

It would be very desirable to run the docker container with files like the following

```python
from gym_unity.envs import UnityEnv
# pre-built binaries
env_name = ""Project/Prebuilt/GridWorld""
env = UnityEnv(env_name, worker_id=0, use_visual=True)
print(str(env))
```

But perhaps it's not a good idea to update binaries directly in the github repo. Maybe you could consider releasing the pre-built binaries in the Github Release page of your repo? Or have a simple script to build all example envs or download pre-built envs from somewhere. ",hello great thanks great work eta current installation process without convoluted addition great docker come example would like run simple following build every like process would desirable run docker container like following python import print perhaps good idea update directly maybe could consider release page simple script build example somewhere,issue,positive,positive,positive,positive,positive,positive
586359810,"@taesiri It doesn't work anymore. See

![image](https://user-images.githubusercontent.com/5555347/74548229-75f62b80-4f1b-11ea-9434-78db134316c7.png)

I think `list_of_scenes` needs to be adjusted
```
# list_of_scenes = list(Path('/content/my-agents/UnitySDK/Assets/ML-Agents/Examples/').rglob('*.unity'))
list_of_scenes = list(Path('/content/my-agents/Project/Assets/ML-Agents/Examples/').rglob('*.unity'))
``` 

Then I can choose a scene, but then when I try to build it, I have another error

![image](https://user-images.githubusercontent.com/5555347/74548385-bd7cb780-4f1b-11ea-80a9-cd85318bfd7b.png)

I feel like there should be a guide on how to build all example envs headlessly... What if I want to train everything in the server where no GUI is available? The current guides do not seem to cover these instructions.",work see image think need list path list path choose scene try build another error image feel like guide build example want train everything server available current seem cover,issue,negative,positive,positive,positive,positive,positive
586060116,I cancelled the yamato tests. Will add changelog entries after the release branch is merged to master.,add release branch master,issue,negative,neutral,neutral,neutral,neutral,neutral
586059743,"Hi @smartcharith -- yes, it is possible to use a combination of visual and vector observations.",hi yes possible use combination visual vector,issue,negative,neutral,neutral,neutral,neutral,neutral
586058770,Hi @JPhilipp -- thanks for the request.  I'll share this with the team; of course we agree that it's helpful to have intuitive ways to visualize your observations :-),hi thanks request share team course agree helpful intuitive way visualize,issue,positive,positive,positive,positive,positive,positive
586028708,Please fill in the issue template with the requested information.,please fill issue template information,issue,negative,neutral,neutral,neutral,neutral,neutral
586013447,Canceled the Yamato tests manually as this will cause unnecessary load on the system.,manually cause unnecessary load system,issue,negative,negative,negative,negative,negative,negative
585953829,"I just want to drag my model here (in the agent):

![image](https://user-images.githubusercontent.com/44144188/74474557-0e51c900-4ea6-11ea-8de3-63727da3ab4e.png)

My model is created from pytorch (.pth) and converted to .onnx (or .nn).

Do I need the names and constants to do that? If the answer is yes, is there any example how to do it? ",want drag model agent image model converted need answer yes example,issue,negative,negative,neutral,neutral,negative,negative
585934957,"> Note that if you have non-1.0 scale on your Agent's transforms, there's a bug (#3321) in 0.13 (and probably earlier) that draws the rays incorrectly. The behavior of rays with scaled transforms should be more consistent in 0.14.

Sorry, I'm on 0.13 (based on a suggested fix for a Python incompatibility I had with 0.14). I'll try to mention that next time.

> Are you requesting decisions every step? The rays will ""fade"" over time on the steps when the agent doesn't request (and thus doesn't make observations).

I follow the pattern of most of the native examples by requesting a decision every 5 steps. But I still need the ray gizmo to be visually meaningful to me. I'd happily sacrifice it being very literal, in favor of it communicating what I need to see to debug my rays. I bet that's not easy to implement, but maybe best you get clear feedback, even if I don't have proper solutions!

> Definitely going to improve this soon

Great! Thanks!",note scale agent bug probably incorrectly behavior scaled consistent sorry based fix python incompatibility try mention next time every step fade time agent request thus make follow pattern native decision every still need ray visually meaningful happily sacrifice literal favor communicating need see bet easy implement maybe best get clear feedback even proper definitely going improve soon great thanks,issue,positive,positive,positive,positive,positive,positive
585929290,The standalone BC trainer was - but you can now use BC as part of PPO or SAC (and combine it with GAIL or RL if you want!). ,trainer use part sac combine want,issue,negative,neutral,neutral,neutral,neutral,neutral
585915515,"_For brevity, I'll use ""ray"", but all the comments should apply to spherecasts too_

> The Ray sensor gizmos almost never show their true ray length

Can you take a screenshot of this? If the rays hit something, the intended behavior is to only draw the ray up to the hit point. I suppose we could also draw the full ray alpha'd out a bit so you can see how far it goes. Note that if you have non-1.0 scale on your Agent's transforms, there's a bug (https://github.com/Unity-Technologies/ml-agents/issues/3321) in 0.13 (and probably earlier) that draws the rays incorrectly. The behavior of rays with scaled transforms should be more consistent in 0.14.

> * When pausing the view, the gizmos often (but not always) disappear

Are you requesting decisions every step? The rays will ""fade"" over time on the steps when the agent doesn't request (and thus doesn't make observations).

> * There's a toggle between World and Transform view of the rays, but I don't know the intent behind this option, I just want to see the best approximation of the ""real thing""
> * The rays often don't match the agent origin

These might be related to not stepping every frame and/or the order that the rays are cast vs when the gizmos draw. We cache the raycast hit results when we make them (coming from the RayPerceptionSensor.Write method) and display them in the OnDrawGizmos(), so I think the display might be a frame behind. This gets worse if you don't request a decision every frame; we'll keep drawing the previous calls rays and hits. 

This is what the `useWorldPositions` toggle is trying to solve. If it's on, we draw the rays where they were actually cast from, even if the agent has moved. If it's off, we transform the rays by the Agent's current position. But this might make the ray hit positions be inaccurate - if the agent moved forward since the cast, the hit could be ""inside"" a wall now.

> * The gizmos aren't shown in non-play mode (as previously reported and being looked at), nor in some types of heuristic modes (as reported by someone else and looked at)

Definitely going to improve this soon

> * As gizmo rays and spheres are not covered by objects, it makes it difficult to understand their position and direction in 3d. As having them shine through is also a bonus to see them, maybe it's possible to get a best of both worlds visualization here, like by having one lower opacity line drawn shining through, and one full opacity line drawn that gets 3d occluded. I'll post this one as a separate issue too.

Good feedback. I know that [Debug.DrawLine](https://docs.unity3d.com/ScriptReference/Debug.DrawLine.html) has an option for `depthTest`; I'll need to see if there's something similar for gizmo drawing (which is a separate API).

---

Thanks again for the great feedback. I'm going to do a pass on the sensor in the next week or two and hopefully address all of it. I think doing the raycasts during the gizmo drawing (and not just relying on cached results) will hopefully address some of the issues you highlighted here (and also make drawing work outside of play mode)",brevity use ray apply ray sensor almost never show true ray length take hit something intended behavior draw ray hit point suppose could also draw full ray alpha bit see far go note scale agent bug probably incorrectly behavior scaled consistent view often always disappear every step fade time agent request thus make toggle world transform view know intent behind option want see best approximation real thing often match agent origin might related stepping every frame order cast draw cache hit make coming method display think display might frame behind worse request decision every frame keep drawing previous toggle trying solve draw actually cast even agent transform agent current position might make ray hit inaccurate agent forward since cast hit could inside wall shown mode previously heuristic someone else definitely going improve soon covered difficult understand position direction shine also bonus see maybe possible get best visualization like one lower opacity line drawn shining one full opacity line drawn post one separate issue good feedback know option need see something similar drawing separate thanks great feedback going pas sensor next week two hopefully address think drawing hopefully address also make drawing work outside play mode,issue,positive,positive,positive,positive,positive,positive
585892045,Going to create a 2019.3 project instead.,going create project instead,issue,negative,neutral,neutral,neutral,neutral,neutral
585632355,"Awesome, thanks for the help!
For anyone having the same problem and wanting to quickly test their raycasts via heuristic input: 
A simple workaround is calling the heuristic function from the AgentAction function by replacing the action array.",awesome thanks help anyone problem wanting quickly test via heuristic input simple calling heuristic function function action array,issue,positive,positive,positive,positive,positive,positive
585446152,"Thanks for the insight!
Isn't BC becoming deprecated? (I may misremember this part)",thanks insight becoming may misremember part,issue,negative,positive,positive,positive,positive,positive
585445218,"To add to what @harperj suggested - the most direct way would be to collect _lots_ of demonstrations from your big model and use `behavioral_cloning` to clone the behavior over to the new model. This is very similar to the idea of Model Distillation (https://arxiv.org/abs/1503.02531, https://towardsdatascience.com/knowledge-distillation-simplified-dd4973dbc764) in supervised learning. ",add direct way would collect big model use clone behavior new model similar idea model distillation learning,issue,negative,positive,neutral,neutral,positive,positive
585395292,"@chriselion another thing that would help would be the ability to change the `BehaviorType` the agent is using (again, maybe if this causes problems, could we just change during `Awake()` or similar one-time?)

I'm hijacking the agent's `Heuristic()` method to pull actions from the UI so humans can control agents.  This is because Player Brains were removed.  It actually works out better than the Player Brains because I can now use mouse inputs and UI buttons, etc, setting the action in `Heuristic()` and it gets passed to `AgentAction` like normal.  This also allows for using demonstration recorder.  Not sure if that was the intent (see https://forum.unity.com/threads/how-to-use-imitation-learning-in-version-0-13.815481/) but it works well.

It would be great if I could set the `BehaviorType` to heuristic mode when I start the scene and read in my config file which specifies ""human mode"" or ""test mode"" or whatnot.

If you think it's better, I can create a new issue for a feature request instead of hijacking this one.
Thanks!",another thing would help would ability change agent maybe could change awake similar agent heuristic method pull control player brain removed actually work better player brain use mouse button setting action heuristic like normal also demonstration recorder sure intent see work well would great could set heuristic mode start scene read file human mode test mode whatnot think better create new issue feature request instead one thanks,issue,positive,positive,positive,positive,positive,positive
585350937,"Hi @AcelisWeaven -- thanks for the well motivated feature request.  At this time the best you could do to try to make this work would be to capture demonstration data from your old model during inference.  I'm not sure how effective this approach would be for capturing similar behavior in smaller models, but it seems like a reasonable thing to explore.

I'll share the request with the team to discuss whether it's something we'd like to add.",hi thanks well feature request time best could try make work would capture demonstration data old model inference sure effective approach would similar behavior smaller like reasonable thing explore share request team discus whether something like add,issue,positive,positive,positive,positive,positive,positive
585318160,Feel free to leave the issue open until the bug is fixed - we'll close it at that time. Thanks!,feel free leave issue open bug fixed close time thanks,issue,positive,positive,positive,positive,positive,positive
585121036,"Yeah, I forgot to mention that it also works in inference! But Inference obviously only works with a trained model, so I still can't test my implementation very well if I can't control the agent myself.
Thanks for the quick reply though! 
I've never used this github feature; Should I close this thread, since you've logged the bug internally? Or will it be closed once the bug is fixed?",yeah forgot mention also work inference inference obviously work trained model still ca test implementation well ca control agent thanks quick reply though never used feature close thread since logged bug internally closed bug fixed,issue,positive,positive,positive,positive,positive,positive
585020337,"> If you have the printout from the console of the Pushblock run, that would be super helpful. And the `timers.json` file from the `summaries` directory as well.
 I will check again, Thanks for your information! ",console run would super helpful file directory well check thanks information,issue,positive,positive,positive,positive,positive,positive
584914263,We need to merge https://github.com/Unity-Technologies/ml-agents/pull/3414 and retrain the FoodCollecter model,need merge retrain model,issue,negative,neutral,neutral,neutral,neutral,neutral
584876082,"Hi @AcelisWeaven, if it seems like a bug feel free to post it here - otherwise forums would be the best place so others can see it too.",hi like bug feel free post otherwise would best place see,issue,positive,positive,positive,positive,positive,positive
584865085,"Thanks @ervteng !
I've got another question regarding self-play, should I make a question issue or post on the forums?",thanks got another question regarding make question issue post,issue,negative,positive,positive,positive,positive,positive
584811716,"Hi @adverley, thanks for reporting this and we're working on a fix. In the meantime, checkout the previous release of ML-Agents (0.13.1) which shouldn't have this issue. ",hi thanks working fix previous release issue,issue,negative,positive,neutral,neutral,positive,positive
584811351,"The `com.unity.ml-agents` package only exists in the current `master` and will be released in the following ML-Agents release. If you want to install the package, clone `master` rather than `latest_release`. Otherwise, follow the install docs from the `release-0.13.1` branch. ",package current master following release want install package clone master rather otherwise follow install branch,issue,negative,neutral,neutral,neutral,neutral,neutral
584810397,"Hi @Xiromtz, you can see the raycasts during inference mode as well, but only in the scene view. But I was able to replicate that if you're using Heuristic the raycasts don't show. This seems like a bug, and I've logged it with internal tracking number MLA-622.",hi see inference mode well scene view able replicate heuristic show like bug logged internal number,issue,positive,positive,positive,positive,positive,positive
584790006,"Hi @AcelisWeaven, thanks for the suggestion, I'll bring it up with the team and perhaps we can figure out how to do this cleanly. Glad the self-play feature is working for you!",hi thanks suggestion bring team perhaps figure cleanly glad feature working,issue,positive,positive,positive,positive,positive,positive
584782967,"@chriselion 

> How do you know it works without a test?

Added a unit test that checks the basic deleting mechanism between resets.",know work without test added unit test basic mechanism,issue,negative,neutral,neutral,neutral,neutral,neutral
584782514,"![image](https://user-images.githubusercontent.com/5085265/74266428-20224900-4cb9-11ea-8265-949d3e87f185.png)
Bouncer
![image](https://user-images.githubusercontent.com/5085265/74266516-43e58f00-4cb9-11ea-8602-aa8cd27da1d6.png)
Hallway",image bouncer image hallway,issue,negative,neutral,neutral,neutral,neutral,neutral
584748335,"If you have the printout from the console of the Pushblock run, that would be super helpful. And the `timers.json` file from the `summaries` directory as well. ",console run would super helpful file directory well,issue,positive,positive,positive,positive,positive,positive
584473167,"@chriselion Thanks! One aspect to consider when finding a best solution: A developer may (I know I do) use heavy gameObject node copy & pasting & dragging around in the Hierarchy pane when setting up the original rays -- like set up the first ray, then copy it and rotate it for the second ray (if the Direction slider on the first ray isn't sufficient). And that beyond that, many devs may not care about the internal ray name given to it (but there's probably a use case for needing to know it, and have it be exposed in the inspector). Not sure if whatever Unity uses internally for node Ids in the hierarchy could be reused here as name.",thanks one aspect consider finding best solution developer may know use heavy node copy pasting dragging around hierarchy pane setting original like set first ray copy rotate second ray direction slider first ray sufficient beyond many may care internal ray name given probably use case needing know exposed inspector sure whatever unity internally node hierarchy could name,issue,positive,positive,positive,positive,positive,positive
584462107,"> Hi @tingfx4yu, can you try running your environment with PPO or SAC and see if you get the same slowdown? If it is, it's in either ML-Agents or the game - otherwise it's likely in your PyTorch code. Thanks!

Yes, currently I use sac to train pushblock environment, the slowdown will also happen. I think it's better for me to check log maybe I can find something wrong with it
Thanks a lot!",hi try running environment sac see get slowdown either game otherwise likely code thanks yes currently use sac train environment slowdown also happen think better check log maybe find something wrong thanks lot,issue,positive,neutral,neutral,neutral,neutral,neutral
584452882,"Hi @tingfx4yu, can you try running your environment with PPO or SAC and see if you get the same slowdown? If it is, it's in either ML-Agents or the game - otherwise it's likely in your PyTorch code. Thanks!",hi try running environment sac see get slowdown either game otherwise likely code thanks,issue,negative,negative,neutral,neutral,negative,negative
584416471,"Hmm, not a bad idea. I've thought about this before (and made similar mistakes) when adding multiple SensorComponents - what I'd really like is that when you add a new SensorComponent, it defaults to a unique name like RayPerceptionSensor_0, RayPerceptionSensor_1, etc.

I'll log something to revisit this and try to improve the workflow.",bad idea thought made similar multiple really like add new unique name like log something revisit try improve,issue,positive,positive,neutral,neutral,positive,positive
584414082,"It's something we can look into (we should at least raise a nice error message instead of a null reference exception). In the meantime, you can see the strings for the layer names that we're looking up here:
https://github.com/Unity-Technologies/ml-agents/blob/d4f764487b57602b4f7c746d67ca37d66dbc60bf/com.unity.ml-agents/Runtime/InferenceBrain/TensorNames.cs#L8-L27

And the constants are used here:
https://github.com/Unity-Technologies/ml-agents/blob/d4f764487b57602b4f7c746d67ca37d66dbc60bf/com.unity.ml-agents/Runtime/InferenceBrain/BarracudaModelParamLoader.cs#L140-L144


Stepping back a minute - are you trying to use your model for ML-Agents inference, or do you just need to be able to run Barracuda (general inference)? If it's the latter, you don't need to follow these conventions.",something look least raise nice error message instead null reference exception see layer looking used stepping back minute trying use model inference need able run barracuda general inference latter need follow,issue,negative,positive,positive,positive,positive,positive
584398103,"> My general impression is that there are now a lot of different classes :
> 
> * Policy
> * TFPolicy
> * NNPolicy
>   It is not always clear to me why there are so many (maybe because we do not have yet the use case for them). Also, the trainer directory is a little bit messy right now. It would be great if files could be grouped better?

I plan to make a separate PR that changes the directory structure into:

common /
- policy /
    - policy.py
    - tf_policy.py
    - nn_policy.py
- optimizer /
    - optimizer.py
    - tf_optimizer.py
- trainer /
    - trainer.py
    - rl_trainer.py

ppo /
sac /

I didn't want to do more changes into this PR. But you're right, a lot of them are unused. The general idea is that the root one (policy, optimizer and trainer) are general interfaces, and the tf_X.py are Tensorflow-specific. So at some point we could have pytorch_policy, pytorch_optimizer, or tf2_policy, tf2_optimizer, etc. 

With that said, wouldn't be opposed to merging TFPolicy and NNPolicy since we currently don't have a non-NN TF-based policy, nor are we likely to have one. ",general impression lot different class policy always clear many maybe yet use case also trainer directory little bit messy right would great could grouped better plan make separate directory structure common policy trainer sac want right lot unused general idea root one policy trainer general point could said would opposed since currently policy likely one,issue,positive,positive,positive,positive,positive,positive
584381145,"Thanks @andrewcoh! Don't hesitate if you have any question, there's still a possibility that it may come from my custom environment somehow :)",thanks hesitate question still possibility may come custom environment somehow,issue,negative,positive,positive,positive,positive,positive
584374687,"@ervteng #3402 started out that way, but in the end I think this is more correct for # of AgentAction steps per Agent.maxSteps.",way end think correct per,issue,negative,neutral,neutral,neutral,neutral,neutral
584355225,"> Hi @tingfx4yu, does the slowdown happen with your environment using our PPO or SAC implementation? This sounds like a memory issue either in the trainer or in the environment, so doing that test would help us debug. Thanks!

Thanks for your replay! I don't use the sac and ppo
Implementation in mlagents but using the algorithms done by pytorch, mlagents was just used as a bridge connect the environment and algorithms. ",hi slowdown happen environment sac implementation like memory issue either trainer environment test would help u thanks thanks replay use sac implementation done used bridge connect environment,issue,positive,positive,positive,positive,positive,positive
584343672,Could you make a jira to add unit test coverage for this? (Don't need to do the test in the PR),could make add unit test coverage need test,issue,negative,neutral,neutral,neutral,neutral,neutral
584320257,"Hi @tingfx4yu, does the slowdown happen with your environment using our PPO or SAC implementation? This sounds like a memory issue either in the trainer or in the environment, so doing that test would help us debug. Thanks!",hi slowdown happen environment sac implementation like memory issue either trainer environment test would help u thanks,issue,positive,positive,positive,positive,positive,positive
584319334,"Hi @Dastyn, thanks for reporting this, I've logged it with internal ID 
MLA-616.

Basically there needs to be more than batch_size number of experiences in the demo file so that you could train at least one batch of demos. But the error message is pretty cryptic. ",hi thanks logged internal id basically need number file could train least one batch demo error message pretty cryptic,issue,positive,positive,neutral,neutral,positive,positive
584318730,"Hello @chriselion !

Thank you very much for your quick reply. 

Is there any guide for that:

 ""**There are several constants that need to be present in the model file, and there's a naming convention that we use to map inputs and outputs to their corresponding tensors**""

Do you have plans to support that? Or make the conversion/implementation easier?",hello thank much quick reply guide several need present model file naming convention use map corresponding support make easier,issue,positive,positive,positive,positive,positive,positive
584316784,"> I'm mostly worried that the model creation is equivalent especially with all of the small gotchas (masking, LSTMs, random number generation, discrete vs. continuous, etc.)

Well that's why we have so many tests!",mostly worried model creation equivalent especially small random number generation discrete continuous well many,issue,negative,neutral,neutral,neutral,neutral,neutral
584314548,"@vincentpierre @anupambhatnagar I agree it's pretty fat. When making it I wasn't able to find a logical place to break it up without something else being left in a broken state (e.g. if we move the Policy graph to another file, then the reward signals are broken, if we just do it for PPO, then the BC module is broken for SAC, etc.). I'll take another look and see if there's a way to do some piece of this. 

Basically there are four main functionality changes:

- LearningModel is no longer instantiated, it consists entirely of static methods. 
- Model creation is moved to the Policy. The contents of NNPolicy _should_ be more or less the contents of PPOModel and a portion of the SACModel. 
- Value function creation and policy updating is moved from the Policy to the Optimizer (PPOOptimizer, SACOptimizer)
- Both PPO and SAC use the same NNPolicy object

The other changes to Reward Signals and GhostTrainer are mostly to adapt to the new position of the Tensorflow graph (i.e. rather than policy.model.tf_node, they're located in policy.tf_node or optimizer.tf_node.) 

@vincentpierre I'm mostly worried that the model creation is equivalent especially with all of the small gotchas (masking, LSTMs, random number generation, discrete vs. continuous, etc.) ",agree pretty fat making able find logical place break without something else left broken state move policy graph another file reward broken module broken sac take another look see way piece basically four main functionality longer entirely static model creation policy content le content portion value function creation policy policy sac use object reward mostly adapt new position graph rather mostly worried model creation equivalent especially small random number generation discrete continuous,issue,positive,positive,neutral,neutral,positive,positive
584307446,@ervteng would it be possible to split this PR into smaller PR's? It would make it easier to understand the changes going in.,would possible split smaller would make easier understand going,issue,negative,neutral,neutral,neutral,neutral,neutral
584291436,There is a lot to look at. Any section that needs attention in particular?,lot look section need attention particular,issue,negative,positive,positive,positive,positive,positive
584252794,"Hi @LewisSim,
Could you give a specific version of Unity that you are using and other details about your setup?  We are testing against Unity 2019.3 on every commit with no issues.  

Could you try to delete your `Library` folder under your Unity project directory to see if that helps.  It's not ideal, but sometimes that helps fix things like this. ",hi could give specific version unity setup testing unity every commit could try delete library folder unity project directory see ideal sometimes fix like,issue,positive,positive,positive,positive,positive,positive
584232118,That's actually quite strange that switching from 16 to 1 envs solved the problem.  I will look into this.  Thanks for bringing it to my attention/using the new self-play trainer!  ,actually quite strange switching problem look thanks new trainer,issue,negative,positive,neutral,neutral,positive,positive
583932183,I also run into the same problem. some one please help. I have been trying to figure this out for days....,also run problem one please help trying figure day,issue,negative,neutral,neutral,neutral,neutral,neutral
583919067,"Recent versions of the barracuda plugin support importing onnx files directly (no need to convert to .nn).

For other questions about models that weren't created with ML-Agents, please ask on the barracuda project: https://github.com/Unity-Technologies/barracuda-release/issues

If you want to use your model for inference on ML-Agents, this isn't well documented or fully supported right now. There are several constants that need to be present in the model file, and there's a naming convention that we use to map inputs and outputs to their corresponding tensors.",recent barracuda support directly need convert please ask barracuda project want use model inference well fully right several need present model file naming convention use map corresponding,issue,positive,positive,neutral,neutral,positive,positive
583915401,"Whoops, the PR was in response to @JPhilipp's forum post https://forum.unity.com/threads/does-use_curiosity-still-exist.824040/

I'll close this off, since it will be merged into master sometime this week (after the release is completed)",whoop response forum post close since master sometime week release,issue,negative,neutral,neutral,neutral,neutral,neutral
583898539,"This is not yet merged into master, by it seems it has been fixed in #3386 (10 minutes before your issue :) )",yet master fixed issue,issue,negative,positive,neutral,neutral,positive,positive
583818776,"Hello @bassmit !

I followed your steps but when I have to drag my model into the agent in Unity I have this following error:

""NullReferenceException: Object reference not set to an instance of an object""

![Captura](https://user-images.githubusercontent.com/44144188/74098840-0b10b300-4b1d-11ea-9622-98e46820fb7f.PNG)

Versions:
· Windows 10
· ML-Agents 0.13.1
· Unity 2018.4.16f1
· Barracuda 0.3.2
*but to convert from onnx to nn I use the scripts of barracuda 0.2.4 because if I use 0.3.2 or higher I have this error:

Traceback (most recent call last):
  File ""onnx_to_barracuda.py"", line 20, in <module>
    onnx2bc.convert(args.source_file, args.target_file, args.trim_unused_by_output, args)
  File ""C:\Users\Ocin\Desktop\pytorch to unity model\onnx_to_barracuda.py"", line 570, in convert
    lambda tensor: get_tensor_data(tensor))
  File ""C:\Users\Ocin\Desktop\pytorch to unity model\barracuda.py"", line 221, in setup_constants
    data = np.reshape(get_tensor_data_lambda(tensor), shape).astype(np.float32))]
  File ""C:\Users\Ocin\Desktop\pytorch to unity model\onnx_to_barracuda.py"", line 570, in <lambda>
    lambda tensor: get_tensor_data(tensor))
  File ""C:\Users\Ocin\Desktop\pytorch to unity model\onnx_to_barracuda.py"", line 336, in get_tensor_data
    floats = struct.unpack('<'+str(int(elems))+'f', tensor.raw_data)
**struct.error: unpack requires a buffer of 4 bytes**

Did you do any more step?

Thank you !",hello drag model agent unity following error object reference set instance object unity barracuda convert use barracuda use higher error recent call last file line module file unity line convert lambda tensor tensor file unity line data tensor shape file unity line lambda lambda tensor tensor file unity line unpack buffer step thank,issue,negative,positive,neutral,neutral,positive,positive
583795356,"Hey @andrewcoh ,

I think I found the issue. I'm a bit embarrassed I didn't try that before.
Replacing `--num-envs 16` by `--num-envs 1` solved the problem... (and training is actually much faster this way)

Here are some Tensorboard screenshots, the change was applied at about 5M steps.
(don't mind the Cumulative Reward, I changed the existential malus at some point)
![image](https://user-images.githubusercontent.com/6735195/74094719-1eecf280-4ae6-11ea-8455-5a9cec5e07c1.png)
![image](https://user-images.githubusercontent.com/6735195/74094723-34621c80-4ae6-11ea-8b8b-e020128107aa.png)
![image](https://user-images.githubusercontent.com/6735195/74094729-3fb54800-4ae6-11ea-9cf7-f02641237899.png)

Now, the agents play well!

Is this a bug or the intended behavior ?",hey think found issue bit embarrassed try problem training actually much faster way change applied mind cumulative reward existential point image image image play well bug intended behavior,issue,negative,positive,positive,positive,positive,positive
583714218,"Soccer only uses raycasts right? In my case I've got raycasts (ball/walls/platforms) + players positions/velocities (normalized) + items informations (held by a player, used by a player, ...)
If there is not enough players, the observations are padded. Edit: everything is relative to the agent position.

I forgot that I changed beta and num_epoch, if I remember correctly it made the agent learn quicker. Another difference I've got is that I now use `--num-envs 16`, where I only used one before. (I trained my agents on a single core VPS :) )",soccer right case got player used player enough edit everything relative agent position forgot beta remember correctly made agent learn another difference got use used one trained single core,issue,negative,positive,neutral,neutral,positive,positive
583675196,"> How do you know it works without a test?

Tracking the size of the dicts using example envs (Hallway, Bouncer). Still need to think of a good unit test for this; we need to emulate On-demand decisions",know work without test size example hallway bouncer still need think good unit test need emulate,issue,negative,positive,positive,positive,positive,positive
583665344,"> Feels like we should have a test that Agent.maxSteps actually triggers a Done at maxSteps

I think we do with the TestAgent",like test actually done think,issue,negative,neutral,neutral,neutral,neutral,neutral
583660253,"> What's the motivation behind this PR? I'm a bit confused 🙃

Trying to do the same thing as we did for the VectorSensor but with masked actions. This will make sure that the user will only be able to call SetActionMask when CollectObservations is called.
It also makes it simpler for us since we do no longer forward calls from the agent to the decision masker.",motivation behind bit confused trying thing masked make sure user able call also simpler u since longer forward agent decision masker,issue,negative,positive,neutral,neutral,positive,positive
583660025,"Is this a fix for a regression that should go into the release branch?

Feels like we should have a test that Agent.maxSteps actually triggers a Done at maxSteps",fix regression go release branch like test actually done,issue,negative,neutral,neutral,neutral,neutral,neutral
583656604,"Ah yes I remember this.  If you can't get it to work as well, please let me know so that we can figure out why.  I think there are differences with my implementation and @LeSphax's. Is your observation space similar to Soccer's? Out of curiousity, which hyperparameter were you missing?

I don't think it is absolutely necessary but it does make for slightly more aggressive offense.",ah yes remember ca get work well please let know figure think implementation observation space similar soccer missing think absolutely necessary make slightly aggressive offense,issue,negative,negative,neutral,neutral,negative,negative
583648443,What's the motivation behind this PR?  I'm a bit confused 🙃 ,motivation behind bit confused,issue,negative,negative,negative,negative,negative,negative
583627115,How do you know it works without a test?,know work without test,issue,negative,neutral,neutral,neutral,neutral,neutral
583627074,"Hi @andrewcoh ,

That makes sense, thanks! Maybe that could be mentioned in the docs somewhere?

Sorry for the lack of context about the okay-ish results. This is in comparison to a good agent that I trained using @LeSphax 's fork (based on ml-agents v0.8).
I actually forgot a PPO hyper-parameter from the trainer_config I used with the fork, so I'll let it train for a few days and I'll let you know if I've got any progress.



If you want to know a bit more about my environment, it looks like this: https://www.youtube.com/watch?v=MZdG-qUug1Q
Basically, a Soccer-like env with platformer controls and items. Each agent can see its nearest teammates and opponents (position, velocity, held/used items), goals positions, ball position (if any), and some raycasting is used for the walls/platforms.
A good agent should learn to: score, defend its goal, run after the ball (and anticipate its respawn), learn to use items, passing the ball.

Edit: Quick question, do you think the existential malus is still needed on Soccer with selfplay?",hi sense thanks maybe could somewhere sorry lack context comparison good agent trained fork based actually forgot used fork let train day let know got progress want know bit environment like basically platformer agent see nearest position velocity ball position used good agent learn score defend goal run ball anticipate learn use passing ball edit quick question think existential still soccer,issue,positive,positive,positive,positive,positive,positive
583611875,"Seems reasonable. It makes sense to handle these together, since the observations might affect the possible actions. The name ""CollectObservations"" sounds less appropriate now, but I don't have a better suggest. ",reasonable sense handle together since might affect possible name le appropriate better suggest,issue,negative,positive,positive,positive,positive,positive
583569516,"Hi @AcelisWeaven,

SAC tends to do pretty poorly when rewards are sparse.  I've trained both Tennis and Soccer with SAC+self-play and have only been able to get good policies when I shape the reward function.

Can you elaborate what you mean by okay-ish results?",hi sac pretty poorly sparse trained tennis soccer able get good shape reward function elaborate mean,issue,positive,positive,positive,positive,positive,positive
583544357,"Hi @andrewcoh ,

Sorry if this is not the right place to ask, but have you tried to train the Soccer env with self-play with SAC instead of PPO?

I've got a similar custom environment and with selfplay+PPO I get some okay-ish results, but with selfplay+SAC, the agents move kinda randomly.

Thanks,",hi sorry right place ask tried train soccer sac instead got similar custom environment get move randomly thanks,issue,negative,negative,neutral,neutral,negative,negative
583542953,"Wow, you're fast, that just hit master yesterday. And I copied that the migration guide without reading it closely :)",wow fast hit master yesterday copied migration guide without reading closely,issue,positive,positive,positive,positive,positive,positive
583439045,"Thanks! Yeah I removed the 2nd academy I had in the 2nd scene and that seemed to fix things. The docs say ""1 academy in the scene"" so I assumed it was 1 per scene, not 1 per game; but it errors if there's 1 per scene. Thanks for the help!",thanks yeah removed academy scene fix say academy scene assumed per scene per game per scene thanks help,issue,positive,neutral,neutral,neutral,neutral,neutral
583291275,"Yes, that makes sense.  Currently each build is used to train its own model, and we name the NN files based on the map size.  We're starting off with smaller map sizes to test things out and experiment on smaller observation spaces to make it easier and faster to iterate on training.  Then once we get a feel for proper rewards and unit ranges, etc, we'll work on larger levels and train new models on those.

If we were able to set vector observation size after reading in our config files, we wouldn't need to make all these builds, we'd just have one build and use different config files instead (or maybe even command line args).  Then we might even be able to work on automating our process a bit.",yes sense currently build used train model name based map size starting smaller map size test experiment smaller observation make easier faster iterate training get feel proper unit work train new able set vector observation size reading would need make one build use different instead maybe even command line might even able work process bit,issue,positive,positive,positive,positive,positive,positive
583170606,"Hmm.... that might be possible with what we're considering but I'm not quite sure. Does each build do its own training and produce its own model? There's no immediate plans to be able to ""resize"" the trained models, for example train on a 50x50 map and run inference on an 8x8 map.",might possible considering quite sure build training produce model immediate able resize trained example train map run inference map,issue,negative,positive,positive,positive,positive,positive
583165000,"> This CLA seems to be for Unity Employees. I am just a student , do i have to sign it?

I don't think I've ever been able to read the whole thing without falling asleep either :) Anybody that wants to contribute a pull request to the repo has to sign in. I think the first section here gives a good explanation of what it's there for: https://www.clahub.com/pages/why_cla",unity student sign think ever able read whole thing without falling asleep either anybody contribute pull request sign think first section good explanation,issue,positive,positive,positive,positive,positive,positive
583158750,"On the current release:
I think you'll need to only have one Academy in the master scene. When you disable the Agent it should remove the Agent from the Academy:
https://github.com/Unity-Technologies/ml-agents/blob/0.13.1/UnitySDK/Assets/ML-Agents/Scripts/Agent.cs#L290-L303
And I believe this happens when you destroy the GameObject that the Agent is attached to.

On the master branch / upcoming release:
You don't need to add an Academy to the scene anymore (and in fact, you can't). The removal code is roughly the same, and called from Agent.OnDisable",current release think need one academy master scene disable agent remove agent academy believe destroy agent attached master branch upcoming release need add academy scene fact ca removal code roughly,issue,negative,negative,negative,negative,negative,negative
583020634,"Thanks for the prompt reply @chriselion and good to hear! I'm not quite sure what the answer to my question is though - so the suggested approach would then be to only have 1 academy for the entire game (rather than 1 per scene)? Performance wise I could see that getting quite heavy, unless we can activate / deactivate agents & brains at will. Or would there be a way to break things up?",thanks prompt reply good hear quite sure answer question though approach would academy entire game rather per scene performance wise could see getting quite heavy unless activate deactivate brain would way break,issue,positive,positive,positive,positive,positive,positive
583014716,"Hi @lukemadera,
You're in luck - as of the next release (tentatively being released next week), Academy will be a singleton instead of a MonoBehavior. These changes are already on the `master` branch if you want to try them out.",hi luck next release tentatively next week academy singleton instead already master branch want try,issue,positive,neutral,neutral,neutral,neutral,neutral
582993976,"hi @lostmsu - i don't think we have this planned, however @surfnerd may have some more insights on it.  Could you describe what you would be looking for in particular?",hi think however may could describe would looking particular,issue,negative,positive,positive,positive,positive,positive
582982721,"Hi @unityjeffrey I am running into an issue that I think is related to having multiple scenes. I just opened this: https://github.com/Unity-Technologies/ml-agents/issues/3371
If I have multiple scenes, should I have 1 academy per scene, or 1 total for all scenes? If 1 total, how do I properly destroy an agent (when the scene unloads with that agent in it) so the Academy knows to stop trying to reference it?",hi running issue think related multiple multiple academy per scene total total properly destroy agent scene agent academy stop trying reference,issue,negative,negative,neutral,neutral,negative,negative
582889912,"> Hi @findlabjie!
> 
> I had no time to push this project further. But I will get back to it the next week and will try to tackle the problem once more.
> 
> Didn't you provide a solution to this issue?! Somebody told calling Done() right after AddReward() is not working properly and instead of using Done() using AgentReset() should do the trick. Somehow the comment about this isn't here anymore... Did you do an edit? If yes, how come?

Sorry, I misunderstood last time, so I deleted it. but I just had a new idea.
I think:
step means : RequestDecision() ->  CollectObservations() -> AgentAction() -> (next)RequestDecision() -> ......
episode means : A period of the Done() and the next Done().
so, i find your code have the same error as mine.  You call function Done() in every step. That's wrong. when you get Positive reward，you don't need to call function Done(), just reward and reset the state you want. 
In your code , DiscCatched() might look something like this:
`
public void DiscCatched()
    {
        AddReward(1.0f);
       //call reset player position function()
       //call reset disc position function()
      //reset some state
      canThrow = true;
    }
`
I thought it might work for you.",hi time push project get back next week try tackle problem provide solution issue somebody told calling done right working properly instead done trick somehow comment edit yes come sorry misunderstood last time new idea think step next episode period done next done find code error mine call function done every step wrong get positive need call function done reward reset state want code might look something like public void reset player position function reset disc position function state true thought might work,issue,positive,negative,neutral,neutral,negative,negative
582856374,"Hi @findlabjie!

I had no time to push this project further. But I will get back to it the next week and will try to tackle the problem once more.

Didn't you provide a solution to this issue?! Somebody told calling Done() right after AddReward() is not working properly and instead of using Done() using AgentReset() should do the trick. Somehow the comment about this isn't here anymore... Did you do an edit? If yes, how come?",hi time push project get back next week try tackle problem provide solution issue somebody told calling done right working properly instead done trick somehow comment edit yes come,issue,negative,positive,neutral,neutral,positive,positive
582789692,"@chriselion Thanks for the response!

For my current environment, I'm using a hex tile map game.  The observations I'm using are values for the status of each tile.  For example empty tiles = 0, player in tile = 1, enemy in tile = 2, etc.  The map is a grid, and I instantiate everything based on the config files which other users can easily edit to modify the game setup.  Maps are currently from 8x8 up to 50x50, so the observations would range from 64 ints up to 2500 ints (don't know if that's a good idea, haven't tried training anything on larger maps yet).

Anyway, would be really nice if the vector observation size could be set during the Behavior Parameters Awake() or something similar.  Currently what I've been doing is make different builds (for others to use) with the vector observation size set in the editor, so I end up making a bunch of builds, 8x8, 12x12, 20x20, etc.",thanks response current environment hex tile map game status tile example empty player tile enemy tile map grid everything based easily edit modify game setup currently would range know good idea tried training anything yet anyway would really nice vector observation size could set behavior awake something similar currently make different use vector observation size set editor end making bunch,issue,positive,positive,neutral,neutral,positive,positive
582781060,"> i have penalty on step in AgentAction: `AddReward(-1f / agentParameters.maxStep);` and `AddReward(-0.5f); `when he hits the wall. I just tried without this wall penalty but it was the same. Agent actually starts to do little circles or spins soon after he learns that the eggs are what he's looking for, but when obsticales comes and eggs are behind the wall then he would start to spin or do circles and never get it. I also tried with different curiosity strength from 0.02 up to 0.5, but no luck there.


I would either remove the penalty on contact with the wall or give it a smaller penalty based on your max possible reward. Try AddReward(-0.005f); or even AddReward(-0.0005f);
@sekne18 ",penalty step wall tried without wall penalty agent actually little soon looking come behind wall would start spin never get also tried different curiosity strength luck would either remove penalty contact wall give smaller penalty based possible reward try even,issue,negative,negative,negative,negative,negative,negative
582773262,"If tou can understand what are they talking so, Yes!",tou understand talking yes,issue,negative,neutral,neutral,neutral,neutral,neutral
582768355,"> Also, you need to sign the CLA before we can merge your changes.

This CLA seems to be for Unity Employees. I am just a student , do i have to sign it?",also need sign merge unity student sign,issue,negative,neutral,neutral,neutral,neutral,neutral
582693697,"Although this should really be done by branching from master, not the release branch",although really done branching master release branch,issue,negative,positive,positive,positive,positive,positive
582685604,"The target should be the release branch. At the the end of the release process, we merge the release branch into master and all these changes will land.",target release branch end release process merge release branch master land,issue,negative,neutral,neutral,neutral,neutral,neutral
582651467,"Also, you need to sign the CLA before we can merge your changes.",also need sign merge,issue,negative,neutral,neutral,neutral,neutral,neutral
582650985,"@MedhaviMonish Can you either apply those changes, or give me permission to? https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/allowing-changes-to-a-pull-request-branch-created-from-a-fork#enabling-repository-maintainer-permissions-on-existing-pull-requests",either apply give permission,issue,negative,neutral,neutral,neutral,neutral,neutral
582650670,Was this up for debate in the weekly meeting about whether or not we should remove this or support it?,debate weekly meeting whether remove support,issue,negative,neutral,neutral,neutral,neutral,neutral
582634772,I will close this issue for now since this isn't an ML-Agents bug.  Thanks for your feedback. ,close issue since bug thanks feedback,issue,negative,positive,positive,positive,positive,positive
582634469,"Could you report a bug [here](https://unity3d.com/unity/qa/bug-reporting) and link to the issue [here](https://issuetracker.unity3d.com/issues/vr-particle-systems-cause-task-dot-rasterdata-dot-indexbuffer-equals-equals-null-error-and-lead-to-graphical-glitches).

It seems like an Editor regression unrelated to ML-Agents.  ",could report bug link issue like editor regression unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
582621830,"Thank you,  we appreciate the feedback.  Your suggestion will be considered internally.",thank appreciate feedback suggestion considered internally,issue,positive,neutral,neutral,neutral,neutral,neutral
582604893,"This is something we're thinking about supporting in the future (the exact mechanism that we'll use is still TBD).

Can you give some specific examples of what sort of observations you want to use this with? For example, are they raycast perception, visual observations, or your own custom observations?",something thinking supporting future exact mechanism use still give specific sort want use example perception visual custom,issue,negative,positive,neutral,neutral,positive,positive
582598753,Thanks. I'm using the very latest stable version -- Unity 2019.3.0f6. (On Windows 10. Currently switched to PC as build platform. It's a rather big project with lots of assets from the Asset Store.),thanks latest stable version unity currently switched build platform rather big project lot asset asset store,issue,positive,positive,positive,positive,positive,positive
582593039,"Hi @JPhilipp,
What version of the editor are you using.  This seems to [have been fixed a while ago](https://issuetracker.unity3d.com/issues/vr-particle-systems-cause-task-dot-rasterdata-dot-indexbuffer-equals-equals-null-error-and-lead-to-graphical-glitches?page=1#comments).",hi version editor fixed ago,issue,negative,positive,neutral,neutral,positive,positive
582262177,"Hi @crzdg 
Have you solved this problem？
I got the same error.
hope your reply and help.",hi got error hope reply help,issue,negative,neutral,neutral,neutral,neutral,neutral
582171797,"But it also couples the public interface of Agent more closely to VectorSensor, whereas before they were decoupled (and we could theoretically change the implementation of how AddVectorObs worked)",also public interface agent closely whereas could theoretically change implementation worked,issue,negative,neutral,neutral,neutral,neutral,neutral
582171282,I'm OK having it in the API. Wondering if `GetAction` or `GetActions` would be a better name though?,wondering would better name though,issue,negative,positive,positive,positive,positive,positive
582170548,"Yes, but it does simplify the Agent code (since it does not forward the calls to addobservation for every type of observation.",yes simplify agent code since forward every type observation,issue,negative,neutral,neutral,neutral,neutral,neutral
582169839,Trying to get more information from Florent (who made the changes in https://github.cds.internal.unity3d.com/unity/UnityInferenceEngine/commit/5b46dde4bb28df7f4c0bc930d6bad221711758c1#diff-8b2383609dc2c5a57e083d2f0b2cbb68) before merging,trying get information florent made,issue,negative,neutral,neutral,neutral,neutral,neutral
582169617,"Seems OK. I'm not sure it actually makes the API any smaller though, just moves it to VectorSensor.",sure actually smaller though,issue,negative,positive,positive,positive,positive,positive
582166003,@TanteVonDante to verify the version numbers please see this link https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Migrating.md#warning-warning-warning ,verify version please see link,issue,negative,neutral,neutral,neutral,neutral,neutral
582100557,"This answers my question and I can work around it.

Do you intend to leave the documentation as-is? It seems like that is confusing for users trying to follow these instructions.",question work around intend leave documentation like trying follow,issue,negative,neutral,neutral,neutral,neutral,neutral
582092582,"@ForrestTrepte we decided to not include examples in the package. you can find the examples in the Project folder in the repo. closing the issue for no.  feel free to reopen it, if needed.",decided include package find project folder issue feel free reopen,issue,positive,positive,positive,positive,positive,positive
581803334,"Where can I check the version? I just started last december with ml and coding, so I'm a total beginner.",check version last total beginner,issue,negative,neutral,neutral,neutral,neutral,neutral
581781111,"> 
> 
> maybe I do something wrong, but when I use --load the model acts like it never trained before.
> 
> I wanted to use it instead of curriculum learning. I wanted to train like 50.000 steps and change the environment afterwards. So the training will get harder step by step. Is this possible?

What mlagents version are you using? 0.13.0 had a bug which made the step count reset in the tensorboard summaries. The model continued training as intended though (I think). Fixed in 0.13.1.",maybe something wrong use load model like never trained use instead curriculum learning train like change environment afterwards training get harder step step possible version bug made step count reset model continued training intended though think fixed,issue,negative,negative,negative,negative,negative,negative
581764676,"maybe I do something wrong, but when I use --load the model acts like it never trained before.

I wanted to use it instead of curriculum learning. I wanted to train like 50.000 steps and change the environment afterwards. So the training will get harder step by step. Is this possible? ",maybe something wrong use load model like never trained use instead curriculum learning train like change environment afterwards training get harder step step possible,issue,negative,negative,negative,negative,negative,negative
581739384,"Yes, exactely! 
Besides, this might be useful to be able to increase this learning rate between two following curriculum lessons (without hacking Python API ;) ).
",yes besides might useful able increase learning rate two following curriculum without hacking python,issue,positive,positive,positive,positive,positive,positive
581698388,"We have made ml-agents a package on the master branch. Doing so, we isolated the code to test it better (including raycasts). This should resolve the issue.",made package master branch isolated code test better resolve issue,issue,negative,positive,positive,positive,positive,positive
581698063,"Hi, 
We removed the ""Reset On Done"" option is the current master branch. To terminate an agent without resetting it, you can destroy its game object directly.",hi removed reset done option current master branch terminate agent without destroy game object directly,issue,negative,negative,negative,negative,negative,negative
581667851,I updated the config.yml so that it _should_ keep entries for `Discussion / General Questions` and `Installation / Setup` but link to the forum instead.,keep discussion general installation setup link forum instead,issue,negative,positive,neutral,neutral,positive,positive
581653043,Hi @nic126 github issues is for bugs and feature requests. Could you please post your message in the Unity ML Agents forum. here's a [link](https://forum.unity.com/forums/ml-agents.453/) to the forum page. Thanks! ,hi feature could please post message unity forum link forum page thanks,issue,positive,positive,positive,positive,positive,positive
581652302,"@fedetask @MarkTension As of last week we have a unity forum for ML Agents. Github issues will be reserved for bugs and feature requests only. It would be great if you could continue the discussion in the forum so that its visible to other ML Agents users as well. Thanks! 

Closing the issue since it's not a bug/feature request.",last week unity forum reserved feature would great could continue discussion forum visible well thanks issue since request,issue,positive,positive,positive,positive,positive,positive
581651600,thanks @JPhilipp for spotting the missing definition. We will update the docs accordingly.,thanks spotting missing definition update accordingly,issue,negative,neutral,neutral,neutral,neutral,neutral
581641893,"> Soccer .nn as-is isn't working - seems like there is a different number of vector obs in the NN vs. in the behavior parameters

@ervteng sorry, I should have told you.  There were duplicate raycast observations so I removed them and am retraining right now.",soccer working like different number vector behavior sorry told duplicate removed right,issue,negative,negative,neutral,neutral,negative,negative
581640799,"Do you mean to stop training, change the learning rate and then resume training from the same point using --load?",mean stop training change learning rate resume training point load,issue,negative,negative,negative,negative,negative,negative
581610533,Soccer .nn as-is isn't working - seems like there is a different number of vector obs in the NN vs. in the behavior parameters,soccer working like different number vector behavior,issue,negative,neutral,neutral,neutral,neutral,neutral
581550539,"Hi @MarkTension,
The best place to discuss this would be on [barracuda's github issues](https://github.com/Unity-Technologies/barracuda-release/issues). We just copy their conversion script.

BTW, The eventual plan is to have everything go through ONNX; ml-agents will hopefully have beta support for ONNX export in the next release.",hi best place discus would barracuda copy conversion script eventual plan everything go hopefully beta support export next release,issue,positive,positive,positive,positive,positive,positive
581534556,"If you really want to use the 0.13.1 release with barracuda 0.5.0, I put the fixes on the `hotfix-0.13.1-barracuda0.5.0` branch. ",really want use release barracuda put branch,issue,negative,positive,positive,positive,positive,positive
581494832,"@anupambhatnagar: I was thinking of updating the names, but then I noticed that in the latest master branch Learning-Environment-Create-New.md says to use the com.unity.ml-agents inside a new Unity project. When I tried this, none of the example materials were pulled into my project. So I'm not sure how this should work.

Did something go wrong on my end when I was testing this? Should examples be included with com.unity.ml-agents? Or should there be a separate examples package? Or should Learning-Environment-Create-New.md be modified to avoid using examples content?",thinking latest master branch use inside new unity project tried none example project sure work something go wrong end testing included separate package avoid content,issue,negative,positive,positive,positive,positive,positive
581486507,"Just came across this, many thanks. ",came across many thanks,issue,negative,positive,positive,positive,positive,positive
581432518,"Hi @TanteVonDante,

Here I mean the "".csv is unusable after a --load because it is overwritten"", yet it works for the model, note that the model starts from where it stopped,  the environment starts fresh.

You can get the data from the tensorflow generated summary (opening it and converting to csv), but them, there is no point in having it to save a unusable .csv just to bloat the disk.",hi mean unusable load yet work model note model stopped environment fresh get data summary opening converting point save unusable bloat disk,issue,positive,negative,neutral,neutral,negative,negative
581337812,"Hard to say. Have you tried seeing what happens when you change beta?

How I understand entropy is:
When all actions have almost same probability, then Entropy will be high
When one action has near 1 probability, Entropy will be low

In the beginning of training, almost all actions have same probability. After some training, some actions get higher probability (in the direction of getting more rewards), and entropy is reduced over time.

I guess in your situation a cause could be your changing environment. I don't know how much it changes, and how often, but if the agent has to relearn every so many steps, then it's weights will need to readjust after such an update, which results into a more random policy, thus higher entropy.  Maybe test this by increasing N? If true, you'll probably see an increase in the entropy after each N.

Another thing could be that a changing environment every so many steps is very hard to learn for the agent, and the agent is in a local minimum of chance level policy. Can only be true though, if the reward and entropy go down a bit at first I guess. 

",hard say tried seeing change beta understand entropy almost probability entropy high one action near probability entropy low beginning training almost probability training get higher probability direction getting entropy reduced time guess situation cause could environment know much often agent relearn every many need readjust update random policy thus higher entropy maybe test increasing true probably see increase entropy another thing could environment every many hard learn agent agent local minimum chance level policy true though reward entropy go bit first guess,issue,positive,positive,positive,positive,positive,positive
581329456,"Hi,
Is there a way to change (increase) the learning rate between 2 --load ?
Thanks!
D.",hi way change increase learning rate load thanks,issue,positive,positive,positive,positive,positive,positive
581276090,"hey caioc2,

I have exactly the same problem. There is no use for --load in my opinion. I thought it will continue my session, but its starts a new one. If I would like to start a new session I would do it without --load.
Is there another option to continue the training?

Thank you for your help.",hey exactly problem use load opinion thought continue session new one would like start new session would without load another option continue training thank help,issue,positive,positive,positive,positive,positive,positive
581160706,"Hey all, 
I will fix the documentation.  0.4.0 is the version to use for the latest release.  Thanks. ",hey fix documentation version use latest release thanks,issue,negative,positive,positive,positive,positive,positive
581144521,"I had the same with Baracuda 0.5.0 on Unity 2019.2, Win 10.
Return to 0.4.0 works fine.",unity win return work fine,issue,positive,positive,positive,positive,positive,positive
581135186,"I had same problem, and this solution works fine for me.
**System:** Windows 10
**Unity Versions:** 2019.3.0f6
It can be associated with Unity Versions. I did not find any information about this problem in Docs",problem solution work fine system unity associated unity find information problem,issue,negative,positive,positive,positive,positive,positive
580988643,"Then I kindly ask to review its design.

Is it any usefull to have a csv that is not complete and **overrides** the older when --load clearly is a continuation of a training and not really a new one?

For any purposes of ploting a graph or garthering statistics, having half a the points makes no sense.",kindly ask review design complete older load clearly continuation training really new one graph statistic half sense,issue,positive,positive,positive,positive,positive,positive
580982066,"ah, sorry about that. I intended to write that comment on the other issue you submitted. https://github.com/Unity-Technologies/ml-agents/issues/3326 
We will look into this issue and make the necessary fix.",ah sorry intended write comment issue look issue make necessary fix,issue,negative,negative,negative,negative,negative,negative
580973929,"@anupambhatnagar

Sorry if I'm being pedantic, are you sure that you understood what's happening?

If [this video](https://www.youtube.com/watch?v=UoEQICke7N4&feature=youtu.be) is ""intended behavior"" instead of [this](https://www.youtube.com/watch?v=XE9hwIsIqk4&feature=youtu.be), without a single line of text in the documentation, no error and no warning whatsoever, then please close this ticket.

Thanks",sorry pedantic sure understood happening video intended behavior instead without single line text documentation error warning whatsoever please close ticket thanks,issue,negative,positive,neutral,neutral,positive,positive
580961156,"Is it possible to initialize (one time) the size via script?  I have a dynamic environment where users can configure thru json config files.  I would like to be able to initialize the observation space based on the config.
I understand the neural networks need to be sized, etc and can't be changed.  But could we initialize the value sometime before they are instantiated?",possible initialize one time size via script dynamic environment configure would like able initialize observation space based understand neural need sized ca could initialize value sometime,issue,positive,positive,positive,positive,positive,positive
580930342,@MarkTension thanks for sending the request. I'll discuss this feature with the team at large and let you know if we plan to add it in the future.,thanks sending request discus feature team large let know plan add future,issue,negative,positive,positive,positive,positive,positive
580929819,@MarcoMeter we have decided to move discussions to the new ML Agents forum. Github will be used for bugs and feature requests only. Could you please post your question in the unity forum? Here's the link https://forum.unity.com/forums/ml-agents.453/,decided move new forum used feature could please post question unity forum link,issue,negative,positive,positive,positive,positive,positive
580929230,@MarcoMeter thanks for your input. We shall discuss this design choice with the team at large.,thanks input shall discus design choice team large,issue,negative,positive,positive,positive,positive,positive
580927411,@ForrestTrepte If you look under Assets in the Unity Project panel you will find these materials there. They might be under a modified name. In case the materials are not there could you please submit a PR with the updated documentation?,look asset unity project panel find might name case could please submit documentation,issue,positive,neutral,neutral,neutral,neutral,neutral
580858991,"@unityjeffrey I see you mentioned Unity SDK (C#) codebase. Is there a NuGet package, that can be used to interact with Unity-based environment for ML agents similar to `mlagents` on pip?",see unity package used interact environment similar pip,issue,negative,neutral,neutral,neutral,neutral,neutral
580851812,"Hi @caioc2,
I agree that this doesn't help with the training aspect of this problem. Yet it would still be beneficial to anyone who is running inference and wants to build for IL2CPP.  You still get performance benefits by building with IL2CPP.  We fully understand that this doesn't fix the training issue, which is why we asked for the extra comment.  

That being said, we will be spending more time fixing the IL2CPP training issue.  Thanks for you comment. ",hi agree help training aspect problem yet would still beneficial anyone running inference build still get performance building fully understand fix training issue extra comment said spending time fixing training issue thanks comment,issue,positive,positive,neutral,neutral,positive,positive
580828100,"Hi,

As someone who have tried out stubs for this problem, here are my 2 cents:

Stubs does not work when building the standalone to run multiple environments for training, which is the main point of ever using IL2CPP, **speed up training**.
Using it in the editor makes not much sense, since the Unity editor uses  Mono backend.

While the IL2CPP is a much needed fix, I think this is not the fix. It is a difficult issue which may be only solved with having directives in the GRPC and or some other clever idea to be found.

",hi someone tried problem work building run multiple training main point ever speed training editor much sense since unity editor mono much fix think fix difficult issue may clever idea found,issue,negative,positive,neutral,neutral,positive,positive
580773498,"Thanks, Chris. Ml-agents is such an awesome project because of developers like you.",thanks awesome project like,issue,positive,positive,positive,positive,positive,positive
580464021,"OK, I think I know what's going on. I believe this is just a visual bug, but it's definitely something we should fix.
When we do the raycasting here:
https://github.com/Unity-Technologies/ml-agents/blob/4ba8b973369435fc76fc67b32a753536d2f126c9/com.unity.ml-agents/Runtime/Sensor/RayPerceptionSensor.cs#L219-L228
we cast up to `rayLength` units, and save the hit fraction. This fraction is based on `rayLength` units.

When doing the gizmo drawing here
https://github.com/Unity-Technologies/ml-agents/blob/4ba8b973369435fc76fc67b32a753536d2f126c9/com.unity.ml-agents/Runtime/Sensor/RayPerceptionSensorComponentBase.cs#L127-L128
We try to use the hit fraction to interpolate between the start and end points. Since these start and end points are affected by the scale, they won't necessarily be `rayLength` units apart. So we'd need to scale the hit fraction (for display only) by `rayLength / rayDirection.magnitude`.

I think a cleaner fix would be to use the scaled ray length (`rayDirection.magnitude`) instead of `rayLength` for raycasting and hit fraction determination in PerceiveStatic. This means that the effective length of the rays would change with the scale; the downside is that this would break existing trained models [edit: would break existing models with non-1 scale].

I'll have a PR for this tomorrow or early next week. In the meantime, I'm pretty confident that training with a RayPerceptionSensor and scale will still work.",think know going believe visual bug definitely something fix cast save hit fraction fraction based drawing try use hit fraction interpolate start end since start end affected scale wo necessarily apart need scale hit fraction display think cleaner fix would use scaled ray length instead hit fraction determination effective length would change scale downside would break trained edit would break scale tomorrow early next week pretty confident training scale still work,issue,positive,positive,positive,positive,positive,positive
580373949,"Thanks, the screenshot helps a lot. Looking into it now, I get the feeling the scale is being applied twice somehow.",thanks lot looking get feeling scale applied twice somehow,issue,positive,positive,positive,positive,positive,positive
580322344,"> I'll look into this tomorrow.
> 
> Can you explain more what are you seeing and what do you expect? I would expect that when the scale changes, the length of the rays also increase.

The expected: the red sphere drawn representing the hit point should be the actual hitpoint. However, when the scale changes, the red sphere is drawn to somewhere else, scaling with the scale.

I don't know if the observation feed into the RL component has the same problem or not. The below screenshot shows this bug when I change the training area's scale to 1.5.
<img width=""1440"" alt=""Screen Shot 2020-01-30 at 10 59 42"" src=""https://user-images.githubusercontent.com/17922409/73466369-c718f380-434f-11ea-85c8-debe2693be59.png"">
",look tomorrow explain seeing expect would expect scale length also increase red sphere drawn hit point actual however scale red sphere drawn somewhere else scaling scale know observation feed component problem bug change training area scale screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
580076611,"Hi @cyz2727327,
Sorry for the delay on this. This team doesn't support Barracuda (the inference engine); you can find more information about it here https://docs.unity3d.com/Packages/com.unity.barracuda@0.4/manual/index.html

If you have problems converting/loading a model, the best place to as is at their github repo: https://github.com/Unity-Technologies/barracuda-release/issues

",hi sorry delay team support barracuda inference engine find information model best place,issue,positive,positive,positive,positive,positive,positive
580075329,"I'll look into this tomorrow.

Can you explain more what are you seeing and what do you expect? I would expect that when the scale changes, the length of the rays also increase.",look tomorrow explain seeing expect would expect scale length also increase,issue,negative,neutral,neutral,neutral,neutral,neutral
580019544,"> This feature has been added in #3194. Thanks for the initial PR!

Does that mean this feature will be available in the next release? ",feature added thanks initial mean feature available next release,issue,negative,positive,neutral,neutral,positive,positive
580019083,This feature has been added in https://github.com/Unity-Technologies/ml-agents/pull/3194.  Thanks for the initial PR!,feature added thanks initial,issue,negative,positive,neutral,neutral,positive,positive
580014269,"@Hunter-Unity, is this something you still want to merge?",something still want merge,issue,negative,neutral,neutral,neutral,neutral,neutral
580013567,"Hey @AcelisWeaven,
Can you update the comment on the function stub to make it clear that training still doesn't work with this change?  After that, I will push a branch to our repo with your commit and merge it. ",hey update comment function stub make clear training still work change push branch commit merge,issue,positive,positive,positive,positive,positive,positive
580008079,"Hey @AcelisWeaven,
Thanks for the information!  Very helpful. I will bring this up with the team and consider merging it. ",hey thanks information helpful bring team consider,issue,positive,positive,positive,positive,positive,positive
580007192,"> Does training still work in the editor with this?

Yes it does. There should be no issue here, the file is marked as Standalone-only.
![image](https://user-images.githubusercontent.com/6735195/73405988-a58b1e00-42f5-11ea-9f40-1f33b675fe0a.png)


> Does this break any other behavior on windows?

Sadly there is an issue I forgot to mention : The compilation works, but when running as a standalone, it runs in Inference mode.
`Couldn't connect to trainer on port 5005 using API version API-14-dev0. Will perform inference instead.`
Switching back to Mono and it works. But I'm not sure the issue is related to this PR. Also, back on v0.8, I had a similar issue because of code stripping being enable (I don't know if this has been solved). With IL2CPP is can only set it to Low or higher. I've been trying IL2CPP Release and Debug.

> Could you provide a little more information on how you tested this?

I was able to compile both a custom environment and a freshly-cloned Tennis (both envs are similar), whereas I couldn't before. Both only work in Editor mode only with IL2CPP.

If it matter, here's the command I use for in-editor training:
```
mlagents-learn trainer_config.yaml --run-id=myrun --train
```
And for standalone training:
```
mlagents-learn.exe trainer_config.yaml --train --env=MyEnv --save-freq=10000 --run-id=myrun
```

TLDR: Inference and training in editor works, training with a standalone build doesn't :)",training still work editor yes issue file marked image break behavior sadly issue forgot mention compilation work running inference mode could connect trainer port version dev perform inference switching back mono work sure issue related also back similar issue code stripping enable know set low higher trying release could provide little information tested able compile custom environment tennis similar whereas could work editor mode matter command use training train training train inference training editor work training build,issue,negative,positive,neutral,neutral,positive,positive
579970844,"Hey @AcelisWeaven,
Thanks for your contribution.  Does this break any other behavior on windows?  Does training still work in the editor with this?  Could you provide a little more information on how you tested this?  
Thanks!
Chris",hey thanks contribution break behavior training still work editor could provide little information tested thanks,issue,positive,positive,neutral,neutral,positive,positive
579967738,"I will close this issue for now.  If you run into other installation issues, please post to the [Unity ML-Agents Forum](https://forum.unity.com/forums/ml-agents.453/).
Cheers.",close issue run installation please post unity forum,issue,negative,neutral,neutral,neutral,neutral,neutral
579967201,"Hi @ShivamMukherjee,
please follow [the instructions from the latest release](https://github.com/Unity-Technologies/ml-agents/blob/latest_release/docs/Installation.md).  The master branch is the branch we actively develop in.  The instructions for installation on the master branch will be updated over the next couple of days.  Thank you. ",hi please follow latest release master branch branch actively develop installation master branch next couple day thank,issue,positive,positive,positive,positive,positive,positive
579887497,"So, it finally learned to move properly with correct orientation  in the tunnel and reaching to the end of tunnel. It took 7M steps for the agent to learn that behavior. I am sure this learning time can be brought down with fine tuning of hyperparameters. ",finally learned move properly correct orientation tunnel reaching end tunnel took agent learn behavior sure learning time brought fine tuning,issue,positive,positive,positive,positive,positive,positive
579606731,"Sure, I'll have to check my company policy for approval to sign the CLA. Stand by...",sure check company policy approval sign stand,issue,positive,positive,positive,positive,positive,positive
579594356,"The instructions say to add a new script directly to the Agent object. When the script is added, it is a blank MonoBehaviour. Later, the script is edited to inherit from Agent, which has a  ```[RequireComponent(typeof(BehaviorParameters))]``` attribute. Arguably, Unity could detect this new attribute when the script is recompiled and automatically add the required component, but that is not how it works. The result is that in the Final Editor Setup step someone could be confused at the step ```Change Decision Interval from 1 to 10.``` because they wouldn't see any script having that property.

One way to fix this would be to change the instructions so that the script is created and inherits from Agent and then, later, attaching the script to the object so that the BehaviourParameters component will be added.",say add new script directly agent object script added blank later script inherit agent attribute unity could detect new attribute script automatically add component work result final editor setup step someone could confused step change decision interval would see script property one way fix would change script agent later script object component added,issue,negative,negative,neutral,neutral,negative,negative
579551499,Will update migration guide later,update migration guide later,issue,negative,neutral,neutral,neutral,neutral,neutral
579412452,Hi I decided to just use Unity as opposed mlagents to solve my problem. closing.,hi decided use unity opposed solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
579395941,Is there a way to use multiple brains trained for the same agent to perform different actions. Or is it better to train multiple agents to perform different actions having only one brain,way use multiple brain trained agent perform different better train multiple perform different one brain,issue,negative,positive,neutral,neutral,positive,positive
579270542,"Thanks for the extra response, I'll have a go with behavioral cloning and see if that makes a significant difference.

I have one last question about the training performance: Looking at my CPU, GPU and memory utilisation, it is not clear what is constraining the training speed. CPU usage for each environment is between 0.5 and 1%, CPU usage for the main Python process is around 17%, less than 1% for the others. GPU cuda usage is around 30% and copy around 15%. Memory is only using a few GB.

What is the limiting factor that prevents any of this from running faster and using more of my system resources? I thought maybe Python was stuck on one core, but CPU usage is fairly evenly spread between all 8 cores - none are near 100%.",thanks extra response go behavioral see significant difference one last question training performance looking memory clear constraining training speed usage environment usage main python process around le usage around copy around memory limiting factor running faster system thought maybe python stuck one core usage fairly evenly spread none near,issue,positive,positive,positive,positive,positive,positive
579045583,Thanks for catching this. We updated the relevant documentation.,thanks catching relevant documentation,issue,negative,positive,positive,positive,positive,positive
579042126,"Thanks for catching this, I implemented basically the same fix in https://github.com/Unity-Technologies/video-recorder/pull/3.",thanks catching basically fix,issue,negative,positive,positive,positive,positive,positive
578893892,"The communication time depends a lot on the amount of data that needs to be exchanged between Unity and Python. Python needs time to select actions and we have ongoing efforts to make the training faster. Unfortunately, there is not much you can do by modifying the game to speed Python up (besides reducing the amount of data exchanged).
When doing reinforcement learning, the Agent must be making the decisions. If you want, you can record some demonstrations and use them to do [behavioral cloning](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Imitation-Learning.md).",communication time lot amount data need unity python python need time select ongoing make training faster unfortunately much game speed python besides reducing amount data reinforcement learning agent must making want record use behavioral,issue,negative,negative,negative,negative,negative,negative
578563288,"I am not very familiar with them too. I'll try. However, the issue hasn't happened again, not even with Enligthen threads enabled. If I'll have news I'll post here",familiar try however issue even news post,issue,negative,positive,positive,positive,positive,positive
578510177,"Hey, 
A oke, so float properties works both ways? That is nice. Maybe I can make it work some how with multiple agents as well (it might be a bit janky but a well).
Thanks I will play with it in the near future.",hey float work way nice maybe make work multiple well might bit well thanks play near future,issue,positive,positive,positive,positive,positive,positive
578409840,"MY BAD!! It's working now, I forgot to add the --load parameter this time. Looks GREAT, thank you",bad working forgot add load parameter time great thank,issue,negative,positive,neutral,neutral,positive,positive
578409659,"Ok tried that, this time it didn't end. However, the model doesn't seem to have remembered anything. It looks as confused as when it was initially executed. Step count seems back to the beginning. ",tried time end however model seem anything confused initially executed step count back beginning,issue,negative,negative,negative,negative,negative,negative
578409114,"ill give that a try today, thanks
",ill give try today thanks,issue,negative,negative,negative,negative,negative,negative
578407441,"This is still an issue.
To veryfiy Debug.Log(vectorAction.Length) in AgentAction, it is not the same as set in the editor when startet without mlagents-learn.
Hoever, vectorAction.Length is set correct when stated via the training environment.
",still issue set editor without set correct stated via training environment,issue,negative,neutral,neutral,neutral,neutral,neutral
578402669,"Hi Vincent, thanks for the info.

Is 5ms the typical length of one of these requests? So 200 steps/sec is the max one environment can manage.

I'm using num-envs to speed up training but only saw a 2x to 3x speed up even with 8 or 16 envs running. Does this mean that the limiting factor is the speed python can process and send out actions and there is nothing I can do to speed that up without reducing the complexity of the model?

My environment can only support a single agent so I don't think I can do any batching of RequestDecision. It also requires a decision at every update as it is turn based. I do use action masking and sometimes there is only one action available to the agent in a specific state. Could I choose that action for the agent and still have it learn from the decision or would it not be able to learn anything if I don't call RequestDecision that update?

Thanks again for your support",hi vincent thanks typical length one one environment manage speed training saw speed even running mean limiting factor speed python process send nothing speed without reducing complexity model environment support single agent think also decision every update turn based use action sometimes one action available agent specific state could choose action agent still learn decision would able learn anything call update thanks support,issue,positive,positive,neutral,neutral,positive,positive
578356234,"Hi,
Agent.DecideAction is very expensive when training as Data must be sent to Python and back at every call. Unity must also wait for Python to pick an action for each Agent as well as training the neural network of the Agents. 
One way to reduce this time is to batch all Agents decision request together (calling request decision for all Agents at the same fixed update) This allows for fewer (although larger) messages to be sent.
Sending fewer messages is also an option (decreasing the number of calls to RequestDecision per seconds) although this can negatively affect your training (making it harder for Agents to learn since the decisions will be further apart)
Reducing the size of the neural network being trained is also an option but the Agents might not be as good.",hi expensive training data must sent python back every call unity must also wait python pick action agent well training neural network one way reduce time batch decision request together calling request decision fixed update although sent sending also option decreasing number per although negatively affect training making harder learn since apart reducing size neural network trained also option might good,issue,positive,negative,neutral,neutral,negative,negative
578355440,"Hi,
If you think this is a bug, could you fill out the bug report form?
I do not have enough information to reproduce the error you are describing.",hi think bug could fill bug report form enough information reproduce error,issue,negative,neutral,neutral,neutral,neutral,neutral
578251112,"Also, Is it possible that the time penalty of (-0.01f) is making the agent to end the episode early and not allowing to explore it to the end of the tunnel ? ",also possible time penalty making agent end episode early explore end tunnel,issue,negative,positive,neutral,neutral,positive,positive
578239162,"Hi,
If you have a single Agent in the scene, I would recommend using the [FloatProperties Side Channel](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Python-API.md#floatpropertieschannel). It allows you to exchange float values from C# to Python. I would say that the equivalent of gym's `info` are Side Channels.  ",hi single agent scene would recommend side channel exchange float python would say equivalent gym side,issue,negative,negative,neutral,neutral,negative,negative
578236791,"Hi @andrewcoh ,
Do you mean difference between last step's distance to target and current distance to target  as an observation ?
or adding the difference as a reward.",hi mean difference last step distance target current distance target observation difference reward,issue,positive,negative,negative,negative,negative,negative
578073770,"Hello:)

Thank you all for the suggestions, I will try them all out.

@mbaske Thank you for the links to your resources, your projects are fantastic!

",hello thank try thank link fantastic,issue,positive,positive,positive,positive,positive,positive
578033137,Yes it seems to work after a first test. Closing the issue.,yes work first test issue,issue,negative,positive,positive,positive,positive,positive
577980474,Getting .3f per timestep is also more valuable than ending an episode by reaching the goal.  Did you try something like the difference between last step's distance to target and current distance to target?,getting per also valuable ending episode reaching goal try something like difference last step distance target current distance target,issue,positive,neutral,neutral,neutral,neutral,neutral
577961212,@dlindmark This was a bug that was fixed in the 0.13.1 release (PR for the fix is here https://github.com/Unity-Technologies/ml-agents/pull/3219) . Can you try that out and see if it helps?,bug fixed release fix try see,issue,negative,positive,neutral,neutral,positive,positive
577960803,This was a bug that was fixed in the 0.13.1 release; see https://github.com/Unity-Technologies/ml-agents/pull/3219 for the fix.,bug fixed release see fix,issue,negative,positive,neutral,neutral,positive,positive
577960545,"OK, closing this, please reopen if you hit any other errors around this.",please reopen hit around,issue,negative,neutral,neutral,neutral,neutral,neutral
577959721,Sorry for the late update. Recent versions of ML Agents support tensorflow 2.0.0. ,sorry late update recent support,issue,negative,negative,negative,negative,negative,negative
577959545,"Unfortunately, we never found the root cause of the missing models. We have since increased the min version to 2018.x and updated the assets accordingly, and they no longer exhibit this problem.",unfortunately never found root cause missing since min version asset accordingly longer exhibit problem,issue,negative,positive,neutral,neutral,positive,positive
577959451,"@devedse Unfortunately, we never found the root cause of this. We have since increased the min version to 2018.x and updated the assets accordingly, and they no longer exhibit this problem.",unfortunately never found root cause since min version asset accordingly longer exhibit problem,issue,negative,positive,positive,positive,positive,positive
577954207,"The total length of the tube is 14 units. The agent is maximum able to reach upto 8.5 units.  So it is never within this range.
if distance to target <4.0f -- reward +0.6f.
In the image it is able to reach upto 8 units only. So agent is getting +0.3f reward per time step. I can give it a try by normalizing the reward . Do you think increasing the number of hidden layer or by increasing the learning rate would help.

If you mean by progressing towards the goal without resetting the environment when collide with the wall, I have tried that too. In that case agent is never able to learn to move with correct orientation.

",total length tube agent maximum able reach never within range distance target reward image able reach agent getting reward per time step give try reward think increasing number hidden layer increasing learning rate would help mean towards goal without environment collide wall tried case agent never able learn move correct orientation,issue,positive,positive,neutral,neutral,positive,positive
577951046,"Is the agent within this range 

>  if distance to target <4.0f -- reward +0.6f

in that image? If so, it's more rewarding for the agent to never finish the episode because it can get .6 every timestep that it's in range of the goal.  

In place of those proximity bonuses, you could use something like (max_distance - current_distance) / max_distance where max_distance is the maximum distance the agent can have to the target. The reason it is divided by max_distance is to normalize the reward to between 0 and 1.  Another thing to try that may be better is progress towards the goal.",agent within range distance target reward image rewarding agent never finish episode get every range goal place proximity could use something like maximum distance agent target reason divided normalize reward another thing try may better progress towards goal,issue,positive,positive,positive,positive,positive,positive
577940955,"Yes, I end the episode if the agent crashes into the wall. Using Done()",yes end episode agent wall done,issue,negative,neutral,neutral,neutral,neutral,neutral
577940792,I think you should have no problem learning with one brain.  Do you end the episode if the agent crashes into the wall i.e. call done?,think problem learning one brain end episode agent wall call done,issue,negative,neutral,neutral,neutral,neutral,neutral
577939573,I think it is too difficult for a single brain to learn the task of going up and also coming down. For this purpose it might be beneficial if I train with two brains. One for going up and other for coming down. Using GivenBrain function of the agent class.,think difficult single brain learn task going also coming purpose might beneficial train two brain one going coming function agent class,issue,negative,negative,negative,negative,negative,negative
577931212,"The actions are::
Actions Provided:
Forward Axis: Two action-- forward movement and no action
upDownRotateAxis : Three actions -- Two Action for up and down rotation and no action
leftRightRotateAxis : Three actions -- Two Action for left and right rotation and no action",provided forward axis two action forward movement action three two action rotation action three two action left right rotation action,issue,negative,positive,positive,positive,positive,positive
577893640,"> I made the AgentInfo property of the Agent private. So yes I am hiding it. What part of the Public API are we passing it to? AgentReset()? I think it is okay, it is more of a delayed event.

`AgentInfo` is passed to `RequestDecision`
https://github.com/Unity-Technologies/ml-agents/blob/0f9318675df2cdba24248a88dfc25ff919060b3e/UnitySDK/Assets/ML-Agents/Scripts/Policy/IPolicy.cs#L21

`AgentInfo` has a public `done` property.

https://github.com/Unity-Technologies/ml-agents/blob/0f9318675df2cdba24248a88dfc25ff919060b3e/UnitySDK/Assets/ML-Agents/Scripts/Agent.cs#L36

",made property agent private yes part public passing think event public done property,issue,negative,neutral,neutral,neutral,neutral,neutral
577812719,"Hello @andrewcoh ,
I changed the reward mechanism . The reward mechanism I am using right now is::
Used SetReward() function for providing reward.
•	Time penalty (-0.01f) using AddReward function
•	-0.8f reward for  colliding with tunnel boundary
•	+1 reward for reaching the final target.
•      if distance to target >=12.0f -- reward = +0.01f
•      if distance to target <12.0f  and >= 8.0f  -- reward = +0.1f
•      if distance to target  <8.0f  and >= 4.0f  -- reward = +0.3f
•      if distance to target <4.0f  -- reward +0.6f
•      if speed <1.4f -- reward = -0.6f

Observations collected are ::
Vector Observation Space: 
•	Adding 12 rays each at 30 degree apart in the form of cone. Detecting objects the tunnel 
        boundary and target at the end.
•	Adding current position of the agent as an observation.
•	Adding the target position as an observation
•       Adding distance to the target as an observation
Note:: Environment reset when agent collides with the tunnel boundary or reach the final target cube at the end of tunnel.

This are the training result after training the agent for 2.5 M time steps.
![image](https://user-images.githubusercontent.com/30369775/73011819-e355e700-3dc9-11ea-8e80-48a712a0cd1e.png)
![image](https://user-images.githubusercontent.com/30369775/73011978-30d25400-3dca-11ea-86e2-84c8a5175a9f.png)
The training converged agent learned some behavior but it is not able to solve the problem completely.
![image](https://user-images.githubusercontent.com/30369775/73012355-f4ebbe80-3dca-11ea-8b3d-8282003d967b.png)
My environment is like the image given above. The agent starts from the start point and need to reach to the target while maintaining correct orientation. Right now it is just able to reach to the point as shown in the image even after 2.5 M steps. I don't think it is too difficult of a problem to be able to solve by ML-Agent. I am not sure where I am going wrong. Do you think curriculum learning would be able to solve it?

Edit::
1). The problem of agent slowing down is solved by providing negative reward for slow speed. Now agent maintains a constant speed while moving.


",hello reward mechanism reward mechanism right used function providing reward time penalty function reward tunnel boundary reward reaching final target distance target reward distance target reward distance target reward distance target reward speed reward collected vector observation space degree apart form cone tunnel boundary target end current position agent observation target position observation distance target observation note environment reset agent tunnel boundary reach final target cube end tunnel training result training agent time image image training agent learned behavior able solve problem completely image environment like image given agent start point need reach target correct orientation right able reach point shown image even think difficult problem able solve sure going wrong think curriculum learning would able solve edit problem agent providing negative reward slow speed agent constant speed moving,issue,positive,positive,neutral,neutral,positive,positive
577810782,"Hi, I am not familiar with Enlighten threads. Could you try without the Enlighten threads and see if you are able to train then? ",hi familiar enlighten could try without enlighten see able train,issue,positive,positive,positive,positive,positive,positive
577806037,Using the `--load` argument will resume the training where it ended. It will save for example the number of training steps that have happened in the previous training session. My guess is that you need to increase the `max_steps` argument in the training configuration `yaml` file. `--load` is to **continue** training not restart a training session with a previously trained behavior as starting point.,load argument resume training ended save example number training previous training session guess need increase argument training configuration file load continue training restart training session previously trained behavior starting point,issue,negative,negative,negative,negative,negative,negative
577803216,"> AgentInfo has a public done member on it. It's not really removing the API, just hiding it in one place arbitrarily and passing it to another as part of our public API.

I made the AgentInfo property of the Agent private. So yes I am hiding it. What part of the Public API are we passing it to? `AgentReset()`? I think it is okay, it is more of a delayed event.",public done member really removing one place arbitrarily passing another part public made property agent private yes part public passing think event,issue,negative,positive,neutral,neutral,positive,positive
577794649,">  I think it is okay to prevent the user to seeing if the Agent is Done since it is an internal state that is only relevant for the Policy, not to the game mechanic.

`AgentInfo` has a public `done` member on it.  It's not really removing the API, just hiding it in one place arbitrarily and passing it to another as part of our public API.  ",think prevent user seeing agent done since internal state relevant policy game mechanic public done member really removing one place arbitrarily passing another part public,issue,negative,positive,neutral,neutral,positive,positive
577788593,"Hi @JohnBergago and @MarcoMeter,
I've logged this feature request as MLA-557 and will update this ticket as changes happen.   Thanks.",hi logged feature request update ticket happen thanks,issue,negative,positive,positive,positive,positive,positive
577745590,"Update: since it was definitely stuck, I terminated using `CTRL-C`, and I obtained a segfault error.
```
INFO:mlagents.trainers:Learning was interrupted. Please wait while the graph is generated.
Segmentation fault (core dumped)
```
I'll follow up with more information if this happens again. Trainings are long but I'll try to keep you updated.",update since definitely stuck error learning interrupted please wait graph segmentation fault core follow information long try keep,issue,negative,negative,neutral,neutral,negative,negative
577692193,"Here is the command line dump when it attempts to rerun then exists immediately


INFO:mlagents_envs:Connected new brain:
thirdRun?team=0
INFO:mlagents.trainers:Hyperparameters for the PPOTrainer of brain thirdRun:
        trainer:        ppo
        batch_size:     1024
        beta:   0.005
        buffer_size:    10240
        epsilon:        0.2
        hidden_units:   16
        lambd:  0.95
        learning_rate:  0.0003
        learning_rate_schedule: linear
        max_steps:      5000000
        memory_size:    256
        normalize:      False
        num_epoch:      3
        num_layers:     2
        time_horizon:   64
        sequence_length:        64
        summary_freq:   1000
        use_recurrent:  False
        vis_encode_type:        simple
        reward_signals:
          extrinsic:
            strength:   1.0
            gamma:      0.99
        summary_path:   thirdRun_thirdRun
        model_path:     ./models/thirdRun-0/thirdRun
        keep_checkpoints:       5
2020-01-23 10:26:30.135661: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
INFO:mlagents.trainers:Loading Model for brain thirdRun?team=0
INFO:mlagents.trainers:Saved Model
INFO:mlagents.trainers:List of nodes to export for brain :thirdRun?team=0
INFO:mlagents.trainers: is_continuous_control
INFO:mlagents.trainers: version_number
INFO:mlagents.trainers: memory_size
INFO:mlagents.trainers: action_output_shape
INFO:mlagents.trainers: action
INFO:mlagents.trainers: action_probs
Converting ./models/thirdRun-0/thirdRun/frozen_graph_def.pb to ./models/thirdRun-0/thirdRun.nn
IGNORED: StopGradient unknown layer
GLOBALS: 'is_continuous_control', 'version_number', 'memory_size', 'action_output_shape'
IN: 'vector_observation': [-1, 1, 1, 18] => 'main_graph_0/hidden_0/BiasAdd'
IN: 'epsilon': [-1, 1, 1, 2] => 'mul'
OUT: 'action', 'action_probs'
DONE: wrote ./models/thirdRun-0/thirdRun.nn file.
INFO:mlagents.trainers:Exported ./models/thirdRun-0/thirdRun.nn file",command line dump rerun immediately connected new brain brain trainer beta epsilon linear normalize false false simple extrinsic strength gamma binary use loading model brain saved model list export brain action converting unknown layer done wrote file file,issue,positive,negative,negative,negative,negative,negative
577640442,Is there a resolution for this issue that will work for Cloud Builds?,resolution issue work cloud,issue,negative,neutral,neutral,neutral,neutral,neutral
577611597,"Rather than having a single agent with swappable brains, I suggest you create multiple agents with each one being responsible for a specific task. For the drone example, you would first train an agent to stabilize and fly the drone towards a given direction. The direction vector is part of the agent's observation space, it could for instance point towards a randomized target position. After your agent has learned this behaviour, create a second one for higher level tasks like finding targets. This agent is trained to create the direction vector as its output value, which is then fed to the first agent.
I've build a couple of example projects using this type of setup:
https://github.com/mbaske/robot-ants
https://github.com/mbaske/angry-ai
https://github.com/mbaske/ml-drone-collection",rather single agent brain suggest create multiple one responsible specific task drone example would first train agent stabilize fly drone towards given direction direction vector part agent observation space could instance point towards target position agent learned behaviour create second one higher level like finding agent trained create direction vector output value fed first agent build couple example type setup,issue,positive,positive,positive,positive,positive,positive
577590946,"Just be aware that these parameters are only of use for the training algorithm.
They are not intended to give you a better view on the agents training inside your environment.",aware use training algorithm intended give better view training inside environment,issue,negative,positive,positive,positive,positive,positive
577580410,"This depends on how much data is being collected for training. In the trainer_config the parameter ""buffer_size"" specifies how much data is going to be gathered.
And of course the more epochs (num_epochs) are used for training, the longer the optimization process takes.

However, changing these values to shift or shrink the break is not comes of course with a high impact on how the training algorithm performs.",much data collected training parameter much data going course used training longer optimization process however shift shrink break come course high impact training algorithm,issue,negative,positive,positive,positive,positive,positive
577577421,@MarcoMeter Yes that's exactly what I mean. Thanks now it all make sense. Anyways do you know if their is a way to reduce it? Maybe instead of every other second it would happen every minute?,yes exactly mean thanks make sense anyways know way reduce maybe instead every second would happen every minute,issue,positive,negative,neutral,neutral,negative,negative
577563027,"I imagine the switching between different brains is to observe agent behavior at each stage... although it's a bit weird that it loads a new one every time. Right now I am doing a similar task with one brain, which I imagine is the intended use for curriculum learning.",imagine switching different brain observe agent behavior stage although bit weird new one every time right similar task one brain imagine intended use curriculum learning,issue,negative,negative,neutral,neutral,negative,negative
577561122,"You could train one model to accomplish certain skills like flying forward, backward and rotating.
A second one could be trained afterwards which just utilizes the already trained movement model to do stuff like navigation.

So you would successively build a hierarchy of models.

(This is just a potential concept. I'm not too familiar with the most recent version of ml-agents)",could train one model accomplish certain like flying forward backward rotating second one could trained afterwards already trained movement model stuff like navigation would successively build hierarchy potential concept familiar recent version,issue,positive,positive,positive,positive,positive,positive
577557520,"Do you mean that the environments visually pause for a moment?

If so, this is due to the flow of the training algorithm.
Talking in the case of PPO, at first data is sampled from every agent.
The collected data is then used to update the agent's policy (this is the observed break).
Data is always collected with the most recent policy.",mean visually pause moment due flow training algorithm talking case first data every agent collected data used update agent policy break data always collected recent policy,issue,negative,negative,neutral,neutral,negative,negative
577442085,"Changes from initial version:
* Added MLAgentsExamples namespace
* Added Agent.LazyInitialize()
* Updated the ModelOverrider so that it takes (behaviorName, asset path) pairs on the command line.",initial version added added asset path command line,issue,negative,neutral,neutral,neutral,neutral,neutral
577438503,"Hi @MarcoMeter,
EGL is being explored but we don't have a timeline on it yet.  I'll create an internal ticket for it and we'll update this thread when we have more information.  Thanks. ",hi yet create internal ticket update thread information thanks,issue,positive,positive,neutral,neutral,positive,positive
577437788,glad you were able to get it working :),glad able get working,issue,negative,positive,positive,positive,positive,positive
577430605,"On the question of the `IsDone()`, I think of it more like an event than a change of state. I think it is okay to prevent the user to seeing if the Agent is Done since it is an internal state that is only relevant for the Policy, not to the game mechanic. 
I am in general more in favor of removing APIs that are not used most of the time. If there is really a use case for knowing if an Agent is done and has yet to reset, I am curious...

On the SetReward topic, I think you are very unanimous that it is useful (so I won't fight too long) but I think it introduces a lot of confusion (that I don't think renaming would entirely solve). I *personally* think adding a reward should be an **irrevocable** action. ",question think like event change state think prevent user seeing agent done since internal state relevant policy game mechanic general favor removing used time really use case knowing agent done yet reset curious topic think unanimous useful wo fight long think lot confusion think would entirely solve personally think reward irrevocable action,issue,positive,positive,neutral,neutral,positive,positive
577427082,@surfnerd My bad! You are right. I was just using the values wrong. Thanks.,bad right wrong thanks,issue,negative,negative,negative,negative,negative,negative
577425106,"> Remove SetReward, allow passing a final reward to Done()

@ervteng I think there are situations other than termination states where we might want the functionality of SetReward",remove allow passing final reward done think termination might want functionality,issue,positive,neutral,neutral,neutral,neutral,neutral
577377462,"> Seems like it could be a little late but I also agree with @ervteng . The SetReward enables implementation of reward functions that would not be possible otherwise. Should we name it something more explicit like SetFullStepReward so that it's less confusing?

Some other possibilities:

- Change name to something like SetIncrementalReward()
- Remove SetReward, add method to reset reward to 0 (ResetIncrementalReward()?)
- Add method to get current reward (GetIncrementalReward?)
- Remove SetReward, allow passing a final reward to Done()",like could little late also agree implementation reward would possible otherwise name something explicit like le change name something like remove add method reset reward add method get current reward remove allow passing final reward done,issue,positive,negative,neutral,neutral,negative,negative
577371078,Seems like it could be a little late but I also agree with @ervteng . The SetReward enables implementation of reward functions that would not be possible otherwise.   Should we name it something more explicit like SetFullStepReward so that it's less confusing?,like could little late also agree implementation reward would possible otherwise name something explicit like le,issue,positive,negative,negative,negative,negative,negative
577353958,"thank you for your comment @caioc2.  we also have this mindset of trying to not introduce too many new features but try and make the core codebase more stable.  As we improve our version of Unity support, we are also striving to ensure bug fixes are applied to previous versions so you are not having to update new versions just to get the fix.  I've also passed some of your feedback to the Barracuda team here at Unity as well.

Again, we really appreciate feedback.",thank comment also trying introduce many new try make core stable improve version unity support also striving ensure bug applied previous update new get fix also feedback barracuda team unity well really appreciate feedback,issue,positive,positive,positive,positive,positive,positive
577346983,Agree with @ervteng. `SetReward()` serves a very specific purpose that is of value in a number of different kinds of games and environments. ,agree specific purpose value number different,issue,positive,neutral,neutral,neutral,neutral,neutral
577335949,"I'm not in support of removing `SetReward()`. It makes things less confusing a little bit but introduces many cases which are harder to implement. For instance, if you have an incremental reward for velocity/survival but if the Agent hits a wall or falls off the platform, you want to kill it and give -1 reward. 

So you put the `SetReward(-1)` and `Done()` in the collider OnTriggerEnter. Without `SetReward` or any way to read the current reward, you'll have no idea how much incremental reward was gathered by the time you hit the callback, and the Agent gets some unknown reward > -1. ",support removing le little bit many harder implement instance incremental reward agent wall platform want kill give reward put done without way read current reward idea much incremental reward time hit agent unknown reward,issue,positive,positive,neutral,neutral,positive,positive
577332851,@andrewcoh @ervteng can you guys look and leave feedback.  Arguing for/against this is out of my area of expertise for the removal of `SetReward`,look leave feedback area removal,issue,negative,neutral,neutral,neutral,neutral,neutral
577328902,"~Before approving this, I'd like to see some of the training results with these changes.  I'm not sure how modifying the reward function in this way will change things.~",like see training sure reward function way change,issue,positive,positive,positive,positive,positive,positive
577259596,"As a user since the 0.5, I like what ml-agents propose and I hope for a great tool in the future, but this are my 2 cents on what I think it should improve.

I've seen many new features come to life, various changes to the usage and implementation which sounds good, but many bugs are not being look at or are ""low priority"" and still open.

IMHO, it would be much more porwefull to offer a simple tool which  may lack some fancy stuff but is very reliable. In general, every Unity package seems to be always in preview and never reach the quality of a release.

Just to cite some of the things that have been a problem and I think could be handled better:

- Versions (which may seems to be in focus now) - Having to update to a new version to solve old bugs and refactor all your code, to discover new bugs. Also having completely different models from different versions, **without a solid explanation why and which changes are related to it**.
- IL2CPP compiler
- Barracuda plagued with bugs. I think the idea to have a universal inference engine is amazing, but if you are not willing to invest the necessary amount of resources to build such tool with great quality, then it's best to use something else that may not fit perfectly your purpose but is well tested and documented.
- Fast paced development of new things while slow pace to fix or improve core features.

TL;DR - stabillity and heavily tested core functionality is a must, one can't build and test new algorithms when the core framework cant be trusted.

Anyway, I'm looking forward it, transparency is always a good sign.




",user since like propose hope great tool future think improve seen many new come life various usage implementation good many look low priority still open would much offer simple tool may lack fancy stuff reliable general every unity package always preview never reach quality release cite problem think could handled better may focus update new version solve old code discover new also completely different different without solid explanation related compiler barracuda plagued think idea universal inference engine amazing willing invest necessary amount build tool great quality best use something else may fit perfectly purpose well tested fast paced development new slow pace fix improve core heavily tested core functionality must one ca build test new core framework cant anyway looking forward transparency always good sign,issue,positive,positive,positive,positive,positive,positive
577251897,"Hi @lazyvainglory,
The realTimeSinceStartup property on the Time class explicitly states it’s not affected by timeScale.  If you are seeing this, can you file a bug?  Please test it out.
Cheers,
Chris",hi property time class explicitly affected seeing file bug please test,issue,negative,neutral,neutral,neutral,neutral,neutral
577219697,@chriselion Thanks. But they get affected too. If I could figure out the speed of the training maybe I could do a `time += (Time.deltaTime * trainingSpeed)` ,thanks get affected could figure speed training maybe could time,issue,negative,positive,positive,positive,positive,positive
577217543,"Check out https://docs.unity3d.com/ScriptReference/Time.html - I think Time.time or Time.realtimeSinceStartup are what you want. There's nothing ML-Agents-specific, though.",check think want nothing though,issue,negative,neutral,neutral,neutral,neutral,neutral
577198754,"@Robin2091 any chance you could share frozen graph of your model (or ONNX file)? no need for trained one, random weights are good enough. Thanks!",robin chance could share frozen graph model file need trained one random good enough thanks,issue,positive,positive,neutral,neutral,positive,positive
577190201,On branch ```release-0.12.0``` the parameters are there and it works. Thank you for your support.,branch work thank support,issue,positive,neutral,neutral,neutral,neutral,neutral
577038726,"@andrewcoh 
Do you know or could you check if EGL is a suitable alternative to X?",know could check suitable alternative,issue,negative,positive,positive,positive,positive,positive
576963682,"You don't seem to be on the correct tag for version 0.12.

Try 
`git checkout 0.12.0`
and then try running again.",seem correct tag version try git try running,issue,negative,neutral,neutral,neutral,neutral,neutral
576933268,"I don't have any of these parameters. Training and Inference Configuration are also not available in my version.

This is the inspector of the academy in my project:
![walljump_academy](https://user-images.githubusercontent.com/32358434/72851484-0d18ec00-3cac-11ea-897b-9d4f7b67b600.PNG)
",training inference configuration also available version inspector academy project,issue,negative,positive,positive,positive,positive,positive
576896330,I'd suggest just a positive bonus for every car that crosses (like .01) and some penalty for collisions/cars waiting for green.,suggest positive bonus every car like penalty waiting green,issue,positive,positive,neutral,neutral,positive,positive
576894030,"> Reward for agent facing towards target(cube at the end of tunnel) = 0.001f * m_FacingDot

may be enabling the agent to get positive reward for just facing the target (but not actually moving toward it). I would remove this. Secondly,

> Reward for moving toward target = 0.003f* m_MovingTowardsDot

should be less (more negative) than the timestep penalty so that the agent doesn't figure out how to constantly be moving towards the target but never actually reaching it.",reward agent facing towards target cube end tunnel may agent get positive reward facing target actually moving toward would remove secondly reward moving toward target le negative penalty agent figure constantly moving towards target never actually reaching,issue,positive,negative,neutral,neutral,negative,negative
576874697,"Currently, we still only support running headlessly with x server.  Also, I see you mentioned visual observations, which cannot be collected by an agent when running in headless mode. See [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Limitations.md#headless-mode)",currently still support running server also see visual collected agent running headless mode see,issue,negative,neutral,neutral,neutral,neutral,neutral
576819758,"Can you tell me what names of the reset parameters are in your Wall Jump Academy script attached to your academy in the wall jump environment?  It should look like this 

![walljumpreset](https://user-images.githubusercontent.com/54679309/72832596-08b3eb00-3c3a-11ea-9e20-118232a9c0c0.png)
",tell reset wall jump academy script attached academy wall jump environment look like,issue,negative,neutral,neutral,neutral,neutral,neutral
576613571,Found a similar issue and the migration instructions. Will try those,found similar issue migration try,issue,negative,neutral,neutral,neutral,neutral,neutral
576556793,"@MinneStephanie2 : Thank you for your help. I set the env_name to None. Then I pressed play in unity. But I got the following error: `Couldn't connect to trainer on port 5004 using API version API-13. Will perform inference instead.` So I looked into issue [3128](https://github.com/Unity-Technologies/ml-agents/issues/3128) to resolve. But, I am still getting The Unity environment took too long to respond error.",thank help set none play unity got following error could connect trainer port version perform inference issue resolve still getting unity environment took long respond error,issue,negative,negative,neutral,neutral,negative,negative
576459336,"Hello @andrewcoh ,
I performed a training by removing the intermediate rewards and by increasing the number of max steps up to 800K. But the performance did not improved and some strange trends were observed during training. The training graphs obtained are as given below:
![image](https://user-images.githubusercontent.com/30369775/72763284-026d3280-3b98-11ea-848b-5d549db74a0b.png)
![image](https://user-images.githubusercontent.com/30369775/72763339-3a747580-3b98-11ea-807b-1c280ec3d8ab.png)
The reward mechanism I am using now is as given below:

- Time penalty (-1f/ agentParameters.maxStep)

- Reward for agent facing towards target(cube at the end of tunnel) = 0.001f * m_FacingDot

- Reward for moving toward target = 0.003f* m_MovingTowardsDot

- -0.2f reward for colliding with tunnel boundary.

- +1 reward for reaching the final target.

The agent moves very slowly after the training, and tends to turn around(full 180) and try to go out of tunnel. I am providing negative reward in order to prompt agent to finish episode fast, instead it moves very slowly and still get positive reward. I am not able to understand this behavior , any suggestion would be helpful.


",hello training removing intermediate increasing number performance strange training training given image image reward mechanism given time penalty reward agent facing towards target cube end tunnel reward moving toward target reward tunnel boundary reward reaching final target agent slowly training turn around full try go tunnel providing negative reward order prompt agent finish episode fast instead slowly still get positive reward able understand behavior suggestion would helpful,issue,positive,positive,neutral,neutral,positive,positive
576458333,"@andrewcoh Thank you so much, it made things so much easier",thank much made much easier,issue,positive,positive,positive,positive,positive,positive
576385516,Please fill in the issue template. ,please fill issue template,issue,negative,neutral,neutral,neutral,neutral,neutral
576332119,"If I run `mlagents-learn --help`, this shows up:

`Traceback (most recent call last):`
`File ""c:\python37\lib\runpy.py"", line 193, in _run_module_as_main`
`    ""__main__"", mod_spec)`
`File ""c:\python37\lib\runpy.py"", line 85, in _run_code`
`    exec(code, run_globals)`
`File ""C:\Python37\Scripts\mlagents-learn.exe\__main__.py"", line 4, in <module>`
`File ""c:\python37\lib\site-packages\mlagents\trainers\learn.py"", line 15, in <module>`
`    from mlagents import tf_utils`
`File ""c:\python37\lib\site-packages\mlagents\tf_utils\__init__.py"", line 1, in <module>`
`    from mlagents.tf_utils.tf import tf as tf  # noqa`
`File ""c:\python37\lib\site-packages\mlagents\tf_utils\tf.py"", line 3, in <module>`
`    import tensorflow as tf  # noqa I201`
`ModuleNotFoundError: No module named 'tensorflow'`

Where my Tensorflow version is 2.0.0 (<2.1,>=1.7) and I'm running 64-bit Python 3.7.3.

Edit: Works with Tensorflow 1.15.0. Unsure why ML-Agents doesn't support version 2.0 when it explicitly downloads Tensorflow 2.0.0 and related dependencies.

@chriselion is it possible to reopen the issue or should I open a new one?",run help recent call last file line file line code file line module file line module import file line module import file line module import module version running python edit work unsure support version explicitly related possible reopen issue open new one,issue,positive,positive,neutral,neutral,positive,positive
576254293,"hey,
You can try to set env_name none. Then you have to press play in unity itself.",hey try set none press play unity,issue,negative,neutral,neutral,neutral,neutral,neutral
576253656,"@lazyvainglory 
> Let me change the question. What is the highest negative reward and the lowest positive reward. Is it 3 decimal +/- .001 or 6 decimal +/- 000001?

Your agent will always try to get a reward as high as posible. You can go as low in that reward as you want. 


",let change question highest negative reward positive reward decimal decimal agent always try get reward high go low reward want,issue,positive,positive,neutral,neutral,positive,positive
576206210,"thank you very much mbaske. it works!

Do you know if it would make more sense to use ""SetReward"" or ""AddReward""? ",thank much work know would make sense use,issue,negative,positive,positive,positive,positive,positive
576202496,"@andrewcoh, The 3rd bullet is for giving negative reward to the agent if there are a lot of cars that have a speed of 0. It's for forcing the agent to let the cars drive without stopping. 

The goal of the ai is to make an intersection as efficient as possible. 

The observation are the amount of cars in front of the traffic light, the state of the light (green, red, yellow) and the average waiting time of the cars in front of the light.

The states are 8 different options. The agent has to set the int between 0 and 7 to choose the state.

",bullet giving negative reward agent lot speed forcing agent let drive without stopping goal ai make intersection efficient possible observation amount front traffic light state light green red yellow average waiting time front light different agent set choose state,issue,positive,positive,neutral,neutral,positive,positive
576198862,"Hey thank you for the response. Everything within the frame is the purple frame is the agent view. 
The additional vectors were a little 3x3 grid around the agent: each pixel being represented as a one-hot encoding of [outside-grid, off-pixel, on-pixel]. 

Your first guess is a problem I had earlier, but I fixed it by either using a single pixel for the agent location (it needs to decide which adjacent pixel it will flip or not flip), or the colors within the agent's red circle would turn purple when pixel was turned on like in the picture.  (I gave agent a bigger circle instead of a single pixel because a single pixel was concerned it was not significant enough for the CNN with the shrinking mapsize after each convolution ).

What over the weekend solved my problem was making the agent's view ego-centric, (keeping the agent centered in the view, mario-style). This was apparently a better representation of the agent's state because I guess it contains more change and diversity in the data, and maybe it's easier to ascribe value to changes of fixed pixels on the screen.






",hey thank response everything within frame purple frame agent view additional little grid around agent first guess problem fixed either single agent location need decide adjacent flip flip color within agent red circle would turn purple turned like picture gave agent bigger circle instead single single concerned significant enough shrinking convolution weekend problem making agent view keeping agent centered view apparently better representation agent state guess change diversity data maybe easier ascribe value fixed screen,issue,positive,positive,neutral,neutral,positive,positive
576160198,"> I also think that my codelines are wrong?

Yes, they are. You're passing floats where Vector3s are expected. Try this
`Vector3 targetOffset = Target.localPosition - transform.localPosition;`
`Vector3 directionToTarget = targetOffset.normalized;`
`float distanceToTarget = targetOffset.magnitude;`
`float distanceToTargetX = Mathf.Abs(targetOffset.x);`
`float distanceToTargetY = Mathf.Abs(targetOffset.y);`
`float distanceToTargetZ = Mathf.Abs(targetOffset.z);`",also think wrong yes passing try vector vector float float float float,issue,negative,negative,negative,negative,negative,negative
576131537,"my agent can move in a 3-dimensional space. His task is to touch a surface that appears randomly somewhere. If he succeeds, both agent and target are repositioned randomly. If he does not, both will be randomly repositioned as well. In the ml-agents examples, set reward was used, so I thought that would suit my task, wouldn't it?

I also think that my codelines are wrong?
``` 
float distanceToTargetx = Vector3.Distance(this.transform.localPosition.x, Target.localPosition.x);
float distanceToTargety = Vector3.Distance(this.transform.localPosition.y, Target.localPosition.y);
float distanceToTargetz = Vector3.Distance(this.transform.localPosition.z, Target.localPosition.z);
",agent move space task touch surface randomly somewhere agent target randomly randomly well set reward used thought would suit task would also think wrong float float float,issue,negative,negative,negative,negative,negative,negative
576077031,"You can do this by adding a GameObject as a child of the agent, rotate the child 90 degrees in the appropriate direction and then add a raycast script to the child.  In the BehaviorParameters script on the agent, check the box ""Use Child Sensors."" This way, the agent will use observations collected by its children too. ",child agent rotate child appropriate direction add script child script agent check box use child way agent use collected,issue,negative,positive,positive,positive,positive,positive
576075135,The `SetReward` function is to set the reward of an entire episode.  You may be expecting the behavior of `AddReward` which accumulates the per time step reward over an agent's episode.,function set reward entire episode may behavior per time step reward agent episode,issue,positive,neutral,neutral,neutral,neutral,neutral
576072075,"A time step penalty (2nd bullet) may not be right for this type of problem. This is often used if an agent's goal is to end an episode as quickly as possible (find a goal state etc). Also, I don't completely understand the the 3rd bullet.  

Can you describe the problem you're trying to solve and possible give some more info on how you're building the observation/action space?",time step penalty bullet may right type problem often used agent goal end episode quickly possible find goal state also completely understand bullet describe problem trying solve possible give building space,issue,negative,positive,neutral,neutral,positive,positive
576071279,There is no bound on what can be passed to AddReward.  The range of -1 to 1 is recommended for learning stability purposes.,bound range learning stability,issue,negative,neutral,neutral,neutral,neutral,neutral
575996303,"Try using a curriculum 

https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Curriculum-Learning.md

The idea is you start with a simple task then slowly scale to the main objective.",try curriculum idea start simple task slowly scale main objective,issue,negative,negative,neutral,neutral,negative,negative
575948385,"@MinneStephanie2  > 
> When you use AddReward than the value has to be between -1 and 1.

Let me change the question. What is the highest negative reward and the lowest positive reward. Is it 3 decimal  +/- .001 or 6 decimal  +/- 000001?

@mbaske Thank you.",use value let change question highest negative reward positive reward decimal decimal thank,issue,positive,negative,neutral,neutral,negative,negative
575943189,@benfknzen This is a really old issue - can you create a new one and add your logs there?,really old issue create new one add,issue,negative,positive,positive,positive,positive,positive
575938575,"I had this working in older versions.

I am getting this same issue now as you did in your original post, ran through and did the same things as mentioned in your posts. Recloned the repository.

Thoughts?",working older getting issue original post ran repository,issue,negative,positive,positive,positive,positive,positive
575910969,"> Is it possible to know the accumulated reward on a specific agent?

You can call an agent's `GetCumulativeReward()` method.",possible know reward specific agent call agent method,issue,positive,neutral,neutral,neutral,neutral,neutral
575910609,"> When you use AddReward than the value has to be between -1 and 1.

AFAIK, it is _recommended_ to keep rewards in this range, but training still seems to work fine if I don't. What's the reasoning behind this recommendation?",use value keep range training still work fine reasoning behind recommendation,issue,negative,positive,neutral,neutral,positive,positive
575850948,"Hey @surfnerd,
in Unity-Editor my game is working fine. Like I said if I click on ""build and run"" the game works also fine (because Java starts - one Agent needs Java to play). But if I build a executable the agent who needs Java is not moving (because Java doesn´t start). Is there a problem with my navmash-agent ",hey game working fine like said click build run game work also fine one agent need play build executable agent need moving start problem,issue,positive,positive,neutral,neutral,positive,positive
575848260,This seems like an issue with unity and not ml agents.  You can try posting on the unity physics forums [here](https://forum.unity.com/forums/physics.78/),like issue unity try posting unity physic,issue,negative,neutral,neutral,neutral,neutral,neutral
575844770,Fixes have been merged into master.  I will close this now.  Thank you again for your feedback.,master close thank feedback,issue,negative,neutral,neutral,neutral,neutral,neutral
575842736,"Is that image the agent view?  Also, can you elaborate on what the additional vector observations are that helped?

My first guess is that in the purely visual setting the red agent pixel is blocking the pixel that it's affecting so it cannot observe the true state.  Thus, it would be impossible to know if the pixel the agent was sitting on is in the correct state or not because it is always 'red' in the observation. I may be misunderstanding the problem setting. Please let me know!",image agent view also elaborate additional vector first guess purely visual setting red agent blocking affecting observe true state thus would impossible know agent sitting correct state always observation may misunderstanding problem setting please let know,issue,negative,positive,neutral,neutral,positive,positive
575823728,Thanks for your report.  Those are my typos.  I’ll fix them quick.  Thanks 😂 ,thanks report fix quick thanks,issue,positive,positive,positive,positive,positive,positive
575800783,We currently do not have a model trained for the soccer environment and there is no heuristic method implemented.,currently model trained soccer environment heuristic method,issue,negative,neutral,neutral,neutral,neutral,neutral
575725182,"Currently, there is no experimental configuration that supports outputting the nn file every k steps.  However, a policy checkpoint is saved every  k steps using the value from the --save-freq command line argument. 

You may be able to hack it if you want to get an nn every k steps using the `export_graph` function in the trainer_controller.  I think
calling `self._export_graph()` after `self._save_model` in the trainer controller (line 226) and modifying the file path in `export_model` in tf_policy.py so that on each export, files are not overwritten would work. 

Alternatively, you can kill the training run after 400k steps which will save to an nn and then rerun with the --load flag to restart from the stopped position.  Rename the file so that it does not get overwritten after the 500k steps. This is a bad suggestion but may be simplest.

I agree that this is a feature we should have and I have logged it internally as the ticket MLA-538.",currently experimental configuration file every however policy saved every value command line argument may able hack want get every function think calling trainer controller line file path export would work alternatively kill training run save rerun load flag restart stopped position rename file get bad suggestion may agree feature logged internally ticket,issue,negative,negative,neutral,neutral,negative,negative
575718933,"Policies can only change the action after a decision interval has elapsed which corresponds to the number of fixed updates that occur (1 fixed update per frame).  So, the Heuristic function will only be called every n fixed updates. On the other hand, update is called far more frequently than fixed update, so it makes sense that you observe this behavior.  Try turning the decision interval down to 1 and maybe your agent will seem more responsive.",change action decision interval number fixed occur fixed update per frame heuristic function every fixed hand update far frequently fixed update sense observe behavior try turning decision interval maybe agent seem responsive,issue,positive,positive,neutral,neutral,positive,positive
575718258,"Hello, yes, I followed these instructions. But it is still not working.",hello yes still working,issue,negative,neutral,neutral,neutral,neutral,neutral
575389841,"Without nesting and adding a menu group:
![Screen Shot 2020-01-16 at 3 08 35 PM](https://user-images.githubusercontent.com/6877802/72570547-1a7c4380-3872-11ea-854e-666f151dce8f.png)
",without menu group screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
575378982,Looks good. Worth noting in the migration guide as a slight behavior change in some cases (only if onDemandDecision?),good worth migration guide slight behavior change,issue,positive,positive,positive,positive,positive,positive
575305625,I didn't know how to revert my changes and thought that creating a new pull request would be a safer option.,know revert thought new pull request would option,issue,negative,positive,positive,positive,positive,positive
575269198,"Hi @devedse,
I've filed this under MLA-535 in our internal tracker.  We will prioritize this and update this issue when we have an update. ",hi internal tracker update issue update,issue,negative,neutral,neutral,neutral,neutral,neutral
575268562,"Hey @devedse,
I apologize for not getting back to you.  It has been quite busy lately.  I'd like to investigate further but may not get to it for a while. I'll add the bug label and file this in our internal tracker.  I'd like to get to the bottom of it.",hey apologize getting back quite busy lately like investigate may get add bug label file internal tracker like get bottom,issue,positive,negative,neutral,neutral,negative,negative
575245461,"Thank you very much! I'll look into it, and probably tomorrow I can say if it worked :D

~~Just to understand where I went wrong, the problem was that I installed `ml-agents` from PyPi (which has the `latest_release` tag) instead of installing the `ml-agents` package from the repository directly (using `pip install -e ./`), then I followed the documentation in `master`, correct?~~

EDIT: Ok, now I see it was because of the `UnitySDK` and not the `ml-agents` package. I cloned the `latest_release` tag of the repo and then followed instructions on `master`. Sorry for the confusion. I think I can close this for now. If I still get trouble I'll ask to reopen.

Thanks again!",thank much look probably tomorrow say worked understand went wrong problem tag instead package repository directly pip install documentation master correct edit see package tag master sorry confusion think close still get trouble ask reopen thanks,issue,negative,negative,negative,negative,negative,negative
575238594,"The documentation you linked to is for the very latest development version of ml-agents. Please use https://github.com/Unity-Technologies/ml-agents/blob/latest_release/docs/Learning-Environment-Create-New.md instead. The directions are mostly the same, but the section on [Add an Empty GameObject to Hold the Academy](https://github.com/Unity-Technologies/ml-agents/blob/latest_release/docs/Learning-Environment-Create-New.md#add-an-empty-gameobject-to-hold-the-academy) is still necessary in all the released versions of ml-agents.",documentation linked latest development version please use instead mostly section add empty hold academy still necessary,issue,negative,positive,positive,positive,positive,positive
575059366,"@surfnerd , do you have any updates or ideas or shall we leave this issue as stale?",shall leave issue stale,issue,negative,negative,negative,negative,negative,negative
575057372,"If u want to increase ur training speed with unvisible agents u alternativly can use ""--no-graphics"" in ur training-command.
[Training ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-ML-Agents.md)
",want increase ur training speed unvisible use ur training,issue,negative,neutral,neutral,neutral,neutral,neutral
574970344,"I have try it,but it works well in windows,but when post in linux ,it has an error""Infinity or NaN floating point numbers appear when calculating the transform matrix for a Collider. Scene hierarchy path ""trianArea (4)/53a2b34d56e1b""
 
(Filename: ./Modules/Physics/Collider.cpp Line: 226)""",try work well post error infinity nan floating point appear calculating transform matrix scene hierarchy path line,issue,negative,neutral,neutral,neutral,neutral,neutral
574969874,"yes,and I made in windows,but I want ti post into linux

",yes made want ti post,issue,negative,neutral,neutral,neutral,neutral,neutral
574929120,@chriselion Hi Chris thanks for pointing me to the right direction! ,hi thanks pointing right direction,issue,negative,positive,positive,positive,positive,positive
574920528,"> Unfortunately, Agent doesn't qualify for this because it's abstract.

For now 😈 ",unfortunately agent qualify abstract,issue,negative,negative,negative,negative,negative,negative
574915745,"> Might want to eventually expose the queue parameter, but looks good.

If we ever build a multithreaded trainer_controller, we'll pass the queue parameter on creation of the AgentManager",might want eventually expose queue parameter good ever build multithreaded pas queue parameter creation,issue,positive,positive,positive,positive,positive,positive
574865361,"@ChunWayne The tutorial is out of date - Brain ScriptableObjects were removed in version 0.11. If you're following https://www.immersivelimit.com/tutorials/unity-ml-agents-penguins, that originally used 0.8.2, which you can download from here https://github.com/Unity-Technologies/ml-agents/releases/tag/0.8.2. They also have directions on how the tutorial has been changed in recent versions.",tutorial date brain removed version following originally used also tutorial recent,issue,negative,positive,positive,positive,positive,positive
574815245,"@mnsmuts 
If there is a NaN in your observations, you should definitely try to find out where it's coming from and fix it. I just added some better NaN checks on the C# side (https://github.com/Unity-Technologies/ml-agents/pull/3221), so if you apply those changes, you should get the callstack for where the observations are being added.",nan definitely try find coming fix added better nan side apply get added,issue,positive,positive,positive,positive,positive,positive
574813298,"The number of visible agents would increase the complexity of what needs to be rendered, but not the complexity of the physics solver/training. So, more visible agents might add some rendering overhead.",number visible would increase complexity need complexity physic visible might add rendering overhead,issue,negative,neutral,neutral,neutral,neutral,neutral
574812217,Can you clarify? Do you mean you've trained an agent and you'd now like to release a game with the trained agent embedded in it?,clarify mean trained agent like release game trained agent,issue,negative,negative,negative,negative,negative,negative
574806904,Maybe there are something else you need to keep fixed. ,maybe something else need keep fixed,issue,negative,positive,neutral,neutral,positive,positive
574797137,Yes I have it fixed to the same seed as the one from the demo. Hence I posted this as a bug. ,yes fixed seed one hence posted bug,issue,negative,positive,neutral,neutral,positive,positive
574787384,"In this case you are assuming the environment will always be fixed. In that case if you take the exact same action for every steps, you will be able to retrace the demonstration. 

However the obstacle tower environment is not a fixed environment. So you won't be able to retrace the demo unless you fix the seed that varies the generation of the environment(and maybe other things that might vary, for example physics in unity). ",case assuming environment always fixed case take exact action every able retrace demonstration however obstacle tower environment fixed environment wo able retrace unless fix seed generation environment maybe might vary example physic unity,issue,negative,positive,positive,positive,positive,positive
574652890,"Also, when I want to create a brain by right clicking, I do not see an ML-agents option as well as compared to this tutorial I found online.

![image](https://user-images.githubusercontent.com/59901029/72436446-629c5700-37db-11ea-8f41-fab44e7fb2ad.png)

is there something wrong with my installation?

Thank you!",also want create brain right see option well tutorial found image something wrong installation thank,issue,negative,negative,negative,negative,negative,negative
574609386,"I can give you code actually

```python
def replay(path, **other_params):
    brain_params, brain_infos, _ = demo_loader.load_demonstration(str(path))
    #some initialization and setting up
    env.reset()
     #starting from 1 because it has the previous_action
    for binfo in brain_infos[1:]:
        #process_info extracts the vector of the previous_action
        _,_,_, info = process_info(binfo)
        _,_,_, newinfo = env.step(info.previous_action)
```


My goal is to be able to take in a demonstration and take the same steps as those in the demo in order to return to the same location as the final step in it (the demo)",give code actually python replay path path setting starting vector goal able take demonstration take order return location final step,issue,negative,positive,positive,positive,positive,positive
574553252,"Yea that sorted it, thanks for all your help!",yea sorted thanks help,issue,positive,positive,positive,positive,positive,positive
574460285,@vincentpierre I made some changes to `get_action` to get the worker_id through and get the right memories/previous actions. ,made get get right,issue,negative,positive,positive,positive,positive,positive
574445583,"Yes, I will be rerunning a training without those intermediate rewards. Here are all the charts generated during training to provide you with more information. Hope they are helpful.
![image](https://user-images.githubusercontent.com/30369775/72395535-b1b88e00-36ee-11ea-88f1-1e613eb97d1c.png)

",yes training without intermediate training provide information hope helpful image,issue,positive,neutral,neutral,neutral,neutral,neutral
574435906,Hmm unfortunately that figure doesn't give a lot of information.  I'd suggest rerunning without the two intermediate rewards mentioned in my previous comment.,unfortunately figure give lot information suggest without two intermediate previous comment,issue,negative,negative,negative,negative,negative,negative
574432592,"This is the cumulative reward chart generated over the training period. 
![image](https://user-images.githubusercontent.com/30369775/72393343-72d30a00-36e7-11ea-8a3d-8a050d229657.png)
I am not sure what you mean by episodic reward.",cumulative reward chart training period image sure mean episodic reward,issue,positive,positive,neutral,neutral,positive,positive
574428347,"I'm suspicious of 
- +0.8f reward for reaching to a minitarget with correct orientation
- 0.5f reward for reaching a minitarget with incorrect orientation
They might be introducing weird optima that can be hard to identify/understand.

Can you remove these/mini target and try running again? Try running for a lot more timesteps (you can kill the training run if it converges before it reaches the max steps and your model will be saved). When training, does the episodic reward seem to stabilize at a value near 1?  ",suspicious reward reaching correct orientation reward reaching incorrect orientation might weird hard remove target try running try running lot kill training run model saved training episodic reward seem stabilize value near,issue,positive,negative,negative,negative,negative,negative
574421358,"> Looks good, do we need to consider this for a hotfix release?

I think so - this and the Tensorboard fix should probably be a hotfix for 0.13. What do you think?",good need consider release think fix probably think,issue,negative,positive,positive,positive,positive,positive
574418584,"I am using PPO for training and max steps in my training are 600,000. The reward mechanism i am using is ::
Used SetReward() function for providing reward.

Time penalty (-1f/ agentParameters.maxStep)

Reward for agent facing towards target(cube at the end of tunnel) = 0.001f * m_FacingDot

Reward for moving toward target = 0.001* m_MovingTowardsDot

+0.8f reward for reaching to a minitarget with correct orientation.

-0.5f reward for reaching a minitarget with incorrect orientation.

-0.2f reward for colliding with tunnel boundary.

+1 reward for reaching the final target.

Here target is a cube at the end of tunnel , to provide positive reward to make the agent reach the end of tunnel . I am using minitargets as intermediate points at which smaller reward is provided to the agent for reaching those points with correct position and orientation.
",training training reward mechanism used function providing reward time penalty reward agent facing towards target cube end tunnel reward moving toward target reward reaching correct orientation reward reaching incorrect orientation reward tunnel boundary reward reaching final target target cube end tunnel provide positive reward make agent reach end tunnel intermediate smaller reward provided agent reaching correct position orientation,issue,positive,positive,neutral,neutral,positive,positive
574398705,"Can you share your reward function?

Also, assuming your reward function encourages the agent to pass through the tunnel without colliding with anything, I'd recommend using PPO over SAC (SAC learns inherently more random policies) and to also try running for more timesteps to allow convergence. ",share reward function also assuming reward function agent pas tunnel without anything recommend sac sac inherently random also try running allow convergence,issue,positive,negative,negative,negative,negative,negative
574383317,"Hi @andrewcoh , thanks for replying.
In my case, I am training an agent to pass through a tunnel perfectly, without colliding with the walls. After training, in one episode agent is able to pass through a particular part of tunnel in a correct manner while in the next episode it is not doing the same or passing through that particular part including some collisions. That creates difference in performance and not acceptable.
I wonder if there is a way to remove this variation.",hi thanks case training agent pas tunnel perfectly without training one episode agent able pas particular part tunnel correct manner next episode passing particular part difference performance acceptable wonder way remove variation,issue,positive,positive,positive,positive,positive,positive
574372737,"The policies learned by PPO/SAC are typically stochastic.  Assuming that the different behaviors aren't significantly different in performance, this isn't unexpected.",learned typically stochastic assuming different significantly different performance unexpected,issue,negative,negative,neutral,neutral,negative,negative
574350357,"That's something we've deprecated (Online BC). Usually the learning would require lots of data, and  real time playing is a little bit too slow to generate enough data for learning. ",something usually learning would require lot data real time little bit slow generate enough data learning,issue,negative,negative,negative,negative,negative,negative
574349678,You could refer to https://github.com/Unity-Technologies/ml-agents/blob/ac4b1a4b91b48cf498c17913b9a68222208a9661/docs/Training-ML-Agents.md for more details. You don't need to name the model. ,could refer need name model,issue,negative,neutral,neutral,neutral,neutral,neutral
574323413,"I guess I've expressed myself poorly. I was thinking about real time learning, while the player plays with the agent (bot) and makes the agent learn from the player at that moment.",guess expressed poorly thinking real time learning player agent bot agent learn player moment,issue,negative,negative,neutral,neutral,negative,negative
574322779,"Hey xiaomaogy,
thanks for your reply! So instead of --train, --load, and do I have to name the model?
`mlagents-learn <trainer-config-file> --run-id=<run-identifier> --load` this would be correct?",hey thanks reply instead train load name model load would correct,issue,negative,positive,positive,positive,positive,positive
574321889,"What do you mean by ""if I try to retrace the trajectory""? Could you provide more detailed steps?",mean try retrace trajectory could provide detailed,issue,negative,positive,neutral,neutral,positive,positive
574320980,You could use --load option. ,could use load option,issue,negative,neutral,neutral,neutral,neutral,neutral
574320127,"For all of the ppo and sac, we are training while the game is running. We just speed up the game so that the training become faster. ",sac training game running speed game training become faster,issue,negative,negative,negative,negative,negative,negative
574310289,I would recommend that you follow the windows guide. https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation-Windows.md,would recommend follow guide,issue,negative,neutral,neutral,neutral,neutral,neutral
574282045,"Just been able to try it and no luck. Ive attached pictures as well of the console.
![pipinstallmlagents](https://user-images.githubusercontent.com/14904154/72366419-217c4800-36f2-11ea-8299-ddaca55fd5d0.PNG)
",able try luck attached well console,issue,positive,positive,positive,positive,positive,positive
574164083,"If I recall correctly, the tensorboard graph continued for mlagents version < 0.13.0. I prefered it that way, since it makes it for cleaner graphs.",recall correctly graph continued version way since cleaner,issue,negative,neutral,neutral,neutral,neutral,neutral
574107810,"In case it helps, I think also you have to effectively disable Extrinsic rewards by setting their strength to 0. Extrinsic rewards are enabled by default, as a result I have found that you have to include them in your reward config but set them to 0.",case think also effectively disable extrinsic setting strength extrinsic default result found include reward set,issue,positive,positive,positive,positive,positive,positive
574067019,"Nope, I don't think so. I believe your main issue is because windows does not recognize pip3. Let me know if the above solution works for you! ",nope think believe main issue recognize pip let know solution work,issue,negative,positive,positive,positive,positive,positive
574063937,"Hi reubenwong97, yea I'm using windows... I will try that and thanks for the help!

If this doesnt resolve the problem, can you think of anything else which may cause this? E.g file locations? 
All my files are being stored in C:\Users\NAME

",hi yea try thanks help doesnt resolve problem think anything else may cause file,issue,positive,positive,positive,positive,positive,positive
574021656,"Hello, are you on Windows OS? If you are using windows, then using pip will do, do not use pip3. Try pip install mlagents and see if it works!",hello o pip use pip try pip install see work,issue,negative,neutral,neutral,neutral,neutral,neutral
574021131,"Hello! I have decided to update to 0.13.0 and it works now! However, a note for the docs in 0.12.0, I think there is an inconsistency in the documentation for the Python API. It still talks about Learning Brains when they are no longer used in 0.12.0. 

Thanks!",hello decided update work however note think inconsistency documentation python still learning brain longer used thanks,issue,negative,positive,positive,positive,positive,positive
573982350,"I used to be able to connect to the trainer but no longer works. It's not like I changed anything, it just stopped working.

I get this...

INFO:mlagents_envs:Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
INFO:mlagents.trainers:UnityEnvironment worker 0: environment stopping.

If I dont run the trainer Unity complains it can't connect to 5004 as an info line on the Unity Console.
",used able connect trainer longer work like anything stopped working get listening port start training pressing play button unity editor worker environment stopping dont run trainer unity ca connect line unity console,issue,negative,positive,positive,positive,positive,positive
573979803,"Is the Agent set to the ""default"" option (instead of ""inference"" or ""heuristic only"") ?",agent set default option instead inference heuristic,issue,negative,neutral,neutral,neutral,neutral,neutral
573942140,"(slack comments for posterity)

> the things I don’t like about ResetReward was that it does 2 things depending on the flag - resets the incremental, and resets the total.
> we agreed that
> 1) ResetReward should reset the whole state, and only be called if the agent is actually done
> 2a) GetIncrementalReward() should return and reset the incremental reward OR
> 2b) RewardStep() should return the incremental reward, and remove GetIncrementalReward from the API",slack posterity like depending flag incremental total agreed reset whole state agent actually done return reset incremental reward return incremental reward remove,issue,positive,positive,neutral,neutral,positive,positive
573930349,"I'm using tensorflow gpu version.
I'm experimenting on my local linux machine. 
It happens in both '--no-graphics mode' is turned on or off.
",version local machine mode turned,issue,negative,neutral,neutral,neutral,neutral,neutral
573915613,"In this case, you are adding two positions, and each position would have 3 observations, so the correct size is 6. 

Not sure why when you change 6 to 20, it would train, but when it is 6 it doesn't train. Probably something else in your setup that is misconfigured. ",case two position would correct size sure change would train train probably something else setup,issue,negative,positive,positive,positive,positive,positive
573909622,"You can scroll to the top of these exceptions, and you will probably find the true error. ",scroll top probably find true error,issue,negative,positive,positive,positive,positive,positive
573908678,Yes it should work. You can refer to the pyramids environment here: https://github.com/Unity-Technologies/ml-agents/blob/master/config/sac_trainer_config.yaml,yes work refer environment,issue,negative,neutral,neutral,neutral,neutral,neutral
573908177,"Do you know if you are using the tensorflow cpu or the gpu version? Also are you using the xvfb or no graphics mode or xserver to render?
",know version also graphic mode render,issue,negative,neutral,neutral,neutral,neutral,neutral
573801054,"Thank you for your answer. Initially I didn't understand what that means, but with your explanation everything is clear now.",thank answer initially understand explanation everything clear,issue,positive,positive,neutral,neutral,positive,positive
573793841,"Hi @RK4444, in the most recent `master` branch, we changed the way steps are reported - they are now per-agent rather than per environment. Since there are 12 agents in the 3DBall scene, we'd expect there to be 12 times more steps than before. 

However, this change has _not_ been officially released yet. We recently changed our release process so that `master` is the active development branch. For the latest stable release (0.13), check it out using this command:

`git clone --branch latest_release https://github.com/Unity-Technologies/ml-agents.git`
",hi recent master branch way rather per environment since scene expect time however change officially yet recently release process master active development branch latest stable release check command git clone branch,issue,positive,positive,neutral,neutral,positive,positive
573790503,"Hi @MarkTension, side channels were released with ML-Agents v0.13. There's no official support yet for moving arbitrary data from C# to Python, but you can see some example implementations of Side Channels [here](https://github.com/Unity-Technologies/ml-agents/tree/master/ml-agents-envs/mlagents_envs/side_channel). ",hi side official support yet moving arbitrary data python see example side,issue,negative,negative,neutral,neutral,negative,negative
573753223,"yeah that's pretty messy isn't it? It also messes up the relative time. You'll get all the info, but i've also not found a way to deal with that yet",yeah pretty messy also relative time get also found way deal yet,issue,negative,positive,neutral,neutral,positive,positive
573749087,"Thank you. I tried to add --load on my second training. But in tensorboard, the graph restart from the left so it's really messy to read anything",thank tried add load second training graph restart left really messy read anything,issue,negative,negative,neutral,neutral,negative,negative
573747826,"If it's learned anything then you'd probably want to add --load to that run-id to continue where you ended, and increase max-steps. 

You say you think it hasn't learned yet after 50k steps? Maybe you want to check if it learned anything at all by checking the progress on tensorboard. If it learned to increase its cumulative reward to some degree then you might want to continue training. If nothing at all happened after 50k steps then you might have some error and you'd probably have to make some changes to your hyperparameter settings or environment if it's a custom one. ",learned anything probably want add load continue ended increase say think learned yet maybe want check learned anything progress learned increase cumulative reward degree might want continue training nothing might error probably make environment custom one,issue,positive,neutral,neutral,neutral,neutral,neutral
573660118,"Same in 2020.1.0a17, tried adding the following the the `csproj` file without any luck:
```
  <dependentAssembly>
    <assemblyIdentity name=""System.Interactive.Async"" publicKeyToken=""94bc3704cddfc263"" culture=""neutral"" />
    <bindingRedirect oldVersion=""0.0.0.0-3.0.3000.0"" newVersion=""3.0.1000.0"" />
  </dependentAssembly>
```
however @Sohojoe dll replacement file does the trick

**Edit** (17.March.20): using mlagents 0.14.1 + unity 2020.1.0b1 yields the same error - the dll above fixed that as well @surfnerd ",tried following file without luck neutral however replacement file trick edit unity error fixed well,issue,negative,positive,neutral,neutral,positive,positive
573593510,"OK thanks, so just the display, not the actual demonstration?",thanks display actual demonstration,issue,positive,positive,neutral,neutral,positive,positive
573499268,"Hi @mnsmuts,
This is a known bug in the display of demonstration files. Sorry for the trouble...",hi known bug display demonstration sorry trouble,issue,negative,negative,negative,negative,negative,negative
573431330,Is it possible to retrieve environment initialization parameters from the demonstrations file?,possible retrieve environment file,issue,negative,neutral,neutral,neutral,neutral,neutral
573387289,I'm having the same problem too. I tried reinstalling everything but nothing seems to work. I would appreciate it if you were to tell me what I'm doing wrong. I tried unity 18.4.15f1 and 2019.2.15f1. I was following the tutorial from https://www.youtube.com/watch?v=axF_nHHchFQ - 1:47:33. The ML-Agents isn't there. I think that something might have changed so I would appreciate if someone linked me to a solution.,problem tried everything nothing work would appreciate tell wrong tried unity following tutorial think something might would appreciate someone linked solution,issue,negative,negative,negative,negative,negative,negative
573327237,"![image](https://user-images.githubusercontent.com/481372/72206634-0cf14300-3499-11ea-9d6d-d46de879c338.png)
This fix worked for me, thank you! Just remember to Reimport the package after making the changes.",image fix worked thank remember reimport package making,issue,negative,neutral,neutral,neutral,neutral,neutral
573248515,"After offline discussions, we decided we need to hash out some of the details of how we want to move Reward Providers forward.  We can reopen this PR to build upon what has already been done here.",decided need hash want move reward forward reopen build upon already done,issue,positive,neutral,neutral,neutral,neutral,neutral
573217039,I'd like for both of you to look at the ResetReward/InternalResetReward.  These are APIs that have carried over from the Agent reward API.  It feels weird to have two references to a reward provider to keep backward-compatibility.  Thoughts?,like look carried agent reward weird two reward provider keep,issue,positive,negative,negative,negative,negative,negative
573194628,Making a new PR based on this PR since a lot of things has changed. ,making new based since lot,issue,negative,positive,positive,positive,positive,positive
573148699,"Fixed, just updated to 0.4.0-preview and it worked with no issues, thank you so much @mantasp .",fixed worked thank much,issue,negative,positive,positive,positive,positive,positive
573138928,"Hi @smartcharith,
We don't have any built-in visualization of the graph currently. There's a discussion here https://github.com/Unity-Technologies/ml-agents/issues/2518 of how to modify the model saving and visualize with Tensorboard (https://github.com/tensorflow/tensorboard/blob/master/docs/r1/graphs.md)",hi visualization graph currently discussion modify model saving visualize,issue,negative,neutral,neutral,neutral,neutral,neutral
573082338,"Hmm. Got another similar problem when doing some tensorflow tutorials. Noticed that my manually installed `cudnn64_7.dll` was not the only one. It existed one in `C:\Program Files\Unity\Hub\Editor\2019.2.7f2\Editor` aswell. And that one was a different version. 

I am not sure how that one got there? Is unity delivered with one? 

In any case, after making sure I loaded my `cudnn64_7.dll` the tensorflow tutorials started to work. Still got the same problem above when running unity editor I cannot run a training using gpu.

Not a critical problem at this point. Since my current use case do not get a big performance increase when using gpu. ",got another similar problem manually one one aswell one different version sure one got unity one case making sure loaded work still got problem running unity editor run training critical problem point since current use case get big performance increase,issue,negative,positive,positive,positive,positive,positive
573014272,"@fog9001 Barracuda 0.4.0-preview shipped yesterday. Could you please try to update your Barracuda version to this one (via Unity Package Manager) and see if it fixed your issue.
**Note:** that you might need to ""re-import"" .nn files in you project after this upgrade. You can do it by right-clicking on .nn file and picking ""Reimport"". ",fog barracuda shipped yesterday could please try update barracuda version one via unity package manager see fixed issue note might need project upgrade file reimport,issue,negative,positive,neutral,neutral,positive,positive
572958308,"Hi, thanks for the answer, yes I have tried loading the model and training again for a while but without success. I have noticed that there is something called checkpoints during the learning but I am not sure if it's possible to start the learning from some checkpoint before the drop. If it was possible I think it might it might help.",hi thanks answer yes tried loading model training without success something learning sure possible start learning drop possible think might might help,issue,positive,positive,positive,positive,positive,positive
572931816,"@niskander what do you mean with that exactly? i have penalty on step in AgentAction:  `AddReward(-1f / agentParameters.maxStep);` and `AddReward(-0.5f); `when he hits the wall. I just tried without this wall penalty but it was the same. Agent actually starts to do little circles or spins soon after he learns that the eggs are what he's looking for, but when obsticales comes and eggs are behind the wall then he would start to spin or do circles and never get it. I also tried with different curiosity strength from 0.02 up to 0.5, but no luck there.",mean exactly penalty step wall tried without wall penalty agent actually little soon looking come behind wall would start spin never get also tried different curiosity strength luck,issue,negative,negative,negative,negative,negative,negative
572792162,"@chriselion I've added a pretty-printed version of the RunOptions when `--debug` is used, and updated based on other comments.",added version used based,issue,negative,neutral,neutral,neutral,neutral,neutral
572744548,"@chriselion I actually haven't had this issue since I reduced the size of the experience buffer (I had went a little ham with it earlier; it was 90 million, reduced to 2 million). ",actually issue since reduced size experience buffer went little ham million reduced million,issue,negative,negative,neutral,neutral,negative,negative
572743448,Did you try saving and loading the model after the sharp drop to see how it's behaving? It's possible that it ran into a bug in the code or got stuck doing something. ,try saving loading model sharp drop see possible ran bug code got stuck something,issue,negative,negative,neutral,neutral,negative,negative
572742490,I would remove the moving/bumping penalties. Try with a constant step penalty instead (to encourage picking up the eggs faster). ,would remove try constant step penalty instead encourage faster,issue,negative,neutral,neutral,neutral,neutral,neutral
572601758,"it seems that i need to have the barracuda package and that there is no brains anymore ?
so i have to update myself on that first maybe thats the issue :)
sorry to bother",need barracuda package brain update first maybe thats issue sorry bother,issue,negative,negative,negative,negative,negative,negative
572561934,"thx for your answers :)
it says ""/Library/Frameworks/Python.framework/Versions/3.6/bin/mlagents-learn""
i dont even know what environment means in this context ?

i assume that i have only one and its somewhat global ? or does unity only look inside its own project-folder or something ?",dont even know environment context assume one somewhat global unity look inside something,issue,negative,neutral,neutral,neutral,neutral,neutral
572560439,"> Hi @IRiViI,
> ML-Agents on master doesn't have textActions anymore. You can find the documentation from the latest release [here](https://github.com/Unity-Technologies/ml-agents/tree/latest_release/docs)

A thank you! my bad.",hi master find documentation latest release thank bad,issue,negative,negative,neutral,neutral,negative,negative
572540107,"It's possible that you have mlagents installed outside of the current virtual environment. Try to run` where mlagents-learn`, this will show you if you have installed mlagents outside of the current enviroment or in my case i had 2 versions. Basically it's an installation problem. Other than that try solutions from here: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/FAQ.md#environment-connection-timeout",possible outside current virtual environment try run show outside current case basically installation problem try,issue,negative,neutral,neutral,neutral,neutral,neutral
572531266,"> ml-agents had major updates in 1 year, I doubt it will work until you create a new project or try to migrate it.

thx yea maybe but i downloaded the latest master and everything is fresh (python, mlagents, tf, unity ...). the only thing i kept was the scripts that i wrote for the agent,academy,area and they have no errors.",major year doubt work create new project try migrate yea maybe latest master everything fresh python unity thing kept wrote agent academy area,issue,negative,positive,positive,positive,positive,positive
572526406,"ml-agents had major updates in 1 year, I doubt it will work until you create a new project or try to migrate it.",major year doubt work create new project try migrate,issue,negative,positive,neutral,neutral,positive,positive
572335092,"Hi @guillefix, this bug should have been fixed by PR #3181 and it should work now in both `master` and the new 0.13.0 release. 

FYI the `develop` branch has been deprecated; `master` contains the latest changes. You can use tags to get the official versioned releases. ",hi bug fixed work master new release develop branch master latest use get official,issue,negative,positive,positive,positive,positive,positive
572325252,"Diffs aren't very useful, so here's how Soccer ""Academy"" GameObject looks now
![image](https://user-images.githubusercontent.com/6877802/72027702-d7e4b680-3234-11ea-8662-2702f468ca9b.png)
",useful soccer academy image,issue,negative,positive,positive,positive,positive,positive
572302462,Do you want to have any methods (or examples) for serializing out RunOptions? How are you supposed to produce the input file for `mlagents-run-experiment`? (looks like `RunOptions._asdict()` is want you want https://stackoverflow.com/a/26180604/224264),want supposed produce input file like want want,issue,positive,neutral,neutral,neutral,neutral,neutral
572239905,"Hi @surfnerd 
I am not using the ML-agents scripts to train models as I am working on a custom architecture. I suppose I will either use what I've written or try using the ml-agents/mlagents/trainers/demo_loader.py . Thank you though.",hi train working custom architecture suppose either use written try thank though,issue,negative,neutral,neutral,neutral,neutral,neutral
572220538,"Ah, sorry for my misunderstanding.  I hope you were able to get it to work.",ah sorry misunderstanding hope able get work,issue,negative,neutral,neutral,neutral,neutral,neutral
572135109,"I have changed some of the parameters and tried to train it again but unfortunately, it performed even worse with the same sudden extremely bad behavior after a certain amount of steps. Can anybody point me on how to improve the learning of more complicated behaviors? What is the best practice of changing the parameters? How do I recognize what to change and how?",tried train unfortunately even worse sudden extremely bad behavior certain amount anybody point improve learning complicated best practice recognize change,issue,negative,negative,negative,negative,negative,negative
572119295,"Hi @Solliet,
The demonstrations are made in Unity, and then used by the trainer in python.  This sounds very much like what you are trying to achieve.  Once you record your demo file in unity you can specify it in your training config for pre-training the GAIL network.  ",hi made unity used trainer python much like trying achieve record file unity specify training network,issue,negative,positive,positive,positive,positive,positive
571994688,"Hey @surfnerd,

Sorry for the delay.

I will setup a git repo with my code for review as soon as possible. 
I need to refacctor my code in advance.

I appreciate your help! Thanks for the support!",hey sorry delay setup git code review soon possible need code advance appreciate help thanks support,issue,positive,negative,neutral,neutral,negative,negative
571914191,"Hi @surfnerd, thank you for the reply. Unfortunately not much. You see, going through with Unity means that  I'd need a way to parse the demonstrations from python. If I understood this correctly, the demonstrations are used from unity which is a no go for me.

I resorted to using a window from pyglet to capture the keys but I am wondering if there is a more appropriate way to do this.",hi thank reply unfortunately much see going unity need way parse python understood correctly used unity go window capture wondering appropriate way,issue,negative,positive,positive,positive,positive,positive
571851546,"> Think it's good, let's just get another pair of eyes on it.

Cool, let's wait for Andrew to take a look before merging",think good let get another pair cool let wait take look,issue,positive,positive,positive,positive,positive,positive
571840034,You will need to convert your model to ONNX yourself to import with Barracuda.  ,need convert model import barracuda,issue,negative,neutral,neutral,neutral,neutral,neutral
571835048,"@surfnerd Thank you for your reply. Just some things I need to clarify. So Barracuda supports Tensorflow 2 and custom models? Can I use:
https://github.com/onnx/tensorflow-onnx to convert my TF2 model to onnx, or can Barracuda do that.
Thanks",thank reply need clarify barracuda custom use convert model barracuda thanks,issue,positive,positive,positive,positive,positive,positive
571830536,"Yes, the concept makes sense. Even I was thinking something similar to start with. What I am thinking is once the training is done , I have the trained brain. I will put the model into unity. In the agent script in the AgentAction method , where the discrete actions are present , I will replace those actions for a particular movement with a command to display a particular text instruction on the screen. That way for a particular user state the trained brain will try to take a discrete action, by calling it from a particular branch. But instead of trained brain able to take an action it will be displaying text.
@surfnerd  would you suggest any improvement or correction.",yes concept sense even thinking something similar start thinking training done trained brain put model unity agent script method discrete present replace particular movement command display particular text instruction screen way particular user state trained brain try take discrete action calling particular branch instead trained brain able take action text would suggest improvement correction,issue,positive,positive,positive,positive,positive,positive
571811753,"Unit tests on this branch ran in 2:15 vs 03:40 on latest commit to master.
https://circleci.com/gh/Unity-Technologies/ml-agents/8752?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link
https://circleci.com/gh/Unity-Technologies/ml-agents/8740?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link",unit branch ran latest commit master,issue,negative,positive,positive,positive,positive,positive
571810447,"Hi @rajatpaliwal,
I was thinking of something very simple, such as mapping it's action space to text.  If you are using a discrete action space, each action could be translated to text, manually, by you.  As of right now, we don't have anything that does this.  

The way I imagined it was that you would train an agent on your environment and then, if you like the results, perhaps you could use that as your ""guidance helper"" for your user based on the user controlled agent.  

For example I you had an agent that moved (up, down, left, right) that solved mazes, you could train it, then put the model into unity.  Then based on your user's agents observations, you could suggest an action to the user by requesting an action from the trained model. 

If you were to use a discrete action space you could then map these actions to text:
 
**Agent Action Space:**
```[ 0, 1, 2, 3]``` 

**Array of text for action:**
```[""up"", ""down"", ""left"", ""right""]```


I'm not sure how easy/hard this would be.  Does the concept make sense?

Like I said, we don't have any code to support this at the moment, but in theory, you could write the code to accomplish this.
 ",hi thinking something simple action space text discrete action space action could text manually right anything way would train agent environment like perhaps could use guidance helper user based user agent example agent left right could train put model unity based user could suggest action user action trained model use discrete action space could map text agent action space array text action left right sure would concept make sense like said code support moment theory could write code accomplish,issue,positive,positive,positive,positive,positive,positive
571740744,"@chriselion sorry, didn't get a chance to try yet but will post here once I do",sorry get chance try yet post,issue,negative,negative,negative,negative,negative,negative
571732605,"Hi @surfnerd , can you elaborate how we can display to the user what the brain thinks based on the user's state.",hi elaborate display user brain based user state,issue,negative,positive,positive,positive,positive,positive
571715234,"Hi @MarkTension, there's no reason why more than 36x36 won't work (it will work just fine) - but since your grid is 36x36, bigger resolutions will introduce redundant information that the CNN will have to learn to ignore, and it will take longer to train. 

Glad to see it's working out! ",hi reason wo work work fine since grid bigger introduce redundant information learn ignore take longer train glad see working,issue,negative,positive,positive,positive,positive,positive
571714402,"Yes, that worked perfectly!
I only searched through the recent issues in here, since i thought this might be a MLAgents-only issue.
Thank you!",yes worked perfectly recent since thought might issue thank,issue,positive,neutral,neutral,neutral,neutral,neutral
571702883,"Hi @Solliet,
Please see the [Imitation Learning Docs](https://github.com/Unity-Technologies/ml-agents/blob/latest_release/docs/Training-Imitation-Learning.md) on how to create demonstrations for learning.  Let me know if that helps you at all.",hi please see imitation learning create learning let know,issue,positive,negative,negative,negative,negative,negative
571702827,@niskander Any luck with applying those changes? Just want to make sure it helps before I close the bug in our tracker...,luck want make sure close bug tracker,issue,positive,positive,positive,positive,positive,positive
571702239,"Hi @Laporasus, 
A quick google search revealed [this solution](https://answers.unity.com/questions/756380/raycast-ignore-itself.html) to a similar problem.  Please let me know if that helps.",hi quick search revealed solution similar problem please let know,issue,negative,positive,positive,positive,positive,positive
571701520,"Hi @IRiViI,
ML-Agents on master doesn't have textActions anymore.  You can find the documentation from the latest release [here](https://github.com/Unity-Technologies/ml-agents/tree/latest_release/docs)",hi master find documentation latest release,issue,negative,positive,positive,positive,positive,positive
571700690,"Hi @tomret,
What are you using the TFPlugin for?  It is not supported by ML-Agents.  Do you have any more information about when this happens?  Have you added any code?  Please provide more information for us, or the community to help you.",hi information added code please provide information u community help,issue,positive,neutral,neutral,neutral,neutral,neutral
571699964,"Hi @Robin2091,
~Unfortunately, TensorFlow 2 isn't officially supported by ML-Agents at the moment.~

**Edit:**
Sorry Tensorflow 2.0 is supported, but custom models are not.  This won't work for ML-Agents, but you can try to make it work with [Barracuda](https://docs.unity3d.com/Packages/com.unity.barracuda@0.3/manual/index.html#getting-unity-barracuda).   You can export your model to ONNX and try to see if it will work with Barracuda in Unity.  ",hi robin officially edit sorry custom wo work try make work barracuda export model try see work barracuda unity,issue,negative,negative,negative,negative,negative,negative
571584774,"> hi @IRiViI,
> I don't have an estimate from the barracuda team on when ONNX importer changes will be ready. We're also blocked on tf2onnx supporting tensorflow 2.0 (see [onnx/tensorflow-onnx#691](https://github.com/onnx/tensorflow-onnx/issues/691)),
> 
> Note that the tight coupling that I mentioned between the tensor names will remain for the immediate future.

Thank you for the information. The tight coupling seems reasonable. I think I will just play within python for now and try to tackle this problem lateron when (maybe) some changes to ml agents are ready. ",hi estimate barracuda team importer ready also blocked supporting see note tight coupling tensor remain immediate future thank information tight coupling reasonable think play within python try tackle problem maybe ready,issue,positive,positive,neutral,neutral,positive,positive
571556606,"Thanks @surfnerd - I realise that I need to be careful about using the correct terminology with ML. By ""update"" I meant ""continue to train the model"" after initial training. I'll investigate the --load option @Rodnyy mentioned.",thanks need careful correct terminology update meant continue train model initial training investigate load option,issue,positive,positive,neutral,neutral,positive,positive
571514073,"Thank you for your input. I went about and tested it in the end. Indeed a little slower to train in the beginning, but it actually reaches a higher optimum! Green line is trained with visual observation of rendertexture, red has a vector observation. In time, it is a little slower to train because it has more parameters. Also, the higher optimum is is not because of having more parameters for visual observation, because when doubling number of hidden units for the vector observation run, it doesn't increase its performance.

Hope that helps anyone with the same question! Additionally, @ervteng , why did you say  (of exactly 36x36 and not bigger)? Did you test a decreased performance with bigger pixel spaces? Thanks!
 
![screenshot](https://user-images.githubusercontent.com/16321685/71885324-6f191380-313a-11ea-852d-bd35b91e4987.png)

* Actually, later on in the training it reaches a much higher optimum even. ~ 1/3 higher cumulative reward.",thank input went tested end indeed little train beginning actually higher optimum green line trained visual observation red vector observation time little train also higher optimum visual observation doubling number hidden vector observation run increase performance hope anyone question additionally say exactly bigger test performance bigger thanks actually later training much higher optimum even higher cumulative reward,issue,positive,positive,positive,positive,positive,positive
571352169,"Hey @chechulinYuri,
Please refer to https://github.com/Unity-Technologies/ml-agents/issues/3167#issuecomment-571351815

for a potential workaround for this issue.",hey please refer potential issue,issue,negative,neutral,neutral,neutral,neutral,neutral
571351815,"![Screen Shot 2020-01-06 at 2 52 45 PM](https://user-images.githubusercontent.com/1356616/71854820-34f53680-3094-11ea-8cbc-8c907e7ed9ad.png)

This is where I found that file.  You can open the `Google.Protobuf.dll.meta` file with a text editor.

Modify the line
```
        Exclude Android: 1
```
to 
```
        Exclude Android: 0
``` 
and then modify 
```
  - first:
      Android: Android
    second:
      enabled: 0
      settings:
        CPU: ARMv7
```
to
```  - first:
      Android: Android
    second:
      enabled: 1
      settings:
        CPU: ARMv7
```",screen shot found file open file text editor modify line exclude android exclude android modify first android android second first android android second,issue,negative,positive,neutral,neutral,positive,positive
571350928,"So, I just did a sort of hacky thing and got it to work.  You can edit the Google.Protobuf.dll.meta file.  Flip all of the Android platform enabled flags to 1.  Then you should be able to build for Android.",sort hacky thing got work edit file flip android platform able build android,issue,negative,positive,positive,positive,positive,positive
571349461,"You may be able to copy it, and only include it for android.  ",may able copy include android,issue,negative,positive,positive,positive,positive,positive
571347082,"Hi @chechulinYuri,
Please refer to #3167,  It seems to be a barracuda related issue.  My apologies for the miscategorization. ",hi please refer barracuda related issue,issue,negative,neutral,neutral,neutral,neutral,neutral
571346608,"I believe this is related to #3125, which I have mistakenly closed.  I will reopen it. ",believe related mistakenly closed reopen,issue,negative,negative,neutral,neutral,negative,negative
571346327,"Hi @whalefood,
It looks like this DLL is included with Barracuda and is not available for those platforms according to their configuration in their package.  I have assigned @mantasp for him to take a look.",hi like included barracuda available according configuration package assigned take look,issue,negative,positive,positive,positive,positive,positive
571339040,"Unfortunately, we don't yet have an explicitly multiagent algorithm for cooperative settings.  If you include something that captures the other agents in an agent's observations though (i.e. raycasts that detect the tag of an agent), you may get emergent cooperative behavior. This is basically what is done in the hide and seek paper referenced in @kevtan's comment.  ",unfortunately yet explicitly algorithm include something agent though detect tag agent may get emergent behavior basically done hide seek paper comment,issue,negative,negative,negative,negative,negative,negative
571253337,"I can reproduce the deprecation warning using h5py==2.10.0 in our unit tests with
```
pytest ml-agents/mlagents/trainers/tests/test_sac.py::test_sac_save_load_buffer
```
but I don't get the `io.UnsupportedOperation` that you're seeing. This might be due to a difference in platforms (I'm on Mac, looks like you're on Windows).

The fix for the deprecation warning is in https://github.com/Unity-Technologies/ml-agents/pull/3165 and I'll make sure that goes into the next release; in the meantime, can you try applying the changes locally and see if they help? (You can ignore the `test_constraints_*_version.txt` changes, those just affect our CI).",reproduce deprecation warning unit get seeing might due difference mac like fix deprecation warning make sure go next release try locally see help ignore affect,issue,negative,positive,neutral,neutral,positive,positive
571251455,"In particular, here's the code that setups up the different possible networks: https://github.com/Unity-Technologies/ml-agents/blob/298df2e122e7d9dcc5faed143a0c7934769a46d7/ml-agents/mlagents/trainers/models.py#L252-L418
(controlled by the `vis_encode_type` option in the trainer config)",particular code different possible option trainer,issue,negative,positive,neutral,neutral,positive,positive
571248242,@surfnerd Could you merge this pull request (I see you approved it) so I can close it?,could merge pull request see close,issue,negative,neutral,neutral,neutral,neutral,neutral
571248239,"Hi @MarkTension, the network in ML-Agents isn't pretrained on any images. I think in your case a visual observation (of exactly 36x36 and not bigger) would perform better than a flattened vector obs, as the convolutional layers are better at learning spatial relationships between the pixels, as you've suggested. ",hi network think case visual observation exactly bigger would perform better vector convolutional better learning spatial,issue,positive,positive,positive,positive,positive,positive
571246834,"Hi @niskander,
Thank you for your feedback.  I've logged this in our internal tracker as MLA-491.  @chriselion as kindly already created #3165 to check against different versions in CI.  We will provide any updates here as they come.",hi thank feedback logged internal tracker kindly already check different provide come,issue,positive,positive,positive,positive,positive,positive
571239984,"Hi @guillefix,
Thanks for the report.  This, indeed, sounds like a bug.  I've logged this in our internal tracker as  MLA-490.  Any updates to the issue will be posted here.  ",hi thanks report indeed like bug logged internal tracker issue posted,issue,positive,positive,neutral,neutral,positive,positive
571235902,"Hi @niskander - what version of h5py are you using? Looks like we've only tested up to 2.9.0, but 2.10.0 came out in December.",hi version like tested came,issue,negative,neutral,neutral,neutral,neutral,neutral
571235041,"Hi @MarkTension,
I feel like the general thoughts are that if you can use vector observations you may have an easier time training.  Though there are scenarios where you may want to switch to visual observations.  Perhaps @ervteng could provide some more input here.",hi feel like general use vector may easier time training though may want switch visual perhaps could provide input,issue,positive,positive,neutral,neutral,positive,positive
571234101,"Hi @fredleefarr,
The model that you highlighted in the editor was added there after training.  If you'd like to update your NN model in the editor, look for the end of the training log to see where it is.  You can then drag that file into the editor and use it for inference if you'd like. ",hi model editor added training like update model editor look end training log see drag file editor use inference like,issue,negative,negative,neutral,neutral,negative,negative
571233656,Maybe @andrewcoh could provide some input here as well.,maybe could provide input well,issue,negative,neutral,neutral,neutral,neutral,neutral
571194459,"Many thanks @Rodnyy, will look at the --load option.",many thanks look load option,issue,negative,positive,positive,positive,positive,positive
571125113,"Thanks Kevtan for answer, I tried through Python API and found my mistake. Thanks.",thanks answer tried python found mistake thanks,issue,positive,positive,positive,positive,positive,positive
571066728,"Hi, as far as I know, it is possible to continue training the model but not the way you tried. You simply have to run the training command again and add ""--load"" at the end also do not forget to increase the max steps in the configuration if you already hit it. during previous learning. If you still have any questions regarding this topic I am sure I have seen more detailed answers about this topic here.",hi far know possible continue training model way tried simply run training command add load end also forget increase configuration already hit previous learning still regarding topic sure seen detailed topic,issue,negative,positive,positive,positive,positive,positive
570939121,"hi Chris, thank you so much for such a prompt response!",hi thank much prompt response,issue,negative,positive,positive,positive,positive,positive
570821773,"This fixed my problem! Yes, that would hopefully save people time in the future. I've created a pull request!",fixed problem yes would hopefully save people time future pull request,issue,positive,positive,neutral,neutral,positive,positive
570821096,"Hi @kevtan,
The installation instructions linked to from the instructions in the link in your comment tell users to install the Barracuda package from Package Manager.  Would you suggest we add a reminder on the Setting up a learning environment page?",hi installation linked link comment tell install barracuda package package manager would suggest add reminder setting learning environment page,issue,negative,neutral,neutral,neutral,neutral,neutral
570817845,"Hi @MinneStephanie2, I'm doing my best to answer with my limited reinforcement learning knowledge, but I'm not sure you can explicitly _force_ your 4 agents to work as a team. The best thing you can do is to implement a reward schedule that incentivizes the agents to work together. That is, dish out rewards in such a way that the optimal strategy would involve some degree of cooperation with the other agents. If designed correctly, the agents would (hopefully) learn a policy that resembles cooperation.

For reference, consider [OpenAI's Hide and Seek paper](https://openai.com/blog/emergent-tool-use/) where the agents were playing a team-based version of hide and seek. There was no explicit programming to tell the agents to interact with (or cooperate with for that matter) each other, but the rewards given were team-based.

> Agents are given a team-based reward; hiders are given a reward of +1 if all hiders are hidden and -1 if any hider is seen by a seeker. Seekers are given the opposite reward, -1 if all hiders are hidden and +1 otherwise.

In addition to having team-based rewards/penalties, another feature that could encourage inter-agent cooperation is if the agents had access to the state of the other agents on their team. Again, you can find a great example of this in the Hide and Seek paper referenced above. Hope this helps!",hi best answer limited reinforcement learning knowledge sure explicitly work team best thing implement reward schedule work together dish way optimal strategy would involve degree designed correctly would hopefully learn policy reference consider hide seek paper version hide seek explicit tell interact matter given given reward given reward hidden hider seen seeker given opposite reward hidden otherwise addition another feature could encourage access state team find great example hide seek paper hope,issue,positive,positive,positive,positive,positive,positive
570752150,"Completed the original task and local benchmarks were showing slightly better performance. Instead of taking up around 76% of the total demonstration loading time, the `BrainInfo.from_agent_proto` calls now only take up about 72% of the time. Also, trial runs that used to take around `0.016` seconds now take around `0.010` seconds.",original task local showing slightly better performance instead taking around total demonstration loading time take time also trial used take around take around,issue,positive,positive,positive,positive,positive,positive
570749544,@chriselion Thank you for taking the time to write all the detailed comments. They were super helpful!,thank taking time write detailed super helpful,issue,positive,positive,positive,positive,positive,positive
570742692,Looks good overall. I'll let you and @andrewcoh decide whether to keep the separation or merge them.,good overall let decide whether keep separation merge,issue,negative,positive,positive,positive,positive,positive
570722597,"This file was deleted in a recent PR, so I'm going to close this.
https://github.com/Unity-Technologies/ml-agents/pull/2969/files#diff-3bf514220b467cbeeb9cea2a25eac9cc",file recent going close,issue,negative,neutral,neutral,neutral,neutral,neutral
570721302,Are you going to add the tests for the ValueError conditions that I suggested? it's optional but would give a bit more coverage.,going add optional would give bit coverage,issue,negative,neutral,neutral,neutral,neutral,neutral
570706869,"Thank you! Will check it out maybe later. 
I was also exploring doing what I want with just the visual input, and making a shsader that drew what I wanted to encode in the array.
But doing it with arrays directly may be better in my case",thank check maybe later also exploring want visual input making drew encode array directly may better case,issue,positive,positive,positive,positive,positive,positive
570704484,"Hi @surfnerd ,
I think from your response , what you mean is to create a let's say Python API to provide user's input to the neural network and output the result in the form of text instructions on the screen. If you can elaborate more on your idea in terms of implementing it in the Unity project, it would be great.",hi think response mean create let say python provide user input neural network output result form text screen elaborate idea unity project would great,issue,positive,positive,positive,positive,positive,positive
570692493,"Hi @rajatpaliwal,
One way an agent could provide guidance is for you to train an agent to actually perform the behaviors you want and then during inference you could send the user’s input to the neural network and give them feedback however you choose.  You’d just need to hook the brain into whatever UI components you want to use to display to the user what the brain thinks they should be doing based on the user’s state.  
So instead of modifying transforms or something, you could translate that into text like “move left” or “open door”, if that makes sense.  Basically translating your action space to text on your own.",hi one way agent could provide guidance train agent actually perform want inference could send user input neural network give feedback however choose need hook brain whatever want use display user brain based user state instead something could translate text like move left open door sense basically action space text,issue,negative,positive,neutral,neutral,positive,positive
570691579,"Hey @crzdg,
I think you need to reward your agent for behavior you want to encourage it to perform.  Giving it a random reward will give it random behavior.  
Calling done in AgentAction will not help either since your local episode will be completed every step.  

Without your code I cannot effectively help you debug what’s happening with your environment.
Is your repository public? Could you share your code somehow?
",hey think need reward agent behavior want encourage perform giving random reward give random behavior calling done help either since local episode every step without code effectively help happening environment repository public could share code somehow,issue,positive,negative,neutral,neutral,negative,negative
570500181,"I did some further research


**Trying to _AddReward_ and _Done_ in the _AgentAction_ Method.**

```
    public override void AgentAction(float[] vectorAction)
    {
           AddReward(Random.Range(-1f, 1f));
           Done()
    }
```

Does lead to the same outcome. Only one episode is completed.

**Trying to set _ResetOnDone_ to _False_ and manually call _AgentReset_**

```
    public void FixedUpdate()
    {
        transform.LookAt(receiverTarget.transform.localPosition);
        if (canThrow)
        {
            canThrow = false;
            RequestDecision();
            return;
        }

        if (throwIsOver)
        {
            Done();
            AgentReset();
            return;
        }
        
    }

```

In this case one episode is done as well. 
Weirdly only the _CollectObservations_ Method gets called the second time when reaching _RequestDecision_. There is no second _AgentAction_ executed.

Can you confirm the correctness of those behaviours?
",research trying method public override void float done lead outcome one episode trying set manually call public void false return done return case one episode done well weirdly method second time reaching second executed confirm correctness,issue,negative,negative,negative,negative,negative,negative
570458794,"I just find the problem has been solved in #2956 
Because the logic 
```
// If a communicator is enabled/provided, then we assume we are in
// training mode. In the absence of a communicator, we assume we are
// in inference mode.
```
has been remove. 
But the problem still exists in the latest version. Looking forward to v0.13 😀",find problem logic communicator assume training mode absence communicator assume inference mode remove problem still latest version looking forward,issue,negative,positive,positive,positive,positive,positive
570423343,"@guillefix This turned out to be easier than I initially expected. PR is here https://github.com/Unity-Technologies/ml-agents/pull/3148, but the folks I need to review it are on holidays/PTO until Monday.",turned easier initially need review,issue,negative,neutral,neutral,neutral,neutral,neutral
570330753,All PRs should target `master`. `develop` has been retired.,target master develop retired,issue,negative,neutral,neutral,neutral,neutral,neutral
570310572,"Sorry for the delay, this slipped through the cracks over the holidays.

So you want the inference to be performed by python? This should be possible in the latest version - if you're using `mlagents-learn` just omit the `--train` parameter from the commandline.",sorry delay slipped want inference python possible latest version omit train parameter,issue,negative,neutral,neutral,neutral,neutral,neutral
570301389,I also logged a longer-term issue for us to add better testing around this; we have a few unit tests but no integration tests that run training with the gym wrapper.,also logged issue u add better testing around unit integration run training gym wrapper,issue,negative,positive,positive,positive,positive,positive
570301095,"@weiziqian - this is fixed on `master` branch now, and will be in the next release. Thanks for the report!",fixed master branch next release thanks report,issue,negative,positive,positive,positive,positive,positive
570298139,"> I'm OK with the summary frequency ""messiness"" since this can already happen with multiple environments to day. I'll defer on the ""agent steps"" change to others who know it better.

@chriselion It actually won't with multiple environments, since there's a specific workaround in trainer_controller that walks through a batch of steps one-by-one and only records the summary at the exact step that needs to be recorded. 

We could do something similar by moving the `_write_summary` call into `process_trajectory`, and only calling it when the total steps exceeds the required steps. ",summary frequency messiness since already happen multiple day defer agent change know better actually wo multiple since specific batch summary exact step need could something similar moving call calling total,issue,negative,positive,positive,positive,positive,positive
570291411,"> From what I can tell, we're returning different types based on whether we're in single or multi agent mode, which is ugly at best. I need confirmation this is actually the intended behavior.

Yep, it's unfortunately intended behavior. The issue is that there is no multi-agent interface for Gym (at least, no official one). What we do is just return a List for each of the obs, reward, dones as returned by `step`. 

Returning a dict as info is fine. Ideally we'd convert the BatchedStepResult tuple into a dict of the values, but the Gym interface spec (http://gym.openai.com/docs/) doesn't specify a type for the values of the info dict, so we're technically in spec. 
",tell different based whether single agent mode ugly best need confirmation actually intended behavior yep unfortunately intended behavior issue interface gym least official one return list reward returned step fine ideally convert gym interface spec specify type technically spec,issue,positive,positive,neutral,neutral,positive,positive
570286444,"It's a bit tricky to export `.nn` more than once b/c of how Tensorflow handles frozen graphs, but possible. 

There is a workaround however. Without making changes to ML-Agents, it IS possible to save `.ckpt` files more than once (in fact, if you look in your models folder, there should be a bunch of them named by step, and you can increase the number with `the --keep-checkpoints` commandline option. 

Now, in the folder that these are all saved, edit the text file called `checkpoint` and replace `model_checkpoint_path` with the one you want. Then run `mlagents-learn` with the `--load` parameter, it will then load THIS checkpoint instead of the latest, and quitting using Ctrl+C immediately after it starts training will export it to `.nn`. ",bit tricky export frozen possible however without making possible save fact look folder bunch step increase number option folder saved edit text file replace one want run load parameter load instead latest immediately training export,issue,negative,positive,positive,positive,positive,positive
570275681,"Hi @surfnerd!

Thanks for the quick reply. Sorry not clarifying _ReceivingAction_ Method is used to tell the **Agent** the disc has been receiveid. Receiveid means either **Catched**, **Grounded** or **OutOfBounce**.

For example the disc has touched ground and therefore is **Grounded**, the **Disc** calls _DiscGrounded_ on the Agent. The _DiscGrounded_ does _AddReward_ an sets the _throwIsOver_ flag.

I added Code snippets to the first comment and changed ReceivingAction to DiscEndstate.

",hi thanks quick reply sorry method used tell agent disc either grounded example disc touched ground therefore grounded disc agent flag added code first comment,issue,negative,positive,neutral,neutral,positive,positive
570263367,"Hi @crzdg,
Thanks for sharing your issue.  Is there any reason you are calling AgentReceivingAction instead of overriding AgentAction?  I’m not sure what the purpose of this function is.  Usually all of the observations happen in CollectObservations, and the actions happen in AgentAction.  It sounds like you are doing some things outside of this.  In the bouncer example, requestDecision is called inside of the FixedUpdate method, but not much else is done there.  

Can you ensure that you are doing your actions and observations in the appropriate methods?  
Cheers,
Chris",hi thanks issue reason calling instead sure purpose function usually happen happen like outside bouncer example inside method much else done ensure appropriate,issue,positive,positive,positive,positive,positive,positive
570156179,"Or well, to basically have some way to pick out the best .nn models would be good.",well basically way pick best would good,issue,positive,positive,positive,positive,positive,positive
570152030,"This morning the run finally completed 😃. Unity still used a ton of memory though. The interesting part is, that even after the run completed and Unity was idling in the Editor it would still consume about 26gb's of memory.

What I then tried was pressing the play button to see how the newly trained model would perform, when I did this Unity went in a ""Not responding"" state and still used about 22 gb of memory:

![image](https://user-images.githubusercontent.com/2350015/71659640-9a6dbe00-2d48-11ea-8d21-722ede1f61ea.png)

After restarting Unity everything worked again and the newly trained model performs great 😄 ",morning run finally unity still used ton memory though interesting part even run unity editor would still consume memory tried pressing play button see newly trained model would perform unity went state still used memory image unity everything worked newly trained model great,issue,positive,positive,positive,positive,positive,positive
570011114,"Okay, I was able to fix the issue by changing `pip3` to `pip`.  Now, `pip list` yields the following:

```
➜ pip list         
Package              Version             Location                                                 
-------------------- ------------------- ---------------------------------------------------------
...
mlagents             0.12.1              /Users/tankevin/Documents/GitHub/ml-agents/ml-agents     
mlagents-envs        0.12.1              /Users/tankevin/Documents/GitHub/ml-agents/ml-agents-envs
...
```

And `conda list` yields the following:
```
➜ conda list        
# packages in environment at /Users/tankevin/anaconda3/envs/unity:
#
# Name                    Version                   Build  Channel
...
mlagents                  0.12.1                    dev_0    <develop>
mlagents-envs             0.12.1                    dev_0    <develop>
...
```

After doing some more digging, I found that the reason I was having errors was because my `pip3` was not installing into my anaconda environment. A quick `which` check gave the following:

```
➜ which pip 
/Users/tankevin/anaconda3/envs/unity/bin/pip
➜ which pip3
/usr/local/bin/pip3
```

I hope this helps anyone who's interested in installing these packages in the future!",able fix issue pip pip pip list following pip list package version location list following list environment name version build channel develop develop digging found reason pip anaconda environment quick check gave following pip pip hope anyone interested future,issue,positive,positive,positive,positive,positive,positive
570010229,"Hi @harperj thanks for the link to the installation guide! I hadn't seen it before but it seems to be exactly what I need. I've followed the instructions carefully but, regrettably, am still running into issues. I created a brand new Anaconda environment (Python 3.7.4), cloned the `latest_release` tagged version of the repository (0.12.1), and performed the `pip3 install -e ./` steps for both the `mlagents` and `mlagents_envs` subdirectories but still am getting errors. For instance,

```
➜ python
Python 3.7.4 (default, Nov 22 2019, 15:36:53) 
[Clang 7.3.0 (clang-703.0.31)] :: Intel(R) Corporation on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Intel(R) Distribution for Python is brought to you by Intel Corporation.
Please check out: https://software.intel.com/en-us/python-distibution
>>> import mlagents
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'mlagents'
>>> ^D
```

Here are the results of running `pip list` and `conda list`, respectively. I don't see the mlagents packages being listed here. Are they supposed to be?

```
➜ pip list          
Package     Version            
----------- -------------------
certifi     2019.9.11          
mkl-fft     1.0.15             
mkl-random  1.1.0              
mkl-service 2.3.0              
numpy       1.17.4             
pip         19.1.1             
setuptools  41.0.1.post20191122
six         1.12.0             
TBB         0.1                
wheel       0.31.0 
```

```
➜ conda list          
# packages in environment at /Users/tankevin/anaconda3/envs/unity:
#
# Name                    Version                   Build  Channel
bzip2                     1.0.8                         0    intel
certifi                   2019.9.11                py37_0    intel
icc_rt                    2020.0                intel_166    intel
intel-openmp              2020.0                intel_166    intel
intelpython               2020.0                        1    intel
libffi                    3.2.1                        11    intel
mkl                       2020.0                intel_166    intel
mkl-service               2.3.0                    py37_0    intel
mkl_fft                   1.0.15           py37ha68da19_3    intel
mkl_random                1.1.0            py37ha68da19_0    intel
numpy                     1.17.4           py37ha68da19_4    intel
numpy-base                1.17.4                   py37_4    intel
openssl                   1.1.1d                        0    intel
pip                       19.1.1                   py37_0    intel
python                    3.7.4                         3    intel
setuptools                41.0.1                   py37_0    intel
six                       1.12.0                   py37_0    intel
sqlite                    3.29.0                        0    intel
tbb                       2020.0                intel_166    intel
tbb4py                    2020.0             py37_intel_0    intel
tcl                       8.6.4                        24    intel
tk                        8.6.4                        29    intel
wheel                     0.31.0                   py37_3    intel
xz                        5.2.4                h1de35cc_7    intel
zlib                      1.2.11               h1de35cc_7    intel
```

Also, I know that `demo_loader.py` has no main function; I was just trying to run it to try to figure out all the import issues. Thanks for the suggestion about importing and then timing it though! That's precisely what I plan to do once I figure this out.",hi thanks link installation guide seen exactly need carefully regrettably still running brand new anaconda environment python tagged version repository pip install still getting instance python python default clang corporation type help copyright license information distribution python brought corporation please check import recent call last file line module module running pip list list respectively see listed supposed pip list package version pip post six wheel list environment name version build channel pip python six wheel also know main function trying run try figure import thanks suggestion timing though precisely plan figure,issue,positive,positive,positive,positive,positive,positive
569994227,"Hi @zergy,
You can change the training configuration in the Academy of the environment you are using by clicking on it in the scene view.  The timeScale property could be set to 1 in order for you to run the simulation in real time instead of sped up time.",hi change training configuration academy environment scene view property could set order run simulation real time instead sped time,issue,negative,positive,positive,positive,positive,positive
569915129,"Unfortunately, that didn't work either. I couldn't pull the model into the intended place. The field always remained empty. 
I reset everything back to vanilla and reinstalled it. I must have made a mistake in the installation. Now everything seems to work as it should.
Thank you and Happy New Year.",unfortunately work either could pull model intended place field always empty reset everything back vanilla must made mistake installation everything work thank happy new year,issue,negative,positive,neutral,neutral,positive,positive
569837941,"@guillefix The way I'd like to do this is with the ISensor interface (https://github.com/Unity-Technologies/ml-agents/blob/master/UnitySDK/Assets/ML-Agents/Scripts/Sensor/ISensor.cs) with GetCompressionType() returning `SensorCompressionType.None`, and propagating this to the Observation data (https://github.com/Unity-Technologies/ml-agents/blob/master/UnitySDK/Assets/ML-Agents/Scripts/Sensor/Observation.cs). But there are some implicit assumptions in both the python and C# code that ""uncompressed => vector observation (1D)"" and ""compressed => visual observation (3D)"" so there might be a few gotchas in various places.

The proto files are located in https://github.com/Unity-Technologies/ml-agents/tree/master/protobuf-definitions/proto/mlagents_envs/communicator_objects but with the change I had in mind, it shouldn't require any changes to the protos.",way like interface observation data implicit python code uncompressed vector observation compressed visual observation might various proto change mind require,issue,negative,neutral,neutral,neutral,neutral,neutral
569832734,"Sure here's the [config file](https://github.com/afewvowels/unity_00_15_ml-agents_infinite_road/blob/master/infinite%20road/Assets/Config/CarAgent.json) that I'm currently training on:

```json
{
    ""measure"" : ""progress"",
    ""thresholds"" : [0.05, 0.1, 0.15, 0.3, 0.5],
    ""min_lesson_length"" : 100,
    ""signal_smoothing"" : true,
    ""parameters"" :
    {
        ""horizontal"" : [0.0, 1.0, 1.0, 1.0, 1.0, 1.0],
        ""vertical"" : [0.0, 0.0, 0.0, 0.0, 1.0, 1.0],
        ""hDist"" : [0.0, 5.0, 5.0, 8.0, 10.0, 12.0],
        ""vDist"" : [0.0, 1.0, 1.0, 1.0, 1.0, 1.5],
        ""nMin"" : [5.0, 15.0, 20.0, 25.0, 30.0, 35.0],
        ""nMax"" : [20.0, 20.0, 25.0, 35.0, 40.0, 50.0],
        ""rMin"" : [4.0, 4.0, 5.0, 5.0, 5.0, 5.0],
        ""rMax"" : [5.0, 5.0, 7.0, 7.0, 7.0, 7.0],
        ""minLength"" : [3.0, 3.0, 4.0, 8.0, 12.0, 15.0],
        ""maxLength"" : [4.0, 4.0, 7.0, 12.0, 18.0, 25.0]
    }
}
```

If I set my academy reset parameters to match the first stage of the curriculum values and then do a build, then the curriculum training proceeds according to the documentation. At set intervals in the training progress the academy reset parameters are changed to the values listed in the curriculum, and the scene rebuilds using these values and the scene grows in complexity.

![Good Params](https://raw.githubusercontent.com/afewvowels/unity_00_15_ml-agents_infinite_road/master/images/params_good.png)

If I use parameters (like below) that differ from the initial values in the curriculum and do a build, then the curriculum values are overridden by the build's academy reset parameters and the curriculum never takes place, I just get to the `max_steps` as set in the `trainer_config.yaml` on only one set of academy reset parameters (the parameters set on the academy in the scene when it was built).

![Bad params](https://raw.githubusercontent.com/afewvowels/unity_00_15_ml-agents_infinite_road/master/images/params_bad.png)

So here is the terminal output (sorry, the terminal output is using a build that uses different float values than the one that I listed above so there's a bit of a mismatch in exact values!). It looks like ml-agents loads the scene along with it's parameters (red box) then overrides with the curriculum values (blue box) but the end result (last image) is definitely not stage 1 of the training but instead a very twisty and difficult road to drive:

![Override?](https://raw.githubusercontent.com/afewvowels/unity_00_15_ml-agents_infinite_road/master/images/mismatch.png)

![Definitely not easy mode](https://raw.githubusercontent.com/afewvowels/unity_00_15_ml-agents_infinite_road/master/images/difficult.png)

Let me know if anything else you'd like to know, thanks for your help!",sure file currently training measure progress true horizontal vertical set academy reset match first stage curriculum build curriculum training proceeds according documentation set training progress academy reset listed curriculum scene scene complexity good use like differ initial curriculum build curriculum build academy reset curriculum never place get set one set academy reset set academy scene built bad terminal output sorry terminal output build different float one listed bit mismatch exact like scene along red box curriculum blue box end result last image definitely stage training instead twisty difficult road drive override definitely easy mode let know anything else like know thanks help,issue,positive,positive,neutral,neutral,positive,positive
569821959,"I believe https://github.com/Unity-Technologies/ml-agents/pull/3136 will fix this, but I want to run it by some other folks that are more familiar with the gym interface.",believe fix want run familiar gym interface,issue,negative,positive,positive,positive,positive,positive
569818210,"Thanks
I have been looking at how I could do it. I figured how to add a new model, but I can't figure out how the python-Unity communication works. I got stuck trying to understand the grpc code.. The `.proto` files are not present in the repo as far as I can see?",thanks looking could figured add new model ca figure communication work got stuck trying understand code present far see,issue,negative,positive,positive,positive,positive,positive
569799138,"Hi @kevtan -- did you follow the full [installation guide](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and specifically the part about installation for development?

Another issue I see with what you're doing here is that `demo_loader.py` has no main function; so nothing will be executed when you run this file.  If you're looking to benchmark it you'd need to have a script that imports `mlagents.trainers.demo_loader.XYZ` and then uses/times it.",hi follow full installation guide specifically part installation development another issue see main function nothing executed run file looking need script,issue,negative,positive,positive,positive,positive,positive
569798051,"Hi @afewvowels, this sounds like a bug if I'm understanding what you wrote correctly.  Could you share an example of the curriculum config you're using and the behavior you see? ",hi like bug understanding wrote correctly could share example curriculum behavior see,issue,positive,neutral,neutral,neutral,neutral,neutral
569796449,"Thanks @weiziqian - this does look like a bug. I've logged it as MLA-486 in our internal tracker, and we should have a fix for it soon.",thanks look like bug logged internal tracker fix soon,issue,positive,positive,neutral,neutral,positive,positive
569773490,"Hi @MHDante,
The Unity coding standard says
```
Spell words using correct US-English spelling. Note that there are a few legacy exceptions that use GB-English that we must preserve, but do not add new ones.
```
(sorry, I don't know of a public version of this for reference). MonoBehaviour is one of these legacy exceptions, but ML-Agents shouldn't use that spelling.",hi unity standard spell correct spelling note legacy use must preserve add new sorry know public version reference one legacy use spelling,issue,negative,negative,neutral,neutral,negative,negative
569770846,"hi @IRiViI,
I don't have an estimate from the barracuda team on when ONNX importer changes will be ready. We're also blocked on tf2onnx supporting tensorflow 2.0 (see https://github.com/onnx/tensorflow-onnx/issues/691),

Note that the tight coupling that I mentioned between the tensor names will remain for the immediate future.",hi estimate barracuda team importer ready also blocked supporting see note tight coupling tensor remain immediate future,issue,positive,positive,neutral,neutral,positive,positive
569769318,"@guillefix Sorry, this isn't supported at the moment, but we do plan to add it in the next month or so. The ID for this in our internal tracker is MLA-345.",sorry moment plan add next month id internal tracker,issue,negative,negative,negative,negative,negative,negative
569768650,@zergy Please make a new issue for your question. This issue is almost a year and a half old and doesn't appear very related.,please make new issue question issue almost year half old appear related,issue,negative,positive,neutral,neutral,positive,positive
569767901,"Hi @nic126,
You should see something like this in the logs after training is done:
```
Converting ./models/testNoWallDir-0/noWallChaserBrain/frozen_graph_def.pb to ./models/testNoWallDir-0/noWallChaserBrain.nn
```
(I might not have the names exactly right). You'll need to copy that `noWallChaserBrain.nn` into your TFModels directory, then you should be able to assign it in the Behavior Parameters > Model (click on the little circle next to the field).",hi see something like training done converting might exactly right need copy directory able assign behavior model click little circle next field,issue,negative,positive,positive,positive,positive,positive
569766391,"Please make sure you're looking at the right version of the documentation for the code that you're using. The version on the master branch may have changed since the last release. 

You can get the version from the most recent release here: https://github.com/Unity-Technologies/ml-agents/blob/latest_release/docs/Learning-Environment-Create-New.md
or for example for the 0.12.1 release here https://github.com/Unity-Technologies/ml-agents/blob/0.12.1/docs/Learning-Environment-Create-New.md",please make sure looking right version documentation code version master branch may since last release get version recent release example release,issue,positive,positive,positive,positive,positive,positive
569762161,"I'm OK with the summary frequency ""messiness"" since this can already happen with multiple environments to day. I'll defer on the ""agent steps"" change to others who know it better.",summary frequency messiness since already happen multiple day defer agent change know better,issue,negative,positive,positive,positive,positive,positive
569758658,"@kevtan and @chriselion  thanks for replying.  Yes,  I am using ML agents (0.10.0) which is an older version. I am going forward with setting the same size of visual observation and rendertexture as @chriselion  suggested.",thanks yes older version going forward setting size visual observation,issue,positive,positive,positive,positive,positive,positive
569750191,"Hi again @unityjeffrey, Its been a while and i guess you're very busy but i was hoping to know if there has been anything helpful regarding this topic. Thanks.",hi guess busy know anything helpful regarding topic thanks,issue,positive,positive,positive,positive,positive,positive
569737100,"I just found this tutorial that shows ML-Agents being used with ECS: https://www.youtube.com/watch?v=Nj1rZKl51b0

Does this mean we can expect something soon?     :-)",found tutorial used mean expect something soon,issue,negative,negative,negative,negative,negative,negative
569463674,Thank you @afewvowels you probably saved my bachelor thesis. I don't know how it happened but after running the development installation it suggested to me to upgrade the pip and now everything works even without the virtual environment... Thank you once more...,thank probably saved bachelor thesis know running development installation upgrade pip everything work even without virtual environment thank,issue,positive,neutral,neutral,neutral,neutral,neutral
569438582,"Hi @chechulinYuri,
Have you installed the Barracuda plugin from package manager?  It includes the google protobuf library and is required for ml-agents to work. ",hi barracuda package manager library work,issue,negative,neutral,neutral,neutral,neutral,neutral
569427229,"is there any news about this bug? I can't use a trained network in my android\iOS game because il2cpp is required for these platforms 
what should I do to be able to build for andoird\iOS with il2cpp backend?",news bug ca use trained network game able build,issue,negative,positive,neutral,neutral,positive,positive
569421435,"Have you tried creating a new virtual environment and following the [installation instructions](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) for development?

```
cd ml-agents-envs
pip3 install -e ./
cd ..
cd ml-agents
pip3 install -e ./
```

I had a lot of trouble getting ml-agents up and running without a [virtual environment](https://docs.python.org/3/tutorial/venv.html).",tried new virtual environment following installation development pip install pip install lot trouble getting running without virtual environment,issue,negative,negative,neutral,neutral,negative,negative
569407880,I think the issue should be opened because I don't have any solution for my problem,think issue solution problem,issue,negative,neutral,neutral,neutral,neutral,neutral
569403272,"@surfnerd  thank you for the reply, but there is nothing similar with my issue.

> Could you turn off dead code stripping in your Xcode project

I don't have an Xcode project because errors pop up right after I press the Build button in unity",thank reply nothing similar issue could turn dead code stripping project project pop right press build button unity,issue,negative,positive,neutral,neutral,positive,positive
569402542,So I tried to update Unity to the newest version but it did not help either. Then I tried to play around with Academy script a bit and I changed the port there to 5005 as the command line said but nothing except the number in the Unity notification changed.,tried update unity version help either tried play around academy script bit port command line said nothing except number unity notification,issue,positive,neutral,neutral,neutral,neutral,neutral
569327866,"hi @RogueCMD, 
from your command prompt you are in a directory below the root directory:
```D:\Program DLL\GDev\ml-agents-master\ml-agents```
you should be in
```D:\Program DLL\GDev\ml-agents-master\```

Please try from that directory and reopen this issue if you still have this same issue.
",hi command prompt directory root directory please try directory reopen issue still issue,issue,negative,neutral,neutral,neutral,neutral,neutral
569327414,I am going to close this in favor of #2595 and have updated our internal issue MLA-94 with this issue.,going close favor internal issue issue,issue,negative,neutral,neutral,neutral,neutral,neutral
569327187,Please follow this issue for updates on this bug. #2595 ,please follow issue bug,issue,negative,neutral,neutral,neutral,neutral,neutral
569326756,"Hi @chechulinYuri,
Could you turn off `dead code stripping` in your Xcode project?  This may prevent these build errors from popping up.  We are tracking a similar issues for ML-Agents not working with IL2CPP, and this sounds related.  Thanks for your feedback.",hi could turn dead code stripping project may prevent build similar working related thanks feedback,issue,negative,neutral,neutral,neutral,neutral,neutral
569324944,@TheoLvs Regarding using custom training scripts from multiple sources-- you could use the low-level [environment API](https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents-envs/mlagents_envs/environment.py) or the gym wrapper to split observations out to multiple trainers and merge them back into a single message to the environment.  The environment APIs are what you'd use if you want this type of custom control.,regarding custom training multiple could use environment gym wrapper split multiple merge back single message environment environment use want type custom control,issue,negative,negative,neutral,neutral,negative,negative
569277813,I am having the same issue with the latest release...when I pass a --time-scale argument (i.e. mlagents-learn config/trainer_config.yaml --run-id=firstRun **--time-scale=100** --train) it is giving me the same error. Has anyone found a solution?,issue latest release pas argument train giving error anyone found solution,issue,negative,positive,positive,positive,positive,positive
569237801,"Please also see this issue
https://github.com/Unity-Technologies/ml-agents/issues/1047#issuecomment-409660869",please also see issue,issue,negative,neutral,neutral,neutral,neutral,neutral
569232400,"Hey, 

if I understand u right u need to load a already trained model. See the [https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-ML-Agents.md](doc) on command line training options `--load` to:

```
--load: If set, the training code loads an already trained model to initialize the neural network before training. The learning code looks for the model in models/<run-id>/ (which is also where it saves models at the end of training). When not set (the default), the neural network weights are randomly initialized and an existing model is not loaded.
```",hey understand right need load already trained model see command line training load load set training code already trained model initialize neural network training learning code model also end training set default neural network randomly model loaded,issue,negative,negative,negative,negative,negative,negative
569172540,"Found it! Don't know why it didn't appear the first time I searched for it, but for those that are curious the `.proto` files can be found `ml-agents/protobuf-definitions/proto/mlagents_envs/communicator_objects/` here.",found know appear first time curious found,issue,negative,positive,neutral,neutral,positive,positive
569166970,"Hi, to piggyback on this question, I would like to try evolution algorithm (https://arxiv.org/abs/1704.07075) which require no training a NN model. My thought is to use gym as an environment and just perform the steps based on calculating the reward.

Anyways, it all works until i want to visualize what is happening, I think the steps are happening too fast, is there a way to slow it so i can see what is happening?

ps. i'm on mlagents 0.10.1 right now, and using the gym-unity folder that came with that version.
I can't do --slow since i'm not training. I don't think the train_model = true still applies.",hi question would like try evolution algorithm require training model thought use gym environment perform based calculating reward anyways work want visualize happening think happening fast way slow see happening right folder came version ca slow since training think true still,issue,positive,positive,neutral,neutral,positive,positive
568918518,"> I am having same problem..how do you solve it?

Hi. I used ""UnitySDK"" from latest_release. Then use speed slider in Academy component in Unity.",problem solve hi used use speed slider academy component unity,issue,negative,neutral,neutral,neutral,neutral,neutral
568846543,"Hey @elliott-omosheye,

I didn't work on this PR in a while but last time I discussed it with the maintainers it was in this issue https://github.com/Unity-Technologies/ml-agents/issues/2559.

The status at that time was `We are still quite interested in the contribution! We are planning on taking a look at is as part of a wider look at multi-agent, which we will be taking soon.`

I don't mind doing the rebase myself if/when we want to merge it.",hey work last time issue status time still quite interested contribution taking look part look taking mind rebase want merge,issue,positive,positive,positive,positive,positive,positive
568809495,This has diverged a lot from master. I wouldn't mind doing a rebase but I can't figure out why this PR wasn't merged/declined months ago and don't want to waste the effort if the maintainers have a reason why they didn't like this.,lot master would mind rebase ca figure ago want waste effort reason like,issue,negative,negative,negative,negative,negative,negative
568790073,Thank you! I really confused releases. The latest one works fine.,thank really confused latest one work fine,issue,negative,positive,positive,positive,positive,positive
568777411,"Related to issue [2739](https://github.com/Unity-Technologies/ml-agents/issues/2739). Our Tennis and Soccer example environments may be helpful.  Additionally, we are working on adding a self-play feature to the toolkit for exactly this case.  The issue referenced above contains a link to a self-play PR that people have had luck with.",related issue tennis soccer example may helpful additionally working feature exactly case issue link people luck,issue,positive,positive,positive,positive,positive,positive
568758208,"> Hi @IRiViI
> With respect to the particular error that you're hitting - we look for a few particular constants in the barracuda file at load time:
> https://github.com/Unity-Technologies/ml-agents/blob/0.12.1/UnitySDK/Assets/ML-Agents/Scripts/InferenceBrain/BarracudaModelParamLoader.cs#L141-L145
> 
> In the normal workflow, these constants are added to the tensorflow model e.g. here:
> https://github.com/Unity-Technologies/ml-agents/blob/0.12.1/ml-agents/mlagents/trainers/models.py#L64-L69
> 
> You'll need to make sure these constants are set in your keras model before converting to barracuda.
> 
> Unfortunately, the conventions for how the python code sets up the model and how the barracuda importer looks for tensors are pretty tightly coupled, so you might hit similar problems even if you get past this one. The strings are all in https://github.com/Unity-Technologies/ml-agents/blob/0.12.1/UnitySDK/Assets/ML-Agents/Scripts/InferenceBrain/TensorNames.cs.
> 
> We'll eventually use ONNX instead of .nn files, but it's not ready yet; we need a few fixes from the barracuda team to make the import process easier.
> 
> Note that if you just want to use barracuda for inference, you can do that without ML-Agents (and ONNX import should work fine). The documention for that is with the package: https://docs.unity3d.com/Packages/com.unity.barracuda@0.3/manual/index.html

Thanks a lot for the references! I already found a few of them but not the list of all string values! Do you have an indication when the new version will be ready? If it will be a month or two it might be worth waiting for me =).",hi respect particular error look particular barracuda file load time normal added model need make sure set model converting barracuda unfortunately python code model barracuda importer pretty tightly coupled might hit similar even get past one eventually use instead ready yet need barracuda team make import process easier note want use barracuda inference without import work fine package thanks lot already found list string indication new version ready month two might worth waiting,issue,positive,positive,positive,positive,positive,positive
568621878,"This will be added in the next release, and is already possible on the `master` branch. You can apply the changes to the Unity-side SDK from this PR: https://github.com/Unity-Technologies/ml-agents/commit/ebad90f564bc03678da167ea40e6ec2fc1574a87

Basically this change allows you to add a RayPerceptionSensor to a child gameobject - so you could add a ""BackwardsSensor"" child to the agent, and flip the transform 180 degrees.",added next release already possible master branch apply basically change add child could add child agent flip transform,issue,negative,neutral,neutral,neutral,neutral,neutral
568606622,Do you have any more information on what it was doing when it froze? Was there an exception on the python side? Tensorflow shouldn't directly affect anything that happens with the editor.,information froze exception python side directly affect anything editor,issue,negative,positive,neutral,neutral,positive,positive
568591758,"As @kevtan mentioned, this is an older version. The latest version has a different (and IMHO simpler, but I'm biased) setup for this; the observation will always be the size of the rendertexture and there's no other dimension parameters that need to be set.

Two other things to note:
* In older versions, the size mismatch probably would lead to a problem, but I'm not sure if it would have complained immediately or more subtly further down the pipeline.
* There's a lower bound to the size of visual observations that we can support (20x20 by default). Some better error messages for this were just added last week.",older version latest version different simpler setup observation always size dimension need set two note older size mismatch probably would lead problem sure would immediately subtly pipeline lower bound size visual support default better error added last week,issue,negative,positive,positive,positive,positive,positive
568590925,"Hi @IRiViI 
With respect to the particular error that you're hitting - we look for a few particular constants in the barracuda file at load time: 
https://github.com/Unity-Technologies/ml-agents/blob/0.12.1/UnitySDK/Assets/ML-Agents/Scripts/InferenceBrain/BarracudaModelParamLoader.cs#L141-L145

In the normal workflow, these constants are added to the tensorflow model e.g. here:
https://github.com/Unity-Technologies/ml-agents/blob/0.12.1/ml-agents/mlagents/trainers/models.py#L64-L69 

You'll need to make sure these constants are set in your keras model before converting to barracuda.

Unfortunately, the conventions for how the python code sets up the model and how the barracuda importer looks for tensors are pretty tightly coupled, so you might hit similar problems even if you get past this one. The strings are all in https://github.com/Unity-Technologies/ml-agents/blob/0.12.1/UnitySDK/Assets/ML-Agents/Scripts/InferenceBrain/TensorNames.cs.

We'll eventually use ONNX instead of .nn files, but it's not ready yet; we need a few fixes from the barracuda team to make the import process easier.

Note that if you just want to use barracuda for inference, you can do that without ML-Agents (and ONNX import should work fine). The documention for that is with the package: https://docs.unity3d.com/Packages/com.unity.barracuda@0.3/manual/index.html
",hi respect particular error look particular barracuda file load time normal added model need make sure set model converting barracuda unfortunately python code model barracuda importer pretty tightly coupled might hit similar even get past one eventually use instead ready yet need barracuda team make import process easier note want use barracuda inference without import work fine package,issue,positive,positive,neutral,neutral,positive,positive
568575673,Update: Removing the raySensor3D component from the observation list fixed it. Thanks for the help. (Again!) @ervteng ,update removing component observation list fixed thanks help,issue,positive,positive,positive,positive,positive,positive
568574374,"Yes it does. If you're using only RayPerceptionSensor, the VectorObservations parameter in the agent should be 0. What're the other 295 vector obs?",yes parameter agent vector,issue,negative,neutral,neutral,neutral,neutral,neutral
568573626,"@ervteng Yes, I am. Now, I have the rayPerceptionSensorComp3D set as an observation. Is that a bad idea? I have heard it works on its own.",yes set observation bad idea work,issue,negative,negative,negative,negative,negative,negative
568573119,"This looks correct - are you absolutely sure that you're using the right model file?

The model file that's exported shows 328 observations, which is the right number.",correct absolutely sure right model file model file right number,issue,negative,positive,positive,positive,positive,positive
568566524,"Hi @Blueeyesjt1, it's a little confusing but you shouldn't add the RayPerceptionSensor obs to the VectorObservationSpaceSize. 

When you quit Python training, the terminal should print out something like this:
```
Converting ./models/booger-0/PushBlock/frozen_graph_def.pb to ./models/booger-0/PushBlock.nn
GLOBALS: 'is_continuous_control', 'version_number', 'memory_size', 'action_output_shape'
IN: 'vector_observation': [-1, 1, 1, 224] => 'main_graph_0/hidden_0/BiasAdd'
IN: 'action_masks': [-1, 1, 1, 7] => 'strided_slice_1'
OUT: 'action_probs', 'concat_2', 'action'
DONE: wrote ./models/booger-0/PushBlock.nn file.
INFO:mlagents.trainers:Exported ./models/booger-0/PushBlock.nn file
```

Can you post your output? It'll help a lot in debugging. Thanks!",hi little add quit python training terminal print something like converting done wrote file file post output help lot thanks,issue,positive,positive,neutral,neutral,positive,positive
568560155,"I agree that this isn't the simplest example.  However I would disagree that the curriculum is being advanced based on progress not relevant to each brain.  The curriculum for each of the brains is advanced based only on data relevant to that brain.  Each brain will have its own trainer, and the data from the trainer is used to advance the curriculum.",agree example however would disagree curriculum advanced based progress relevant brain curriculum brain advanced based data relevant brain brain trainer data trainer used advance curriculum,issue,positive,positive,positive,positive,positive,positive
568495561,"![image](https://user-images.githubusercontent.com/59098194/71364157-5ff66b00-2569-11ea-8fca-f20fbdbecf15.png)
![image](https://user-images.githubusercontent.com/59098194/71364245-a1871600-2569-11ea-8d5d-9055ab265b48.png)
![image](https://user-images.githubusercontent.com/59098194/71364293-c7141f80-2569-11ea-8c12-cd6408319aa6.png)

(I deleted my other question because it was unrelated to the topic.)

But now, I'm having an issue when try to test my agent AFTER training and adding the model to the inspector. Adding more observations does not fix this. What should I do? If I add an extra 33, then the error also adds another 33. It keeps asking for +33 over and over again the more I add on!",image image image question unrelated topic issue try test agent training model inspector fix add extra error also another add,issue,negative,neutral,neutral,neutral,neutral,neutral
568473763,"I also tried with the latest changes and I noticed the same behavior. If it can help, the agent_list (containing agent ids and all their info) contains only one agent at the step when the agent is done.

More importantly, when I’m playing with different seeds for the environment, this behavior sometimes disappear for no reason.",also tried latest behavior help agent one agent step agent done importantly different environment behavior sometimes disappear reason,issue,negative,positive,positive,positive,positive,positive
568453915,"Here's the last snapshot from Friday:
https://drive.google.com/open?id=12PKfydD9lMUX3mtwgmEMSu0_uhusF7DR

And a new one from today:
https://drive.google.com/file/d/1bHMs-2HassYa0EoeoGe3u_iYctBwt530/view?usp=sharing

Strange thing is though that the one from Friday was about 3gb while the one from today is 10gb.

Latest Task Manager:

![image](https://user-images.githubusercontent.com/2350015/71356485-b1f7ba80-2582-11ea-8194-eb54158737f5.png)

Latest tensorboard:
![image](https://user-images.githubusercontent.com/2350015/71356472-a0aeae00-2582-11ea-9b7b-f13ae9e119a8.png)",last snapshot new one today strange thing though one one today latest task manager image latest image,issue,negative,positive,positive,positive,positive,positive
568446027,"> HI @IRiViI at present we do not support this functionality. However, in theory it's possible to import a Keras model into Unity. The keras model needs to be saved into onnx format and then convert it into barracuda using the onnx to barracuda converter and then use it in Unity. This sounds simple in theory but the reality might be different.

Thank you for your reponse and sorry for my late reply. Too bad that it's not that simple using the current tools. I will probably try to check out what I can achieve using python code and the ml agents env tools. The reason why I wanted to intergrate keras into unity is because I want to move freely trough my environment during training. Hopefully there will be another way. 
Thanks again for your reply",hi present support functionality however theory possible import model unity model need saved format convert barracuda barracuda converter use unity simple theory reality might different thank sorry late reply bad simple current probably try check achieve python code reason unity want move freely trough environment training hopefully another way thanks reply,issue,positive,negative,neutral,neutral,negative,negative
568429396,"I totally agree on the need of having to solve this for @Tobbse 's issue as I'm also running into that problem:

![image](https://user-images.githubusercontent.com/2350015/71351723-b1f0be00-2574-11ea-9996-8ed79030e730.png)

In the example above I'd want to get the .nn model of one of these peaks as apparently the model performed best there.

Now it's almost like following the stock trade market 😆 ",totally agree need solve issue also running problem image example want get model one apparently model best almost like following stock trade market,issue,positive,positive,positive,positive,positive,positive
568344616,"Hi @rajatpaliwal were you able to figure this out? It also looks like you're using an old version of the Unity ML-Agents SDK (seeing as you have a Learning Brain, which has now been replaced in favor of the Behavior Parameters component attached to Agents).",hi able figure also like old version unity seeing learning brain favor behavior component attached,issue,positive,positive,positive,positive,positive,positive
568338807,"Could you be a little bit more specific when you say ""the agent does not initialize?"" It sounds like nothing is crashing, but I'm not quite sure. Are you training with your environment using the low-level Python API or the `mlagents-learn` command line tool?",could little bit specific say agent initialize like nothing quite sure training environment python command line tool,issue,negative,positive,positive,positive,positive,positive
568335770,"I pulled in the latest changes, ran the same experiment as described in the issue and was able to replicate the same problem. It seems like once one of the ball drops the environment only registers the information of the agent that dropped the ball. In particular, the `max_reached` and `rewards` fields of the BrainInfo object also collapse to length-one lists. I'm not sure what's going on though. Can someone more knowledgeable let us know what the root cause of this issue is?",latest ran experiment issue able replicate problem like one ball environment information agent ball particular object also collapse sure going though someone knowledgeable let u know root cause issue,issue,negative,positive,positive,positive,positive,positive
568294100,config/trainer_config.yaml is specifying the path to the config file _from the current directory_.  Make sure you are in the directory that contains the config folder when you execute the command.,path file current make sure directory folder execute command,issue,negative,positive,positive,positive,positive,positive
568293901,"Hi,

I assume you're looking at the docs on master. Try these instead https://github.com/Unity-Technologies/ml-agents/tree/latest_release",hi assume looking master try instead,issue,negative,neutral,neutral,neutral,neutral,neutral
568248689,"> You can see how to fix it on #1534 .

Thanks for your advice. It worked！",see fix thanks advice,issue,negative,positive,positive,positive,positive,positive
568237825,"Hi @RogueCMD, I am assuming you git cloned or downloaded the master branch .In the root folder there will be a config folder and a trainer_config.yaml file ( '..\\ml-agents-master\\ml-agents-master\\config\\trainer_config.yaml').Have you tried checking whether it is present.Alternatively you can try out building a sample executable  Unity file and then set the environment to /path/to/buildfile ('https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Executable.md') .Otherwise you can try out running mlagent_learn command('https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md').Im guessing you have properly setup the mlagent_envs ,ml_agent via the setup instructions provided at readme.",hi assuming git master branch root folder folder file tried whether try building sample executable unity file set environment try running command guessing properly setup via setup provided,issue,negative,neutral,neutral,neutral,neutral,neutral
568079086,"Interesting - and you have ""UseChildSensors"" checked? I've been able to add such a sensor to an empty child gameobject and it does work. What's the error you're getting?",interesting checked able add sensor empty child work error getting,issue,negative,positive,positive,positive,positive,positive
568068802,"Hi @MarkTension, you're right, when OnDemandDecisions isn't enabled, an agent takes a step every FixedUpdate, but only asks the Barracuda model/Python code for a new action every DecisionInterval steps, repeating the action in between. 

Depending on how CPU-intensive the AgentAction method is, you might see some speedup when switching to OnDemandDecisions as you're seeing. ",hi right agent step every barracuda code new action every action depending method might see switching seeing,issue,negative,positive,positive,positive,positive,positive
568067747,"Hi @Blueeyesjt1, are you using the UnitySDK folder from the `master` branch of ML-Agents? What's the hierarchy of GameObjects under your Agent look like?",hi folder master branch hierarchy agent look like,issue,negative,neutral,neutral,neutral,neutral,neutral
568056905,Were you able to create the executable following all the instructions here https://github.com/Unity-Technologies/ml-agents/blob/latest_release/docs/Learning-Environment-Executable.md#building-the-3dball-environment ?,able create executable following,issue,negative,positive,positive,positive,positive,positive
568051693,"I am not sure to get your question but if it can help:

- Build your game in Unity (I recomend you to start with provided examples, say 3DBall)
- Create a python script where you want
- In your script, define env_name as the absolute path of your built game, for me it looks like
`env_name = ""D:/Users/Valentin/Desktop/test/ml-agents/UnitySDK/Builds/ball.exe""
`
- Instantiate a UnityEnvironment with your env_name (don't forget to import what you need)
`env = UnityEnvironment(base_port = 5006, file_name=env_name, seed=1)
`
- Executing this little script should launch your game, then you can start working on interacting with it

Hope this helps

**EDIT**: Start with a simple python script, Jupyter Notebook can make it more difficult",sure get question help build game unity start provided say create python script want script define absolute path built game like forget import need little script launch game start working hope edit start simple python script notebook make difficult,issue,positive,negative,negative,negative,negative,negative
568049056,"Hi @Pabmyster,

My first recommendation is to check the docs of the latest_release tag here: https://github.com/Unity-Technologies/ml-agents/tree/latest_release

Were you able to create the build following the directions in the section ""Build the 3DBall environment"" of https://github.com/Unity-Technologies/ml-agents/blob/latest_release/docs/Learning-Environment-Executable.md ?

If you were able to create the build, the simplest way to run is to use the --env flag to specify the path to your build i.e. in envs.  (See https://github.com/Unity-Technologies/ml-agents/blob/latest_release/docs/Learning-Environment-Executable.md#training-the-environment)

Otherwise, let me know where you got stuck.",hi first recommendation check tag able create build following section build environment able create build way run use flag specify path build see otherwise let know got stuck,issue,negative,positive,positive,positive,positive,positive
568035301,One _guess_ on why the training would be slowing down is if your memory is getting really fragmented with the creation and deletion of materials/textures/etc.  It may also explain the heap size of the unity process.  I'll see if I can find anything with some of my own spelunking. ,one training would memory getting really fragmented creation deletion may also explain heap size unity process see find anything,issue,negative,positive,positive,positive,positive,positive
568033813,Do you happen to have the memory snapshot files you can send to me?  Or post to google drive or something.  I don't think I'll be able to run your game for as long :P ,happen memory snapshot send post drive something think able run game long,issue,negative,positive,neutral,neutral,positive,positive
567992187,"Sorry for the delay. We have this logged as MLA-348 in our tracker, but it's not currently scheduled to be worked on.

The current master branch also has a new feature called Side Channels which can be used to send arbitrary data between the python and C# processes
https://github.com/Unity-Technologies/ml-agents/blob/dfe9c11518f1de73f36bf85c36477372481e1598/ml-agents-envs/mlagents_envs/side_channel/side_channel.py#L16
https://github.com/Unity-Technologies/ml-agents/blob/dfe9c11518f1de73f36bf85c36477372481e1598/UnitySDK/Assets/ML-Agents/Scripts/SideChannel/SideChannel.cs#L19
",sorry delay logged tracker currently worked current master branch also new feature side used send arbitrary data python,issue,negative,negative,neutral,neutral,negative,negative
567990438,hi @devedse - we put the Conda instructions back in since the last release.  https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation-Windows.md,hi put back since last release,issue,negative,neutral,neutral,neutral,neutral,neutral
567960262,"@MarkTension `model.save('saved_model/my_model') ` will save model in Keras format, if you want to feed it to Barracuda TF converter you have to convert it to Tensorflow format.
Here are some discussions on how this can be done: https://stackoverflow.com/questions/45466020/how-to-export-keras-h5-to-tensorflow-pb",save model format want feed barracuda converter convert format done,issue,positive,neutral,neutral,neutral,neutral,neutral
567904661,"> The recent ISensor interface changes give us a chance to bring some of this back in a different way. I think we can add a new ""compression type"" to transmit a Google.Protobuf.WellKnownTypes.Any, and provide a corresponding user-defined hook in the python side to decode this into an observation.

Is there any current work on bringing the custom observation back in the form you described? Or an issue for it?",recent interface give u chance bring back different way think add new compression type transmit provide corresponding hook python side decode observation current work custom observation back form issue,issue,negative,positive,neutral,neutral,positive,positive
567874350,I think it's good to keep this issue open since @unityjeffrey said he'll keep us posted. Let's wait until a final decision has been made in regards to the Conda documentation.,think good keep issue open since said keep u posted let wait final decision made documentation,issue,negative,positive,positive,positive,positive,positive
567855812,"Another thing I saw was that the time between snapshots also gradually increases.

At the fist few snapshots I see there's a difference of about 50 seconds per 1000 steps.

At the end this duration has increased to about 220 seconds:

```
INFO:mlagents.trainers: race_7: RaceGameLearningBrain: Step: 1000. Time Elapsed: 52.796 s Mean Reward: -38.610. Std of Reward: 17.850. Training.
INFO:mlagents.trainers: race_7: RaceGameLearningBrain: Step: 2000. Time Elapsed: 102.428 s Mean Reward: -34.189. Std of Reward: 20.628. Training.
INFO:mlagents.trainers: race_7: RaceGameLearningBrain: Step: 3000. Time Elapsed: 152.381 s Mean Reward: -23.578. Std of Reward: 20.009. Training.
INFO:mlagents.trainers: race_7: RaceGameLearningBrain: Step: 4000. Time Elapsed: 206.723 s Mean Reward: -15.534. Std of Reward: 16.491. Training.
INFO:mlagents.trainers: race_7: RaceGameLearningBrain: Step: 5000. Time Elapsed: 257.479 s Mean Reward: -7.267. Std of Reward: 10.994. Training.

...
...
...

INFO:mlagents.trainers: race_7: RaceGameLearningBrain: Step: 2319000. Time Elapsed: 317379.718 s Mean Reward: 1467.813. Std of Reward: 1560.897. Training.
INFO:mlagents.trainers: race_7: RaceGameLearningBrain: Step: 2320000. Time Elapsed: 317599.201 s Mean Reward: 601.707. Std of Reward: 293.869. Training.
INFO:mlagents.trainers: race_7: RaceGameLearningBrain: Step: 2321000. Time Elapsed: 317824.258 s Mean Reward: 1292.860. Std of Reward: 1052.086. Training.
INFO:mlagents.trainers: race_7: RaceGameLearningBrain: Step: 2322000. Time Elapsed: 318044.004 s Mean Reward: 1013.822. Std of Reward: 1115.018. Training.
INFO:mlagents.trainers: race_7: RaceGameLearningBrain: Step: 2323000. Time Elapsed: 318266.715 s Mean Reward: 1757.583. Std of Reward: 2141.269. Training.
```

And for completeness, the last Tensorboard output:
![image](https://user-images.githubusercontent.com/2350015/71244686-612e5a80-2313-11ea-8b1f-a9426be90bf1.png)

And smoothed:
![image](https://user-images.githubusercontent.com/2350015/71244884-c4b88800-2313-11ea-92c5-00df8dd37ce6.png)
",another thing saw time also gradually fist see difference per end duration step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training completeness last output image image,issue,positive,negative,negative,negative,negative,negative
567854610,"I did a bit of scrolling through the snapshot as well and think it might be a bit higher then 200-300MB due to there being quite a lot of instances for some specific 1MB objects. For example there's 935 instances of Texture2D.

However I also agree that this doesn't seem to add up to 20gb.

This morning however I checked again and saw the memory usage dropped to 14GB:
![image](https://user-images.githubusercontent.com/2350015/71244262-68a13400-2312-11ea-8d2f-0aec8a117988.png)

It could possibly just be an artifact of the Garbage collector not running that often, but I'm not really sure.",bit snapshot well think might bit higher due quite lot specific example however also agree seem add morning however checked saw memory usage image could possibly artifact garbage collector running often really sure,issue,positive,positive,positive,positive,positive,positive
567731417,"Sorry, your original assumption is correct.  AgentAction is called every FixedUpdate.  The action that is executed is changed at every request for a decision which occurs every x FixedUpdates where x = Decision Interval.  So, in your original question, yes AgentAction would be called 10,000 times and the action would change every 10 FixedUpdates.",sorry original assumption correct every action executed every request decision every decision interval original question yes would time action would change every,issue,positive,positive,neutral,neutral,positive,positive
567706719,"@andrewcoh Did you mean to make a distinction between AgentAction and FixedUpdate? AgentAction is called every environment step which is called on FixedUpdate. 

I briefly looked through the code, and it looks like the episode length is the sum of the lengths of ""trajectories"" which are lists of `AgentExperiences`. In `Agent.cs`, the agent experience is recorded only when `m_RequestDecision `is true, so I think my initial assumption is correct. 
",mean make distinction every environment step briefly code like episode length sum agent experience true think initial assumption correct,issue,positive,positive,neutral,neutral,positive,positive
567689028,"For an episode with 1000 steps, the total number of calls to AgentAction is 1000.  The decision interval is the number of FixedUpdates called in your scene between agent decisions.",episode total number decision interval number scene agent,issue,negative,neutral,neutral,neutral,neutral,neutral
567614712,"Hmm, this is pretty weird.  The amount of total memory in the snapshot is definitely less than ~20GB.  It's more on the scale of around 200-300MB.  I'll take a look at the project again and see if I can find anything else. ",pretty weird amount total memory snapshot definitely le scale around take look project see find anything else,issue,positive,negative,neutral,neutral,negative,negative
567553097,"I've had limited success training on visual algorithms and found agents that use the RaycastPerception component to converge on a solution much faster but you'll have to experiment with that for yourself.

The items you listed could be passed to the agent in the CollectObservations method. Just remember to set the space size under vector observations in the Behavior Parameters (visible in the Unity editor) to match the number of ```AddVectorObs(obs)``` you add in ```CollectObservations()```!:
```c#
public override void CollectObservations()
{
     AddVectorObs(currentHealth);
     AddVectorObs(currentMana);
     AddVectorObs(experience);
}
```

You could control the agent with a vector action space, just use 3 branches.

```c#
public override float[] AgentAction(float[] vectorAction)
{
     var moveAction = Mathf.FloorToInt(vectorAction[0]);
     var rotateAction = Mathf.FloorToInt(vectorAction[1]);
     var attackAction = Mathf.FloorToInt(vectorAction[2]);

     switch (moveAction)
     {
          case 1:
               agentRB.AddForce(transform.forward, ForceMode.VelocityChange);
               break;
          case 2:
               agentRB.AddForce(-transform.forward, ForceMode.VelocityChange);
               break;
     }

     switch (rotateAction)
     {
          case 1:
               transform.Rotate(0.0f, 5.0f, 0.0f);
               break;
          case 2:
               transform.Rotate(0.0f, -5.0f, 0.0f);
               break;
     }

     switch(attackAction)
     {
          case 1:
               DoAgentAttack(1);
               break;
          case 2:
               DoAgentAttack(2);
               break;
          ...
          case n:
               DoAgentAttack(n);
               break;
     }
}
```

 The agent can perform a single action on each branch per agent action. So technically the agent moves, turns, and attacks every time this function is called, but we use a branch size 1 greater than the supplied number of actions so there's an empty action in case the agent only wants to perform a single action. In the example below if the agent just wanted to move forwards, its vectorAction would look like this:

```c#
vectorAction[0] = 1.0f; // This moves forward
vectorAction[1] = 0.0f; // This is an undefined rotation action so nothing happens
vectorAction[2] = 0.0f // This is an undefined attack action so nothing happens
```",limited success training visual found use component converge solution much faster experiment listed could agent method remember set space size vector behavior visible unity editor match number add public override void experience could control agent vector action space use public override float float switch case break case break switch case break case break switch case break case break case break agent perform single action branch per agent action technically agent turn every time function use branch size greater number empty action case agent perform single action example agent move forward would look like forward undefined rotation action nothing undefined attack action nothing,issue,positive,positive,neutral,neutral,positive,positive
567527682,Issue was fixed on Barracuda side and fix should appear in the next release of Barracuda,issue fixed barracuda side fix appear next release barracuda,issue,negative,positive,neutral,neutral,positive,positive
567423198,"@surfnerd , I just took a screenshot when Unity was using ~20gb of memory. When I then opened this snapshot the Unity memory usage spiked to ~27gb so that's why in the screenshot below the memory usage is higher.

Anyway, the snapshot:
![image](https://user-images.githubusercontent.com/2350015/71164469-6de97a80-224f-11ea-8195-d575063bf813.png)

And a screenshot of the whole Table sorted by reference count:
![image](https://user-images.githubusercontent.com/2350015/71165475-2d8afc00-2251-11ea-8fce-5b4b7de65435.png)

It seems there's a few gigabytes here and there in for example shaders.

And Task Manager:
![image](https://user-images.githubusercontent.com/2350015/71164485-780b7900-224f-11ea-993a-7f389e1d1366.png)

Is there anything else you would like to see in this Snapshot?",took unity memory snapshot unity memory usage spiked memory usage higher anyway snapshot image whole table sorted reference count image example task manager image anything else would like see snapshot,issue,negative,positive,positive,positive,positive,positive
567341931,"Finally, I managed to delete it, thanks... But the error is still there. Unity still throws me ""The communicator was unable to connnect..."" error. 
",finally delete thanks error still unity still communicator unable error,issue,negative,negative,neutral,neutral,negative,negative
567255543,"This is what I was trying to get at in https://github.com/Unity-Technologies/ml-agents/issues/3033#issuecomment-562321239 - I think you have mlagents installed outside of the current virtual environment, which could happen if you ran `pip install mlagents` outside of the virtual environment. You should be able to see where it's installed with `where mlagents-learn`.",trying get think outside current virtual environment could happen ran pip install outside virtual environment able see,issue,negative,positive,positive,positive,positive,positive
567249548,"(summarizing offline discussion)
Subsequent PR will remove `policy` from `Trainer`, and move these to SACTrainer and PPOTrainer respectively (this will get rid of the need for their `sac_policy` and `ppo_policy`). Where RLTrainer uses `self.policy`, it will swap to using get_policy() with the behavior_name_id.",discussion subsequent remove policy trainer move respectively get rid need swap,issue,negative,neutral,neutral,neutral,neutral,neutral
567231933,"If you've found this useful, it's possible others would too. Feel free to submit a pull request following our contributing guidelines https://github.com/Unity-Technologies/ml-agents/blob/master/CONTRIBUTING.md",found useful possible would feel free submit pull request following,issue,positive,positive,positive,positive,positive,positive
567231349,Thank you for the feedback. I will bring up some alternatives for the load/run-id functionality in our next team meeting. ,thank feedback bring functionality next team meeting,issue,negative,neutral,neutral,neutral,neutral,neutral
567205895,"Okay... I actually don't know why am i having so much issues with this but im so confiused right now. I uninstalled mlagents and it still shows me its installed. in fact i found out it is installed right after i create new enviroment. I literally don't know what to do, is there any way i can uninstall this and get rid of it completely?
![Error](https://user-images.githubusercontent.com/25244849/71121963-d436b480-21df-11ea-89bc-7d1ff0256e48.PNG)
I must also mention i have uninstalled python and everything and installed ml-agents 0.10.0 with python=3.6 and tensorflow=1.7.1",actually know much right uninstalled still fact found right create new literally know way get rid completely error must also mention uninstalled python everything,issue,negative,positive,positive,positive,positive,positive
567196885,"Most likely the agent was trained for the base 50k steps, the _timers.json file says 50,001 but I haven't looked at this type of file before so I'm not sure if that's where the model's trained step count lives. I've got a fresh run going now for 500,000 steps and I'll let you know if the newly trained model exhibits the same performance and behavioral issues discussed above.",likely agent trained base file type file sure model trained step count got fresh run going let know newly trained model performance behavioral,issue,positive,positive,neutral,neutral,positive,positive
567185709,For how many timesteps did you let the agent with visual obs/raycasts run?,many let agent visual run,issue,negative,positive,positive,positive,positive,positive
567172954,Thanks for the update.  Can you also take a memory snapshot with the Memory profiler?  I'd like to see what is taking up so much memory. ,thanks update also take memory snapshot memory profiler like see taking much memory,issue,positive,positive,positive,positive,positive,positive
567041183,"Update, I've found a solution. 

One thing I did before executing the model was (like in the grid-world example) force-rendering the camera's to the rendertextures with:

```
if (targetCamera != null && agentInput != null)
{
    targetCamera.Render();
    agentInput.Render();
}
```

I thought this'd be sufficient, but apparently I needed to wrap that in a coroutine to make sure it's done before feeding it in the model:

```
IEnumerator renderFrame()
{
  if (targetCamera != null && agentInput != null)
  {
      targetCamera.Render();
      agentInput.Render();
  }
}
```

Call it before the barracuda model execution
`StartCoroutine(""renderFrame"");`

That did the trick or me",update found solution one thing model like example camera null null thought sufficient apparently wrap make sure done feeding model null null call barracuda model execution trick,issue,positive,positive,positive,positive,positive,positive
567032970,Issue added to our internal tracker.,issue added internal tracker,issue,negative,neutral,neutral,neutral,neutral,neutral
567027365,Added this issue to our internal tracker.,added issue internal tracker,issue,negative,neutral,neutral,neutral,neutral,neutral
567024970,"At the moment I can't tell if we would be able to support this network in the near future, but I added this report to our internal issue tracker.",moment ca tell would able support network near future added report internal issue tracker,issue,negative,positive,positive,positive,positive,positive
566956388,"Well, now we have a whole bunch of snapshot files so I couldn't really see one file making the difference. Besides that a CLI option could be added to enable/disable this feature.",well whole bunch snapshot could really see one file making difference besides option could added feature,issue,negative,positive,positive,positive,positive,positive
566954023,"True, but then we'd end up with a whole bunch of .nn files :)",true end whole bunch,issue,negative,positive,positive,positive,positive,positive
566891957,"Thanks for your replies, I still feel that the current example does not quite make sense. 

WallJump is training multiple different brains with the curriculum system. And it is engaging the next level of the curriculum based on progress which is not relevant in the new brains. How is this different from not just training 3 separate brains entirely? Because I thought the curriculum system was about progressing a single brain in a more guided manner. 

Perhaps a second simple example can be made to demonstrate iteratively training a single brain via curriculum. ",thanks still feel current example quite make sense training multiple different brain curriculum system engaging next level curriculum based progress relevant new brain different training separate brain entirely thought curriculum system single brain manner perhaps second simple example made demonstrate iteratively training single brain via curriculum,issue,positive,positive,neutral,neutral,positive,positive
566866659,"You're using the wrong version of pip3.  You need to use python3's venv and its pip3.  I was using conda, and it doesn't work -- nor does virtualenv.  Here's the guide:  https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Using-Virtual-Environment.md  ",wrong version pip need use python pip work guide,issue,negative,negative,negative,negative,negative,negative
566835511,"Sure, you can see by [this footage](https://gfycat.com/threadbareglisteningboa) that I'm not getting very good performance. That's a standalone scene with a single agent in it with a camera sensor @ 128x128px (that image on the billboard is a render texture of the agent camera) and I can't even get 24fps playback, let alone for training. Here's [my repo](https://github.com/afewvowels/unity_00_07_ml-agents_warehouse) for the project.

I had a lot of success working with the RayPerception component alone both with performance and results, but only with PPO. I've had a lot of problems with agents only doing a single action with SAC. I'll post some pictures from [this repo on chameleon agents](https://github.com/afewvowels/unity_00_09_ml-agents_chameleon) where the agents picked a favorite color (green) and ignored the reward structure and immediately changed to the green color from the randomized start color.

Unfortunately in the warehouse scene I've changed my Agent script a bit and the old models I trained on SAC aren't using the same inputs and vector spaces (I'll have to retrain) but across 2 or 3 training sessions the agent settled on rotating in a single direction and doing no other actions after about 50k-100k steps.

EDIT: Thanks for responding!

DOUBLE EDIT: First link was to repo, not to the footage

TRIPLE EDIT: [Here's some footage](https://youtu.be/DWS-o6cuJOk) of the same scene with a PPO agent using just the raycast component. There's 15 concurrent instances of the arena running at 20x speed no problems!",sure see footage getting good performance scene single agent camera sensor image billboard render texture agent camera ca even get playback let alone training project lot success working component alone performance lot single action sac post chameleon picked favorite color green reward structure immediately green color start color unfortunately warehouse scene agent script bit old trained sac vector retrain across training session agent settled rotating single direction edit thanks double edit first link footage triple edit footage scene agent component concurrent arena running speed,issue,positive,positive,positive,positive,positive,positive
566819983,"Hi,
@andrewcoh . Thanks for replying. The problem is solved , the issue was in the case number defined under each branch for every action. They were not starting from 1 but were numbered something else so, the agent was not able to reach those actions.
Sorry for bothering you.

Thanks ,
Rajat ",hi thanks problem issue case number defined branch every action starting something else agent able reach sorry thanks,issue,negative,positive,neutral,neutral,positive,positive
566814255,"You should be able to add both CameraSensor and RayPerception scripts to an agent without any problem. 

Can you be more specific regarding not having good results?",able add agent without problem specific regarding good,issue,positive,positive,positive,positive,positive,positive
566811032,"Fundamentally, the implementations are the same. The majority of the differences stem from the fact that we have to support the many different types of action/observation spaces that can be built in Unity.  These references are exactly what I'd recommend to understand the basics of PPO/SAC.

Good luck.",fundamentally majority stem fact support many different built unity exactly recommend understand good luck,issue,positive,positive,positive,positive,positive,positive
566809805,"Hi,

Can you copy the actual code from your AgentAction function?",hi copy actual code function,issue,negative,neutral,neutral,neutral,neutral,neutral
566773297,"Hi @unixunion -- what you're calling out are two separate features we take advantage of in the WallJump environment: 1) using different brains based on changes in the environment and 2) using curriculum learning to gradually make the environment more difficult over time.

In this case each of the brains has its own curriculum.  You might consider a different version of the environment where we don't use separate brains at all, and use a curriculum to train a single ""WallJumpBrain"".

As far as what to use in practice -- it really depends on your use case.  If you have multiple distinct behaviors you'd like to learn, and you can programmatically determine when a different behavior should be used, multiple brains/behaviors might be appropriate.

If you have a complex environment where learning is initially very slow (for example if the rewards are sparse) it might be beneficial to use curriculum learning to guide training.",hi calling two separate take advantage environment different brain based environment curriculum learning gradually make environment difficult time case brain curriculum might consider different version environment use separate brain use curriculum train single far use practice really use case multiple distinct like learn programmatically determine different behavior used multiple might appropriate complex environment learning initially slow example sparse might beneficial use curriculum learning guide training,issue,positive,negative,neutral,neutral,negative,negative
566720660,"Well, if the tooling would generate a .nn file during creation of a snapshot then your issue would also be tackled :)",well tooling would generate file creation snapshot issue would also tackled,issue,negative,neutral,neutral,neutral,neutral,neutral
566709035,"> Can you link to the algorithm used (or explain it in a comment)?

Unfortunately this rabbit hole is going pretty deep. It turns out our old way of doing normalization was technically ""wrong"" - we were taking the mean across all the observations of all the agents in the scene then taking the running mean/std of those means rather than individually. We can't do this anymore since the Trainer receives trajectories that are not across agents. 

This implementation _should_ be correct, but training performance on Crawler and Reacher is worse by about 10-15% final reward. I'm investigating other ways to do normalization that are both correct and effective, e.g. https://github.com/rll/rllab/blob/master/rllab/envs/normalized_env.py",link algorithm used explain comment unfortunately rabbit hole going pretty deep turn old way normalization technically wrong taking mean across scene taking running rather individually ca since trainer across implementation correct training performance crawler reacher worse final reward investigating way normalization correct effective,issue,negative,negative,neutral,neutral,negative,negative
566691708,"I'm wondering if there might be a way to export a .nn file from checkpoints that are not the latest checkpoint?
This would be very useful to me as I would sometimes after a long training session like to use policies at specific points of time during the training.
For example, I have a project that involves some very unstable training, which is likely my own fault. But nevertheless I would like to use the policy given at the checkpoint with the highest reward which is not nessecarily the latest one.",wondering might way export file latest would useful would sometimes long training session like use specific time training example project unstable training likely fault nevertheless would like use policy given highest reward latest one,issue,positive,positive,positive,positive,positive,positive
566688138,"HI @IRiViI at present we do not support this functionality.  However, in theory it's possible to import a Keras model into Unity. The keras model needs to be saved into onnx format and then convert it into barracuda using the onnx to barracuda converter and then use it in Unity.  This sounds simple in theory but the reality might be different.",hi present support functionality however theory possible import model unity model need saved format convert barracuda barracuda converter use unity simple theory reality might different,issue,positive,neutral,neutral,neutral,neutral,neutral
566464843,"@surfnerd , Hi, had a bit of a busy period so sorry for the delay in updates. Yesterday I made a [fix to the code](https://github.com/devedse/DS-MLUnityPrivate/commit/e83afb2a870f4b7762ba588c58fdad86a3880ba4) to now destroy materials on recreation of the map. I'm not sure if this has fixed the issue though because after one night of training I'm using about 8gb of memory again:

![image](https://user-images.githubusercontent.com/2350015/70984267-72c8f580-20ba-11ea-877b-996841efdbcd.png)

Whether this has fixed the issue is to be seen. I'll keep the training running for a few days and will inform you on the progress.

All commits I made:
https://github.com/devedse/DS-MLUnityPrivate/commits/master",hi bit busy period sorry delay yesterday made fix code destroy recreation map sure fixed issue though one night training memory image whether fixed issue seen keep training running day inform progress made,issue,negative,positive,neutral,neutral,positive,positive
566236853,"Cache miss:
![image](https://user-images.githubusercontent.com/6877802/70941042-713af700-2000-11ea-92e1-5362558f6b1c.png)

Cache hit (precommit only)
![image](https://user-images.githubusercontent.com/6877802/70941058-7b5cf580-2000-11ea-90ad-bd1ffbd5df1e.png)

Cache hit (precommit + venv)
![image](https://user-images.githubusercontent.com/6877802/70941924-6b461580-2002-11ea-98ea-39f24c893408.png)

Total time for the run goes from 1:44 uncached to 0:42 cached.",cache miss image cache hit precommit image cache hit precommit image total time run go,issue,negative,neutral,neutral,neutral,neutral,neutral
566188593,Thanks for the report. I've got a PR to fix the location here https://github.com/Unity-Technologies/ml-agents/pull/3093,thanks report got fix location,issue,negative,positive,positive,positive,positive,positive
566030191,"Hey guys, 

I got the same Error on training with the VisuallHallWay-example. I use tensorflow-gpu on cpu I cant reproduce the error. On every other example it works fine. Here my errorlog:


```
INFO:mlagents.envs:Start training by pressing the Play button in the Unity Editor.
INFO:mlagents.envs:
'Academy' started successfully!
Unity Academy name: Academy
        Number of Training Brains : 0
        Reset Parameters :


2019-12-16 12:48:22.495341: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-12-16 12:48:22.698308: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:23:00.0
totalMemory: 8.00GiB freeMemory: 6.64GiB
2019-12-16 12:48:22.704420: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1423] Adding visible gpu devices: 0
2019-12-16 12:48:23.316615: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-16 12:48:23.319443: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:917]      0
2019-12-16 12:48:23.321846: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:930] 0:   N
2019-12-16 12:48:23.324508: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6416 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:23:00.0, compute capability: 6.1)
INFO:mlagents.envs:Hyperparameters for the PPOTrainer of brain VisualHallway:
        trainer:        ppo
        batch_size:     64
        beta:   0.01
        buffer_size:    1024
        epsilon:        0.2
        hidden_units:   128
        lambd:  0.95
        learning_rate:  0.0003
        learning_rate_schedule: linear
        max_steps:      5.0e5
        memory_size:    256
        normalize:      False
        num_epoch:      3
        num_layers:     1
        time_horizon:   64
        sequence_length:        64
        summary_freq:   1000
        use_recurrent:  True
        vis_encode_type:        simple
        reward_signals:
          extrinsic:
            strength:   1.0
            gamma:      0.99
        summary_path:   ./summaries/firstRun_VisualHallway
        model_path:     ./models/firstRun-0/VisualHallway
        keep_checkpoints:       5
2019-12-16 12:48:24.583849: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1423] Adding visible gpu devices: 0
2019-12-16 12:48:24.586676: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-16 12:48:24.589644: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:917]      0
2019-12-16 12:48:24.591726: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:930] 0:   N
2019-12-16 12:48:24.594429: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6416 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:23:00.0, compute capability: 6.1)
2019-12-16 12:48:26.067053: E T:\src\github\tensorflow\tensorflow\stream_executor\cuda\cuda_dnn.cc:396] Loaded runtime CuDNN library: 7605 (compatibility version 7600) but source was compiled with 7003 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.
2019-12-16 12:48:26.076044: F T:\src\github\tensorflow\tensorflow\core\kernels\conv_ops.cc:712] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)
Process Process-1:
Traceback (most recent call last):
  File ""C:\Users\marku\Anaconda3\envs\ml-agents-gpu\lib\multiprocessing\connection.py"", line 312, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
BrokenPipeError: [WinError 109] Die Pipe wurde beendet

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\marku\Anaconda3\envs\ml-agents-gpu\lib\multiprocessing\process.py"", line 258, in _bootstrap
    self.run()
  File ""C:\Users\marku\Anaconda3\envs\ml-agents-gpu\lib\multiprocessing\process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""c:\_github\ml-agents\ml-agents-envs\mlagents\envs\subprocess_env_manager.py"", line 89, in worker
    cmd: EnvironmentCommand = parent_conn.recv()
  File ""C:\Users\marku\Anaconda3\envs\ml-agents-gpu\lib\multiprocessing\connection.py"", line 250, in recv
    buf = self._recv_bytes()
  File ""C:\Users\marku\Anaconda3\envs\ml-agents-gpu\lib\multiprocessing\connection.py"", line 321, in _recv_bytes
    raise EOFError
EOFError

(ml-agents-gpu) C:\_GitHub\ml-agents>
```",hey got error training use cant reproduce error every example work fine start training pressing play button unity editor successfully unity academy name academy number training brain reset binary use found device name major minor visible device interconnect strength edge matrix device memory physical device name bus id compute capability brain trainer beta epsilon linear normalize false true simple extrinsic strength gamma visible device interconnect strength edge matrix device memory physical device name bus id compute capability loaded library compatibility version source compatibility version binary install upgrade library match building make sure library loaded compatible version compile configuration check parent process recent call last file line err true die pipe handling exception another exception recent call last file line file line run file line worker file line file line raise,issue,positive,positive,positive,positive,positive,positive
565988432,"By looking more closely to the issue, I realise that there is no TFModel for this particular example. Do you guys have any NN model, even if it does not perform well. I just want to look at the gameplay, and if it is good then I'll train one myself and contribute to the project.",looking closely issue particular example model even perform well want look good train one contribute project,issue,positive,positive,positive,positive,positive,positive
565812717,"This was caused by an error in my custom config file missing the ""s"" off the end of reward signals: ",error custom file missing end reward,issue,negative,negative,negative,negative,negative,negative
565758525,"There's a section in the Migration Guide on this, let me know if there's anything that could be made clearer https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Migrating.md#steps-to-migrate-1",section migration guide let know anything could made clearer,issue,negative,neutral,neutral,neutral,neutral,neutral
565758475,"Yes, it will automatically add to the agents observations; you don't need to account for it the Behavior Parameters' Space Size.",yes automatically add need account behavior space size,issue,negative,neutral,neutral,neutral,neutral,neutral
565748646,"Hm, I'll have to look into that.

I am using a low initial entropy because I'm using a discrete action space. The [docs](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-SAC.md) suggest an initial entropy coefficient of `0.05` - `0.5` for discrete.",look low initial entropy discrete action space suggest initial entropy coefficient discrete,issue,negative,positive,neutral,neutral,positive,positive
565737070,"In the end I managed to make it work using SAC, it was a bug in the code that delayed the calculations of one frame.

However, why are you using such a low initial entropy?
On 14 Dec 2019, 15:56 +0100, Keith Smith <notifications@github.com>, wrote:
> .NET Framework 4.x does not result in working SAC agents, they still converge on a set of behaviors that does not maximize rewards but instead favors a single type of behavior. I got blue chameleons this time.
> I'm going back to PPO.
> These were my training parameters, and a link to the repo if you want to try to replicate the behavior yourself:
> default:
>    trainer: sac
>    batch_size: 128
>    buffer_size: 50000
>    buffer_init_steps: 0
>    hidden_units: 128
>    init_entcoef: 1.0
>    learning_rate: 3.0e-4
>    learning_rate_schedule: constant
>    max_steps: 1.0e5
>    memory_size: 256
>    normalize: false
>    num_update: 1
>    train_interval: 1
>    num_layers: 2
>    time_horizon: 64
>    sequence_length: 64
>    summary_freq: 1000
>    tau: 0.005
>    use_recurrent: false
>    vis_encode_type: simple
>    reward_signals:
>        extrinsic:
>            strength: 1.0
>            gamma: 0.99
>
> Chameleon:
>    vis_encode_type: nature_cnn
>    init_entcoef: 0.25
>    buffer_init_steps: 5000
>    reward_signals:
>        curiosity:
>            strength: 0.02
>            gamma: 0.99
>            encoding_size: 256
> https://github.com/afewvowels/unity_00_09_chameleon
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",end make work sac bug code one frame however low initial entropy smith wrote framework result working sac still converge set maximize instead single type behavior got blue time going back training link want try replicate behavior default trainer sac constant normalize false tau false simple extrinsic strength gamma chameleon curiosity strength gamma thread reply directly view,issue,positive,negative,neutral,neutral,negative,negative
565724848,".NET Framework 4.x does not result in working SAC agents, they still converge on a set of behaviors that does not maximize rewards but instead favors a single type of behavior. I got blue chameleons this time.

I'm going back to PPO.

These were my training parameters, and a link to the repo if you want to try to replicate the behavior yourself:

```yaml
default:
    trainer: sac
    batch_size: 128
    buffer_size: 50000
    buffer_init_steps: 0
    hidden_units: 128
    init_entcoef: 1.0
    learning_rate: 3.0e-4
    learning_rate_schedule: constant
    max_steps: 1.0e5
    memory_size: 256
    normalize: false
    num_update: 1
    train_interval: 1
    num_layers: 2
    time_horizon: 64
    sequence_length: 64
    summary_freq: 1000
    tau: 0.005
    use_recurrent: false
    vis_encode_type: simple
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99

Chameleon:
    vis_encode_type: nature_cnn
    init_entcoef: 0.25
    buffer_init_steps: 5000
    reward_signals:
        curiosity:
            strength: 0.02
            gamma: 0.99
            encoding_size: 256
```

https://github.com/afewvowels/unity_00_09_chameleon",framework result working sac still converge set maximize instead single type behavior got blue time going back training link want try replicate behavior default trainer sac constant normalize false tau false simple extrinsic strength gamma chameleon curiosity strength gamma,issue,positive,negative,negative,negative,negative,negative
565663501,"I recently started training on SAC because I read how much more efficient it was and discovered the same behavior of inaction. I followed a tutorial on making a chameleon and came down in the morning to find 25 green chameleons which totally ignored their surroundings and the reward signals. They just immediately changed colors from the random start color to green and stayed there. Another simple test chamber that had an agent, a ball, and a goal resulted in the agent spinning in circles after 50k steps, despite having moved the ball into the goal several times earlier in the training.

I'm going back to PPO and having good results, I'd suggest you try the same.

EDIT: I just realized I never set the .NET framework in my SAC projects to 4.x and I'm wondering if that would explain the aberrant behavior. I'm going to change this setting and retrain and will let you know how that goes. It's frustrating seeing the agents not responding to the environment's reward structure.",recently training sac read much efficient discovered behavior inaction tutorial making chameleon came morning find green totally surroundings reward immediately color random start color green stayed another simple test chamber agent ball goal agent spinning despite ball goal several time training going back good suggest try edit never set framework sac wondering would explain aberrant behavior going change setting retrain let know go seeing environment reward structure,issue,positive,neutral,neutral,neutral,neutral,neutral
565521805,"Hmmm, it seems that the UnitySDK and Python trainer are using different definitions for an Episode:

Here is the code from the UnitySDK.Academy.EnvironmentReset:

        void EnvironmentReset()
        {
            m_StepCount = 0;
            m_EpisodeCount++;
            AcademyReset();
        }

You can see that m_EpisodeCount is being incremented with AcademyReset. The UnitySDK obviously considers an Episode to be the steps between AcademyResets.

However, from what you said, and from the python code it appears that the python trainer considers an Episode to be the steps between AgentReset and Done.

It would be nice if the UnitySDK and the python Trainers agreed on the definition of an Episode.",python trainer different episode code void see obviously episode however said python code python trainer episode done would nice python agreed definition episode,issue,positive,positive,positive,positive,positive,positive
565240245,"Initial packages didn't contain the side_channel directory.

After switching to find_packages(), the installed packages from https://test.pypi.org/project/mlagents/0.12.1.dev1/ worked for me.",initial contain directory switching worked,issue,negative,neutral,neutral,neutral,neutral,neutral
565209341,Thanks for the clarification! This is helpful. IMO it would be useful to have this definition in the documentation.,thanks clarification helpful would useful definition documentation,issue,positive,positive,positive,positive,positive,positive
565132684,"Hi @Phong13, thank you for the in-depth investigation!

AcademyReset is the global reset function - it's not meant for ""normal"" use. This is because within one scene, there are many Agents, and each Agent could end their episode at different times. We wouldn't want to reset all the agents if only one is finished! 

An episode with respect to the trainer is between an Agent.Reset and an Agent.Done. In most training scenarios using our trainer codebase, AcademyReset is never called since the agents just run until Done and reset themselves. An AcademyReset is used only when the Python code needs to force-reset all the agents (such as, need to change the environment parameters for the sampler or the next lesson in the curriculum), or in the case of the OpenAI gym wrapper, where the algorithm needs to call reset(). 

For your use case, I'd recommend doing the generalization randomization in the AgentReset, or simply using the sampler. Hopefully that helps!",hi thank investigation global reset function meant normal use within one scene many agent could end episode different time would want reset one finished episode respect trainer training trainer never since run done reset used python code need need change environment sampler next lesson curriculum case gym wrapper algorithm need call reset use case recommend generalization randomization simply sampler hopefully,issue,positive,positive,positive,positive,positive,positive
565060832,"Closing this, just saw that there's a start offset and an end offset, didn't realize I could set those differently to get the effect I wanted.

Love the toolkit!",saw start offset end offset realize could set differently get effect love,issue,positive,positive,positive,positive,positive,positive
565049045,"Depends what you mean, 
But what I do in my case is using a render texture as visual observations, 
and besides attaching it to the Agent's sensor script, also importing it into my agent.cs script. 
This allows looking at pixel level for instance. ",mean case render texture visual besides agent sensor script also script looking level instance,issue,negative,negative,negative,negative,negative,negative
564909494,"Solve by changing the reward structure and by tuning the hyperparameters.
~80% accuracy

",solve reward structure tuning accuracy,issue,positive,neutral,neutral,neutral,neutral,neutral
564785621,"Not exactly:

The bug is that without a sampler file, agents will over-fit and the
symptoms are as described.

Previously I was using ML-Agents v.08. In this version the sampler file did
not exist. It was not necessary because ML-Agents would automatically call
AcademyReset periodically (I think after each batch). The batch was the
episode. There was no bug because the AcademyReset was being called
regularly which randomized my initial conditions and the brain would
generalize. When I restarted with --load, the reward would recover to its
previous level very quickly.

Then I upgraded to v.10, BUT DID NOT ADD the sampler file (I did not
initially understand that it was needed). The bug would happen in this
scenario. Without the sampler file AcademyReset is only called once at the
beginning of training or when there is a switch to a new cirriculum stage.
There is only one episode per ciriculum stage. My agents would train on the
same start conditions for many-many hours. They would over-fit. If I
stopped and restarted with --load, AcademyReset would be called and the
initial conditions would change. My over-fitted-agent would have a hard
time with the different start conditions, reward woud be low, and training
would take a long time to recover. The depth of the loss-in-reward was
random because it depended on how close the new random start conditions
were to what had been trained previously. The final trained agent wasn't
great because it was over-fitted to one start condition (not generalized).
This is the bug.

Now I have added a sampler file and AcademyReset (new episode) is being
called regularly again. My agents are training nicely on a wide variety of
start conditions. If I restart with --load the reward recovers very quickly
and resumes as expected.

If there is no sampler file, then there is only one episode per cirriculum
stage and you will get an over-fitted agent. The symptoms are as described
in this bug.

On Wed, Dec 11, 2019 at 11:36 AM Ervin T. <notifications@github.com> wrote:

> Hi @Phong13 <https://github.com/Phong13>, thanks for the feedback! Just
> so I understand correctly, you're using a sampling configuration, right?
> This bug doesn't happen if you're not using --sampler?
>
> We do have one sampler config 3dball_generalize.yaml included with the
> repo.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/2932?email_source=notifications&email_token=ACWVKEM6DOSMRY4AH2KMM6DQYE6KJA5CNFSM4JO3XDR2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGUJ6OA#issuecomment-564698936>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACWVKEIKPXOIMIQAXOTQVALQYE6KJANCNFSM4JO3XDRQ>
> .
>


-- 
Ian Deane
DigitalOpus <http://digitalopus.ca>
Twitter: @DigitalOpus <https://twitter.com/DigitalOpus>
",exactly bug without sampler file previously version sampler file exist necessary would automatically call periodically think batch batch episode bug regularly initial brain would generalize load reward would recover previous level quickly add sampler file initially understand bug would happen scenario without sampler file beginning training switch new stage one episode per stage would train start would stopped load would initial would change would hard time different start reward low training would take long time recover depth random close new random start trained previously final trained agent great one start condition generalized bug added sampler file new episode regularly training nicely wide variety start restart load reward quickly sampler file one episode per stage get agent bug wed wrote hi thanks feedback understand correctly sampling configuration right bug happen sampler one sampler included reply directly view twitter,issue,positive,positive,neutral,neutral,positive,positive
564698936,"Hi @Phong13, thanks for the feedback! Just so I understand correctly, you're using a sampling configuration, right? This bug doesn't happen if you're not using `--sampler`?

We do have one sampler config `3dball_generalize.yaml` included with the repo - but you're right it's not obvious that it's a sampler config just from looking at it!",hi thanks feedback understand correctly sampling configuration right bug happen sampler one sampler included right obvious sampler looking,issue,negative,positive,positive,positive,positive,positive
564318841,"Hi @guillefix,
Glad that's working better now.

The editor always tries to connect on port 5004 
https://github.com/Unity-Technologies/ml-agents/blob/164d1ab98efc620b2e8c18e680e5fc99c19d69f1/UnitySDK/Assets/ML-Agents/Scripts/Academy.cs#L284
but you're trying to connect on 5006 which won't work.

As a side note, it looks like you're using recent code from master (side_channels were added after 0.12.0); you're probably better off using the `latest_release` tag (https://github.com/Unity-Technologies/ml-agents/tree/latest_release)",hi glad working better editor always connect port trying connect wo work side note like recent code master added probably better tag,issue,positive,positive,positive,positive,positive,positive
564308958,"I redownloaded ml-agents Unity SDK, and now it works without the errors.
However, I have the same issue as @wightwhale , with the API not connecting to Unity. I am running the 3D ball environment on Unity, and I have this on python

```
import matplotlib.pyplot as plt
import numpy as np
import sys

from mlagents.envs.environment import UnityEnvironment
from mlagents import envs
import mlagents.envs.side_channel
from mlagents.envs.side_channel.engine_configuration_channel import EngineConfig, EngineConfigurationChannel

env_name = None # to use the Unity editor
train_mode = True  # Whether to run the environment in training or inference mode
engine_configuration_channel = EngineConfigurationChannel()
env = UnityEnvironment(base_port = 5006, file_name=env_name, side_channels = [engine_configuration_channel])
```

and it hangs there, even though I press the play button on Unity",unity work without however issue unity running ball environment unity python import import import import import import import none use unity editor true whether run environment training inference mode even though press play button unity,issue,positive,positive,positive,positive,positive,positive
564293467,"Hi @eliofreitas,
Unfortunately we don't have a timeline for if/when online imitation learning will be brought back. I added your feedback to our internal tracker (MLA-422).",hi unfortunately imitation learning brought back added feedback internal tracker,issue,negative,negative,neutral,neutral,negative,negative
564285919,"Hi,
The ""Number of Training Brains"" log was misleading, and was cleaned up in the 0.12.0 release. Closing this issue.",hi number training brain log misleading release issue,issue,negative,neutral,neutral,neutral,neutral,neutral
564268857,Let's make an Installation template too (can be separate pr),let make installation template separate,issue,negative,neutral,neutral,neutral,neutral,neutral
564150330,@albertoxamin nice - I only could answer that question because I encountered it as well. Also ran into the module import error. glad to see you made an mr for that. ,nice could answer question well also ran module import error glad see made,issue,positive,positive,positive,positive,positive,positive
564082575,"Great, thank you. By the way - are you planning on providing a notebook that does some actual training in the future? I vaguely remember there existing one for PPO at some point, but I can't find it anymore.",great thank way providing notebook actual training future vaguely remember one point ca find,issue,positive,positive,neutral,neutral,positive,positive
563879586,Might be useful to add a description of why this change is being made in the commit message.,might useful add description change made commit message,issue,positive,positive,positive,positive,positive,positive
563878253,"If I can suggest a few easy improvements to help other people avoid this problem.

Add a comment in the source code mentioning the Sample file for the method Academy.AcademyReset so that if anyone tries to override it to randomize start conditions they likely need to use the --sampler parameter.

Also this documentation page with the section on Resetting the environment needs to mention the --sampler parameter:

https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Design-Academy.md

It should also have a link to this page about generalization.


https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Generalized-Reinforcement-Learning-Agents.md",suggest easy help people avoid problem add comment source code sample file method anyone override randomize start likely need use sampler parameter also documentation page section environment need mention sampler parameter also link page generalization,issue,negative,positive,positive,positive,positive,positive
563859854,"I finally discovered the cause of this!

One of the differences between v.08 and v.10 is the sampling file. I discovered that AcademyReset was only being called once in v.10 whereas in v.08 it was called frequently. This explains the dramatic drop when I use --load since the model has over-fitted to my initial random start conditions (which never change until I --load). When I add a sampling file, AcademyReset is being called frequently again and the agent is generalizing to different random start conditions. When I restart with --load there is only a very short drop in the reward.

I think you can close this if you like. Although you might want to add some example sampling .yaml files for the examples. There are none as of V.11. Any of the examples that do randomized start conditions in AcademyReset will have this problem. They will over-fit to one set of random start conditions and if restarted, there will be a huge loss in reward progress that needs to be re-learned.",finally discovered cause one sampling file discovered whereas frequently dramatic drop use load since model initial random start never change load add sampling file frequently agent different random start restart load short drop reward think close like although might want add example sampling none start problem one set random start huge loss reward progress need,issue,positive,negative,negative,negative,negative,negative
563839828,"So after days of thinking what could went wrong in my ml-agents.. i ended up creating a project from start and its working perfectly now... Not sure what was wrong, however i did manage to make it work so for everyone having the same issue, your problem might be in your vesions Good luck.",day thinking could went wrong ended project start working perfectly sure wrong however manage make work everyone issue problem might good luck,issue,positive,positive,neutral,neutral,positive,positive
563536528,"I've discovered this is probably proxy related. I'm behind a corporate proxy and it's causing all kinds of issues with package manager currently and it seems to interfere with the newer implementation of ml agents. I see you mentioned changing the port, do you think this could have to do with my new issue. I was operating on lower <.10 version of ml-agents without this issue. 
I noticed in your docs 

Cause: You have assigned HTTP_PROXY and HTTPS_PROXY values in your environment variables. Solution: Remove these values and try again.

However these values are suggested to be enabled for the package manager and unity hub.

Configuring your proxy server
When using a proxy server, configure the HTTP_PROXY and HTTPS_PROXY environment variables for the Unity Package Manager to use when performing requests against the Unity package registry.
https://docs.unity3d.com/Manual/upm-network.html",discovered probably proxy related behind corporate proxy causing package manager currently interfere implementation see port think could new issue operating lower version without issue cause assigned environment solution remove try however package manager unity hub proxy server proxy server configure environment unity package manager use unity package registry,issue,negative,negative,neutral,neutral,negative,negative
563510077,"Hi @chriselion ,
If the Behavior Type is ""Inference Only"" and there is no NN Model, the Unity Editor will throw
```
MissingReferenceException: The variable m_Model of BehaviorParameters doesn't exist anymore.
You probably need to reassign the m_Model variable of the 'BehaviorParameters' script in the inspector.
```
which means the external python process cannot control the agents in inference mode.

BTW, in previous versions, if I set the `train_mode` in `env.reset()` (e.g. `env.reset(train_mode=False)`), both agents in Unity Editor and executor would automatically run in inference mode. But now this configuration seems not working.",hi behavior type inference model unity editor throw variable exist probably need reassign variable script inspector external python process control inference mode previous set unity editor executor would automatically run inference mode configuration working,issue,negative,negative,neutral,neutral,negative,negative
563507561,I have now updated to the latest versions and verified the files and both reports v12. However my anaconda environment wont connect to unity still. I downloaded the latest version of unity 2019.2.15f1 as well.,latest however anaconda environment wont connect unity still latest version unity well,issue,negative,positive,positive,positive,positive,positive
563506196,I installed the barracuda preview package and now it doesn't have the errors but it still wont connect to unity.,barracuda preview package still wont connect unity,issue,negative,neutral,neutral,neutral,neutral,neutral
563495528,i made this change as I was walking some folks at Neurips through the workflow and couldn't find it.,made change walking could find,issue,negative,neutral,neutral,neutral,neutral,neutral
563495441,"i believe so.  many of the first time users walk through the installation doc as a first step.  Since its unlikely they will see the depricated docs sections, its important we call it out there as we don't want to an abandonment of the funnel",believe many first time walk installation doc first step since unlikely see important call want abandonment funnel,issue,negative,positive,positive,positive,positive,positive
563494110,"This is already linked from https://github.com/Unity-Technologies/ml-agents/tree/master/docs#deprecated-docs - do we really need to add another place where we say ""we don't really support this but...""?",already linked really need add another place say really support,issue,negative,positive,positive,positive,positive,positive
563489792,"hi @Taikatou - we are currently working on more multi-agent.  However, you can check out Arena (uses ML-Agents from a team at Oxford).  https://sites.google.com/view/arena-unity/home?authuser=0",hi currently working however check arena team,issue,negative,neutral,neutral,neutral,neutral,neutral
563469742,"Could someone give me a hint on how to set up my NN when using both, visual and vector observations? My agent only has a few vector observations and actions that initially don't require anything larger than 2 x 128 hidden units. Do I need to considerably increase the NN size after adding 84x84 pixel visual observations on top of that? Is there some rule of thumb along the lines of ""adding visual observations of a given size is equivalent to amount x of additional vector observations""? Thanks!",could someone give hint set visual vector agent vector initially require anything hidden need considerably increase size visual top rule thumb along visual given size equivalent amount additional vector thanks,issue,positive,positive,neutral,neutral,positive,positive
563467155,"Nothing surprising from the barracuda conversion:
```
Converting ./gpu_inference.pb to ./gpu_inference.nn
WARNING:tensorflow:From /Users/chris.elion/code/ml-agents/ml-agents/mlagents/trainers/tensorflow_to_barracuda.py:1541: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.

IGNORED: StopGradient unknown layer
GLOBALS: 'is_continuous_control', 'version_number', 'memory_size', 'action_output_shape'
IN: 'vector_observation': [-1, 1, 1, 894] => 'main_graph_0/hidden_0/BiasAdd'
IN: 'epsilon': [-1, 1, 1, 2] => 'mul'
OUT: 'action', 'action_probs'
DONE: wrote ./gpu_inference.nn file.
```
I'm going to have to let the Barracuda folks handle this after the conference.",nothing surprising barracuda conversion converting warning name please use instead unknown layer done wrote file going let barracuda handle conference,issue,negative,positive,positive,positive,positive,positive
563434578,"Hi @BlueFisher,
In 0.12.0, you can specify that Agents run inference even if there's a python process attached by changing the Behavior Type in the Behavior Parameters configuration to ""Inference Only""

![image](https://user-images.githubusercontent.com/6877802/70472004-34608480-1a83-11ea-96a2-098e1f3afa05.png)
",hi specify run inference even python process attached behavior type behavior configuration inference image,issue,negative,neutral,neutral,neutral,neutral,neutral
563427537,Is it possible to share some kind of graphics context between two unity instance ?,possible share kind graphic context two unity instance,issue,positive,positive,positive,positive,positive,positive
563421577,"Thanks @chriselion - I realised that I still had the ""Behavior Type"" drop-down set to ""Heuristic Only"". I switched to ""Default"" and now the training is racing along nicely.",thanks still behavior type set heuristic switched default training racing along nicely,issue,positive,positive,positive,positive,positive,positive
563408940,Hi @fredleefarr - the model file doesn't get updated automatically. There's a better description of how to set it here: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md#after-training,hi model file get automatically better description set,issue,negative,positive,positive,positive,positive,positive
563407216,"> Over all this PR looks good to me, but since it's rather large it would be good to hear some idea of what cases you've tested. Have you retrained all scened using cloud training for this branch? Have you tested it on Windows?

+1 for cloud training",good since rather large would good hear idea tested cloud training branch tested cloud training,issue,positive,positive,positive,positive,positive,positive
563402322,"Thanks, Cpu inference is perfectly fine for me now, no rush 👍 ",thanks inference perfectly fine rush,issue,positive,positive,positive,positive,positive,positive
563399455,"Hi @DVonk,
Sorry for the problem. You're correct that the notebook is out of date.

If you run
```python
env.step()
```
after creating the `UnityEnvironment`, the brain information will be synced and `env.external_brain_names` will be non-empty.

We'll make sure the notebook gets tested more automatically in the future, to prevent things like this.",hi sorry problem correct notebook date run python brain information make sure notebook tested automatically future prevent like,issue,negative,neutral,neutral,neutral,neutral,neutral
563371750,Thanks @albertoxamin. We need some better checks to make sure these docs stay up-to-date; hopefully we'll get to that soon.,thanks need better make sure stay hopefully get soon,issue,positive,positive,positive,positive,positive,positive
563369647,"Thanks @fog9001 - I'll take a look today and see if anything jumps out, but it will probably need the Barracuda team to look into this, and they're all at the NeurIPS conference this week.",thanks fog take look today see anything probably need barracuda team look conference week,issue,negative,positive,positive,positive,positive,positive
563368822,"Unity engine currently supports only one GPU at the time, the same limitation applies to Barracuda. The only way to use both cards is to run two instances of Unity standalone build and assigning them to different cards, but if you need outputs from both networks in the same instance of application, then you would need to do some sort of IPC and send data between them.",unity engine currently one time limitation barracuda way use run two unity build different need instance application would need sort send data,issue,negative,neutral,neutral,neutral,neutral,neutral
563365383,Thanks for catching this. As soon as you sign the CLA I'll merge it in.,thanks catching soon sign merge,issue,negative,positive,positive,positive,positive,positive
563362574,"Hi @guillefix,
What are the errors you're seeing? I get a lot of warnings in 2019.2.15f1 (haven't tried .13 yet) but no errors.",hi seeing get lot tried yet,issue,negative,neutral,neutral,neutral,neutral,neutral
563351479,@yangbo That checkbox no longer exists since version 0.11.0. Can you please open a new issue explaining your problem?,longer since version please open new issue explaining problem,issue,negative,positive,neutral,neutral,positive,positive
563221975,"Thanks @olgn, that solved that issue. However I encountered another one because the example was outdated. So I opened #3055 with the fixes.",thanks issue however another one example outdated,issue,negative,negative,negative,negative,negative,negative
563134301,"Heya, I was directed here from [this](https://github.com/Unity-Technologies/ml-agents/issues/2984) feature request. Where exactly can I find the part that covers the tensorboard C# variable functionality? Will my input still be relevant?",directed feature request exactly find part variable functionality input still relevant,issue,negative,positive,positive,positive,positive,positive
563130445,"Hello, Someone could add barracuda label ?",hello someone could add barracuda label,issue,negative,neutral,neutral,neutral,neutral,neutral
563121681,"Thanks anyway! Yeah I think It really has to do with when I call this piece of code in Unity instead of how the model works, because it does work within the update loop. 

Already tried things like calling the model with  [WaitForEndOfFrame ](https://docs.unity3d.com/ScriptReference/WaitForEndOfFrame.html) , but that didn't help either",thanks anyway yeah think really call piece code unity instead model work work within update loop already tried like calling model help either,issue,positive,positive,positive,positive,positive,positive
563110820,"Hi,

Sure, sharing Zip file containing ´frozen_graph_def.pb´ and model (.nn).

https://drive.google.com/file/d/1lQ6JmyLSGdHcpRyK88wQIbQhr93wtAp0/view?usp=sharing

Thank you @chriselion ",hi sure zip file model thank,issue,positive,positive,positive,positive,positive,positive
563024071,"Hi @fog9001,
Can you share a copy of the frozen graph that is output from training? Even if it just has random weights (no training run), that should help us diagnose the differences.

cc @mantasp ",hi fog share copy frozen graph output training even random training run help u diagnose,issue,positive,negative,negative,negative,negative,negative
563016877,"For anyone trying to use models from Unity in TensorFlow.js, here is the approach I took.

* Install Unity ML-Agents for development by following [this guide here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md#installing-for-development)
* In `ml-agents/mlagents/trainers/tf_policy.py` script, add the following lines to the `export_model` function to export a TensorFlow SavedModel.  Note that the required graph nodes are different for continuous action space (i.e. the agent takes action on float values) and discrete action space (i.e. the agent takes action on integer values), so you will need to uncomment the respective lines accordingly:

```
def export_model(self):
        """"""
        Exports latest saved model to .nn format for Unity embedding.
        """"""

        with self.graph.as_default():
            graph_def = self.graph.as_graph_def()

            # BEGINNING OF ADDED CODE:
            # To learn more about nodes of the graph, uncomment the following lines:
            # for node in graph_def.node:
            #     print(""-------"")
            #     print(node.name)
            #     print(node.input)
            #     print(node.attr)
 
            # Uncomment for discrete vector action space:
            # vectorInputNode = self.graph.get_tensor_by_name(""vector_observation:0"")
            # actionMaskInput = self.graph.get_tensor_by_name(""action_masks:0"")
            # actionOutputNode = self.graph.get_tensor_by_name(""action:0"")
            # sigs = {}
            # sigs[tf.compat.v1.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] = \
                tf.saved_model.signature_def_utils.predict_signature_def( \
                    {""in"": vectorInputNode, ""actionMask"": actionMaskInput}, {""out"": actionOutputNode})
            
            # Uncomment for continuous vector action space:
            # vectorInputNode = self.graph.get_tensor_by_name(""vector_observation:0"")
            # epsilonInputNode = self.graph.get_tensor_by_name(""epsilon:0"")
            # actionOutputNode = self.graph.get_tensor_by_name(""action:0"")
            # sigs = {}
            # sigs[tf.compat.v1.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] = \
            #     tf.saved_model.signature_def_utils.predict_signature_def( \
            #         {""in"": vectorInputNode,""epsilon"": epsilonInputNode}, {""out"": actionOutputNode})



            
            builder = tf.compat.v1.saved_model.Builder(self.model_path + ""/SavedModel/"")
            builder.add_meta_graph_and_variables( \
                self.sess, \
                [tf.saved_model.tag_constants.SERVING], \
                signature_def_map=sigs, \
                strip_default_attrs=True)
            builder.save()                                
            # END OF ADDED CODE
           
            target_nodes = "","".join(self._process_graph())
            
            output_graph_def = graph_util.convert_variables_to_constants(
                self.sess, graph_def, target_nodes.replace("" "", """").split("","")
            )
            frozen_graph_def_path = self.model_path + ""/frozen_graph_def.pb""
            with gfile.GFile(frozen_graph_def_path, ""wb"") as f:
                f.write(output_graph_def.SerializeToString())
            tf2bc.convert(frozen_graph_def_path, self.model_path + "".nn"")
            logger.info(""Exported "" + self.model_path + "".nn file"")
     
```

* the exported SavedModel should be able to be converted to a web model by the latest [tensorflowjs-converter](https://github.com/tensorflow/tfjs) (tfjs version 1.4.0, currently)

If anyone can clarify the different output nodes: `action` vs. `action_probs/action_probs`, please let me know!",anyone trying use unity approach took install unity development following guide script add following function export note graph different continuous action space agent action float discrete action space agent action integer need respective accordingly self latest saved model format unity beginning added code learn graph following node print print print print discrete vector action space action continuous vector action space epsilon action epsilon builder end added code file able converted web model latest version currently anyone clarify different output action please let know,issue,positive,positive,positive,positive,positive,positive
562930514,I have installed Barracuda preview and I still get the errors,barracuda preview still get,issue,negative,neutral,neutral,neutral,neutral,neutral
562882569,you have the wrong version of tensorflow installed. as stated in the baselines installation instructions [here](https://github.com/openai/baselines#installation) tensorflow 1.14 is the latest tf version supported - and you have 2.0 installed,wrong version stated installation latest version,issue,negative,neutral,neutral,neutral,neutral,neutral
562830254,"What's wierd is that is used to work and i didn't change anything. Is it possilbe that there's an error in my script or smth? The error shows that the line 99 was triggered and it should recieve parameters, so is it just because it can't connect or it could also be the script?
And this are my results from pip freeze:

```absl-py==0.8.1
astor==0.8.0
attrs==19.3.0
backcall==0.1.0
bleach==3.1.0
cachetools==3.1.1
certifi==2019.11.28
chardet==3.0.4
cloudpickle==1.2.2
colorama==0.4.1
cycler==0.10.0
decorator==4.4.1
defusedxml==0.6.0
entrypoints==0.3
gast==0.2.2
google-auth==1.7.2
google-auth-oauthlib==0.4.1
google-pasta==0.1.8
grpcio==1.25.0
h5py==2.10.0
idna==2.8
importlib-metadata==1.2.0
ipykernel==5.1.3
ipython==7.10.1
ipython-genutils==0.2.0
ipywidgets==7.5.1
jedi==0.15.1
Jinja2==2.10.3
jsonschema==3.2.0
jupyter==1.0.0
jupyter-client==5.3.4
jupyter-console==6.0.0
jupyter-core==4.6.1
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.0
kiwisolver==1.1.0
Markdown==3.1.1
MarkupSafe==1.1.1
matplotlib==3.1.2
mistune==0.8.4
# Editable install with no version control (mlagents==0.10.0)
-e c:\ml-agents-0.10.0\ml-agents
# Editable install with no version control (mlagents-envs==0.10.0)
-e c:\ml-agents-0.10.0\ml-agents-envs
more-itertools==8.0.1
nbconvert==5.6.1
nbformat==4.4.0
notebook==6.0.2
numpy==1.17.4
oauthlib==3.1.0
opencv-contrib-python==4.1.1.26
opt-einsum==3.1.0
pandocfilters==1.4.2
parso==0.5.1
pickleshare==0.7.5
Pillow==6.2.1
prometheus-client==0.7.1
prompt-toolkit==2.0.10
protobuf==3.11.1
pyasn1==0.4.8
pyasn1-modules==0.2.7
Pygments==2.5.2
pyparsing==2.4.5
pypiwin32==223
pyrsistent==0.15.6
python-dateutil==2.8.1
pywin32==227
pywinpty==0.5.7
PyYAML==5.2
pyzmq==18.1.1
qtconsole==4.6.0
requests==2.22.0
requests-oauthlib==1.3.0
rsa==4.0
Send2Trash==1.5.0
six==1.13.0
tensorboard==1.15.0
tensorflow==1.15.0
tensorflow-estimator==1.15.1
termcolor==1.1.0
terminado==0.8.3
testpath==0.4.4
tornado==6.0.3
traitlets==4.3.3
urllib3==1.25.7
wcwidth==0.1.7
webencodings==0.5.1
Werkzeug==0.16.0
widgetsnbextension==3.5.1
wincertstore==0.2
wrapt==1.11.2
zipp==0.6.0```
`
And thank you for your time and help really.",used work change anything error script error line triggered ca connect could also script pip freeze install version control install version control thank time help really,issue,negative,positive,positive,positive,positive,positive
562811575,"> My fault! Did not have Control ticked in Academy script. Cheers

Can you show how to tick the Control in Academy script? Thanks.",fault control ticked academy script show tick control academy script thanks,issue,negative,positive,positive,positive,positive,positive
562799038,"One thing I didn't realize earlier is that we lost some of the exception information coming from here
https://github.com/Unity-Technologies/ml-agents/blob/22520fe4724c7c592afcc37069e723036b5521f5/UnitySDK/Assets/ML-Agents/Scripts/Grpc/RpcCommunicator.cs#L92-L103
because this is the expected path when doing inference. Not the end of the world though.",one thing realize lost exception information coming path inference end world though,issue,negative,neutral,neutral,neutral,neutral,neutral
562797116,"Can you paste the results from running `pip freeze`?  If you're installing things with `pip install mlagents==...` then you shouldn't need to do `pip install -e` - the first one installs the packages from the python package repository, the second one makes an ""editable"" install from the local source.

Besides that and the https://github.com/Unity-Technologies/ml-agents/blob/master/docs/FAQ.md#environment-connection-timeout I'm running out of ideas...",paste running pip freeze pip install need pip install first one python package repository second one install local source besides running,issue,negative,positive,neutral,neutral,positive,positive
562687183,"Sorry, I retract what I had said. It looks like you are using a another supported way to invoke barracuda.  I should have kept reading :P.   Maybe @mantasp could help out here.",sorry retract said like another way invoke barracuda kept reading maybe could help,issue,positive,negative,negative,negative,negative,negative
562679527,"Otherwise, you may need to have your tensor in `NHWC` format.  Which means you'll need to add the batch size to the beginning. ",otherwise may need tensor format need add batch size beginning,issue,negative,neutral,neutral,neutral,neutral,neutral
562679015,"Hi @MarkTension,
According do the barracuda docs, it looks like you might need to pass two tensors in a Dictionary.
https://docs.unity3d.com/Packages/com.unity.barracuda@0.3/manual/index.html#execute-the-model

Could you try this and let us know if you get the results you are expecting?
Thanks.",hi according barracuda like might need pas two dictionary could try let u know get thanks,issue,positive,positive,positive,positive,positive,positive
562631225,I discovered the issue was that my SDK version on the windows machine was using mlagents 11 to build the project.,discovered issue version machine build project,issue,negative,neutral,neutral,neutral,neutral,neutral
562622667,"Yes, using the sharedMaterial property may or may not be appropriate for what you are doing.  You will need to destroy any material clones that you’ve created using the `Destroy` function as you stated.",yes property may may appropriate need destroy material destroy function stated,issue,negative,positive,neutral,neutral,positive,positive
562588173,"I'm currently retraining the application and it seems to have not really been resolved as the memory usage of Unity is now at around 7gb.

Could it be that simply setting .sharedMaterial isn't good enough?

If that's the case, is there another solution or should I Dispose the materials themselves somehow? (Should this be done by doing something like `Destroy(ga.GetComponent<MeshRenderer>().material)`)",currently application really resolved memory usage unity around could simply setting good enough case another solution dispose somehow done something like destroy,issue,positive,positive,positive,positive,positive,positive
562514856,"@surfnerd , thanks for this, I'm sorry for the confusion around this.

I couldn't find a way to Dispose a material.

What I'll try is actually setting the meshRenderer.sharedMaterial instead of the meshRenderer.material. I'm not sure if this will solve the issue as well? (The only thing I could find on google was that sharedMaterial returns the actual reference whereas .material returns a copy for this specific object. But it doesn't state anything about setting it).

So for now I'll implement this patch:
Old
```
renderer.material = foundPiece.Mat;
```

New
```
renderer.sharedMaterial = foundPiece.Mat;
```


Another question I have is, do I also need to do something similar for the MiniMap? I'm using the following code to update the images there:
```
var img = ga.GetComponent<Image>();
img.sprite = foundPiece.Sprite;
```",thanks sorry confusion around could find way dispose material try actually setting instead sure solve issue well thing could find actual reference whereas copy specific object state anything setting implement patch old new another question also need something similar following code update image,issue,positive,positive,neutral,neutral,positive,positive
562489864,"Okay, I've reinstalled everything and made new enviroment, however i still recieve the same error.",everything made new however still error,issue,negative,positive,positive,positive,positive,positive
562431720,"> Hi,
> If I change my setup so that I have version 0.12.0 of the python library, and version 0.10.0 of the C# code, I get this same error.
> 
> One thing that could be happening is that you might have multiple versions of mlagents installed, one in a virtual environment (like venv or conda) and another ""system"" install. So even if `pip show mlagents` says 0.10.0, when you run `mlagents-learn` it might be executing a different version of the script.
> 
> I would recommend
> 
> 1. run `pip uninstall -y mlagents`
> 2. run `mlagents-learn` - this should give an error like ""No such file or directory"" (I'm on mac but it would be similar on windows). If it doesn't give an error, you need to figure out what is being run and remove it. On mac and linux, you can run `which mlagents-learn` to get the path of the executable (I believe `where` is the equivalent windows command)
> 3. run `pip install mlagents==0.10.0` (preferably using venv or conda)

Hi,

I did all the steps you wrote and the problem is still there. Do you think deleting ml-agents completely and installing it again could solve problem?

EDIT: Alright, i did decided to reinstall everything and as i tought, i found 2 python versions installed(removed one), directed to `cd ml-agents-envs` and ran `pip install -e ./` and returns me this error:` ERROR: mlagents 0.10.0 has requirement mlagents-envs==0.10.0, but you'll have mlagents-envs 0.8.2 which is incompatible.`. If i run `pip install mlagents-envs==0.8.2`, `pip install -e ./` still returns the same error. Is there any way of deleeting everything and fresh installing so i wont be facing this issues while install? 
Installing from `cd ml-agents` still returns me `ERROR: Package 'mlagents' requires a different Python: 3.7.5 not in '>=3.6,<3.7'` after my upgrade on python. 
At the end of `pip install mlagents` i see this: 
`Installing collected packages: mlagents-envs
  Found existing installation: mlagents-envs 0.8.2
    Uninstalling mlagents-envs-0.8.2:
      Successfully uninstalled mlagents-envs-0.8.2
Successfully installed mlagents-envs-0.10.0`
which means i have 2 versions? i did delete one but it was mlagents-envs-0.10.0 which just get reinstalled. Its a mess i know... Best way prob. is to delete completely everything and reinstall.",hi change setup version python library version code get error one thing could happening might multiple one virtual environment like another system install even pip show run might different version script would recommend run pip run give error like file directory mac would similar give error need figure run remove mac run get path executable believe equivalent command run pip install preferably hi wrote problem still think completely could solve problem edit alright decided reinstall everything tought found python removed one directed ran pip install error error requirement run pip install pip install still error way everything fresh wont facing install still error package different python upgrade python end pip install see collected found installation successfully uninstalled successfully delete one get mess know best way prob delete completely everything reinstall,issue,negative,positive,positive,positive,positive,positive
562381296,Would it be possible to add the logic to save it every checkpoint?,would possible add logic save every,issue,negative,neutral,neutral,neutral,neutral,neutral
562371453,"Yep, it makes sense, since it's episodes completed since last policy update. Every time it prints out a message, it's one policy update. So no episode was completed between the two messages, hence it's nan. By default SAC updates every environment step, so it's unlikely you'll see many non-nans. ",yep sense since since last policy update every time message one policy update episode two hence nan default sac every environment step unlikely see many,issue,negative,neutral,neutral,neutral,neutral,neutral
562368238,"@ervteng that's what I was thinking but does it make sense, in the screenshot, that an episode was completed and yet the next few returns were still NaN? 
I can try to save the output to double check, but I don't see a single non-NaN in the returns. 

On the other hand, it appears to be training ""correctly"" as far as I can see.  I just want to make sure these NaN values don't affect the training (which seems to be the case). ",thinking make sense episode yet next still nan try save output double check see single hand training correctly far see want make sure nan affect training case,issue,positive,positive,positive,positive,positive,positive
562361845,"Hi @niskander, the code outputs nan there (when running in `--debug` mode) when there hasn't been a completed episode since the last policy update. Since SAC updates policy much more frequently than PPO, the probability of getting a nan in that message is much higher. However, the step logs happen at a much longer interval, so usually there has been a completed episode by then. The nans are entirely harmless. 

@chriselion you're probably right, we should output None or Invalid in that message (and coincidentally in the CSV log). ",hi code nan running mode episode since last policy update since sac policy much frequently probability getting nan message much higher however step happen much longer interval usually episode entirely harmless probably right output none invalid message coincidentally log,issue,negative,positive,neutral,neutral,positive,positive
562356357,"If it helps, the Cumulative Reward graph in tensorboard contains valid values. ",cumulative reward graph valid,issue,positive,neutral,neutral,neutral,neutral,neutral
562354485,"@chriselion This is literally all I'm seeing (it's never not NaN), despite the rewards always being valid in  the ""Step"" logs. So why would `cumulative_returns_since_policy_update `always be empty? ",literally seeing never nan despite always valid step would always empty,issue,negative,negative,neutral,neutral,negative,negative
562351766,"I think the `NaN`s are coming from here
https://github.com/Unity-Technologies/ml-agents/blob/0796a1b5ff68a9e9b2b456d7088245edb2c50add/ml-agents/mlagents/trainers/sac/trainer.py#L241-L244
when `self.cumulative_returns_since_policy_update` is empty. It should be harmless, but we should probably output 0 or None in that situation.

@ervteng Any other thoughts?",think nan coming empty harmless probably output none situation,issue,negative,negative,neutral,neutral,negative,negative
562339704,I can reproduce similar behavior with our examples (it's only NaN sometimes but not every time). I'll look into it.,reproduce similar behavior nan sometimes every time look,issue,negative,neutral,neutral,neutral,neutral,neutral
562337292,"Please take a look at the suggestions I made on https://github.com/Unity-Technologies/ml-agents/issues/3033#issuecomment-562321239 - I think they apply here too. It's possible that even if you `pip install` a specific version of mlagents, running `mlagents-learn` will run a different version.
",please take look made think apply possible even pip install specific version running run different version,issue,negative,neutral,neutral,neutral,neutral,neutral
562321239,"Hi,
If I change my setup so that I have version 0.12.0 of the python library, and version 0.10.0 of the C# code, I get this same error.

One thing that could be happening is that you might have multiple versions of mlagents installed, one in a virtual environment (like venv or conda) and another ""system"" install. So even if `pip show mlagents` says 0.10.0, when you run `mlagents-learn` it might be executing a different version of the script.

I would recommend

1.  run `pip uninstall -y mlagents`
2.  run `mlagents-learn` - this should give an error like ""No such file or directory"" (I'm on mac but it would be similar on windows). If it doesn't give an error, you need to figure out what is being run and remove it. On mac and linux, you can run `which mlagents-learn` to get the path of the executable (I believe `where` is the equivalent windows command)
3. run `pip install mlagents==0.10.0` (preferably using venv or conda)",hi change setup version python library version code get error one thing could happening might multiple one virtual environment like another system install even pip show run might different version script would recommend run pip run give error like file directory mac would similar give error need figure run remove mac run get path executable believe equivalent command run pip install preferably,issue,negative,neutral,neutral,neutral,neutral,neutral
562316179,"@surfnerd Deleting the xml file does indeed fix the issue. Thanks!

@caioc2 Yeah it would definitely be great if they were actually compatible. ",file indeed fix issue thanks yeah would definitely great actually compatible,issue,positive,positive,positive,positive,positive,positive
562281798,"Sorry, this got automatically closed when we deleted the `develop` branch, and it looks like I can't reopen or change the target branch",sorry got automatically closed develop branch like ca reopen change target branch,issue,negative,negative,negative,negative,negative,negative
562254480,"And right at the beginning I copied the ML-agents folder in unity assets which contained scripts as well as training examples that worked, except for those that required the ""Heuristic"" method

What version of python should be installed for version 0.11 of ml agents?",right beginning copied folder unity asset well training worked except heuristic method version python version,issue,positive,positive,positive,positive,positive,positive
562252609,"Hi @niskander,
We added a `link.xml` file between 0.9 and 0.12 under the Plugins/ProtoBuffer folder.  If you remove this, you should be able to link with your macro modifications.  We are still investigating the best way to handle this issue.  Thank you for your patience. ",hi added file folder remove able link macro still investigating best way handle issue thank patience,issue,positive,positive,positive,positive,positive,positive
562250763,I did not upgrade to version 0.11. I immediately started working in it.,upgrade version immediately working,issue,negative,neutral,neutral,neutral,neutral,neutral
562250058,"hi @SERGZV - if you are upgrading to 0.11, make sure to copy the files from /ml-agents/UnitySDK/Assets/ML-Agents/Scripts/ from the 0.11 release to your project as well. ",hi make sure copy release project well,issue,positive,positive,positive,positive,positive,positive
562248326,"I wrote the code for the lesson version 0.8. However, I originally used version 0.11 since there is no video tutorial for version 0.11. The comments in the video tutorial indicated what changes need to be made to work in version 0.11. Namely, the override virtual method ""Heuristic"" in the agent class, which inherits from the agent class. What changes I need to make to the code to work in version 0.11 I honestly don’t know.",wrote code lesson version however originally used version since video tutorial version video tutorial need made work version namely override virtual method heuristic agent class agent class need make code work version honestly know,issue,negative,positive,positive,positive,positive,positive
562243789,"Since the materials have the (Instance) tag, it tells me you need to dispose of them manually.  This should solve your memory leak.",since instance tag need dispose manually solve memory leak,issue,negative,neutral,neutral,neutral,neutral,neutral
562240182,"I do, however, see your material memory growing significantly.  It grew from around 20MB from about 1 minute after the start to about 70MB 10 minutes later.  

Here is a screen shot of a small portion of the Material memory.  
![Screen Shot 2019-12-05 at 9 51 14 AM](https://user-images.githubusercontent.com/1356616/70260317-deb76f80-1744-11ea-81bc-408d3d0b7868.png)


Please see make sure you are correctly disposing of materials that are no longer being used, or ensure to mark them as shared if you are going to reuse them.  I would guess that this could be a leak from the random generation of your track, but I'm not sure.  

I hope this helps.",however see material memory growing significantly grew around minute start later screen shot small portion material memory screen shot please see make sure correctly longer used ensure mark going reuse would guess could leak random generation track sure hope,issue,positive,positive,positive,positive,positive,positive
562239778,"hi @SERGZV - which version of the C# code from ML-Agents are you using (i.e. 0.8, 0.9, etc..)?  When you upgrade to 0.11, you will need to upgrade both the C# scripts and Python.",hi version code upgrade need upgrade python,issue,negative,neutral,neutral,neutral,neutral,neutral
562237241,"So the large amount of `INT64`, `String`, and `Byte[]` seems to be coming from the profiler itself, which is unfortunate noise.  I'll keep digging...",large amount string coming profiler unfortunate noise keep digging,issue,negative,negative,negative,negative,negative,negative
562236919,"Hi @albertoxamin,
This import is coming from OpenAI baselines, not ml-agents. It's likely that they're not compatible with the latest version of tensorflow.",hi import coming likely compatible latest version,issue,negative,positive,positive,positive,positive,positive
562235379,"Hey @devedse,
I was able to reproduce the leak without training.  I'm just running the RaceGameScene and seeing large buffers of `UINT64` and `System.Byte[]`.  I'm still not sure if it's coming from ML-Agents or not.  I will dig a bit further.",hey able reproduce leak without training running seeing large still sure coming dig bit,issue,negative,positive,positive,positive,positive,positive
562234920,"So. For training ""ML-Agents"" I use unity version 2019.2.12f1
In Project settings> Player: I changed ""NET 2.x"" to ""NET 4.x""
Using the ""unity hub"" downloaded and installed a new version of unity jdk sdk and ndk
(cool done now no need to go and download from external pages, if only ML-Agents and the right version of Python could also be installed)
The python version was installed 3.6.6 since everywhere where I didn’t read it was advised to install this particular 3.6.x. However, since you said python, you also need to update
I uninstalled the old python version and installed the python version 3.8.0.
And launched the console again. Now in the console when entering commands:
<<mlagents-learn config/trainer_config.yaml --curriculum=config/curricula/Penguin --run-id=Project_0 --train>> & <<pip3 install mlagents.>>
I get an error: Is not an internal or external command, executable program or batch file.
Now no matter what version of python I install, I get the same error in the console :-(
The point cannot be in the wrong path or command, because the path and command were saved in my notebook and all the time I just copied them to the console like this time. This problem appeared just after reinstalling python",training use unity version project player net net unity hub new version unity cool done need go external right version python could also python version since everywhere read advised install particular however since said python also need update uninstalled old python version python version console console entering train pip install get error internal external command executable program batch file matter version python install get error console point wrong path command path command saved notebook time copied console like time problem python,issue,negative,positive,neutral,neutral,positive,positive
562233131,"Hey, that's great thanks. It could very well be some misunderstanding for me about how specific things in Unity work but let's see.

I don't use visual observations yet. The inputs of the ml agent are a bunch of Ray traces that input booleans wether they hit the road or hit the grass.",hey great thanks could well misunderstanding specific unity work let see use visual yet agent bunch ray input wether hit road hit grass,issue,positive,positive,positive,positive,positive,positive
562231787,"I think it is part of a big problem which is stated in the issue #2595, and yet has no solution.

IMHO, we **need** both enabled at the same time, Mono is far slower than il2cpp. For complex scenes we could easily speed up the training by 2x or more just by having it compiled with il2cpp.",think part big problem stated issue yet solution need time mono far complex could easily speed training,issue,negative,positive,neutral,neutral,positive,positive
562227065,I got the invite.  I'll see if I can take some time today to run a profiling trace myself.  We haven't seen any memory leaks with our nightly training sessions. But maybe you are hitting a case that we haven't covered.  Thanks for your cooperation and help debugging this.,got invite see take time today run trace seen memory nightly training session maybe case covered thanks help,issue,positive,positive,positive,positive,positive,positive
562146686,"Something like:

VisualObservation ----->  VisualEncoder -----> Visual
VectorObservation -----> VectorEncoder -----> Vector

Concat(Visual, Vector) ----> Output Stream(s)

You can dig exacly how it is [here](https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/models.py)",something like visual vector visual vector output stream dig,issue,negative,neutral,neutral,neutral,neutral,neutral
562136142,"GPU's are faster for big amout of data, be it from big networks with convolutional layers and visual obsevations and/or large batch sizes.
If you are using small networks with a few vector observations and/or small batch sizes, CPU is usually better.",faster big data big convolutional visual large batch size small vector small batch size usually better,issue,negative,positive,neutral,neutral,positive,positive
562111072,"I think that's it, thanks! I haven't resolved the issue yet, but I know where to look now (Anaconda/pip). In any case, it doesn't seem to be a bug in mlagents so I'll close this bug report.",think thanks resolved issue yet know look case seem bug close bug report,issue,positive,positive,positive,positive,positive,positive
562107526,@chriselion I have the up to date python mlagents installed its just updating from 0.8 to 10 or even using any version of 10 -> breaks a lot of the python api on windows and the jupyter notebook doesnt seem to want to work with a blank project,date python even version lot python notebook doesnt seem want work blank project,issue,negative,neutral,neutral,neutral,neutral,neutral
562098551,"Yes that is perfect, many thanks.",yes perfect many thanks,issue,positive,positive,positive,positive,positive,positive
562075008,"When digging through the Memory Profile I see a lot of arrays with empty values, e.g.:
![image](https://user-images.githubusercontent.com/2350015/70228391-9607b280-1754-11ea-84f0-1306213c7acb.png)

The same happens with arrays for UINT64 and String. Loads of 0 values or null values.",digging memory profile see lot empty image string null,issue,negative,negative,neutral,neutral,negative,negative
562057907,"I've been running the memory profiler.

Screenshot of memory usage just after start:
![image](https://user-images.githubusercontent.com/2350015/70224318-5e493c80-174d-11ea-80a5-5c8b153574ba.png)

Screenshot of memory usage afte 5 minutes of running:
![image](https://user-images.githubusercontent.com/2350015/70224342-66a17780-174d-11ea-8359-6205ec27dca3.png)

And the diff:
![image](https://user-images.githubusercontent.com/2350015/70224459-98b2d980-174d-11ea-88dc-d0eb7f4fd8bd.png)

I've also created a private repository which I will add you to. To actually start training I'm currently using the TrainRace.cmd script from our fork of ml-agents:
https://github.com/devedse/ml-agents/blob/master/TrainRace.cmd

You'll receive an invite for the other private repository shortly :smile:",running memory profiler memory usage start image memory usage running image image also private repository add actually start training currently script fork receive invite private repository shortly smile,issue,negative,positive,neutral,neutral,positive,positive
561979864,"> Hi Jan,
> A common cause of this is a mismatch between the versions python and c# code. Can you please tell me
> 
> * the version number when you run `pip show mlagents` (or `pip3`)
> * the value of `Academy.kApiVersion` or `k_ApiVersion` - this should be something like ""API-123""
>   ?
> 
> I'm not sure exactly what versions the penguin tutorial uses, but if you have a specific version of the python package, you can get the corresponding C# code from https://github.com/Unity-Technologies/ml-agents/tree/x.y.z/, where `x.y.z` is the version.

Hi,
1) So my version is: 0.10.0, i have installed ml-agents 0.8.2 because i just wanted to learn basics of this and most tutorials are in older versions.

2) My kApiVersion is ""API-10"";

Okay, so i tried training my agent without curriculum and it didnt start again. Although i was able to train my agents in the past, but not with this project",hi common cause mismatch python code please tell version number run pip show pip value something like sure exactly penguin tutorial specific version python package get corresponding code version hi version learn older tried training agent without curriculum didnt start although able train past project,issue,positive,positive,positive,positive,positive,positive
561977184,"Hi Jan,
A common cause of this is a mismatch between the versions python and c# code. Can you please tell me 
* the version number when you run `pip show mlagents` (or `pip3`)
* the value of `Academy.kApiVersion` or `k_ApiVersion` - this should be something like ""API-123""
?

I'm not sure exactly what versions the penguin tutorial uses, but if you have a specific version of the python package, you can get the corresponding C# code from https://github.com/Unity-Technologies/ml-agents/tree/x.y.z/, where `x.y.z` is the version.",hi common cause mismatch python code please tell version number run pip show pip value something like sure exactly penguin tutorial specific version python package get corresponding code version,issue,positive,positive,positive,positive,positive,positive
561968622,"hi - we think the problem is that we switched the port for training in 0.11.  And if you had a version mismatch between the C# SDK and Python, it would never hit the error because of the port change.

Can you make sure both the C# code and Python code is updated using the same version and see if that fixes your issue?",hi think problem switched port training version mismatch python would never hit error port change make sure code python code version see issue,issue,negative,positive,positive,positive,positive,positive
561967553,"@jinhueck @Taikatou - if you're using version 0.8.2 of the python library, you'll also need to use the same version of the C# code. You can find that here https://github.com/Unity-Technologies/ml-agents/tree/0.8.2 - there's an option to download the the repo at that revision.",version python library also need use version code find option revision,issue,negative,neutral,neutral,neutral,neutral,neutral
561966134,"@mbaske Just a heads up that we changed the ""Use Heuristic"" bool to an enum in 0.12.0, which is equivalent to your change. Going to close this.",use heuristic bool equivalent change going close,issue,negative,neutral,neutral,neutral,neutral,neutral
561898470,You could add me as a collaborator and remove me whenever you feel.  ,could add collaborator remove whenever feel,issue,negative,neutral,neutral,neutral,neutral,neutral
561888950,I'll do some investigation with the memory profiler tomorrow first. Would it be possible to share a private repository?,investigation memory profiler tomorrow first would possible share private repository,issue,negative,positive,neutral,neutral,positive,positive
561879808,"Hi @thomasdingemanse,
The error message makes it sound like it can't find mlagents at all (not just mlagents.envs.environment). 

I'm not an anaconda expert, but based on https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#using-pip-in-an-environment and https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation-Windows.md#step-3-install-required-python-packages, I think you might need `pip` instead of `pip3` - my guess is that `pip3` is the ""system"" pip, so it's installing things outside of your conda environment.

If you run `pip debug` (and `pip3 debug`) that should list the path of the executable, which might give more information.",hi error message sound like ca find anaconda expert based think might need pip instead pip guess pip system pip outside environment run pip pip list path executable might give information,issue,negative,positive,positive,positive,positive,positive
561836771,"Thanks @surfnerd, it was the GitHub plugin, removing it fixed the issue",thanks removing fixed issue,issue,negative,positive,positive,positive,positive,positive
561834542,"Ah, I was just looking at the default in the old code.",ah looking default old code,issue,negative,positive,neutral,neutral,positive,positive
561833161,"> default is 20 right now. I think 100 breaks some of our environments.

Hunter tweaked some of the timescales in the env update. 100 causes tunnelling in e.g. Hallway, the cube will go through the wall. ",default right think update hallway cube go wall,issue,negative,positive,positive,positive,positive,positive
561832641,The other definition could be in a `.dll` file as well.  Is there a `.dll` or any plugin that you are using that has the name `ReadOnlyCollectionsInterfaces` ,definition could file well name,issue,negative,neutral,neutral,neutral,neutral,neutral
561829401,"Hi @devedse,
Just a quick look at your screen shots, it seems like the allocation is happening in your code.  Are you sure that there isn't a memory leak in your `RaceTrackScript`?

Could you share your project with us?  Without an in depth view into your code, it will be hard to know what's going on.  You can always install the [Unity Memory profiler](https://docs.unity3d.com/Packages/com.unity.memoryprofiler@0.1/manual/index.html) from the package manager to see what type of objects are leaking. ",hi quick look screen like allocation happening code sure memory leak could share project u without depth view code hard know going always install unity memory profiler package manager see type,issue,negative,positive,positive,positive,positive,positive
561827251,"Hi @albertoxamin,
Are you using any plugins that contain a redefinition of this interface?  ",hi contain redefinition interface,issue,negative,neutral,neutral,neutral,neutral,neutral
561767820,I believe we'll default to the old behavior (timescale = 100) right?,believe default old behavior right,issue,negative,positive,positive,positive,positive,positive
561767265,"Should we add the new command line options to the getting started guide? (The line where they run `mlagents-learn --train`, we'll also need to pass a timescale arg right?). ",add new command line getting guide line run train also need pas right,issue,negative,positive,positive,positive,positive,positive
561763232,"By default, when you connect with python, the academy thinks it's in ""training"" mode. You can change the behavior in Agents (in 0.12.0) by setting their BehaviorParameters Behavior Type to ""Inference Only""; they'll use inference, but the Academy's decision of whether to use the ""training"" or ""inference"" configurations is based solely on whether it's connected to an external processes (i.e. Python).

If you want full control over the settings in 0.12.0, I'd recommend you do the equivalent of Academy.ConfigureEnvironmentHelper inside your Academy.Step implementation (unfortunately doing it in InitializeAcademy will get overwritten)",default connect python academy training mode change behavior setting behavior type inference use inference academy decision whether use training inference based solely whether connected external python want full control recommend equivalent inside implementation unfortunately get,issue,negative,negative,neutral,neutral,negative,negative
561726588,hi @wightwhale - you'll need to install the Barracuda preview package to resolve these (there's now a note in the [Migration Guide](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Migrating.md#steps-to-migrate) about this). We're trying to make this more automatic and will likely do a hotfix release soon.,hi need install barracuda preview package resolve note migration guide trying make automatic likely release soon,issue,negative,neutral,neutral,neutral,neutral,neutral
561600326,"
> hi @migberbay - does training work when you set use_recurrent to false (or remove it)?

Hi @unityjeffrey, yes it does train normally if use_recurrent is set to false or not present.",hi training work set false remove hi yes train normally set false present,issue,negative,negative,negative,negative,negative,negative
561441831,"Sounds good, will add that note tomorrow.",good add note tomorrow,issue,negative,positive,positive,positive,positive,positive
561436495,"Thanks for your quick answer @chriselion

You are right, I can change these parameters in the academy and it is working well, however the environment is always runing on training mode

I made some tests by playing with window width and height and no matter what I chose for the `train_mode` parameter (`True `or `False`) in env.reset(), the game is runing with training mode only, never inference

Am I missing something ? Also I think that it is a great idea to put these parameters as arguments to UnityEnvironment",thanks quick answer right change academy working well however environment always training mode made window width height matter chose parameter true false game training mode never inference missing something also think great idea put,issue,positive,positive,positive,positive,positive,positive
561430616,"Hi @valentinmace,
I think the settings you're looking for are available in the Academy (in Unity):
https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Design-Academy.md#academy-properties
By default, training happens at 80x80 and 100x timescale, and inference is 1280x720 and normal timescale, but you can adjust them in the Academy.

Just a note that the interface for this will change in the next release; these parameters will be set on the python commandline or as arguments to UnityEnvironment.",hi think looking available academy unity default training inference normal adjust academy note interface change next release set python,issue,negative,positive,positive,positive,positive,positive
561428999,"Actually, let's talk about that tomorrow - @surfnerd and I need to figure out if we're specifying dependencies wrong.",actually let talk tomorrow need figure wrong,issue,negative,negative,negative,negative,negative,negative
561421660,Should we also add it to the Basic guide? https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md,also add basic guide,issue,negative,neutral,neutral,neutral,neutral,neutral
561293329,hi @migberbay - does training work when you set use_recurrent to false (or remove it)?,hi training work set false remove,issue,negative,negative,negative,negative,negative,negative
561087781,"I am working on a project that planned on using custom observations as well. We want to use visual observations, but not in an end-to-end fashion. But rather using seperate computer vision algorithms together with semantic segmentation networks to ""interept"" the images. The result is then used as an vector observation. This way we can train the policy both with the true observation that is easy to know from the simulation. But also using a seperate module that estimates these observations from an image.

So we have implemented so that we can send images using the custom observation proto. And in `brain.py` we process these images and changes the values on some of the vector observations. We could of course do this processing already in the `CollectObservations()` function in unity. But our toolchain makes it easier to do that step in python.

",working project custom well want use visual fashion rather computer vision together semantic segmentation result used vector observation way train policy true observation easy know simulation also module image send custom observation proto process vector could course already function unity easier step python,issue,positive,positive,positive,positive,positive,positive
561034026,"Thanks for the suggestion @ervteng, I haven't tried that. Sounds very interesting! I will probably give that a go :) ",thanks suggestion tried interesting probably give go,issue,positive,positive,positive,positive,positive,positive
561003479,"If you're using 0.10, you should follow the documentation from that version: https://github.com/Unity-Technologies/ml-agents/tree/0.10.0/docs

The ""Use Heuristic"" checkbox and Heuristic() method were added in 0.11.0.

Which brain checkbox do you mean? The guide for upgrading from 0.10.0 to 0.11.0 is here: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Migrating.md#migrating-from-ml-agents-toolkit-v010-to-v0110",follow documentation version use heuristic heuristic method added brain mean guide,issue,negative,negative,negative,negative,negative,negative
560994495,"@wardVD Sorry I didn't respond to this at the time! Unfortunately, the code I wrote isn't public, but I can try to answer some questions if you are still working on it.",sorry respond time unfortunately code wrote public try answer still working,issue,negative,negative,negative,negative,negative,negative
560985381,"is the cause only 0.11version and below have this function? if update to 0.11,where is the brain checkbox",cause function update brain,issue,negative,neutral,neutral,neutral,neutral,neutral
560805503,also - opening this up as a more general discussion to see if folks in the community have attempted to do what you are describing above.,also opening general discussion see community,issue,negative,positive,neutral,neutral,positive,positive
560788690,"Hi @robinerd, thank you again for giving the pretraining a go and reporting your results! Have you tried combining GAIL and PreTraining to see if it will train?

There has been evidence in the literature that suggests GAIL works for visual obs better when combined with a [(reward signal](https://arxiv.org/pdf/1904.03438.pdf). Especially in your case, it's possible that the discriminator's neural network just isn't big enough to capture what ""similar"" means between two screens of the game. It's possible that the pretraining will give it enough of a direction to learn its way there. ",hi thank giving pretraining go tried combining pretraining see train evidence literature work visual better combined reward signal especially case possible discriminator neural network big enough capture similar two game possible pretraining give enough direction learn way,issue,positive,positive,neutral,neutral,positive,positive
560699292,"hi @MarkTension - as Chris mentioned, we are working on a feature called SideChannels which is meant to pass data between Unity and Python outside of the training loops.  Here is a few of the PRs and the latest code is in our develop branch.  Let us know if that would work for this request:

https://github.com/Unity-Technologies/ml-agents/pull/2956
Docs - https://github.com/Unity-Technologies/ml-agents/pull/3011",hi working feature meant pas data unity python outside training latest code develop branch let u know would work request,issue,negative,positive,positive,positive,positive,positive
560657185,hi @sstream17 - we've documented your request and will update you if we decide to implement.  part of this is a Tensorflow restriction with the graph and would cause some issues --load with the check points not matching.  will keep you posted,hi request update decide implement part restriction graph would cause load check matching keep posted,issue,negative,neutral,neutral,neutral,neutral,neutral
560647865,"awesome.  going to close for now, but feel free to re-open or a new issue.",awesome going close feel free new issue,issue,positive,positive,positive,positive,positive,positive
560579232,"hi @beluis3d and @HugeKangroo - refer to this issue - https://github.com/Unity-Technologies/ml-agents/issues/2856

We also cleaned up the logging here.  https://github.com/Unity-Technologies/ml-agents/pull/2950

We just pushed out release 0.12 that has these fixes.  Please re-open if this is still persistent.  Than you.",hi refer issue also logging release please still persistent,issue,negative,neutral,neutral,neutral,neutral,neutral
560574270,"Hi, I didn't get the time to make an issue previously but I was able to get similar results with pretraining instead of offline_bc. I set the extrinsic reward strength to 0 and added my demo files in pretraining. So from the point of view of my bot with visual observations I don't see any issue of removing the offline_bc.

Regarding GAIL, I wasn't able to get it to converge. I just got seemingly random action all the time. Maybe it doesn't work good with visual observations, not sure.

Anyway, I can stick to the pretraining, and furthermore I think it's good you're renaming it to behavioral cloning. I was for a long time a bit confused about was pretraining was actually doing.",hi get time make issue previously able get similar pretraining instead set extrinsic reward strength added pretraining point view bot visual see issue removing regarding able get converge got seemingly random action time maybe work good visual sure anyway stick pretraining furthermore think good behavioral long time bit confused pretraining actually,issue,positive,positive,positive,positive,positive,positive
560569085,@ervteng thanks for the reply! I'm closing this issue then since you have added internal tracking and I can work around the problem ,thanks reply issue since added internal work around problem,issue,negative,positive,neutral,neutral,positive,positive
560567881,"Strange.. There was a similar issue just a few days ago, #2981",strange similar issue day ago,issue,negative,negative,neutral,neutral,negative,negative
560530766,"@chriselion 
>Looks good! Can you make sure you update the docs

There are quite a few changes to the docs. I will make a PR to this branch with the docs edits and merge this afterwards.",good make sure update quite make branch merge afterwards,issue,positive,positive,positive,positive,positive,positive
560517448,"Hi @surfnerd,
The version is 0.11.0. I install it from pip. The version for Unity is 2019.2.12f1. Thanks.",hi version install pip version unity thanks,issue,negative,positive,positive,positive,positive,positive
560504858,"Hey @robinerd, glad you figured it out! This seems to be related to the fact that the code expects a decision on the first step. We'll look into fixing it - it should be fixable on the Python side. I've logged the issue with internal reference MLA-392. ",hey glad figured related fact code decision first step look fixing fixable python side logged issue internal reference,issue,negative,positive,positive,positive,positive,positive
560497518,"Timing experiments were disappointing. On GridWorld and Hallway, no noticeable difference. I also modified 3DBall to use 50000 observations, and that showed about a 10% speedup.",timing disappointing hallway noticeable difference also use,issue,negative,negative,negative,negative,negative,negative
560493751,"Just in general, I'm not really sure how resetting works since 0.10. I'm currently using an agent to reset after a certain number of steps. However, I'm unsure at the difference between resetting your environment vs having Tensorflow understand that an 'episode' is complete. The only way I can get episodes to complete and output their values to tensorboard is when a reset happens automatically through an agent.

Preferably there would be some way to make sure that both the in-unity environment is reset for the next run and the episode is marked as complete at the same time, especially as I'm using an RNN and I don't want it to learn that the whole game will randomly reset sometimes. Currently my environment resets and episode resets are decoupled, which I think is causing some weird behavior. For example, my agents are clearly trying to finish the game as quickly as possible, even if it means making bad moves, because the more often the environment resets during one episode, the greater total reward they can earn.",general really sure work since currently agent reset certain number however unsure difference environment understand complete way get complete output reset automatically agent preferably would way make sure environment reset next run episode marked complete time especially want learn whole game randomly reset sometimes currently environment episode think causing weird behavior example clearly trying finish game quickly possible even making bad often environment one episode greater total reward earn,issue,positive,negative,neutral,neutral,negative,negative
560383384,"Here's some additional info.
When debugging the code in pycharm I see some errors that might give a clue to more knowledgeable people:

```
IGNORED: Cast unknown layer
IGNORED: VariableV2 unknown layer
IGNORED: Assign unknown layer
WARNING: rank unknown for tensor conv1/weights while processing node conv1/weights/read
IGNORED: VariableV2 unknown layer
IGNORED: Assign unknown layer
WARNING: rank unknown for tensor conv1/biases while processing node conv1/biases/read
WARNING: rank unknown for tensor conv1/biases while processing node model/conv1/BiasAdd
IGNORED: VariableV2 unknown layer
IGNORED: Assign unknown layer
WARNING: rank unknown for tensor conv2/weights while processing node conv2/weights/read
IGNORED: VariableV2 unknown layer
IGNORED: Assign unknown layer
WARNING: rank unknown for tensor conv2/biases while processing node conv2/biases/read
WARNING: rank unknown for tensor conv2/biases while processing node model/conv2/BiasAdd
IGNORED: VariableV2 unknown layer
IGNORED: Assign unknown layer
WARNING: rank unknown for tensor conv3/weights while processing node conv3/weights/read
IGNORED: VariableV2 unknown layer
IGNORED: Assign unknown layer
WARNING: rank unknown for tensor conv3/biases while processing node conv3/biases/read
WARNING: rank unknown for tensor conv3/biases while processing node model/conv3/BiasAdd
IGNORED: VariableV2 unknown layer
IGNORED: Assign unknown layer
WARNING: rank unknown for tensor conv4/weights while processing node conv4/weights/read
IGNORED: VariableV2 unknown layer
IGNORED: Assign unknown layer
WARNING: rank unknown for tensor conv4/biases while processing node conv4/biases/read
WARNING: rank unknown for tensor conv4/biases while processing node model/conv4/BiasAdd
IGNORED: VariableV2 unknown layer
IGNORED: Assign unknown layer
WARNING: rank unknown for tensor conv5/weights while processing node conv5/weights/read
IGNORED: VariableV2 unknown layer
IGNORED: Assign unknown layer
WARNING: rank unknown for tensor conv5/biases while processing node conv5/biases/read
WARNING: rank unknown for tensor conv5/biases while processing node model/conv5/BiasAdd
WARNING: rank unknown for tensor conv1/biases while processing node model_1/conv1/BiasAdd
WARNING: rank unknown for tensor conv2/biases while processing node model_1/conv2/BiasAdd
WARNING: rank unknown for tensor conv3/biases while processing node model_1/conv3/BiasAdd
WARNING: rank unknown for tensor conv4/biases while processing node model_1/conv4/BiasAdd
WARNING: rank unknown for tensor conv5/biases while processing node model_1/conv5/BiasAdd
WARNING: rank unknown for tensor contrastive-loss/Mean while processing node contrastive-loss/truediv

```",additional code see might give clue knowledgeable people cast unknown layer unknown layer assign unknown layer warning rank unknown tensor node unknown layer assign unknown layer warning rank unknown tensor node warning rank unknown tensor node unknown layer assign unknown layer warning rank unknown tensor node unknown layer assign unknown layer warning rank unknown tensor node warning rank unknown tensor node unknown layer assign unknown layer warning rank unknown tensor node unknown layer assign unknown layer warning rank unknown tensor node warning rank unknown tensor node unknown layer assign unknown layer warning rank unknown tensor node unknown layer assign unknown layer warning rank unknown tensor node warning rank unknown tensor node unknown layer assign unknown layer warning rank unknown tensor node unknown layer assign unknown layer warning rank unknown tensor node warning rank unknown tensor node warning rank unknown tensor node warning rank unknown tensor node warning rank unknown tensor node warning rank unknown tensor node warning rank unknown tensor node warning rank unknown tensor node,issue,negative,negative,negative,negative,negative,negative
560304363,"Sorry, I meet this on 2017.4 when press 'Play' to stop, 
the Unity window freezed even if I force quit in 'Force Quit Applications', 
and my hosts contains '127.0.0.1 localhost'

os: 10.15.1 (19B88)",sorry meet press stop unity window even force quit quit o,issue,negative,negative,negative,negative,negative,negative
560090707,"@chriselion Changing the behavior name to match the filename worked wonders, thank you!
Have a very good day",behavior name match worked thank good day,issue,positive,positive,positive,positive,positive,positive
560086458,"Strange.. I wonder if it's still needed to have the ENABLE_TENSORFLOW in script defines? In the build settings. I thought it is not used any more but that's one thing that might differ.

EDIT: I don't think that's the one.. It probably wouldn't compile even if that was the problem.",strange wonder still script build thought used one thing might differ edit think one probably would compile even problem,issue,negative,negative,neutral,neutral,negative,negative
560075135,"I am training inside unity editor.
![obraz](https://user-images.githubusercontent.com/31181594/69911462-1e681980-141c-11ea-8ea1-0764ceef5312.png)
Yes the Academy is in the same scene in which I try to train.
And the version of unity is: Unity 2019.2.14f1 (64-bit)

Actually I copied my whole project to UnitySDK project, and it works, actually it ""connected"" even without any agents on. But it is not working if I have another project. I am missing something for sure.",training inside unity editor yes academy scene try train version unity unity actually copied whole project project work actually connected even without working another project missing something sure,issue,negative,positive,neutral,neutral,positive,positive
560030563,"I posted the timings in the commit discussion where you pinged me, sorry if that was the wrong place.
Hope the measurements will be of any help!",posted commit discussion sorry wrong place hope help,issue,positive,negative,negative,negative,negative,negative
560030309,"I finally figured out the reason!
The problem happens when an agent is added to the scene but doesn't request any decisions at start. Our agents use on demand decisions, and a coroutine that requests decisions several times per second.
When the error happens, the python environment doesn't even print out the hyper parameters. It crashes before that with a ""KeyError [my-behavior-name]"". I suspect that python gets notified about the behavior and then it starts to use it in some way before there is any data. As said above the crash happens in environment.py, in _get_state.

I hope this is something you could be able to reproduce, and perhaps handle in a better way somehow.",finally figured reason problem agent added scene request start use demand several time per second error python environment even print hyper suspect python notified behavior use way data said crash hope something could able reproduce perhaps handle better way somehow,issue,negative,positive,positive,positive,positive,positive
560029474,"Are you training inside of unity editor or build? They have changed how you train in a build.

Could you provide a screenshot of your settings on the GlobalAcademy component?
Did you remember to add your academy in the scene you're running?

Which unity version are you using?",training inside unity editor build train build could provide component remember add academy scene running unity version,issue,negative,neutral,neutral,neutral,neutral,neutral
560012236,"Here's where we parse the curriculum directory: https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/meta_curriculum.py#L56
Basically the key for the dictionary is the filename.

The behavior name is used as the key to that dictionary here: https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/trainer_util.py#L107-L109

It sounds like the dictionary has key ""ChameleonLearning"" (no space) but it's trying to look up ""Chameleon Learning"" (with a space).

We definitely need to make the error handling better here. and maybe handle unknown behavior names.",parse curriculum directory basically key dictionary behavior name used key dictionary like dictionary key space trying look chameleon learning space definitely need make error handling better maybe handle unknown behavior,issue,positive,positive,neutral,neutral,positive,positive
560008866,"![image](https://user-images.githubusercontent.com/41434155/69904374-3ef58c80-13a6-11ea-911e-0f230731413d.png)
![image](https://user-images.githubusercontent.com/41434155/69904383-4fa60280-13a6-11ea-85c8-bbaf4c7d0c03.png)

@chriselion This is the second ml agents project im trying and the first had ""My behavior"" (the default) and it was fine when the folder was called ""penguin"" and the file ""PenguinLearning.json"" so i don't see how the two are related. Could you clarify on what exactly this behavior name is used for?",image image second project trying first behavior default fine folder penguin file see two related could clarify exactly behavior name used,issue,negative,positive,positive,positive,positive,positive
560008064,"@Numan4221 You need to have a ""default"" section in your trainer config's yaml file. See https://github.com/Unity-Technologies/ml-agents/blob/master/config/trainer_config.yaml#L1 for an example

@AydenWasTaken Can you post the contents of the curriculum folder and file? I think it's expecting a ""Chameleon Behavior.json"" file there, but you're probably going to have an easier time if you remove the space from the behavior name.",need default section trainer file see example post content curriculum folder file think chameleon file probably going easier time remove space behavior name,issue,negative,neutral,neutral,neutral,neutral,neutral
559967480,"Hey. I am having a similar issue but with the newer versions a lot has changed and could not figure it out. Here is the console output:
![image](https://user-images.githubusercontent.com/41434155/69901105-71d75a80-137d-11ea-9843-3c2a18cf084a.png)
Any help would be appreciated!
PS. I am a noob so please forgive any stupidity :)
Ayden",hey similar issue lot could figure console output image help would please forgive stupidity,issue,negative,negative,negative,negative,negative,negative
559628914,"Hello,
I have got same problem in my ml-agent project. I found that the behaviorparameters has failed when :
https://github.com/Unity-Technologies/ml-agents/blob/8a08bc4306dfa897b780afc79ee2bce47c76d47f/UnitySDK/Assets/ML-Agents/Scripts/Policy/BehaviorParameters.cs#L40-L43

I also checked the Academy, It failed at :
https://github.com/Unity-Technologies/ml-agents/blob/8a08bc4306dfa897b780afc79ee2bce47c76d47f/UnitySDK/Assets/ML-Agents/Scripts/Academy.cs#L294-L307

It looks like ml-agent try to detect python process and result with an exception.",hello got problem project found also checked academy like try detect python process result exception,issue,negative,neutral,neutral,neutral,neutral,neutral
559620349,"I forget the exact command but in unity you can create a view to show an image.


Sent from Yahoo Mail for iPhone


On Thursday, November 28, 2019, 6:05 PM, Rajat Paliwal <notifications@github.com> wrote:


Due to proprietary issues I can't share code or images of the environment. But I wonder how did you displayed the images in Visual Studio.

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or unsubscribe.



",forget exact command unity create view show image sent yahoo mail wrote due proprietary ca share code environment wonder displayed visual studio reply directly view,issue,negative,positive,neutral,neutral,positive,positive
559616527,Due to proprietary issues I can't share code or images of the environment. But I wonder how did you displayed the images in Visual Studio.,due proprietary ca share code environment wonder displayed visual studio,issue,negative,negative,neutral,neutral,negative,negative
559582174,"I ran the measurements with the modifications from your commit. I didn't understand where I was supposed to put the code to save the file, but I let it train for a short while to get a timings file saved in summaries folder. So you will see ~1600 environment steps as well, which you can ignore.

As a reminder, this is importing 4 demo files, total 1 GB, with mainly visual data.

**Here's the file:**
[measure-timings-3-0_timers.json.txt](https://github.com/Unity-Technologies/ml-agents/files/3902789/measure-timings-3-0_timers.json.txt)

**The part I think is relevant:**
```
{
  ""name"": ""root"",
  ""total"": 1004.7263779,
  ""count"": 1,
  ""self"": 44.596612099998424,
  ""children"": [
    {
      ""name"": ""demo_to_buffer"",
      ""total"": 426.8387183,
      ""count"": 1,
      ""self"": 46.05535809999992,
      ""children"": [
        {
          ""name"": ""load_demonstration"",
          ""total"": 157.1733402,
          ""count"": 1,
          ""self"": 18.54265760000007,
          ""children"": [
            {
              ""name"": ""read_file"",
              ""total"": 3.925581199999982,
              ""count"": 4,
              ""self"": 3.925581199999982
            },
            {
              ""name"": ""BrainInfo.from_agent_proto"",
              ""total"": 134.70510139999996,
              ""count"": 15514,
              ""self"": 9.737127899998626,
              ""children"": [
                {
                  ""name"": ""BrainInfo.process_pixels"",
                  ""total"": 124.96797350000134,
                  ""count"": 31028,
                  ""self"": 68.33487490000124,
                  ""children"": [
                    {
                      ""name"": ""image_decompress"",
                      ""total"": 56.63309860000009,
                      ""count"": 31028,
                      ""self"": 56.63309860000009
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          ""name"": ""make_demo_buffer"",
          ""total"": 223.61002000000002,
          ""count"": 1,
          ""self"": 223.61002000000002
        }
      ]
    },
```

EDIT: Just realized I should've posted this in the thread I opened initially... let's keep the discussion in the other thread maybe",ran commit understand supposed put code save file let train short get file saved folder see environment well ignore reminder total mainly visual data file part think relevant name root total count self name total count self name total count self name total count self name total count self name total count self name total count self name total count self edit posted thread initially let keep discussion thread maybe,issue,positive,positive,neutral,neutral,positive,positive
559579223,"`UnityEnvironmentException: Parameter file could not be found at config/trainer_config.yaml.`

Your working directory is:

`C:\Users\Abubakar\Downloads>`

config/trainer_config.yaml file is located at ml-agents root directory.

If you want to run from ""Downloads"" you need to use the correct path, In you case it looks like /ml-agents/config/trainer_config.yaml",parameter file could found working directory file root directory want run need use correct path case like,issue,negative,neutral,neutral,neutral,neutral,neutral
559542274,"I am not sure how to debug it but I did display the image in Visual Studio, so I could see it. 
Anyways I can't really know how to solve your problem. If you post some images or post the code on GitHub that could help to give more information.",sure display image visual studio could see anyways ca really know solve problem post post code could help give information,issue,negative,positive,positive,positive,positive,positive
559362645,"Hi @surfnerd,
Could you tell me how you hook up the brains manually?",hi could tell hook brain manually,issue,negative,neutral,neutral,neutral,neutral,neutral
559300978,"hey @anupambhatnagar - i didn't have a chance to review this before it was merged.  i have some suggestions, but it might be faster if I went in and made them myself.  I will do that and send a PR back out.",hey chance review might faster went made send back,issue,negative,neutral,neutral,neutral,neutral,neutral
559238046,"No the mini targets were just the position vectors in the list. They were not visible in the scene. The final target at the end of tunnel is visible. 
Also, is there a way to debug what visual observation is being sent to the brain?",position list visible scene final target end tunnel visible also way visual observation sent brain,issue,negative,neutral,neutral,neutral,neutral,neutral
559236170,Is all the necessary information for the agent present in each visual observation? For example are the mini targets visible? ,necessary information agent present visual observation example visible,issue,negative,neutral,neutral,neutral,neutral,neutral
559234659,"Hi @mattinjersey ,
I tried training with a combination of visual+vector observation . I provided one visual observation. But the training results were not much great. Would you suggest just doing training with visual observation?",hi tried training combination observation provided one visual observation training much great would suggest training visual observation,issue,positive,positive,positive,positive,positive,positive
559234582,"Oh I see, then we see, but I'll let you know when it's done. Have a nice holiday! :)",oh see see let know done nice holiday,issue,positive,positive,positive,positive,positive,positive
559233761,"OK, no rush. It's a holiday here tomorrow and Friday, so I might not get a chance to look at the results until Monday.",rush holiday tomorrow might get chance look,issue,positive,neutral,neutral,neutral,neutral,neutral
559232671,"Closing this since it's fixed on develop, which will soon be merged to master (early next week)",since fixed develop soon master early next week,issue,negative,positive,neutral,neutral,positive,positive
559231796,"@chriselion ah yes I see now, I could do that for sure! I'm going to sleep soon but I can get it done tomorrow and send you the results in the other thread.",ah yes see could sure going sleep soon get done tomorrow send thread,issue,positive,positive,positive,positive,positive,positive
559230073,"Not sure if you saw it, but I tagged you in https://github.com/Unity-Technologies/ml-agents/pull/2989 - if you can apply those changes and paste some timers here, that would give me a better idea of where we should focus. I _suspect_ a lot of the time is in the proto->python conversion (since that's where the image decompression happens) but would like some data to back it up before I change that part of the code.",sure saw tagged apply paste would give better idea focus lot time python conversion since image decompression would like data back change part code,issue,positive,positive,positive,positive,positive,positive
559226936,Thanks! I found it now. Really appreciate the feedback: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Executable.md,thanks found really appreciate feedback,issue,positive,positive,positive,positive,positive,positive
559225025,"Thanks both of you for the details!

I think spreading out the load would be good for my use case. My main struggle currently is not really the total training time but rather the initial time to wait before I can see that it starts correctly.

Thanks chris for the suggestions to test! I will let you know if I try something out :) 

",thanks think spreading load would good use case main struggle currently really total training time rather initial time wait see correctly thanks test let know try something,issue,positive,positive,positive,positive,positive,positive
559220568,"Some issue tracking on our side:
MLA-363 - make sure we don't use numpy.float64 for anything (PR https://github.com/Unity-Technologies/ml-agents/pull/2948)
MLA-345 - float ""compression"" for visual observations (already logged)
MLA-390 - JPEG compression for visual observations 
MLA-391 - demo_loader doesn't close files after loading
MLA-392 - add timers to demo loading
MLA-393 - make_demo_buffer converts each AgentInfo proto twice
",issue side make sure use anything float compression visual already logged compression visual close loading add loading proto twice,issue,negative,positive,positive,positive,positive,positive
559220499,"Hi @unityjeffrey  . I tried by creating multiple environment in the scene, but it resulted in very slow training. I will try by creating a build of the unity game . 
Is creating multiple environment in the scene is not advised?",hi tried multiple environment scene slow training try build unity game multiple environment scene advised,issue,negative,negative,negative,negative,negative,negative
559215719,"Hi @robinerd - to follow up on what Jeff said:

[load_demonstration()](https://github.com/Unity-Technologies/ml-agents/blob/bc1acaf6d2988c07d13620e7702aed67f2c7be78/ml-agents/mlagents/trainers/demo_loader.py#L80)() loads all the data at once; we could potentially change this (and the consumers of the demo) to load only as needed. This wouldn't be faster overall, but would at least spread out the cost. (I also just noticed we don't close the file afterwards, which is a bug)

Visual observations are currently always compressed as PNGs. As you mentioned, we plan to allow for uncompressed (float32) data, and possibly JPEG too. There are some potential problems with JPEG since it's lossy, which would mean that your training data would be different than your inference data. JPEG would potentially give you a smaller file size (I imagine ""real"" game data doesn't compress as PNG well), as well as might decompress faster.

I'm not sure about the performance impact, but I've also got some python changes queued up to make sure that we always keep data (including visual obs) in float32 format (instead of numpy's float64 default). 

If you want to hack on this a bit more:
* supporting float visual observations might be non-trivial, since it'll require both python and C# changes.
* adding support for JPEG should be pretty straightforward, just adding enums in ISensor.SensorCompressionType, and handling it in CameraSensor.GetCompressedObservation. We'll probably need enums for varying compression levels, say 75, 95, and 99.
* we could add timers around the file loading and BrainInfo.from_agent_proto() (where decompression happens) calls in the demo loader, to get an idea of where the time is actually being spent.
* Now that I look at it a little more, it looks like make_demo_buffer() convert from proto twice for each AgentInfo (once as current and once as next), so if that's a non-trivial amount of time, there's good room for improvement.",hi follow jeff said data could potentially change load would faster overall would least spread cost also close file afterwards bug visual currently always compressed plan allow uncompressed float data possibly potential since would mean training data would different inference data would potentially give smaller file size imagine real game data compress well well might decompress faster sure performance impact also got python make sure always keep data visual float format instead float default want hack bit supporting float visual might since require python support pretty straightforward handling probably need compression say could add around file loading decompression loader get idea time actually spent look little like convert proto twice current next amount time good room improvement,issue,positive,positive,neutral,neutral,positive,positive
559192293,Great! I'd be happy to contribute. Let me know how I can help,great happy contribute let know help,issue,positive,positive,positive,positive,positive,positive
559189912,"hi @MarkTension - i can see how this be very useful.  its not currently in our backlog, would you be interested to contribute?  we'd be happy to get your thoughts on design and code.  CC: @ervteng ",hi see useful currently backlog would interested contribute happy get design code,issue,positive,positive,positive,positive,positive,positive
559189223,"hi @Tikoshido - i think for folks in the community to help, we would need to see details on how you have implemented the academy and agents.  it's hard to debug on just on the command line alone",hi think community help would need see academy hard command line alone,issue,negative,negative,negative,negative,negative,negative
559188177,"hey @robinerd - we discussed this a bit yesterday.  I believe the problem is that we are trying to load all the demo data at once, instead of sequentially.  I think this could fix your issue.  CC: @chriselion @ervteng ",hey bit yesterday believe problem trying load data instead sequentially think could fix issue,issue,negative,neutral,neutral,neutral,neutral,neutral
559187418,"hi @rajatpaliwal - have you tried to create a build of the unity game, and run the training via the command line instead?  You could also use --num-envs=N to see if that helps as well.",hi tried create build unity game run training via command line instead could also use see well,issue,negative,negative,negative,negative,negative,negative
559186695,"hi, as @caioc2 mentioned, we do not ship the Unity project binaries as part of the repo.  However, you can go to File -> Build Settings in Unity to build our example projects to the platform you are trying to run on (in this case, it should be MacOS).  Attaching a screenshot for you.  Let me know if this addresses your issue. 
<img width=""623"" alt=""Screen Shot 2019-11-27 at 9 39 09 AM"" src=""https://user-images.githubusercontent.com/34355042/69746746-cff40b80-10f9-11ea-8303-818d0c29b091.png"">
",hi ship unity project part however go file build unity build example platform trying run case let know issue screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
559170833,"""../envs"" is just an example path, where you should replace it with your own path. When you see something `""inside"" `or `<inside>` it means you should replace it with your own values ​​and remove the `""""` or `<>`.
 
It makes no sense to ship binaries (for the examples) when you provide the entire unity project itself, if not, one would need to provide binaries for different OS'es.

If you want to test any example scenes you can use the training command without --env and it will ask you to press ""Play"" in the editor or build any of the examples yourself and setup the --env according.",example path replace path see something inside inside replace remove sense ship provide entire unity project one would need provide different want test example use training command without ask press play editor build setup according,issue,negative,neutral,neutral,neutral,neutral,neutral
559083424,"For anyone getting this issue, in my case this was a BatchNormalization layer that had the flag 'trainable' set to `True` manually, so that the standard `tf.keras.backend.set_learning_phase(0)` to switch the BatchNormalization layer to test mode was not applied.

This apparently leads to the BatchNorm weights not being frozen and, thus, the Barracuda converter fails since the BatchNorm layer does not have the expected data.

Probably it would be good to make sure that `data` is initalized in `get_tensor_data` anyway and fail with some informative error message instead of the error:

```
def get_tensor_data(tensor):
    if isinstance(tensor, np.ndarray):
        return tensor.astype(float)

    dims = get_tensor_dims(tensor)
    elems = np.product(dims)
    
    if tensor.tensor_content:
        # TODO: support other types
        dataType = get_tensor_dtype(tensor)
        if dataType == ""DT_FLOAT"":
            data = struct.unpack('<'+str(elems)+'f', tensor.tensor_content)
        elif dataType == ""DT_INT32"":
            data = struct.unpack('<'+str(elems)+'i', tensor.tensor_content)
        elif dataType == ""DT_BOOL"":
            data = struct.unpack('<'+str(elems)+'?', tensor.tensor_content)
        else:
            print('UNSUPPORTED: data type', dataType)
    
    if tensor.float_val:
        data = tensor.float_val
    if tensor.int_val:
        data = np.array(tensor.int_val, dtype=float)
    if tensor.bool_val:
        data = np.array(tensor.bool_val, dtype=float)

    return np.array(data).reshape(dims)
```",anyone getting issue case layer flag set true manually standard switch layer test mode applied apparently frozen thus barracuda converter since layer data probably would good make sure data anyway fail informative error message instead error tensor tensor return float tensor support tensor data data data else print data type data data data return data,issue,negative,positive,positive,positive,positive,positive
559023201,"> You can put the brain .nn file in Model
> See # 3 of [this guide in the docs](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md#running-a-pre-trained-model)

Thank you very much for your reply, I solved the problem with your method.",put brain file model see guide thank much reply problem method,issue,negative,positive,positive,positive,positive,positive
558970992,"You can put the brain .nn file in Model 
See # 3 of [this guide in the docs](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md#running-a-pre-trained-model)",put brain file model see guide,issue,negative,neutral,neutral,neutral,neutral,neutral
558950236,"I am having same issue with Discrete brain.
CollectObservation quite often is called at wrong times or not at all at start.
And AI generates wrong output when AgentAction is executed.
It also appear to brake action masking, as Agent may attempt to generate output for wrong mask. (Or no mask at all)
I tried to add work around, but couldn't fix it completely.

I have added AI to a board game and only one player at times suppose to be executed through RequestDecision, but it appears that CollectObservations is skipped sometimes. 
And always skipped on the first turn.

In game set up is this:
Academy through AcademyStep triggers RequestDecision on Agent and waits for it to complete.
Only after Agent submitted move, then Academy will execute next agent.",issue discrete brain quite often wrong time start ai wrong output executed also appear brake action agent may attempt generate output wrong mask mask tried add work around could fix completely added ai board game one player time suppose executed sometimes always first turn game set academy agent complete agent move academy execute next agent,issue,negative,negative,negative,negative,negative,negative
558903413,"> Hi there! I had the same problem, and for me it was because I didn't set a brain yet.
> If you don't set a brain in the agent's Behavior parameters, ml-agents will always use Heuristics.
> This is only the case when you're running it in play mode.
> Hope that helps

But in 0.11 version, i don't know how to set a brain, my Begavior parameters inspector is like this

![捕获](https://user-images.githubusercontent.com/16350176/69688882-48f76100-1102-11ea-97be-6da39e2c5ccc.PNG)

the only one thing that i think is related to brain is 'Behavior Name', but it doesn't work.",hi problem set brain yet set brain agent behavior always use case running play mode hope version know set brain inspector like one thing think related brain name work,issue,positive,neutral,neutral,neutral,neutral,neutral
558860073,"> Are there any performance changes (I mean in training speed) ?

Not as far as I can tell (at least on 3DBall). There shouldn't be, given that it's the same # of objects, just that the update buffer is outside the class of the AgentBuffers.",performance mean training speed far tell least given update buffer outside class,issue,negative,negative,negative,negative,negative,negative
558856521,Are there any performance changes (I mean in training speed) ?,performance mean training speed,issue,negative,negative,negative,negative,negative,negative
558855146,"Great, good luck! Also note the time horizon which I note in the video may be an important hyperparameter. 


Sent from Yahoo Mail for iPhone


On Tuesday, November 26, 2019, 5:31 PM, Rajat Paliwal <notifications@github.com> wrote:


Thanks for sharing . Awesome work and explanation in the video. I will try visual observation in my project and will update you on the result.

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or unsubscribe.



",great good luck also note time horizon note video may important sent yahoo mail wrote thanks awesome work explanation video try visual observation project update result reply directly view,issue,positive,positive,positive,positive,positive,positive
558849755,"Looks great. Definitely OK to merge this, and hook it in later.",great definitely merge hook later,issue,positive,positive,positive,positive,positive,positive
558846041,"Thanks for sharing . Awesome work and explanation in the video. I will try visual observation in my project and will update you on the result.
",thanks awesome work explanation video try visual observation project update result,issue,positive,positive,positive,positive,positive,positive
558840360,"I used single environment. Max num Steps didn't seem critical but I did force the game to end after 1 minute, in Unity.
My code is on GitHub if you are interested. Also there I made a video, its in the GitHub readme.
https://github.com/mattinjersey/laserDefenser_ML
",used single environment seem critical force game end minute unity code interested also made video,issue,negative,negative,neutral,neutral,negative,negative
558836810,"Sure, will definitely give it a try. Just a query , did you use single environment or multiple environment in a single scene and what were the maximum number of steps you chose for your problem.",sure definitely give try query use single environment multiple environment single scene maximum number chose problem,issue,negative,positive,neutral,neutral,positive,positive
558835408,"Well I tried a Space Invaders game with many invaders. It did not train well with Vector Observation, and it trained quite well with Visual Observation. I think its worth a shot... it is not too difficult to implement. Generate a low-resolution of the scene so that the datasize is not too big.",well tried space game many train well vector observation trained quite well visual observation think worth shot difficult implement generate scene big,issue,positive,negative,neutral,neutral,negative,negative
558833809,"Hi @mattinjersey . Thanks for replying.
I think what you mean is providing visual observation instead of vector observation. I did not went with that approach as it was mentioned in the documentation that visual observations are not much effective and takes much more longer time to train.
Was your ML problem similar to mine , and does using visual observations improved your performance.",hi thanks think mean providing visual observation instead vector observation went approach documentation visual much effective much longer time train problem similar mine visual performance,issue,positive,positive,neutral,neutral,positive,positive
558831148,I found better results when I provided the ML with images rather than coordinates. ,found better provided rather,issue,negative,positive,positive,positive,positive,positive
558825155,"This PR contains code additions for side channels, tests and code comments but does not have documentation and are not being used by training (the old system is still working)
I would like to get this PR in and then make another with deprecation of the current way to do reset parameters and engine config and the documentation changes.",code side code documentation used training old system still working would like get make another deprecation current way reset engine documentation,issue,negative,positive,neutral,neutral,positive,positive
558799409,Edited to say 5-10 minutes instead of a quarter. I had another issue at the time of posting.,say instead quarter another issue time posting,issue,negative,neutral,neutral,neutral,neutral,neutral
558787529,"Hey @robinerd, this discussion is super helpful, let's still create the issue. Would like to make sure we don't lose any functionality if we remove offline BC, esp. with images. 

One of the reasons for this PR is to get rid of that ambiguity of having several of the same technique in different places (and rename pretraining to BC). 

A side note: I just fixed an [issue](https://github.com/Unity-Technologies/ml-agents/pull/2977) with setting `steps` to 0 in pretraining, for now just set it to some large number to get it to run.",hey discussion super helpful let still create issue would like make sure lose functionality remove one get rid ambiguity several technique different rename pretraining side note fixed issue setting pretraining set large number get run,issue,positive,positive,positive,positive,positive,positive
558783626,"Hi @mgb249, the loss function is actually the same for pretraining and BC - it's the same update, only performed on a model that's also used for PPO. 

When you say only used pretraining, what are the hyperparameters? Hopefully I can recreate your issue on our side. For instance, on our example env Hallway, these two configs produce roughly the same result:

```
Hallway:
	trainer:	ppo
	batch_size:	128
	beta:	0.01
	buffer_size:	1024
	epsilon:	0.2
	hidden_units:	128
	lambd:	0.95
	learning_rate:	0.0003
	max_steps:	5.0e5
	memory_size:	256
	normalize:	False
	num_epoch:	3
	num_layers:	2
	time_horizon:	64
	sequence_length:	64
	summary_freq:	1000
	use_recurrent:	True
	reward_signals:	
	  extrinsic:	
	    strength:	0.0
	    gamma:	0.99
	summary_path:	./summaries/pretraininghallway_Hallway
	model_path:	./models/pretraininghallway-0/Hallway
	keep_checkpoints:	5
	pretraining:	
	  demo_path:	./demos/ExpertHallway.demo
	  strength:	1.0
	  steps:	1000000
```
```
Hallway:
    trainer: offline_bc
    max_steps: 5.0e5
    num_epoch: 5
    batch_size: 64
    batches_per_epoch: 5
    num_layers: 2
    hidden_units: 128
    sequence_length: 16
    use_recurrent: true
    memory_size: 256
    sequence_length: 32
    demo_path: ./demos/ExpertHallway.demo
```

Anyways, keep me posted on the experiment. We're considering combining the offline BC and pretraining features, but want to make sure there's no degradation in performance - thank you for trying them out.",hi loss function actually pretraining update model also used say used pretraining hopefully recreate issue side instance example hallway two produce roughly result hallway trainer beta epsilon normalize false true extrinsic strength gamma pretraining strength hallway trainer true anyways keep posted experiment considering combining pretraining want make sure degradation performance thank trying,issue,positive,positive,positive,positive,positive,positive
558776657,"Before creating any issue I will check if I can get good results uby using only the pretraining reward as you suggested.

I didn't realize that pretraining was essentially using behavioral cloning. I thought it was based on GAIL in some way too. And I was confused what the difference between pretraining and GAIL was. Then it all makes sense now 😄

Is it still interesting to dig into the reason why I don't get good results with GAIL then?",issue check get good pretraining reward realize pretraining essentially behavioral thought based way confused difference pretraining sense still interesting dig reason get good,issue,positive,positive,positive,positive,positive,positive
558772514,"@ervteng Thanks for your reply! I'm glad to try it out, you're all doing a great job with this :)
Right now I'm using only a gail reward signal with 1.0 strength, and I can't seem to get it to converge. 

All actions are randomly fluctuating a lot while the same situation works fairly OK with offline_bc. Have experimented with use_actions and use_vail, and some different encoding sizes.

I will start a discussion issue (will put link here) with more info, maybe we can understand it better together. Maybe it doesn't work well with cameras, or maybe I'm just not using it right.",thanks reply glad try great job right reward signal strength ca seem get converge randomly lot situation work fairly experimented different size start discussion issue put link maybe understand better together maybe work well maybe right,issue,positive,positive,positive,positive,positive,positive
558761662,"@dylansc Thank you. I have defined the Heuristic() method and mapped keys to actions, and everything is working as expected. Also, this method answers my second question as well.
Thank you again. 

Unity should add such changes in the migrating guide.",thank defined heuristic method everything working also method second question well thank unity add guide,issue,positive,neutral,neutral,neutral,neutral,neutral
558753228,"@robinerd thanks for chiming in! If there are indeed cases where BC beats GAIL clearly, it's a strong signal for us that we shouldn't remove BC. It's possible that GAIL is not as good with images as BC is. 

You can already try these features! The current setup lets you combine BC and GAIL with PPO, only that BC is called ""[pretraining](https://github.com/Unity-Technologies/ml-agents/blob/3d7c4b8d3c1ad17070308b4e06bb57d4a80f9a0c/docs/Training-PPO.md#optional-pretraining-using-demonstrations)"". You can set `steps` to 0 to have pretraining last continuously throughout your run. In fact, if you set the strengths of all the `reward_signals` to 0 and pretraining strength to 1.0, it should effectively become BC. 

Another thing I'd play with is setting `use_actions` to `true` for GAIL. This should better force-mimic your playthroughs. It's less useful when combining GAIL with an RL signal, but for pure demonstrations it should work better.

Thanks again for giving these features a real shakedown - always looking for such feedback when deciding what to remove and what to add. ",thanks indeed clearly strong signal u remove possible good already try current setup combine pretraining set pretraining last continuously throughout run fact set pretraining strength effectively become another thing play setting true better le useful combining signal pure work better thanks giving real shakedown always looking feedback remove add,issue,positive,positive,positive,positive,positive,positive
558747859,"Have you defined the Heuristic() method in your agent class? Ctrl-f heuristic() in [this doc](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Design-Agents.md) for more details.

And here's the [Unity input manager documentation](https://docs.unity3d.com/Manual/class-InputManager.html) that should help with mapping actions to keys",defined heuristic method agent class heuristic doc unity input manager documentation help,issue,negative,neutral,neutral,neutral,neutral,neutral
558747246,hey @robinerd - that is likely our eventual plan as well (singleton),hey likely eventual plan well singleton,issue,negative,neutral,neutral,neutral,neutral,neutral
558745410,"@dylansc Thank you for your reply. The model field is already set to None. However, I cannot control the agent. it gives the following error:-

`UnityAgentsException: The Heuristic method was not implemented for the Agent`

 Also, where to map actions to keys?",thank reply model field already set none however control agent following error heuristic method agent also map,issue,negative,neutral,neutral,neutral,neutral,neutral
558744858,Your approach sounds reasonable to me. We've documented your suggestion and will prioritize when appropriate.,approach reasonable suggestion appropriate,issue,negative,positive,positive,positive,positive,positive
558740811,"Regarding your first question, to control the agent yourself, just set the Model field under behavior parameters to none.",regarding first question control agent set model field behavior none,issue,negative,positive,positive,positive,positive,positive
558722822,"Hi,

I just wanted to follow up on my last question. Over the weekend we tried validate that the results are different in regards to behavioral cloning, I trained 10 models in three conditions (30 in total): Pretrain-Sac (ONLY use pretrain), Pretrain-PPO (ONLY use pretrain), BC. The PPO failed to ever get a reward and based on run time speed, it didn't even use the pretrain, the SAC failed to hit the reward, although observationally it did appear to do some non-random movement, and BC consistently got a reward. 

The task is fairly simple, and again I wanted to validate that pretraining works in the same way as in BC assuming the same hyperparameters. If you have an example config file/setting or scene where you've verified that pretraining works for PPO and SAC and gets the same results, can you share them? 

At this point I might bite bullet and just go through the python code and see how loss and weight updates are being calculated in BC versus the pretraining component in the RL models.

Regards,

Michael",hi follow last question weekend tried validate different behavioral trained three total use pretrain use pretrain ever get reward based run time speed even use pretrain sac hit reward although observationally appear movement consistently got reward task fairly simple validate pretraining work way assuming example scene pretraining work sac share point might bite bullet go python code see loss weight calculated versus pretraining component,issue,positive,positive,neutral,neutral,positive,positive
558682928,"Hi there! I had the same problem, and for me it was because I didn't set a brain yet. 
If you don't set a brain in the agent's Behavior parameters, ml-agents will always use Heuristics. 
This is only the case when you're running it in play mode.
Hope that helps ",hi problem set brain yet set brain agent behavior always use case running play mode hope,issue,negative,neutral,neutral,neutral,neutral,neutral
558675507,"Interesting, I am right now comparing gail and offline_bc in a project where I teach an agent to play a game based on visual inputs, to imitate a human playstyle. 

My first impression so far was that I got better results with BC than gail.
If there's any way I can try out these changes I would be happy to do a new comparison and let you know how it works for my case, since I'm anyway going to spend some time optimizing this agent.

Here's a video about my project: https://www.linkedin.com/posts/robin-lindh-nilsson-7a30205b_ai-machinelearning-deeplearning-activity-6604987123860815872-rI1K",interesting right project teach agent play game based visual imitate human first impression far got better way try would happy new comparison let know work case since anyway going spend time agent video project,issue,positive,positive,positive,positive,positive,positive
558668937,"Okay so what I can suggest based on that is to 1. Make sure the agent has enough detailed understanding of the world with its observations. Think of it like, based on that data, does the agent have any possibility to figure out where the window is? What happens if the ray observations don't hit the window for a while?
2. If that is alright consider trying my previous suggestions for the improved rewards ",suggest based make sure agent enough detailed understanding world think like based data agent possibility figure window ray hit window alright consider trying previous,issue,positive,positive,positive,positive,positive,positive
558668382,"It works in train mode:). So I guess the problem is solved now. 
Didn't try train mode at first because I thought something was wrong, while actually it was just a problem with heuristic-mode. Makes sense",work train mode guess problem try train mode first thought something wrong actually problem sense,issue,negative,negative,neutral,neutral,negative,negative
558664252,"You are right, I forgot that they introduced support for multiple branches some time ago. Having a second look at your first screenshot I agree that you should indeed be getting to values. Do you get the same error while training?",right forgot support multiple time ago second look first agree indeed getting get error training,issue,negative,positive,positive,positive,positive,positive
558659641,"Alright, thanks for the clarification! The part about heuristics explains a lot. I think you're wrong though with the discrete vs continuous. Here's [instructions from the documentation](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Design-Agents.md#vector-actions):

> When you specify that the vector action space is Continuous, the action parameter passed to the Agent is an array of control signals with length equal to the Vector Action Space Size property. When you specify a Discrete vector action space type, the action parameter is an array containing integers. Each integer is an index into a list or table of commands. In the Discrete vector action space type, the action parameter is an array of indices. The number of indices in the array is determined by the number of branches defined in the Branches Size property. ",alright thanks clarification part lot think wrong though discrete continuous documentation specify vector action space continuous action parameter agent array control length equal vector action space size property specify discrete vector action space type action parameter array integer index list table discrete vector action space type action parameter array index number index array determined number defined size property,issue,positive,positive,neutral,neutral,positive,positive
558654373,"Thanks. Let me know when it's up to date.

P.S : I find it hilarious that Python is so commonly used for its ease in actual script creation, yet most interfaces rely on cmd/terminal at some point.  It's literally just a bunch of numbers for setup and a text interpreter if one must have python. Probably a 600 line unity editor script. This would bring a lot more usage of ML in the general community.",thanks let know date find hilarious python commonly used ease actual script creation yet rely point literally bunch setup text interpreter one must python probably line unity editor script would bring lot usage general community,issue,positive,positive,neutral,neutral,positive,positive
558529591,Any updates on headless rendering? 2019 is almost at its end,headless rendering almost end,issue,negative,neutral,neutral,neutral,neutral,neutral
558459071,@sg2 @albertoxamin We are working on publicly releasing a docker image with mlagents and all its dependencies pre-installed to support our Docker users and taking away the installation overhead completely. Stay tuned.,working publicly docker image support docker taking away installation overhead completely stay tuned,issue,negative,positive,neutral,neutral,positive,positive
558458418,"@CarlBinneman Thanks for the feedback. We assume that installing python3 is responsibility of the user. There is clear documentation on python.org for installing python3, hence we do not provide documentation towards that end.

Secondly, the user may have multiple versions of python installed on the work environment. E.g. python 2.7 and python 3.6. In this case the usual alias for python 2.7 and python3.6 are `python` and `python3` respectively. Since you installed Python on your system for the first time the system set `python` as an alias for Python3 in your case. We will update the docs keeping this use case in mind.

Lastly, venv is the recommended way for the following reasons:
- It works on all platforms i.e. Mac, Linux, Windows
- It supports maintaining multiple python environments for projects
- Since its already included in Python3, it is straightforward to setup",thanks feedback assume python responsibility user clear documentation python hence provide documentation towards end secondly user may multiple python work environment python python case usual alias python python python python respectively since python system first time system set python alias python case update keeping use case mind lastly way following work mac multiple python since already included python straightforward setup,issue,positive,positive,neutral,neutral,positive,positive
558385406,"Overall looks good, could just use some python unit tests.",overall good could use python unit,issue,negative,positive,positive,positive,positive,positive
558359469,@njustesen Thanks for the report. We think we found and fixed the problem in https://github.com/Unity-Technologies/ml-agents/pull/2965,thanks report think found fixed problem,issue,negative,positive,positive,positive,positive,positive
558348466,Good to hear! Glad you were able to train your tower. ,good hear glad able train tower,issue,positive,positive,positive,positive,positive,positive
558348234,"The spikes are smaller in v0.8 b/c of the recording interval change - they're still there, but averaged over many runs. I'm thinking it has something to do, as you said, with the reward structure, and the fact that all of the 3DBalls start at the same time on initialization (but in subsequent runs, they'll randomize), so if the first couple are low rewards, the first log is low. 

In any case, as long as your agents recover after some time, I wouldn't worry too much - it's just an artifact of reporting, and won't affect your model weights. ",smaller recording interval change still many thinking something said reward structure fact start time subsequent randomize first couple low first log low case long recover time would worry much artifact wo affect model,issue,negative,positive,positive,positive,positive,positive
558283849,"Yea sure I will show you my code 
`public class SpaceAgent : Agent{

    public Transform Target;
    public Transform Ball; 
    public GameObject wall_prefab;
    public int wall_Count = 5;
    public float wall_distance = 5f;
    public float wall_random = 2f;
    const float rayDistance = 1.5f;
    string[] detectableObjects = {  ""wall"" };
    private RayPerception ray;
    private Rigidbody r;
    private List<Transform> walls;

    float[] rayAngles = { 20f-90f, 90f-90f, 160f-90f, 45f-90f, 135f-90f, 70f-90f, 110f-90f };

    void Awake(){
        walls = new List<Transform>();
        for(int i = 1 ; i <= wall_Count; i++){
            GameObject go = Instantiate(wall_prefab);
            walls.Add(go.transform);
        }
    }

    public override void InitializeAgent(){
        base.InitializeAgent();
        r = GetComponent<Rigidbody>();
        ray = GetComponent<RayPerception>();
    }

    public override void AgentReset(){

        for(int i = 1 ; i <= wall_Count; i++){
            walls[i - 1].position = new Vector3(i * wall_distance ,  Random.Range(-wall_random,wall_random) , Random.Range(-wall_random,wall_random) );
        }
        Target.position = new Vector3(29.5f,  Random.Range(-wall_random,wall_random) , Random.Range(-wall_random,wall_random) );
        Ball.position = new Vector3(0.6f,  0f, 0f);
    }

    public override void CollectObservations(){
        AddVectorObs(ray.Perceive(rayDistance, rayAngles, detectableObjects, 0f, 0f));
        AddVectorObs(ray.Perceive(rayDistance, rayAngles, detectableObjects, 0f, 1f));
        AddVectorObs(ray.Perceive(rayDistance, rayAngles, detectableObjects, 0f, -1f));
        AddVectorObs(Target.position);
        AddVectorObs(this.transform.position);
    }

    public override void AgentAction(float[] vectorAction, string textAction){
        Vector3 controlSignal = Vector3.zero;
        controlSignal.x = vectorAction[0];
        controlSignal.y = vectorAction[1];
        controlSignal.z = vectorAction[2];
        controlSignal.Normalize();
        controlSignal *= Time.deltaTime;
        transform.Translate(controlSignal);
    }
    
    public override float[] Heuristic(){
        var action = new float[3];
        action[0] = Input.GetAxis(""Horizontal"");
        action[1] = Input.GetAxis(""Vertical"");
        action[2] = Input.GetAxis(""Zaxis"");
        return action;
    }

    private void OnCollisionEnter(Collision collision){
        float distanceToTarget = Vector3.Distance(transform.position, Target.position);
        if (collision.gameObject.CompareTag(""target"")){
            SetReward(1f);
            Done();
        }
        if (collision.gameObject.CompareTag(""wall"")){
            AddReward( - distanceToTarget / 50f );
            Done();
        }
    }

    private void OnTriggerExit(Collider col){
        if ( col.gameObject.CompareTag(""block"")){
            float distanceToTarget = Vector3.Distance(transform.position, Target.position);
            AddReward( - distanceToTarget / 100f );
            Done();
        }
    }
}


`",yea sure show code public class agent public transform target public transform ball public public public float public float float string wall private ray private private list transform float void awake new list transform go public override void ray public override void new vector new vector new vector public override void public override void float string vector public override float heuristic action new float action horizontal action vertical action return action private void collision collision float target done wall done private void col block float done,issue,negative,positive,neutral,neutral,positive,positive
558280790,Solved this issue by activating another movement script at the start of the training and as it completes one iteration disable the current script and enable the actual agent script.,issue another movement script start training one iteration disable current script enable actual agent script,issue,negative,neutral,neutral,neutral,neutral,neutral
558277085,"Can you tell a bit more about how the agent observes its world? The agent needs to have enough data about the environment to understand which direction the window is. Maybe you could give the agent the vector between itself and the next window, unless you consider that ""cheating"". ",tell bit agent world agent need enough data environment understand direction window maybe could give agent vector next window unless consider cheating,issue,negative,neutral,neutral,neutral,neutral,neutral
558274790,Ah I see the tutorial now. They use continuous in their guide (see the final part where they set agent parameters) so that's probably what you need.,ah see tutorial use continuous guide see final part set agent probably need,issue,negative,neutral,neutral,neutral,neutral,neutral
558273489,"Yea when the ball hits the wall or leave the limit everything resets ( wall/target position )
Do you think that Rayperception distance might affect the training ? I mean like should it be larger or smaller ?
",yea ball wall leave limit everything position think distance might affect training mean like smaller,issue,negative,negative,negative,negative,negative,negative
558272184,"The warning you see is something strange that unity started to do recently which I'm not sure of why. I get that message in ml agents and also several other places in my own code. That variabel is actually assigned from the unity editor, via a custom inspector, but for some reason the compiler doesn't realize that it works in that way so it still complain. It's not affecting the actual running game though.",warning see something strange unity recently sure get message also several code actually assigned unity editor via custom inspector reason compiler realize work way still complain affecting actual running game though,issue,negative,positive,neutral,neutral,positive,positive
558270757,"The reason you only get one float is that you use Space Type: Discrete, meaning your agent chooses between two actions. Then you get only one float and its value is an integer corresponding to the action index. So just do a Mathf.RoundToInt or similar, and do different actions based on that. 
In case you need two floats you can change space type to continuous

Note that the agent will start with heuristic if you run it without training, since you haven't linked a model into its behavior.
",reason get one float use space type discrete meaning agent two get one float value integer corresponding action index similar different based case need two change space type continuous note agent start heuristic run without training since linked model behavior,issue,negative,positive,neutral,neutral,positive,positive
558267994,"Since you give rewards very seldom (""sparse rewards"") the agent will take long time to learn and might never find its way unless you use certain things like curiosity and long batches.

For this case I don't think you need any of that though.

I would recommend trying to have a more continuous reward function to let the agent know all the time if it gets better or worse.
Try to do a small punishment every frame which is based on how far away from the correct direction it is. Maybe something like

AddReward(-distance_to_next_window * 0.01f)

Also try to add a reward for getting through a window successfully.

Also, do you restart the agent if it hits a wall? Or what happens?",since give seldom sparse agent take long time learn might never find way unless use certain like curiosity long case think need though would recommend trying continuous reward function let agent know time better worse try small punishment every frame based far away correct direction maybe something like also try add reward getting window successfully also restart agent wall,issue,positive,positive,positive,positive,positive,positive
558263482,"Thanks, I will be using venvs but am tempted to try conda. Not sure if conda will be more user-friendly than dos box.
Was just curious as to whether I would see issues down the line with current method of setup.

Not to worry, I will await the updated instructions. Many thanks to all involved",thanks try sure do box curious whether would see line current method setup worry await many thanks involved,issue,positive,positive,positive,positive,positive,positive
558262749,"I found the problem! Linux executables usually don't have an extension.
However mlagents-learn looks for a file ending with `.x86_64` ",found problem usually extension however file ending,issue,negative,negative,negative,negative,negative,negative
558261920,"Hm if absolute path doesn't work maybe there's some other issue.
I notice that your --env points to something without file extension, is that the executable file that you would usually run the game with? On Windows for example I need to point to the .exe file. Not sure how it works on Linux. Maybe someone from Unity team can help out better.",absolute path work maybe issue notice something without file extension executable file would usually run game example need point file sure work maybe someone unity team help better,issue,positive,positive,positive,positive,positive,positive
558254648,"Hi @CarlBinneman - i haven't tried your instructions, but known @anupambhatnagar is working to update them with clearer instructions.  a user doesn't necessarily need to use venvs or conda, but it makes it easier if you have multiple python projects locally.

@anupambhatnagar see insntructiosn above from Carl",hi tried known working update clearer user necessarily need use easier multiple python locally see carl,issue,negative,neutral,neutral,neutral,neutral,neutral
558252987,"Ok but why not leaving the document and mark it as ""deprecated"" like the cloud training?

https://github.com/Unity-Technologies/ml-agents/tree/master/docs#cloud-training-deprecated",leaving document mark like cloud training,issue,negative,neutral,neutral,neutral,neutral,neutral
558241927,"I have installed mlagents from pip, in that case what should be the env path?
I have even tried with the absolute path, and I still get this error",pip case path even tried absolute path still get error,issue,negative,positive,positive,positive,positive,positive
558239704,"It seems it doesn't find your build. In the documentation it says you should run it from the directory where you have ML-Agents installed. It seems to still find ml agents, but maybe the --env path needs to be relative to ml agents folder?",find build documentation run directory still find maybe path need relative folder,issue,negative,neutral,neutral,neutral,neutral,neutral
558237148,"So, to be clear : 
The instructions that I posted as working, are not the correct way.
One must follow the instructions provided in your link relating top Anaconda.

Is the above correct? @unityjeffrey ",clear posted working correct way one must follow provided link top anaconda correct,issue,positive,positive,positive,positive,positive,positive
558126492,"Maybe could have something to do with this:
￼

<img width=""1308"" alt=""Screen Shot 2019-11-25 at 1 04 49 PM"" src=""https://user-images.githubusercontent.com/16321685/69539133-2e977900-0f84-11ea-8c99-3abcd869314e.png"">

It's an error message that I get when opening the UnitySDK  in Unity, and after it asks to change the code so as to be more compatible with my version of unity (because the ml-agents SDK has been built in an older version of Unity)
",maybe could something screen shot error message get opening unity change code compatible version unity built older version unity,issue,negative,positive,positive,positive,positive,positive
558112036,"After further research, it seems that the agent only uses the heuristic method, and never without heuristic, because when I change the 
`    public override float[] Heuristic()`
function from returning an array of one to three actions, I do see the length of vectorAction increase in the agentAction function. Strange, since Use Heuristic has not been selected in the Agent's Behavior paramaters",research agent heuristic method never without heuristic change public override float heuristic function array one three see length increase function strange since use heuristic selected agent behavior,issue,negative,negative,neutral,neutral,negative,negative
557997898,"I just ran a test with the 3DBallHard environment using v.8 and the spikes are far less severe. There are some spikes but they appear to be limited to the first ephisode, then the training fully recovers immediately. This is very different from v.11 where it feels like there is a significant loss of training progress that needs to be re-learned with each --load.
",ran test environment far le severe appear limited first training fully immediately different like significant loss training progress need load,issue,negative,positive,positive,positive,positive,positive
557965804,"I did some experiments. Here are the findings.

I tried switching to the ""Walker"" example because it takes longer to train and is more similar to my environments. However the bug does NOT happen with the ""Walker"" environment. When I stop and restart, the reward resumes perfectly regardless of what I do with the --num-envs. I tried using 1, env, 16 envs and switching multiple times between 1 env and 16 envs. No bug. However the walker environment has a very small standard deviation with the reward. I think that might be related.

The bug does happen with the 3DBallHard environment. Even when I use 1 environment (although it recovers very quickly. Here is the tensorflow graph. I didn't change the length of the ephisode for this run. I just restarted multiple times always using --num-envs=1. You can see the downward spikes where the reward drops. Notice that the spikes correspond to ""short"" ephisiode lengths. I think the ephisode length might be related.

![image](https://user-images.githubusercontent.com/11359505/69508016-656c8100-0ee9-11ea-8db3-9748e3b07fc7.png)

The bug is worse if I use --num-envs=16. Here is the same graph, but using --num-envs=16 with multiple restarts. I never changed the num-envs. You can see the dips, but they are deeper and clearly take longer to recover from.

![image](https://user-images.githubusercontent.com/11359505/69508208-f5aac600-0ee9-11ea-9b3c-f823fb206d98.png)

My current hypothesis is that this may be related to the structure of the reward in the environments and the ""short"" ephisode lengths. In my own environments I start a bunch of agents in random poses. Some of these poses are hard to recover from and the agent resets many times very quickly with low rewards. The high reward successful agents take some time to return their rewards. I wonder if, when --load is used, the first few ephisodes contain only low rewards.

",tried switching walker example longer train similar however bug happen walker environment stop restart reward perfectly regardless tried switching multiple time bug however walker environment small standard deviation reward think might related bug happen environment even use environment although quickly graph change length run multiple time always see downward reward notice correspond short think length might related image bug worse use graph multiple never see clearly take longer recover image current hypothesis may related structure reward short start bunch random hard recover agent many time quickly low high reward successful take time return wonder load used first contain low,issue,positive,positive,neutral,neutral,positive,positive
557886027,"I tried putting a dummy agent in the scene, always there. With 1 observation and 1 action.
My problem is still happening. Python gives KeyError sometimes when I start to train, but it's about 1 out of 3 times so I just restart.",tried dummy agent scene always observation action problem still happening python sometimes start train time restart,issue,negative,positive,neutral,neutral,positive,positive
557819188,"I changed a few parameters and a reward system. It became an op tower :) 

Thank you! You can close the issue :)",reward system tower thank close issue,issue,positive,neutral,neutral,neutral,neutral,neutral
557775523,"Which version of MacOS are you using?
On 23 Nov 2019, 00:41 +0100, Maciej Hyla <notifications@github.com>, wrote:
> Same problem here but i downloaded repository as zip, run terminal and receive this messageMacBook-Pro:OSX maciej$ xattr -d com.apple.quarantine macblas.bundle xattr: macblas.bundle: No such xattr: com.apple.quarantine
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",version wrote problem repository zip run terminal receive reply directly view,issue,negative,positive,neutral,neutral,positive,positive
557738039,Ye after modifying the Agent.cs it could know record the data. Thank you a lot!,ye could know record data thank lot,issue,negative,neutral,neutral,neutral,neutral,neutral
557734559,"Same problem here but i downloaded repository as zip, run terminal  and receive this message`MacBook-Pro:OSX maciej$ xattr -d com.apple.quarantine macblas.bundle
xattr: macblas.bundle: No such xattr: com.apple.quarantine`",problem repository zip run terminal receive message,issue,negative,neutral,neutral,neutral,neutral,neutral
557661170,"Cool, will probably redo this PR instead of fixing the merge conflicts.

I'll make sure to update the windows script, although it's probably less important now that we removed the Custom* protos.",cool probably redo instead fixing merge make sure update script although probably le important removed custom,issue,positive,positive,positive,positive,positive,positive
557659848,"@erikfrey Hi, thanks for the clarification. 

So I ended up testing RL with pretraining and did as you specified, and there are some weird effects. Firstly, for SAC, it uses pretraining, BUT, only when I specify the number of steps > 0, but it is not converging (which makes me wonder if parameters used in RL are somehow impacting pretraining, i.e. some parameter that is affecting the weight updates in pretraining??). For PPO, it just doesn't do BC regardless of what I set :/

If you have an example config file/setting or scene where you've verified that pretraining works for PPO and SAC, can you share them? I'm wondering if there is some setting I'm using that is unintentionally causing it to not use pretraining for PPO. Likewise, for at least SAC, I'm pretty befuddled why I'm getting such divergent performance.  If you have two example configs to create equivalent behavior in SAC pretraining and offline BC, that would be great! 

The only thing of note is that the version i'm using according to GIT log is 0.10.1 (I believe it is pulled on october 21st) 

I'll try a few more variations over the weekend. 

I really appreciate the help! 

Best,

Michael",hi thanks clarification ended testing pretraining weird effect firstly sac pretraining specify number converging wonder used somehow pretraining parameter affecting weight pretraining regardless set example scene pretraining work sac share wondering setting unintentionally causing use pretraining likewise least sac pretty getting divergent performance two example create equivalent behavior sac pretraining would great thing note version according git log believe st try weekend really appreciate help best,issue,positive,positive,positive,positive,positive,positive
557653636,does the windows protobuf generation script also need to be updated?,generation script also need,issue,negative,neutral,neutral,neutral,neutral,neutral
557652175,"I think removing the `envs` directory makes sense here.  If there were any sibling directories, or code maybe it would make less sense.  Alas, there is nothing under the `mlagents` folder other than `envs`. ",think removing directory sense sibling code maybe would make le sense ala nothing folder,issue,negative,neutral,neutral,neutral,neutral,neutral
557649379,FWIW this will get better in the next release with the `--version` flag that @vincentpierre added. But can't do that just yet.,get better next release version flag added ca yet,issue,negative,positive,positive,positive,positive,positive
557634276,Having trouble getting Pyramid cloud training to run. Backing that out for now.,trouble getting pyramid cloud training run backing,issue,negative,negative,negative,negative,negative,negative
557632947,"@caioc2 Great! The changes are on develop now, and will be in the next release (hopefully around Dec 2nd).",great develop next release hopefully around,issue,positive,positive,positive,positive,positive,positive
557627323,"If anyone's interested, I decided to use one giant branch that has 1 action per combination. After masking, there's usually a fairly small number of actual legitimate actions remaining. It's working pretty well.",anyone interested decided use one giant branch action per combination usually fairly small number actual legitimate working pretty well,issue,positive,positive,neutral,neutral,positive,positive
557625417,"Hi @MarkTension ,
There are actually two types of Agents in the scene:
* the one under the AreaRenderTexture uses a Render Texture Sensor Component. This agent has its Camera's TargetTexture set to the RenderTexture that the sensor uses.
![image](https://user-images.githubusercontent.com/6877802/69447582-a9a22a80-0d0b-11ea-9267-fe0d68a102ad.png)
* the Agents in Area, Area (1), etc. are configured from the prefab and use a Camera Sensor Component. These agents don't have GridAgent.renderCamera set.
![image](https://user-images.githubusercontent.com/6877802/69447687-de15e680-0d0b-11ea-9a2a-64e3311415ab.png)

I'm not sure about the relative performance of these two approaches; I believe the Camera -> RenderTexture approach is mainly to show an example of how a RenderTexture can be used (and also for display in the scene).

> Additionally, how destructive for the agent will it be without the this

It's fine for most of the agents, but would break the Agent using the RenderTexture",hi actually two scene one render texture sensor component agent camera set sensor image area area prefab use camera sensor component set image sure relative performance two believe camera approach mainly show example used also display scene additionally destructive agent without fine would break agent,issue,negative,positive,neutral,neutral,positive,positive
557607691,"@mantasp  It seems to be working properly Thanks!

I'll test it more deeply later, but so far so good.",working properly thanks test deeply later far good,issue,positive,positive,positive,positive,positive,positive
557597254,@caioc2 could you please try our work in progress branch: https://github.com/Unity-Technologies/ml-agents/tree/develop-barracuda-0.3.x ?,could please try work progress branch,issue,positive,neutral,neutral,neutral,neutral,neutral
557551533,"The run ID was positively correct. But maybe some data got corrupted or something, because trying it again later (having changed between GIT branches) it worked again. Thanks for your input! I'm closing this.",run id positively correct maybe data got corrupted something trying later git worked thanks input,issue,positive,positive,positive,positive,positive,positive
557428304,We can close this issue now. Based on your reply in the other thread I got my question answered.,close issue based reply thread got question,issue,negative,neutral,neutral,neutral,neutral,neutral
557413319,"Hi @CarlBinneman @DVonk thank you for your feedback. Apologies that you are having issues, we recognize that the instructions are unclear for new python users. We are in process of making sure Conda is properly tested as part of our releases, put those instructions back for Anaconda + Win, and improve the venv instructions  . For now - refer to the previous Installation instructions for Windows https://github.com/Unity-Technologies/ml-agents/blob/0.10.1/docs/Installation-Windows.md",hi thank feedback recognize unclear new python process making sure properly tested part put back anaconda win improve refer previous installation,issue,positive,positive,positive,positive,positive,positive
557409048,"I already talked about this in other posts, but I agree with this so much. The Windows instructions especially feel like nobody even tested them before release. I think this issue should have the highest priority out of anything right now.",already agree much especially feel like nobody even tested release think issue highest priority anything right,issue,positive,positive,positive,positive,positive,positive
557360736,"@himanshi4693 - it's OK to ignore the `Number of Training Brains : 0` message. That log is obsolete; starting in the most recent version, we don't send brain parameters until later, so that list of brains is always empty. I'm cleaning it up in https://github.com/Unity-Technologies/ml-agents/pull/2950",ignore number training brain message log obsolete starting recent version send brain later list brain always empty cleaning,issue,negative,negative,neutral,neutral,negative,negative
557356516,"You shouldn't need to work with the data string directly. You can create DemonstrationMetaProto, BrainParametersProto, and AgentInfoProto objects and pass them to the code that I linked to above. The imports for them are at the top of the file: https://gist.github.com/chriselion/3714d05255eea2f9132b96a182fbdcaa#file-convert_demo-py-L12-L16

Yes, the .demo file is just a binary file with protobuf messages in it.",need work data string directly create pas code linked top file yes file binary file,issue,positive,positive,positive,positive,positive,positive
557339649,"It is using the environment to do inference and collect the rewards for reporting to Tensorboard (and so you can see what the agent is up to!). Currently there's no way to run ml-agents without the environment, but technically speaking the environment isn't being used to learn.",environment inference collect see agent currently way run without environment technically speaking environment used learn,issue,negative,neutral,neutral,neutral,neutral,neutral
557334781,"Hmm, as this is an unsupported feature at the moment, I'm not sure the current state of this functionality. But I have gotten it to work simply by removing `Application.isEditor` - you might have to modify it in `Agent.cs` too - in `SendInfoToBrain()`",unsupported feature moment sure current state functionality gotten work simply removing might modify,issue,negative,positive,positive,positive,positive,positive
557289061,"Aha then it makes sense, I have maybe just misunderstood the offline behavior cloning. From reading the documentation I didn't realize that it is also running inference at the same time as it is learning. Do I still need to have my agent in the scene? Thank you very much for your clarification!",aha sense maybe misunderstood behavior reading documentation realize also running inference time learning still need agent scene thank much clarification,issue,negative,positive,positive,positive,positive,positive
557281513,Moving the camera around during training will impact agent actions based on the policy learned from the demo files. This will not impact model training since the only interaction between the environment and model during behavioral cloning is for inference.,moving camera around training impact agent based policy learned impact model training since interaction environment model behavioral inference,issue,negative,neutral,neutral,neutral,neutral,neutral
557271428,"> Hi @Yunao-Shen, just put the folder name and put only the demo files you want in there. For instance, if you have `demo1.demo` and `demo2.demo` you can put them in a folder called `./demos` and put that in your `trainer_config`.
> 
> You can record demos in the build but you'll have to edit DemonstrationRecorder.cs. Remove all mentions of `Application.isEditor` in the if statements and the build will record.

Hi @ervteng , it seems that recording demo in build doesn't work. I modified the DemostrationRecorder.cs . It could generatate .demo file but it's almost empty, only including first line. I tried development build but it didnt solve the problem.",hi put folder name put want instance put folder put record demo build edit remove build record hi recording build work could file almost empty first line tried development build didnt solve problem,issue,negative,positive,neutral,neutral,positive,positive
557234745,"> Hi @mgb249, setting the pretraining_steps to 0 doesn't disable the RL - you also have to set the reward strength of your extrinsic reward to 0. We're working on combining BC and pretraining into one feature - stay tuned!

Also, about my second question, just to clarify, is it ONLY using the demo file (in both BC and for pretraining) - i.e. the environment is not being used ? ",hi setting disable also set reward strength extrinsic reward working combining pretraining one feature stay tuned also second question clarify file pretraining environment used,issue,positive,neutral,neutral,neutral,neutral,neutral
557220050,"Hi @mgb249, setting the pretraining_steps to 0 doesn't disable the RL - you also have to set the reward strength of your extrinsic reward to 0. We're working on combining BC and pretraining into one feature - stay tuned!",hi setting disable also set reward strength extrinsic reward working combining pretraining one feature stay tuned,issue,positive,neutral,neutral,neutral,neutral,neutral
557207086,"Yes, that can be done pretty easily by running mlagents-learn and adding the `--load` parameter, with the same run-id, and quitting immediately using Ctrl+C when the agents start training. This should load the latest saved checkpoint and then export to barracuda. ",yes done pretty easily running load parameter immediately start training load latest saved export barracuda,issue,positive,positive,positive,positive,positive,positive
557206350,"This shouldn't affect the training - I think it's either a reporting issue or randomness in the training. I think you're still OK with using these models.
I've logged the issue with internal reference MLA-365. Please keep us posted with what happens in your experiments so we can diagnose the issue. ",affect training think either issue randomness training think still logged issue internal reference please keep u posted diagnose issue,issue,negative,neutral,neutral,neutral,neutral,neutral
557191138,"I did not use visual observations, so it is possible it might be treating those inputs differently. Anyway, hopefully they'll answer my question, or yours :).

If I don't hear back in a few days, I might just bit the bullet and run a debugger on the python code to see what is getting passed in tensorflow. :/",use visual possible might treating differently anyway hopefully answer question hear back day might bit bullet run python code see getting,issue,negative,neutral,neutral,neutral,neutral,neutral
556943176,"@himanshi4693 As long as you get the message that the academy has started successfully (and the environment executable should launch as well) then you should be golden. For me, it also said that the number of training brains was 0, but I was still able to call `step()` on the environment with actions, get observations, and progress the agent through the environment. Ultimately, these are the things you need to train your RL agent.",long get message academy successfully environment executable launch well golden also said number training brain still able call step environment get progress agent environment ultimately need train agent,issue,positive,positive,positive,positive,positive,positive
556698650,This was a feature I also used in previous versions of the MLAgents toolkit and would like to second the request for it to return as an official feature.,feature also used previous would like second request return official feature,issue,negative,negative,neutral,neutral,negative,negative
556508790,"Thanks @chriselion. I just started looking into protobuf and will see if I can figure it out. 
Can you provide an example of how the data string will look like before getting encoded by protobuf?
Also, Is the .demo file just a binary file created from the protobuf encoding?",thanks looking see figure provide example data string look like getting also file binary file,issue,positive,positive,positive,positive,positive,positive
556455895,"Regarding your last question I have recently seen some strange behaviour that agents use the camera in the scene even though I expect the visual observations should just be stored in the demo file (hence independent of environment while training). I filed an issue (#2944) and will await clarifications there :) Also subbing to this in case you find something! :+1:

EDIT: Forgot to ask, do you also use visual observations?",regarding last question recently seen strange behaviour use camera scene even though expect visual file hence independent environment training issue await also subbing case find something edit forgot ask also use visual,issue,negative,negative,neutral,neutral,negative,negative
556352096,Thank you @harperj :) for now I'm pursuing a workaround where I'm manually buffering past observations and actions to feed them into the network. Maybe this might help other people in the meantime.,thank manually past feed network maybe might help people,issue,positive,negative,negative,negative,negative,negative
556337180,Thanks for the update @wrd90.  We're keeping an eye on TF2 and evaluating when it would make sense to upgrade.  Good luck with your project!,thanks update keeping eye would make sense upgrade good luck project,issue,positive,positive,positive,positive,positive,positive
556000408,"@Smileynator : any ideas on how to train a simulation with my own learning algorithm, using the mlagents env API on jupyter notebook? Details here : https://github.com/Unity-Technologies/ml-agents/issues/2940",train simulation learning algorithm notebook,issue,negative,neutral,neutral,neutral,neutral,neutral
555941933,"In a recent version, ""brain"" files were removed. Instead you now add an extra script to an agent, to give ot the previous ""brain"" settings, also check your academy setting to see if you are actually training, or just going from a NN file",recent version brain removed instead add extra script agent give previous brain also check academy setting see actually training going file,issue,negative,negative,neutral,neutral,negative,negative
555931638,"It seems you installed tensorflow by default, which is incompatible with your old version tensorflow dependencies. (for example, if you installed tensorflow 2.0 before, and switched your CUDA version to 10.1 accordingly, you'll get these error messages, because CUDA 10.1 is not compatible with tensorflow 1.15)
To check this, just open your Anaconda Prompt, type in ""python"", and then type in  ""import tensorflow"", you'll probably get error messages such as ""Could not load dynamic library...""",default incompatible old version example switched version accordingly get error compatible check open anaconda prompt type python type import probably get error could load dynamic library,issue,negative,positive,neutral,neutral,positive,positive
555921070,"Thanks @stale bot for reminding me to update my findings here.

According to [this site](https://www.tensorflow.org/lite/convert/rnn#not_currently_supported), RNNs are not yet supported, but support is planned ""by the end of 2019"". I expect this support to be added only to TF2, therefore I reckon ML-Agents will need to migrate to TF2 as well before this is possible... To my knowledge this is a significant piece of work, especially what concerns saving of models and graphs. Up to now I have not been able to convert the frozen graph (which is no longer supported in TF2) to a SavedModel, because there is a fair bit of coding necessary in the model generation. Due to time restrictions for my work I had to move on to another solution for now.",thanks stale bot update according site yet support end expect support added therefore reckon need migrate well possible knowledge significant piece work especially saving able convert frozen graph longer fair bit necessary model generation due time work move another solution,issue,positive,positive,positive,positive,positive,positive
555858349,"Hi @ervteng and thanks for the suggestion! Yes we have no agents in the scene at start. Right now it works like 50% of the time while sometimes I get the KeyError at start. So I suppose it depends a bit on the timing of whether an agent had time to run a step first.
I will give it a try with a dummy agent and get back to you :+1:",hi thanks suggestion yes scene start right work like time sometimes get start suppose bit timing whether agent time run step first give try dummy agent get back,issue,positive,positive,positive,positive,positive,positive
555854881,"Hi @gauravspatil,
We don't provide any utilities for this. But the demo file format is pretty simple, it contains
1) A DemonstrationMetaProto protobuf message.
2) A BrainParametersProto protobuf message.
3) A sequence of `AgentInfoProto` protobuf messages.
There are a few other details like padding after the DemonstrationMetaProto and encoded size before each of the protos.

I was recently writing some demo files in python and wrote a small utility that handles the extra details; here's the relevant part:
https://gist.github.com/chriselion/3714d05255eea2f9132b96a182fbdcaa#file-convert_demo-py-L101-L115
(you can ignore more of the rest of the file)

So if you're comfortable forming the protobuf objects, that should be enough to get you going.",hi provide file format pretty simple sequence like padding size recently writing python wrote small utility extra relevant part ignore rest file comfortable forming enough get going,issue,positive,positive,neutral,neutral,positive,positive
555848397,@ervteng do you have any insights into generating a .nn file from the last saved checkpoint? Due to the training process crashing @devedse would like to extract the model from the saved checkpoint.,generating file last saved due training process would like extract model saved,issue,positive,negative,neutral,neutral,negative,negative
555754840,"> Anyhow, after I fixed that issue I got rid of the error message in unity, and the training seems to start but then immediately i get a crash in python. I will get back to you with logs soon, but it crashes due to a missing key (our behaviour name) in the _brains in environment.py. I added some debug prints in update_brain_params and I see that most of our behaviours are not added to _brains since they have 0 agent infos. I'm a bit unsure of the order stuff gets initialized, maybe we miss some step in the Unity code.

@robinerd I think I know what's going on here - is it correct that when the environment starts up there might not be any Agents in the scene? The way the code is written currently, it assumes there is at least one Agent in the scene at startup, otherwise it won't spawn the right trainers on the python side. Adding new Agents with new behavior names should be no problem once the first one is initialized. So the quick workaround might be to have an agent with some dummy behavior name in the scene (not dynamically instantiated). If this works, we can fix the problem in the Python code pretty easily and push a patch. ",anyhow fixed issue got rid error message unity training start immediately get crash python get back soon due missing key behaviour name added see added since agent bit unsure order stuff maybe miss step unity code think know going correct environment might scene way code written currently least one agent scene otherwise wo spawn right python side new new behavior problem first one quick might agent dummy behavior name scene dynamically work fix problem python code pretty easily push patch,issue,negative,positive,positive,positive,positive,positive
555744555,"> Hi @watarigasu, can you share your ML-Agents SDK version (the UnitySDK folder) and your Python version? If you're using the latest 0.11 Python code you'll have to update the UnitySDK folder.

ITS OKAY but thank you 
I uninstalled and updated from version 0.6.0 to 0.10.0. 
I went through mountains of hell to install CUDA and the right grpc files and finally am working something. but thank you so much @ervteng ",hi share version folder python version latest python code update folder thank uninstalled version went hell install right finally working something thank much,issue,negative,positive,positive,positive,positive,positive
555727837,"@ervteng As much as i wish that to be true, it looked to me as if the game was making proper linear progression at a low pace, instead of resetting and teleporting agents. But i will check it later once i got the learning down now that it runs normally from my perspective.

I am aware of the benefits of running multiple build clients. But in debugging and testing i rather keep it in editor until i feel i have it roughly working right. Might need some good eductional books on the subject :P

Thanks for all the help, and thanks for the clarification on the term. If it's not in the docs, please put it somewhere so people know what experiences are in the context, i was out of the loop!",much wish true game making proper linear progression low pace instead check later got learning normally perspective aware running multiple build testing rather keep editor feel roughly working right might need good subject thanks help thanks clarification term please put somewhere people know context loop,issue,positive,positive,positive,positive,positive,positive
555686520,"Thanks for the response. I will try experimienting with the --num-envs=1 to see if it only happens when switching between --num-envs=1 and num-envs>1.

Will this affect the training algorithm? Will it confuse PPO if I switch the --num-envs and resume training? The training seems to recover from the dip fairly quickly, (faster than it would take to re-learn the lost knowledge) but slower than I would expect. I would expect the dip to clear itself in one or two resets of each agent. It seems to take MUCH longer than that to recover from the dip.

The dip seems to happen even if I resume training using the same --num-envs. Here is some example output where I do NOT switch the --num-envs. The reward drops from 11-20 to 5-8 when I resume. The reward stays in the lower range for over 30k steps and counting.

FIRST RUN:
```INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacterPhasePrediction: Step: 10092847. Time Elapsed: 1243.707 s Mean Reward: 57.743. Std of Reward: 12.929. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12217743. Time Elapsed: 1253.471 s Mean Reward: 13.209. Std of Reward: 23.204. Training.
INFO:mlagents.envs:Saved Model
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12218743. Time Elapsed: 1274.913 s Mean Reward: 20.419. Std of Reward: 33.086. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12219743. Time Elapsed: 1284.694 s Mean Reward: 11.160. Std of Reward: 20.645. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacterPhasePrediction: Step: 10095847. Time Elapsed: 1284.711 s Mean Reward: 58.667. Std of Reward: 13.603. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12220744. Time Elapsed: 1305.315 s Mean Reward: 17.581. Std of Reward: 28.757. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12221743. Time Elapsed: 1325.829 s Mean Reward: 14.007. Std of Reward: 26.339. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12222743. Time Elapsed: 1335.527 s Mean Reward: 17.353. Std of Reward: 29.131. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacterPhasePrediction: Step: 10098847. Time Elapsed: 1335.536 s Mean Reward: 58.433. Std of Reward: 12.810. Training.
INFO:mlagents.envs:Saved Model
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12223743. Time Elapsed: 1356.651 s Mean Reward: 15.037. Std of Reward: 26.640. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12224743. Time Elapsed: 1366.723 s Mean Reward: 16.577. Std of Reward: 27.787. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12225743. Time Elapsed: 1387.484 s Mean Reward: 18.457. Std of Reward: 31.844. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacterPhasePrediction: Step: 10101847. Time Elapsed: 1387.494 s Mean Reward: 58.037. Std of Reward: 13.871. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12226743. Time Elapsed: 1397.364 s Mean Reward: 16.226. Std of Reward: 29.070. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12227743. Time Elapsed: 1417.817 s Mean Reward: 14.887. Std of Reward: 25.974. Training.
INFO:mlagents.envs:Saved Model
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12228743. Time Elapsed: 1438.794 s Mean Reward: 16.025. Std of Reward: 26.960. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacterPhasePrediction: Step: 10104847. Time Elapsed: 1438.801 s Mean Reward: 58.154. Std of Reward: 14.002. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12229743. Time Elapsed: 1448.802 s Mean Reward: 17.322. Std of Reward: 31.237. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12230745. Time Elapsed: 1469.318 s Mean Reward: 15.368. Std of Reward: 25.114. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12231743. Time Elapsed: 1479.463 s Mean Reward: 13.405. Std of Reward: 25.155. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacterPhasePrediction: Step: 10107847. Time Elapsed: 1479.480 s Mean Reward: 56.817. Std of Reward: 14.140. Training.```

Now I stop [CTRL-C] and restart with exactly the same command line (using the same --num-envs)
Notice that the reward drops from 11-20 range to 5-8 range. The reward stays in the 5-8 range for many thousands of steps.

INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12233054. Time Elapsed: 12.496 s Mean Reward: 1.495. Std of Reward: 1.575. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12234054. Time Elapsed: 23.014 s Mean Reward: 5.634. Std of Reward: 19.058. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12235054. Time Elapsed: 47.078 s Mean Reward: 5.837. Std of Reward: 18.223. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacterPhasePrediction: Step: 10111156. Time Elapsed: 47.090 s Mean Reward: 56.606. Std of Reward: 14.773. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12236054. Time Elapsed: 58.198 s Mean Reward: 7.404. Std of Reward: 25.398. Training.
INFO:mlagents.envs:Saved Model
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12237054. Time Elapsed: 81.132 s Mean Reward: 6.309. Std of Reward: 18.655. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12238054. Time Elapsed: 103.025 s Mean Reward: 6.318. Std of Reward: 22.586. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacterPhasePrediction: Step: 10114156. Time Elapsed: 103.034 s Mean Reward: 56.810. Std of Reward: 14.473. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12239054. Time Elapsed: 113.836 s Mean Reward: 5.784. Std of Reward: 15.890. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12240055. Time Elapsed: 134.933 s Mean Reward: 5.468. Std of Reward: 20.378. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12241054. Time Elapsed: 155.485 s Mean Reward: 7.066. Std of Reward: 22.630. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacterPhasePrediction: Step: 10117156. Time Elapsed: 155.500 s Mean Reward: 55.891. Std of Reward: 14.088. Training.
INFO:mlagents.envs:Saved Model
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12242054. Time Elapsed: 177.000 s Mean Reward: 6.768. Std of Reward: 19.976. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12243054. Time Elapsed: 197.970 s Mean Reward: 6.917. Std of Reward: 20.901. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12244054. Time Elapsed: 208.210 s Mean Reward: 7.074. Std of Reward: 21.412. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacterPhasePrediction: Step: 10120156. Time Elapsed: 208.226 s Mean Reward: 57.065. Std of Reward: 12.698. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12245054. Time Elapsed: 229.102 s Mean Reward: 7.418. Std of Reward: 24.697. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12246054. Time Elapsed: 239.235 s Mean Reward: 8.265. Std of Reward: 24.186. Training.
INFO:mlagents.envs:Saved Model
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12247054. Time Elapsed: 260.504 s Mean Reward: 6.809. Std of Reward: 19.344. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacterPhasePrediction: Step: 10123156. Time Elapsed: 260.517 s Mean Reward: 57.928. Std of Reward: 12.869. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12248054. Time Elapsed: 281.370 s Mean Reward: 8.070. Std of Reward: 25.212. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12249054. Time Elapsed: 291.562 s Mean Reward: 6.238. Std of Reward: 20.732. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacter: Step: 12250054. Time Elapsed: 312.365 s Mean Reward: 7.452. Std of Reward: 22.609. Training.
INFO:mlagents.trainers: HumanoidWalk31: HumanoidCharacterPhasePrediction: Step: 10126156. Time Elapsed: 312.375 s Mean Reward: 58.415. Std of Reward: 12.721. Training.```",thanks response try see switching affect training algorithm confuse switch resume training training recover dip fairly quickly faster would take lost knowledge would expect would expect dip clear one two agent take much longer recover dip dip happen even resume training example output switch reward resume reward stay lower range counting first run step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward stop restart exactly command line notice reward range range reward stay range many step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training saved model step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward training step time mean reward reward,issue,positive,negative,negative,negative,negative,negative
555684373,"Hi @GProulx,
We felt there were a couple of problems with the old approach. First, modifying the custom proto definitions and rebuilding the code was a source of problems for some users, especially on Windows.
The larger problem was that just defining the custom observations and actions wasn't enough for the trainers to be able to do anything with them; there was an expectation by some users that all they would need to do is define the custom protos, and suddenly the trainer would start consuming the observations or producing the actions, whereas it would actually require a lot of customization to enable this, and we had no good examples of this.

The recent ISensor interface changes give us a chance to bring some of this back in a different way. I think we can add a new ""compression type"" to transmit a [Google.Protobuf.WellKnownTypes.Any](https://developers.google.com/protocol-buffers/docs/reference/csharp/class/google/protobuf/well-known-types/any), and provide a corresponding user-defined hook in the python side to decode this into an observation.

Do you have more details on how you were using the Custom Observations and feeding them into the trainer?",hi felt couple old approach first custom proto code source especially problem custom enough able anything expectation would need define custom suddenly trainer would start consuming whereas would actually require lot enable good recent interface give u chance bring back different way think add new compression type transmit provide corresponding hook python side decode observation custom feeding trainer,issue,negative,positive,positive,positive,positive,positive
555630586,"Hi @watarigasu, can you share your ML-Agents SDK version (the UnitySDK folder) and your Python version? If you're using the latest 0.11 Python code you'll have to update the UnitySDK folder. ",hi share version folder python version latest python code update folder,issue,negative,positive,positive,positive,positive,positive
555630290,"Hi @Phong13, we did change how the number of steps were logged when using --num-envs=XX in 0.9. They're now logged as the total steps across environments, rather than assuming one environment (e.g. if you have 8 environments, one recorded ""step"" is actually 8 steps in v0.8, and 1 step in v0.9). This means you'll be recording summaries at a higher frequency and may see dips you won't see in v0.8. 

Does the dip happen with --num-envs=1?",hi change number logged logged total across rather assuming one environment one step actually step recording higher frequency may see wo see dip happen,issue,negative,positive,neutral,neutral,positive,positive
555627037,"@Smileynator an experience is a single action and observation. 

I think what you're seeing isn't a slowdown - it's a behavior of the rendering system at high timescales :P Hopefully what you're also noticing is that things are jumping around in the game. This means the simulation is still running fast, it just is only rendering to the screen at 0.3 FPS. Since you're not using the rendered image as observations for your agent, you're still training. 

Making the game into a build (rather than running in the editor) should provide a decent speedup as well. ",experience single action observation think seeing slowdown behavior rendering system high hopefully also around game simulation still running fast rendering screen since image agent still training making game build rather running editor provide decent well,issue,positive,negative,neutral,neutral,negative,negative
555622219,@himanshi4693 Hmmm I don't have any experience trying to get ML-Agents working with external models. Were you able to figure it out?,experience trying get working external able figure,issue,negative,positive,positive,positive,positive,positive
555488793,"@anupambhatnagar , what I'm running into currently is that the training process crashes sometimes. Sometimes due to something I can't really pinpoint and also a few times due to an out of memory exception.

Is it possible to manually generate a .nn file from the last snapshot without starting to train? (I noticed some issues when I continued training, but lets leave that for another day)",running currently training process sometimes sometimes due something ca really pinpoint also time due memory exception possible manually generate file last snapshot without starting train continued training leave another day,issue,negative,negative,neutral,neutral,negative,negative
555446004,"Ah that actually got me somewhere.
@ervteng  Just to document what happened. I set this up in the editor. With the settings set to 100x timescale, and unlimited FPS.
I think mainly the timescale here, causes something to mess up when one of the two can not keep up.
My expected behaviour here would be ""Run as fast as you can possibly manage, capping at 100x normal game speed""
The behaviour instead was something like ""chug at 0.3FPS"".

Is this something i misunderstood, or is it simply that the neural network and editor are not synced up? I would personally expect unity to call evaluate once per frame/physics step (whatever is set up)
Maybe it has something to do with max physics step and these insanely high settings causing a conflict of unreachable desired speed, and as a result the physics system could not get it's calculations in at the fixed timestep, etc. etc.?

Explaining here what exactly went wrong compared to expectations, might be a great insight for others.

PS: in the docs for the config file it says:
`
batch_size | The number of experiences in each iteration of gradient descent.
`
What is an ""experience"". Amount of time a reward is given? Times agent's Done() is triggered? Steps? Depending on what this means, i want to set it vastly different.",ah actually got somewhere document set editor set unlimited think mainly something mess one two keep behaviour would run fast possibly manage capping normal game speed behaviour instead something like chug something misunderstood simply neural network editor would personally expect unity call evaluate per step whatever set maybe something physic step insanely high causing conflict unreachable desired speed result physic system could get fixed explaining exactly went wrong might great insight file number iteration gradient descent experience amount time reward given time agent done triggered depending want set vastly different,issue,positive,positive,neutral,neutral,positive,positive
555300774,"> Thanks! Will get back at you when I get it working for me

I just got it working. Since there isn't any documentation for the latest version, I downgraded to unity 2018.1.0. From there, I just followed [this medium article](https://medium.com/@indiecontessa/setting-up-a-python-environment-with-tensorflow-on-macos-for-training-unity-ml-agents-faf19d71201) and created my own academy, brain, and agent. I also created my own script to interface with the environment python/genetic_algorithm.py since I don't want to use RL. 

Here is all my code:
https://github.com/iamPres/Bipedal-Walker

Hope this helps",thanks get back get working got working since documentation latest version unity medium article academy brain agent also script interface environment since want use code hope,issue,positive,positive,positive,positive,positive,positive
555298554,"This looks good, thanks for making the clarification! Could you repeat the changes on the `develop` branch and re-target that? Thanks!",good thanks making clarification could repeat develop branch thanks,issue,positive,positive,positive,positive,positive,positive
555296366,"> Will we need to re-record all of them or just the Hallway one?

I reconverted them already",need hallway one already,issue,negative,neutral,neutral,neutral,neutral,neutral
555296125,"> Models will still work as before but the demo files will not.

Will we need to re-record all of them or just the Hallway one?",still work need hallway one,issue,negative,neutral,neutral,neutral,neutral,neutral
555290960,"Just to clarify, existing models will keep working as before? The only thing that would break when upgrading is existing demo files?",clarify keep working thing would break,issue,negative,neutral,neutral,neutral,neutral,neutral
555280459,"Hi @Smileynator, it seems like your neural network code is running quite fast (`PPOPolicy.evaluate` and `update_policy`. That means it's most likely the Unity executable itself. Can you try running with one of the example environments and seeing if it's running slowly? Does it also run slowly without Python?",hi like neural network code running quite fast likely unity executable try running one example seeing running slowly also run slowly without python,issue,negative,negative,negative,negative,negative,negative
555280238,"Hi @chriselion 

I’m really surprised to discover that the project is getting rid of CustomObservation. For me CustomObs is very helpful. For example to return data of variable length on each Step or based on some initials reset parameters. Using ProtoBuf message is also interesting to easily manipulate those received objects in Python directly. 

I already thought about a few options to work around that suppression but can you tell me why that decision was made and if you have some alternative solutions to suggest?

Thanks",hi really discover project getting rid helpful example return data variable length step based reset message also interesting easily manipulate received python directly already thought work around suppression tell decision made alternative suggest thanks,issue,positive,positive,positive,positive,positive,positive
555270599,"I am just going to share that. It's not the most readable to me as to which is which. It is saying my unity environment.step takes 81 seconds?
To clarify, this is a totally empty scene, with 1 agents and 1 academy. it's a simple car and a plane to drive it on. On my end there should not be happening anything intensive.

What i must add, is that i half set up the environment once before. and thus the project had to be upgraded from a version that used ""brain"" files, to the new thing. Maybe that upgrade did something wacky? (even though i replaced all of the code of ML-agents, and have not altered mine IIRC)

I think the neural network settings are the default, i did not bother adjusting it to a custom set yet. And like i said, 3 input floats, 3 output floats. 

`{
  ""name"": ""root"",
  ""gauges"": [
    {
      ""name"": ""CarBrain.mean_reward"",
      ""value"": -1.0,
      ""min"": -1.0,
      ""max"": -1.0,
      ""count"": 10
    }
  ],
  ""total"": 196.0640960826856,
  ""count"": 1,
  ""self"": 27.23873248419713,
  ""children"": [
    {
      ""name"": ""TrainerController.advance"",
      ""total"": 168.82536359848848,
      ""count"": 50001,
      ""self"": 26.59743871954896,
      ""children"": [
        {
          ""name"": ""env_step"",
          ""total"": 138.54210463491776,
          ""count"": 50001,
          ""self"": 106.10325752416313,
          ""children"": [
            {
              ""name"": ""SubprocessEnvManager._take_step"",
              ""total"": 31.966718216435595,
              ""count"": 50001,
              ""self"": 0.7935256180679602,
              ""children"": [
                {
                  ""name"": ""PPOPolicy.evaluate"",
                  ""total"": 31.173192598367635,
                  ""count"": 50001,
                  ""self"": 31.173192598367635
                }
              ]
            },
            {
              ""name"": ""workers"",
              ""total"": 0.47212889431903626,
              ""count"": 50001,
              ""self"": 0.0,
              ""children"": [
                {
                  ""name"": ""worker_root"",
                  ""total"": 193.0269196876839,
                  ""count"": 50001,
                  ""is_parallel"": true,
                  ""self"": 111.64083326897189,
                  ""children"": [
                    {
                      ""name"": ""UnityEnvironment.step"",
                      ""total"": 81.386086418712,
                      ""count"": 50001,
                      ""is_parallel"": true,
                      ""self"": 8.594779898282425,
                      ""children"": [
                        {
                          ""name"": ""UnityEnvironment._generate_step_input"",
                          ""total"": 2.0340816534307233,
                          ""count"": 50001,
                          ""is_parallel"": true,
                          ""self"": 2.0340816534307233
                        },
                        {
                          ""name"": ""communicator.exchange"",
                          ""total"": 70.75722486699885,
                          ""count"": 50001,
                          ""is_parallel"": true,
                          ""self"": 70.75722486699885
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          ""name"": ""update_policy"",
          ""total"": 3.6858202440217553,
          ""count"": 4,
          ""self"": 2.8439364363836717,
          ""children"": [
            {
              ""name"": ""PPOPolicy.update"",
              ""total"": 0.8418838076380837,
              ""count"": 120,
              ""self"": 0.8418838076380837
            }
          ]
        }
      ]
    }
  ]
}`

any clue what is going on? :sweat_smile: ",going share readable saying unity clarify totally empty scene academy simple car plane drive end happening anything intensive must add half set environment thus project version used brain new thing maybe upgrade something wacky even though code mine think neural network default bother custom set yet like said input output name root name value min count total count self name total count self name total count self name total count self name total count self name total count self name total count true self name total count true self name total count true self name total count true self name total count self name total count self clue going,issue,positive,positive,neutral,neutral,positive,positive
555249780,"@ervteng So If I train an agent for 10000 steps and have 5 levels, Do I change scenes every 10000/5 levels?",train agent change every,issue,negative,neutral,neutral,neutral,neutral,neutral
555229970,"So I have been looking into this more now, and I almost got it to work.
I saw that I had done a tweak in the Agent.cs code to get inference mode working so I reverted that and started again:

**First try for inference:**
- disable game object
- Add component MonsterAgent
- Initialize variables in BehaviorParameters and AgentParameters.
- GiveModel(""TrainedTentacles"")
- enable game object.
Result: Give model crashes since the m_PolicyFactory is still null at that time

**Try two for inference:**
Do GiveModel AFTER enabling the object.
**Result: Inference works!**

**Next step: trying to make training work**
As soon as I call GiveModel the python fails with KeyError: 'My Behavior' in environment.py, in _get_state. It makes no sense to do give model in the training, but that's the only way to set different behaviour names on different monster types.
So I edited BehaviorParameters.behaviorName to be settable also. Then I can init my agents properly from script.

So in training mode, we set the behaviorName to a dynamic string how we want the model name to be. And in inference we do GiveModel with the actual name of the trained behaviour.
For that logic I had to also extend our academy like this:
```
public class MadnessAcademy : Academy {
    public static bool isTraining = false;
    public override void AcademyReset()
    {
        isTraining = !GetIsInference();
    }
}
```

It also works with respawning agents as @ervteng pointed out. However since we use multiple behaviours and randomize enemies, we might have agent behaviours disappearing or being added to the scene as we train, based on randomness. This causes the same KeyError in python code as mentioned above.

It is possible though for us to know all agent behavior names at start, but I'm not sure what to do with that information. I suppose we need to keep all the brains active in some way even when they're not in the scene. Maybe we could make sure to always have at least one monster of each behaviour name. Is there any better way?",looking almost got work saw done tweak code get inference mode working first try inference disable game object add component initialize enable game object result give model since still null time try two inference object result inference work next step trying make training work soon call python behavior sense give model training way set different behaviour different monster settable also properly script training mode set dynamic string want model name inference actual name trained behaviour logic also extend academy like public class academy public static bool false public override void also work pointed however since use multiple randomize might agent disappearing added scene train based randomness python code possible though u know agent behavior start sure information suppose need keep brain active way even scene maybe could make sure always least one monster behaviour name better way,issue,positive,positive,neutral,neutral,positive,positive
555195459,"Hi, and thanks! I am using the latest unitysdk folder, but I can double check to remove it and add again tomorrow to make sure I didn't mess anything up while trying to fault find. I did realize today that I forgot to initialize the vector action descriptions array. The thing we do is to add our agent script with AddComponent and then initialize values in the agent parameters script that gets auto-added. All this initialization is done on a game object which is not active, and after that we do SetActive(true). That's to avoid issues with onenable and start being called before initialization is done.

Anyhow, after I fixed that issue I got rid of the error message in unity, and the training seems to start but then immediately i get a crash in python. I will get back to you with logs soon, but it crashes due to a missing key (our behaviour name) in the _brains in environment.py. I added some debug prints in update_brain_params and I see that most of our behaviours are not added to _brains since they have 0 agent infos. I'm a bit unsure of the order stuff gets initialized, maybe we miss some step in the Unity code.
We're using on demand decisions, about 10 times per second from a coroutine.
Sorry if some of the script names above are wrong, I don't have the code in front of me right now.

Maybe @vincentpierre could point us in the right direction of the general approach to spawn an agent and init it, with code. I know we're doing things a bit out of the ordinary :stuck_out_tongue: ",hi thanks latest folder double check remove add tomorrow make sure mess anything trying fault find realize today forgot initialize vector action array thing add agent script initialize agent script done game object active true avoid start done anyhow fixed issue got rid error message unity training start immediately get crash python get back soon due missing key behaviour name added see added since agent bit unsure order stuff maybe miss step unity code demand time per second sorry script wrong code front right maybe could point u right direction general approach spawn agent code know bit ordinary,issue,negative,positive,neutral,neutral,positive,positive
555189353,"Hi, Thanks for that! I will give it a try, should be enough for now.",hi thanks give try enough,issue,negative,positive,neutral,neutral,positive,positive
555171759,"Hi @Irekter, we currently don't support switching scenes during training at the moment, as the Academy object is required to communicate with Python and would get destroyed if the scenes are switched. 

There are workarounds, e.g. setting a Don't Destroy On Load for the Academy and instantiating it only on the first scene, but they're not officially supported. ",hi currently support switching training moment academy object communicate python would get switched setting destroy load academy first scene officially,issue,negative,positive,neutral,neutral,positive,positive
555170460,"Hi @robinerd, are you using the latest ML-Agents UnitySDK in the project? We've seen similar issues when the UnitySDK version is from an older release (0.10 and below) and is used with Python 0.11. 

The respawning during training should be no issue as long as the behavior name stays the same. Even with a different behavior name, a new Trainer would be spawned, just as before. ",hi latest project seen similar version older release used python training issue long behavior name stay even different behavior name new trainer would,issue,negative,positive,positive,positive,positive,positive
555137153,"It's OK for curiosity encoding size to be bigger than observation size. It corresponds to the complexity of information encoded by the curiosity module, not necessarily the dimension. ",curiosity size bigger observation size complexity information curiosity module necessarily dimension,issue,negative,neutral,neutral,neutral,neutral,neutral
555136786,"Hi @droidXrobot, visual hallway isn't all that great at training :P I believe it's quite hard for the CNN to differentiate between the O and the X in the middle. You can try making the problem a bit easier by changing the color of the two targets, or perhaps try changing `vis_encoder_type` to `nature` or `resnet` and increasing the batch size. ",hi visual hallway great training believe quite hard differentiate middle try making problem bit easier color two perhaps try nature increasing batch size,issue,positive,positive,positive,positive,positive,positive
555135199,"Hey @robinerd, fantastic stuff with the monsters in Source of Madness! I wasn't at Unite but saw some of the demos - really cool. 

We did change the way brains are instantiated in 0.11 - we'll look into this issue (@vincentpierre). If you just want to get SAC running right away, try 0.10.1 - that one doesn't contain the change in brain spawning but does have the latest algorithms. Will keep you posted on what we find. ",hey fantastic stuff source madness unite saw demo really cool change way brain look issue want get sac running right away try one contain change brain spawning latest keep posted find,issue,positive,positive,positive,positive,positive,positive
555132925,"That CPU should be fine for training. How big is your neural network? 

After quitting your run, there should be a *.json file in your summaries directory - it should contain some timings for parts of your code - if you're still having problems, would you mind posting the output of that? Thanks!",fine training big neural network run file directory contain code still would mind posting output thanks,issue,positive,positive,positive,positive,positive,positive
555131944,"This is fixed on the develop branch now, and will be in the next release.",fixed develop branch next release,issue,negative,positive,neutral,neutral,positive,positive
555131872,"Continuous should be OK for this problem. Is there a lot of variation in your episodes? std of reward being high isn't necessarily bad as long as it's hitting the target :P

You can try lowering beta for more deterministic behavior if entropy is also high. ",continuous problem lot variation reward high necessarily bad long target try lowering beta deterministic behavior entropy also high,issue,negative,negative,negative,negative,negative,negative
555104999,Thanks! Will get back at you when I get it working for me,thanks get back get working,issue,negative,positive,neutral,neutral,positive,positive
555084292,"> > yes you can. all the python code that creates the TF layers is included. you do not need to touch it for it to work, but if might be a good place to start if you want a custom topology.
> > Take a look at https://github.com/Unity-Technologies/ml-agents/blob/master/python/unitytrainers/models.py#L162
> 
> This link would be helpful to me, but is broken, any new address to get here? Thanks!

**Here you go: https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/models.py**

Please tell me if you figure anything out. I'm trying to do the same thing.",yes python code included need touch work might good place start want custom topology take look link would helpful broken new address get thanks go please tell figure anything trying thing,issue,positive,positive,positive,positive,positive,positive
555023070,"@kevtan , @OliverMathias : I'm working on the same version of ML-Agents and have these behavior parameters set up. I'm able to train in editor using the mlagents-learn command. However, I want to train with my own algorithm implementation so I'm trying to interact with the environment build on jupyter notebook. When I try printing the environment information, I get the following:
![Screenshot_2](https://user-images.githubusercontent.com/38923623/69057830-c17b6500-0a12-11ea-9a72-356180391fab.png)

It says that the number of training brains is zero. Any idea on how to proceed? Thanks in advance!",working version behavior set able train editor command however want train algorithm implementation trying interact environment build notebook try printing environment information get following number training brain zero idea proceed thanks advance,issue,positive,positive,positive,positive,positive,positive
554880120,"I too ran into the same problem, since I forgot to mention the payer brain inputs properly. ",ran problem since forgot mention payer brain properly,issue,negative,neutral,neutral,neutral,neutral,neutral
554802236,"> 
> 
> hi @sonicviz, @DVonk, @devedse - thank you for your feedback, very helpful for us to understand the needs for conda in Windows. For now, please refer to the previous instructions (it should be the same steps). https://github.com/Unity-Technologies/ml-agents/blob/0.10.1/docs/Installation-Windows.md
> 
> Will keep you posted on next steps on Anaconda instructuions for Windows.

Hi, I found this on the third day of trying to set up ML Agents on my setup. First try worked with this guide.
Note: I installed tensorflow-gpu and got it working using the guide, except for one issue that was solved by changing a line in ""setup.py"" according to this thread: https://github.com/Unity-Technologies/ml-agents/issues/1534

I couldn't get things set up without this guide, because some of the info was not available anywhere else.",hi thank feedback helpful u understand need please refer previous keep posted next anaconda hi found third day trying set setup first try worked guide note got working guide except one issue line according thread could get set without guide available anywhere else,issue,positive,positive,neutral,neutral,positive,positive
554796589,"> I saw a code change work-around mentioned in another thread, but I would like to request this as a feature also.

Hi @robinerd, the code change you're referring to is actually really minor. I've created a gist here 
https://gist.github.com/mbaske/64a0e261f02aa7daff459cc1afef0198",saw code change another thread would like request feature also hi code change actually really minor gist,issue,negative,negative,neutral,neutral,negative,negative
554781515,"Possibly two things going on here. Is it expected to show 0 brains learning in the academy, in python console? I'm running another test now in a new project and getting the same, but it proceeds and trains.

It might be that my issue is related to #2844 since we're using both ResetOnDone and OnDemandDecisions. But I can't test right now.",possibly two going show brain learning academy python console running another test new project getting proceeds might issue related since ca test right,issue,negative,positive,positive,positive,positive,positive
554658091,"Thank you! :)

The tower improved quite a lot. I have one more question. The std of Reward is high. What can be the problem? Should I change the config parameters or brain's settings / inputs?

Right now I use continuous space type with two outputs (one for rotate, one for shoot).",thank tower quite lot one question reward high problem change brain right use continuous space type two one rotate one shoot,issue,negative,positive,positive,positive,positive,positive
554586543,"Hi @PabloOlinav,
ml-agents doesn't have any specific requirements on tensorboard or setuptools, so it's coming from one of the other packages. You might try installing `setuptools == 41.0.0` first and see if that helps resolve it.

PS - in the future, please paste the errors and use triple-backticks (```) instead of screenshots - it makes the errors easier for others to search for in the future.",hi specific coming one might try first see resolve future please paste use instead easier search future,issue,positive,positive,neutral,neutral,positive,positive
554585319,Glad that worked. I've got a PR to update the FAQ with this (and few related issues we've been seeing): https://github.com/Unity-Technologies/ml-agents/pull/2920,glad worked got update related seeing,issue,negative,positive,positive,positive,positive,positive
554583322,Thanks. Pulled the latest master and it is available.,thanks latest master available,issue,negative,positive,positive,positive,positive,positive
554537899,"@chriselion you were right I was using a 32-bit version, it is working now thanks.",right version working thanks,issue,negative,positive,positive,positive,positive,positive
554530175,"Sorry, I'm not sure. There are binary wheels of tensorflow for 3.7: https://pypi.org/project/tensorflow/1.15.0/#files
so I would expect it to work. What platform are you on? I've heard of some people accidentally using 32-bit python on windows, which might explain it.

You need to be able to do `pip install ""tensorflow<2.0""`, but if that doesn't work, this isn't the best place to troubleshoot it.",sorry sure binary would expect work platform people accidentally python might explain need able pip install work best place,issue,positive,positive,positive,positive,positive,positive
554514718,"> @ralph367 What version of python are you using? Tensorflow doesn't yet support python3.8 (see [tensorflow/tensorflow#33374](https://github.com/tensorflow/tensorflow/issues/33374)) so if you're using 3.8, you'll need to either downgrade to python 3.7, or wait for them to build new packages.

I installed python 3.7.5 and still facing the same error, is there any needed version for pip as well?",ralph version python yet support python see need either downgrade python wait build new python still facing error version pip well,issue,negative,positive,positive,positive,positive,positive
554494707,Need to retrain the models for these - will kick off a cloud run over the weekend.,need retrain kick cloud run weekend,issue,negative,neutral,neutral,neutral,neutral,neutral
554488511,"@ralph367 What version of python are you using? Tensorflow doesn't yet support python3.8 (see https://github.com/tensorflow/tensorflow/issues/33374) so if you're using 3.8, you'll need to either downgrade to python 3.7, or wait for them to build new packages.",ralph version python yet support python see need either downgrade python wait build new,issue,negative,positive,positive,positive,positive,positive
554487566,"@harperj good call, I already had pylint disabled on the barracuda files (for similar reasons). I undid the changes and disabled flake8 on them.",good call already disabled barracuda similar undid disabled flake,issue,negative,positive,neutral,neutral,positive,positive
554477798,Thanks for catching this. I'll update our examples to use non-square visuals to hopefully prevent anything like this from happening again.,thanks catching update use hopefully prevent anything like happening,issue,positive,positive,positive,positive,positive,positive
554358037,"Thanks! Using only 'python' made it works! Don't know why i haven't tryed it before.

Now, I have the problem while installing the ml-agents, cause it gives me an error whith tensorboard, i dont know if the installation was completed after that.
![Captura](https://user-images.githubusercontent.com/20268684/68946761-0c4b7180-07b4-11ea-94cd-5b23c588b8b4.PNG)
",thanks made work know problem cause error dont know installation,issue,negative,positive,positive,positive,positive,positive
554349503,"Aah it seems that there was a problem with tensorflow 1.15 , and an older version of tensorflow_to_barracuda.py according to https://github.com/Unity-Technologies/ml-agents/issues/2775 . Updated the script to the latest version, and it works:) ",problem older version according script latest version work,issue,negative,positive,positive,positive,positive,positive
554219825,"The solution is much simpler, 
• Go to →Edit → Project settings → Input → Size 2 → In Vertical & Horizontal → Remove assigned buttons, change type to mouse movements, Axis to X axis & Y axis respectively.
• Go to Player brain → Axis continuous player action → Size 2 → Name as Vertical & Horizontal, Index as 0, 1 and Scale as suitable. ",solution much simpler go project input size vertical horizontal remove assigned button change type mouse axis axis axis respectively go player brain axis continuous player action size name vertical horizontal index scale suitable,issue,positive,positive,positive,positive,positive,positive
554129831,"> Hi @mbaske
> T=You are right, we removed the capacity to do training and have a brain running inference at the same time. This was to make it easier to get started with the toolkit (removing the broadcast hub). The fix you did is exactly what I would have done and I do not think there would be any side effects.

Are you planning on making this more intuitive? As said, I find this to be a very basic requirement. Removing the capacity for training and inference at the same time, just to make things slightly easier to use, doesn't sound like a very good idea. If I really just have the options of 'training everything' vs 'training nothing' I'd probably just go back to 0.10 to be honest.",hi right removed capacity training brain running inference time make easier get removing broadcast hub fix exactly would done think would side effect making intuitive said find basic requirement removing capacity training inference time make slightly easier use sound like good idea really everything nothing probably go back honest,issue,positive,positive,positive,positive,positive,positive
554124969,@njustesen Thanks for pointing this out. I added a CI check for it this sort of issue in https://github.com/Unity-Technologies/ml-agents/pull/2910 (as well as fixing it on the develop branch).,thanks pointing added check sort issue well fixing develop branch,issue,positive,positive,positive,positive,positive,positive
554121050,"This should be sufficient, but stacked vectors 40 is quite a lot. I'd try stacked vectors 3 or 4 - or even no stacked vectors since you have the object's velocity as part of the observation. 

Also make sure your observations are normalized either by making them between -1 and 1, or turning on `normalize: true` in the trainer config. ",sufficient quite lot try even since object velocity part observation also make sure either making turning normalize true trainer,issue,positive,positive,positive,positive,positive,positive
554120160,"Yes, that sounds like it should work. But you'll have to either start/stop training manually or rewrite some of the logic to reload the `.demo` file periodically. ",yes like work either training manually rewrite logic reload file periodically,issue,positive,neutral,neutral,neutral,neutral,neutral
554119725,"From the screenshot it looks like your environment is not outputting NaNs, which is good. 

This is nearly an ""infinite horizon"" problem - what you're doing seems to be good. The Trainer will recognize that the agents hit a max step and treat it as if the episode kept going. ",like environment good nearly infinite horizon problem good trainer recognize hit step treat episode kept going,issue,positive,positive,positive,positive,positive,positive
554118752,"Hi @AsadJeewa, if you're planning on modifying `trainer.py`, install ml-agents with `pip install -e ./` in the ml-agents-envs and ml-agents directories, rather than `pip install mlagents`, this will force `mlagents-learn` to point to the files in your github clone. The duplication has to do with how pip works - it makes a copy of the file in the site-packages folder. ",hi install pip install rather pip install force point clone duplication pip work copy file folder,issue,negative,neutral,neutral,neutral,neutral,neutral
554113107,"@SetoKaiba Thanks for doing the initial work on this. I merged https://github.com/Unity-Technologies/ml-agents/pull/2665 to the develop branch, which has support (via tf.compat.v1) for tensorflow2 library.",thanks initial work develop branch support via library,issue,positive,positive,neutral,neutral,positive,positive
554099989,"Thank you. It works. 
Please advise on why there are 2 trainer.py files?
Working on Windows.
My understanding is that one is the github clone and the other is part of the site-packages in anaconda through pip install. But the duplication of files is frustrating. Made the mistake of changing the wrong trainer.py file at first.",thank work please advise working understanding one clone part anaconda pip install duplication made mistake wrong file first,issue,negative,negative,negative,negative,negative,negative
554098416,"My solution right now is to set max steps on the agents, and _not_ reset them on done, with no max steps on the environment. This does work around the issue. Just want to make sure it's normal to have to do that?",solution right set reset done environment work around issue want make sure normal,issue,positive,positive,positive,positive,positive,positive
554076396,I should also mention that setting the max steps to a positive number does seem to resolve the error (but if I set it to a large number sometimes I still see this). In the screenshot it's uncapped. ,also mention setting positive number seem resolve error set large number sometimes still see uncapped,issue,positive,positive,positive,positive,positive,positive
554074973,"Hi @ervteng , thanks for the response. It's actually both issues, I've attached a screenshot to demonstrate: 
![noepisodecompleted_LI (3)](https://user-images.githubusercontent.com/1988272/68895369-1f4c3c00-06f7-11ea-8154-119f6e0f4912.jpg)

It looks like it's trying to update the policy before any episode completes and thus getting nan as a result? Do you know a solution to this other than setting max steps on the agents?",hi thanks response actually attached demonstrate like trying update policy episode thus getting nan result know solution setting,issue,positive,positive,neutral,neutral,positive,positive
554044106,"Hi thank you for your response 

Yes it's all done on the same machine. Speaking of GAIL would it be possible, if I were to stick with online in editor training, to use a generated file from iteration n (if n is the current iteration number) to power agents in iteration n+1 ?",hi thank response yes done machine speaking would possible stick editor training use file iteration current iteration number power iteration,issue,positive,neutral,neutral,neutral,neutral,neutral
554030181,"Hi @ervteng

Thank you your response. 

Right now I add 6 floats to the tower. (One tower and one object are in the scene)
1. Can fire? (0 or 1)
2. Distance between the tower and the object
3. Angle (tower should rotate, it can shoot only forward)
4. Object's forward (x) (object can move only forward)
5. Object's forward (z) (object can move only forward)
6. Object's speed

And I set the stacked vectors to 40 (the bullet will die in 32 fix updates + 8 stacks just in case)",hi thank response right add tower one tower one object scene fire distance tower object angle tower rotate shoot forward object forward object move forward object forward object move forward object speed set bullet die fix case,issue,negative,positive,positive,positive,positive,positive
554013015,"Hi @dorog, from what it sounds like, the second option (if the bullet hit the target, give a reward) is the better option. What you need to make sure is that the tower has some way to observe the velocity of the target. This can be done in two ways: adding the velocity of the target explicitly to the observations, or using Stacked Vectors (will need a bigger network/will take longer to train). What's the observations look like?",hi like second option bullet hit target give reward better option need make sure tower way observe velocity target done two way velocity target explicitly need bigger take longer train look like,issue,positive,positive,positive,positive,positive,positive
554012662,"Hi @mbaske 
T=You are right, we removed the capacity to do training and have a brain running inference at the same time. This was to make it easier to get started with the toolkit (removing the broadcast hub). The fix you did is exactly what I would have done and I do not think there would be any side effects. ",hi right removed capacity training brain running inference time make easier get removing broadcast hub fix exactly would done think would side effect,issue,negative,positive,positive,positive,positive,positive
554009212,Not currently but it would be trivial to add. In `ml-agents/mlagents/trainers/trainer.py` there is a line that reads `stat_mean = float(np.mean(self.stats[key]))` followed by `summary.value.add()`. You can just add another summary value add with np.std instead of np.mean.,currently would trivial add line float key add another summary value add instead,issue,negative,neutral,neutral,neutral,neutral,neutral
554008211,"Hi @josayarh, I take it you're training on the same machine as the player is playing? If so, 0.10 and BC would probably be your best option. It was pretty limited, however, as it only allowed you to have agents that purely learn from the player (and not from any other reward signal), and couldn't learn very complex behavior without an inordinate amount of demonstrations - which is why it was deprecated in 0.11. It's possible to use something like GAIL for online learning as well, though we haven't implemented that feature. ",hi take training machine player would probably best option pretty limited however purely learn player reward signal could learn complex behavior without inordinate amount possible use something like learning well though feature,issue,positive,positive,positive,positive,positive,positive
553967052,"I had this issue, I fixed it by moving my env.

The path had a '[' character, and it seems like Windows doesn't like it !",issue fixed moving path character like like,issue,positive,positive,neutral,neutral,positive,positive
553895898,"We have found what is causing this issue and expect it to be fixed with ML-Agents upgrade to Barracuda 0.3.x, which we expect to happen soon.",found causing issue expect fixed upgrade barracuda expect happen soon,issue,negative,positive,neutral,neutral,positive,positive
553894355,"> Check this link:
> [#2871 (comment)](https://github.com/Unity-Technologies/ml-agents/issues/2871#issuecomment-552179615)

Thank you. 
Maybe they should add this to the beginner tutorial?",check link comment thank maybe add beginner tutorial,issue,negative,neutral,neutral,neutral,neutral,neutral
553846741,"I was following this tutorial, but I noticed that i can still just inherit from ml-agents classes. I guess the video was doing this in an archaic way. Closing this for now",following tutorial still inherit class guess video archaic way,issue,negative,neutral,neutral,neutral,neutral,neutral
553798383,"I've added a `m_UseBarracuda` bool to the `BehaviorParameters` script, which allows me to skip the `IsCommunicatorOn` check for a given agent and return a `BarracudaPolicy` anyway. This seems to be doing what I want, but I'm not sure if it has any other unintended effects. 
However, combining training and inference in a multi-agent environment seems like a rather basic requirement. I wonder why this feature was removed?",added bool script skip check given agent return anyway want sure unintended effect however combining training inference environment like rather basic requirement wonder feature removed,issue,positive,positive,positive,positive,positive,positive
553666018,"@surfnerd 
> Will this have a follow up with docs changes?

Definitely will add a migration guide for it. I don't think there are any docs on the old code, but I can find a place to add a writeup on the new stuff.",follow definitely add migration guide think old code find place add new stuff,issue,negative,positive,neutral,neutral,positive,positive
553654742,"thanks, I had a syntax error where I used a dash instead of an equal sign which fix that error",thanks syntax error used dash instead equal sign fix error,issue,negative,positive,neutral,neutral,positive,positive
553564531,"What about `mlagents_envs.environment`?

I'm also beginning to think the right thing may be to just re-combine the two packages.  Have you thought about that option?",also beginning think right thing may two thought option,issue,negative,positive,positive,positive,positive,positive
553539650,"Those ideas to query a brain without the need of being tied to an agent would be excelent and extend ML-Agents usage for many other cases which actually are impossible or very hack.

It would be amazing to have those features available.",query brain without need tied agent would extend usage many actually impossible hack would amazing available,issue,positive,positive,positive,positive,positive,positive
553535548,"I asked around here but nobody else has used the histograms either.

If you'd like to experiment with it and submit a pull request, we'd be happy to take the change, but otherwise I don't think it's something that we're likely to add.",around nobody else used either like experiment submit pull request happy take change otherwise think something likely add,issue,positive,positive,positive,positive,positive,positive
553534684,"hi @sebjf,
Sorry for the delay on this. I believe the behavior for dead/hung got better with this PR https://github.com/Unity-Technologies/ml-agents/pull/2812, which is also in the 0.11.0 release.",hi sorry delay believe behavior got better also release,issue,negative,neutral,neutral,neutral,neutral,neutral
553427628,"found character '\t' that cannot start any token
in ""config/trainer_config.yaml"", line 291, column 1

You have a tab in your yml file.

On Tue, Nov 12, 2019 at 22:43 nlin3696 <notifications@github.com> wrote:

> (ml-agents)
> C:\Users\user\Documents\GitHub\MachineLearningAgents\ml-agents-master>mlagents-learn
> config/trainer_config.yaml --curriculum=config/curricula/Penguin/
> --run-id=penguin_1 --train
>
> Traceback (most recent call last):
> File
> ""C:\Users\user\AppData\Local\Programs\Python\Python37\Scripts\mlagents-learn-script.py"",
> line 11, in
> load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')()
> File
> ""c:\users\user\documents\github\machinelearningagents\ml-agents-master\ml-agents\mlagents\trainers\learn.py"",
> line 408, in main
> run_training(0, run_seed, options, Queue())
> File
> ""c:\users\user\documents\github\machinelearningagents\ml-agents-master\ml-agents\mlagents\trainers\learn.py"",
> line 205, in run_training
> trainer_config = load_config(trainer_config_path)
> File
> ""c:\users\user\documents\github\machinelearningagents\ml-agents-master\ml-agents\mlagents\trainers\trainer_util.py"",
> line 141, in load_config
> return
>
>
>
>
> *load_config(data_file) File
> ""c:\users\user\documents\github\machinelearningagents\ml-agents-master\ml-agents\mlagents\trainers\trainer_util.py"",
> line 158, in load_config return yaml.safe_load(fp) File
> ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml_init.py"",
> line 162, in safe_load return load(stream, SafeLoader) File
> ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml_init*.py"",
> line 114, in load
> return loader.get_single_data()
> File
> ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\constructor.py"",
> line 41, in get_single_data
> node = self.get_single_node()
> File
> ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\composer.py"",
> line 36, in get_single_node
> document = self.compose_document()
> File
> ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\composer.py"",
> line 55, in compose_document
> node = self.compose_node(None, None)
> File
> ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\composer.py"",
> line 84, in compose_node
> node = self.compose_mapping_node(anchor)
> File
> ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\composer.py"",
> line 133, in compose_mapping_node
> item_value = self.compose_node(node, item_key)
> File
> ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\composer.py"",
> line 64, in compose_node
> if self.check_event(AliasEvent):
> File
> ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\parser.py"",
> line 98, in check_event
> self.current_event = self.state()
> File
> ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\parser.py"",
> line 449, in parse_block_mapping_value
> if not self.check_token(KeyToken, ValueToken, BlockEndToken):
> File
> ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\scanner.py"",
> line 116, in check_token
> self.fetch_more_tokens()
> File
> ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\scanner.py"",
> line 260, in fetch_more_tokens
> self.get_mark())
> yaml.scanner.ScannerError: while scanning for the next token
> found character '\t' that cannot start any token
> in ""config/trainer_config.yaml"", line 291, column 1
>
> I seems I have a similar problem. I reinstalled Python a few times to
> retry and still the same.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/2898?email_source=notifications&email_token=AAKLGSCZ6FJTA5V2YO32NVLQTOO2LA5CNFSM4JMI7DCKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOED5CDII#issuecomment-553263521>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAKLGSHV6EB3JCZHCMRUAQDQTOO2LANCNFSM4JMI7DCA>
> .
>
-- 
*Chris Goy*
Senior Software Developer | ML Agents
San Francisco
<https://unity3d.com/?utm_source=unity3d&utm_medium=email&utm_campaign=company-information-2016-Global-Unity-Email-Sig>
",found character start token line column tab file tue wrote train recent call last file line file line main queue file line file line return file line return file line return load stream file line load return file line node file line document file line node none none file line node anchor file line node file line file line file line file line file line scanning next token found character start token line column similar problem python time retry still thread reply directly view goy senior developer san,issue,negative,positive,neutral,neutral,positive,positive
553263521,"(ml-agents) C:\Users\user\Documents\GitHub\MachineLearningAgents\ml-agents-master>mlagents-learn config/trainer_config.yaml --curriculum=config/curricula/Penguin/ --run-id=penguin_1 --train

Traceback (most recent call last):
  File ""C:\Users\user\AppData\Local\Programs\Python\Python37\Scripts\mlagents-learn-script.py"", line 11, in <module>
    load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')()
  File ""c:\users\user\documents\github\machinelearningagents\ml-agents-master\ml-agents\mlagents\trainers\learn.py"", line 408, in main
    run_training(0, run_seed, options, Queue())
  File ""c:\users\user\documents\github\machinelearningagents\ml-agents-master\ml-agents\mlagents\trainers\learn.py"", line 205, in run_training
    trainer_config = load_config(trainer_config_path)
  File ""c:\users\user\documents\github\machinelearningagents\ml-agents-master\ml-agents\mlagents\trainers\trainer_util.py"", line 141, in load_config
    return _load_config(data_file)
  File ""c:\users\user\documents\github\machinelearningagents\ml-agents-master\ml-agents\mlagents\trainers\trainer_util.py"", line 158, in _load_config
    return yaml.safe_load(fp)
  File ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\__init__.py"", line 162, in safe_load
    return load(stream, SafeLoader)
  File ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\__init__.py"", line 114, in load
    return loader.get_single_data()
  File ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\constructor.py"", line 41, in get_single_data
    node = self.get_single_node()
  File ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\composer.py"", line 36, in get_single_node
    document = self.compose_document()
  File ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\composer.py"", line 55, in compose_document
    node = self.compose_node(None, None)
  File ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\composer.py"", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
  File ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\composer.py"", line 133, in compose_mapping_node
    item_value = self.compose_node(node, item_key)
  File ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\composer.py"", line 64, in compose_node
    if self.check_event(AliasEvent):
  File ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\parser.py"", line 98, in check_event
    self.current_event = self.state()
  File ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\parser.py"", line 449, in parse_block_mapping_value
    if not self.check_token(KeyToken, ValueToken, BlockEndToken):
  File ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\scanner.py"", line 116, in check_token
    self.fetch_more_tokens()
  File ""c:\users\user\appdata\local\programs\python\python37\lib\site-packages\yaml\scanner.py"", line 260, in fetch_more_tokens
    self.get_mark())
yaml.scanner.ScannerError: while scanning for the next token
found character '\t' that cannot start any token
  in ""config/trainer_config.yaml"", line 291, column 1



I seems I have a similar problem. I reinstalled Python a few times to retry and still the same. @ervteng ",train recent call last file line module file line main queue file line file line return file line return file line return load stream file line load return file line node file line document file line node none none file line node anchor file line node file line file line file line file line file line scanning next token found character start token line column similar problem python time retry still,issue,negative,positive,neutral,neutral,positive,positive
553192986,"Hi @J-Travnik,
I *think* this should have gotten better in the 0.11.0 release. We added a timeout on the communication between C# and python here: https://github.com/Unity-Technologies/ml-agents/pull/2812

We also changed the shutdown behavior slightly to wait a bit before killing the spawned process: https://github.com/Unity-Technologies/ml-agents/pull/2620 (although this means we'll end up waiting for `timeout_wait` twice, which is arguably not ideal).

So if I'm reading this all correctly, when the we try to step the environment here:
https://github.com/Unity-Technologies/ml-agents/blob/5194664db8a078a2673edef2cbd82a90975a9b59/ml-agents-envs/mlagents/envs/subprocess_env_manager.py#L99

If the other process is dead, we'll raise the exception here
https://github.com/Unity-Technologies/ml-agents/blob/5194664db8a078a2673edef2cbd82a90975a9b59/ml-agents-envs/mlagents/envs/environment.py#L527

which will get caught here
https://github.com/Unity-Technologies/ml-agents/blob/5194664db8a078a2673edef2cbd82a90975a9b59/ml-agents-envs/mlagents/envs/subprocess_env_manager.py#L119-L130

and `env.close()` will (eventually) kill the process.",hi think gotten better release added communication python also shutdown behavior slightly wait bit killing process although end waiting twice ideal reading correctly try step environment process dead raise exception get caught eventually kill process,issue,negative,positive,positive,positive,positive,positive
553171440,"This seems like a Python install issue, try re-installing `ml-agents` and `ml-agents-envs` using `pip` and try again. ",like python install issue try pip try,issue,negative,neutral,neutral,neutral,neutral,neutral
553171118,"Hi @niskander, you shouldn't be getting the `Mean reward: nan` error regardless of the agent steps. Rather, it should say ""no episode was completed"" if the episode doesn't terminate. If you see a NaN reward found warning, that means one of the rewards was actually a NaN, usually a divide-by-zero. 

You're right, resetting too early makes learning harder. I wouldn't worry if the episodes don't finish in the beginning of training; as the agent learns, the episode will get shorter and you'll see the mean reward being reported. I'd set agent steps to much larger than you think the agent needs to solve the level, and lengthen `summary_freq` in the `trainer_config.yaml` file. ",hi getting mean reward nan error regardless agent rather say episode episode terminate see nan reward found warning one actually nan usually right early learning harder would worry finish beginning training agent episode get shorter see mean reward set agent much think agent need solve level lengthen file,issue,positive,negative,neutral,neutral,negative,negative
553149290,@Phong13 I have raised the issue of using multiple brains for inference with our team. at present this feature is under discussion. ,raised issue multiple brain inference team present feature discussion,issue,negative,neutral,neutral,neutral,neutral,neutral
553146513,"Brains have been deprecated in v0.11. According to the [Migration guide](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Migrating.md#migrating-from-ml-agents-toolkit-v010-to-v011) 
 > Since Brain ScriptableObjects have been removed, you will need to delete all the Brain ScriptableObjects from your Assets folder. Then, add a Behavior Parameters component to each Agent GameObject. You will then need to complete the fields on the new Behavior Parameters component with the BrainParameters of the old Brain.",brain according migration guide since brain removed need delete brain asset folder add behavior component agent need complete new behavior component old brain,issue,negative,positive,positive,positive,positive,positive
553146197,"Yes it happens with version 0.11 too.

To reproduce
     Clean install of ML-Agents v 0.11
     Open the 3DBallHard.Unity scene
     Edit the Ball3DHardAgent.cs so that at the beginning of AgentAction
          Debug.Log(""Value= "" + GetValueEstimate());

Run the scene in inference mode. The console will print ""Value= 0""


     ",yes version reproduce clean install open scene edit beginning run scene inference mode console print,issue,positive,positive,positive,positive,positive,positive
553145460,"The Agent Component requires the component Behavior Parameters to work. If you create a new Agent, the Behavior parameters will be added automatically but it seems that when updating an existing project, the Behavior Parameters are not automatically updated.",agent component component behavior work create new agent behavior added automatically project behavior automatically,issue,negative,positive,positive,positive,positive,positive
553101610,@devedse this sounds like a useful feature. I have added it as a feature request into our backlog.,like useful feature added feature request backlog,issue,positive,positive,positive,positive,positive,positive
553095810,@J-Travnik From where are you able to infer that the parent process doesn't call `wait` or `waitpid`? It would be great if you could provide information on what you ran and the setup you are using. Thanks!,able infer parent process call wait would great could provide information ran setup thanks,issue,positive,positive,positive,positive,positive,positive
553094663,@vincentpierre Was this change affected by a recent code refactor?,change affected recent code,issue,negative,neutral,neutral,neutral,neutral,neutral
553094399,@Phong13 could you please update to the latest version (ml-agents 0.11) and try this again?,could please update latest version try,issue,negative,positive,positive,positive,positive,positive
553089703,"Thanks for the feedback @sg2. To keep the entry bar easy on all platforms (Windows, Mac, Linux) we recommend using virtual envs. Depending on the need for Docker in the community we will consider adding Docker as a first class route to install ml-agents. ",thanks feedback keep entry bar easy mac recommend virtual depending need docker community consider docker first class route install,issue,positive,positive,positive,positive,positive,positive
553068708,"Hi @MarkTension,
You should be able to instantiate a Policy without the agent subclass.  The `Agent` just calls into the `BehaviorParameters.GeneratePolicy` method.  This, indeed, does use the `Academy` to see if training is happening.  

In short, it is possible, you just need to write some extra code to hook it up.  If you have any specific questions, please feel free to follow up on this thread.",hi able policy without agent subclass agent method indeed use academy see training happening short possible need write extra code hook specific please feel free follow thread,issue,positive,positive,positive,positive,positive,positive
553048725,"I could submit a PR with the solution if that would be useful.
",could submit solution would useful,issue,positive,positive,positive,positive,positive,positive
553031316,Please execute `pip show mlagents` to see the version of ml-agents you have installed. In case you are not using 0.11.0 please execute `pip install --upgrade mlagents`. Also do the same with `mlagents-envs`. Hope this helps.,please execute pip show see version case please execute pip install upgrade also hope,issue,positive,neutral,neutral,neutral,neutral,neutral
553019604,@BlonskiP Could you please provide a screenshot of the error message you are receiving and what you did to get the error message. Thanks.,could please provide error message get error message thanks,issue,negative,positive,positive,positive,positive,positive
553016676,Thanks for the posting the resolution to your problem @adbourdages. Since the issue has been resolved I'll close the issue for now. ,thanks posting resolution problem since issue resolved close issue,issue,negative,positive,positive,positive,positive,positive
553015037,Thanks fro the feedback @PabloOlinav We will look into this issue and update the docs accordingly.,thanks fro feedback look issue update accordingly,issue,negative,positive,positive,positive,positive,positive
553012675,"@BlonskiP What's the actual problem? The deprecation warnings from tensorflow are safe to ignore. Overall, it looks like training is working and your reward is increasing over time.

I'm not sure about the cuda warnings; worst case is that you'll use CPU training instead of GPU. If you want to resolve those, you'll need to look around on tensorflow or cuda support.

Tensorflow 1.14.0 is supported on ML Agents 0.10 and 0.11. Tensorflow 1.15 is only supported on ML Agents 0.11.0",actual problem deprecation safe ignore overall like training working reward increasing time sure worst case use training instead want resolve need look around support,issue,positive,neutral,neutral,neutral,neutral,neutral
552907527,"Resolved - another package that I was using was also providing a ToArray Extension method, which seemed to confuse all involved compilers without throwing any errors that really pointed in that direction. I found out by creating a new project and piecewise importing the old project. ",resolved another package also providing extension method confuse involved without throwing really pointed direction found new project piecewise old project,issue,negative,positive,positive,positive,positive,positive
552850875,"Hey again,

unfortunately I was not able to resolve the issue. I could successfully install and run ML-Agents on my personal computer, however on my PC at work there seems to remain the exact same issue as posted above. I cannot re-open the issue, since I didn't create it, I thought I'll just try and comment anyways.

I don't know how to ensure that the ML-Agents python package and UnitySDK are both from the latest release 0.11.0. However, I did the following steps and keep running into these error messages, not being able to connect to my executable or train at any point (I am working on Windows 10):

1. Install latest version of anaconda
2. Create and activate new python anaconda environment (I tried both python 3.7.4 and 3.6.9)
3. install tensorflow (the issue occures with both tf version 1.7.1 and 1.14)
4. with the anaconda environment activated, I cloned the git repo from `git clone https://github.com/Unity-Technologies/ml-agents.git`
5. I ran the following steps from the installation manual to install ml-agents:
```
cd ml-agents-envs
pip3 install -e ./
cd ..
cd ml-agents
pip3 install -e ./
```
6. I am currently working with Unity 2019.3.0b7, but I also tried running ml-agents with 2017.4.34f1
7. I tried both training the 3DBall example environment from the Unity editor itself (`--env=None`) and from an executable, as well as trying to train the RollerBall example.

Maybe there is some obvious mistake or step I am missing here, but I am unable to find it myself, and as already said I was able to successfully install ML-Agents on another computer.

Any help is very much appreciated.
Thanks and kind regards.",hey unfortunately able resolve issue could successfully install run personal computer however work remain exact issue posted issue since create thought try comment anyways know ensure python package latest release however following keep running error able connect executable train point working install latest version anaconda create activate new python anaconda environment tried python install issue version anaconda environment git git clone ran following installation manual install pip install pip install currently working unity also tried running tried training example environment unity editor executable well trying train example maybe obvious mistake step missing unable find already said able successfully install another computer help much thanks kind,issue,positive,positive,positive,positive,positive,positive
552755486,"I find a answer for this.  When hand collision with ground, it's called that /sharedAsseet/scripts/GroundContact.cs.
This file contain Agent.Done().
I must need more learning.
Thanks.",find answer hand collision ground file contain must need learning thanks,issue,positive,positive,positive,positive,positive,positive
552734346,"It should work just fine. 
You have to create your .nn file from the .pb file as shown in the [Barracuda documentation](https://github.com/Unity-Technologies/ml-agents/blob/master/UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda.md). Just manually load the file and give the vector action manually. Have implemented it and the code is [this](https://github.com/Zilch123/Custom-Barracuda-Implementation/tree/master). 
",work fine create file file shown barracuda documentation manually load file give vector action manually code,issue,negative,positive,positive,positive,positive,positive
552716286,"@OliverMathias I just pulled from the remote repository and now I have these scripts and it's all working. Thanks!

**To anyone reading this thread, it seems like ML-Agents no longer uses the Learning/Player Brain abstraction for RL environments. Please do not follow any of the advice given above about setting up the Heuristic and Player brains because they will not work! If you are having similar issues, then pull in the latest changes from the ML-Agents repository.**",remote repository working thanks anyone reading thread like longer brain abstraction please follow advice given setting heuristic player brain work similar pull latest repository,issue,positive,positive,positive,positive,positive,positive
552709543,"Turns out it is fairly easy to decouple the Agent from the Brain. Agent needs to implement an interface:

`    public interface IBrainInputOutputSource
    {
        Agent GetAgent();

        void UpdateVectorAction(float[] actions);

        void UpdateValueAction(float action);

        void UpdateMemoriesAction(List<float> actions);

        List<float> GetMemoriesAction();
    }`

Then the m_agentInfo Dictionary is Dictionary<IBrainInputOutputSource, AgentInfo>.

Everything works and I can use the Brain with any class that implements the IBrainInputOutputSource methods.


",turn fairly easy agent brain agent need implement interface public interface agent void float void float action void list float list float dictionary dictionary everything work use brain class,issue,negative,positive,positive,positive,positive,positive
552698651,"I just realized the mismatch may have occurred when I installed gym_unity. The problem occurred after I had executed ""pip3 install gym_unity"" and perhaps it pulled in version 0.11 of MLAgents when I had been using 0.10 successfully before. I did not expect installing gym_unity to change versions, though I understand now why it did, but the total lack of a useful error message in this case really was problematic for me.",mismatch may problem executed pip install perhaps version successfully expect change though understand total lack useful error message case really problematic,issue,negative,positive,positive,positive,positive,positive
552696159,"Well that guide worked for me. I can train again. I must say these error messages were painfully unhelpful. It took me the whole afternoon to figure out what happened. The first error I got was a timeout from the python side, then I noticed the ""unable to connect"" error in Unity. And I had not consciously updated. It would be nice for future versions to better detect this situation and clearly notify the user.",well guide worked train must say error painfully unhelpful took whole afternoon figure first error got python side unable connect error unity consciously would nice future better detect situation clearly notify user,issue,negative,positive,neutral,neutral,positive,positive
552692979,"Hey there actually is a guide!
https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Migrating.md
Going through that now...",hey actually guide going,issue,negative,neutral,neutral,neutral,neutral,neutral
552692073,"I am also affected by this issue. I did not intentionally update but I did see that I had both 0.10 and 0.11 on my system. I believe I have fixed that so I only have 0.11 installed, but now the broadcasthub seems to be gone from my academy. Is there a tutorial for migrating from 0.10 to 0.11 or a changelist somewhere?

In my case everything had been working, so I was really stumped when I first got this issue.",also affected issue intentionally update see system believe fixed gone academy tutorial somewhere case everything working really first got issue,issue,negative,positive,positive,positive,positive,positive
552687471,"Hi @NickPowers43,
It's on the Agent class: https://github.com/Unity-Technologies/ml-agents/blob/3d7c4b8d3c1ad17070308b4e06bb57d4a80f9a0c/UnitySDK/Assets/ML-Agents/Scripts/Agent.cs#L516-L529
(master branch is current the same as the 0.11.0 release branch)",hi agent class master branch current release branch,issue,negative,neutral,neutral,neutral,neutral,neutral
552683308,"Hello @harperj 
Sorry for delayed reply. I was on a business trip.
I made a minor changes on the `ResetIfDone` in `Agent.cs` code and it works for me
```
 void ResetIfDone()
        {
            // If an agent is done, then it will also
            // request for a decision and an action
            if (IsDone())
            {
                if (agentParameters.resetOnDone)
                {
                    if (agentParameters.onDemandDecision)
                    {
                        if (!m_HasAlreadyReset)
                        {
                            // If event based, the agent can reset as soon
                            // as it is done
                            _AgentReset();
                            m_HasAlreadyReset = true;
                        }
                    }
                    else if (m_RequestDecision)
                    {
                        // If not event based, the agent must wait to request a
                        // decision before resetting to keep multiple agents in sync.
                        _AgentReset();
                    }
                }
                else {
                    if(agentParameters.onDemandDecision){
                        RequestDecision();
                    }
                    else{
                        m_Terminate = true;
                        RequestDecision();
                    }
                }

                // else
                // {
                //     m_Terminate = true;
                //     RequestDecision();
                // }
            }
        }

```
Hope these can help you or others who may meet the same problem.",hello sorry reply business trip made minor code work void agent done also request decision action event based agent reset soon done true else event based agent must wait request decision keep multiple sync else else true else true hope help may meet problem,issue,positive,positive,neutral,neutral,positive,positive
552649863,"The first step in discovering the cause of the problem was realizing that 'ping localhost' didn't work on my machine. Adding the line '127.0.0.1 localhost' to the /etc/hosts file solved the ping issue and in turn solved the freeze.

Somewhere in the RpcCommunicator there must have been socket call to localhost that was not reporting errors correctly. I'm not sure if I should mark this is solved or not...",first step cause problem realizing work machine line file ping issue turn freeze somewhere must socket call correctly sure mark,issue,negative,positive,positive,positive,positive,positive
552547427,"Thank you for your help Tylawoj! It worked out!
I tried over 8 hours and now it works.",thank help worked tried work,issue,positive,neutral,neutral,neutral,neutral,neutral
552539753,Yeah it works good. Now I can train the agent with multiple demos. Thank you a lot!,yeah work good train agent multiple demo thank lot,issue,positive,positive,positive,positive,positive,positive
552537271,"Hi @Yunao-Shen, just put the folder name and put only the demo files you want in there. For instance, if you have `demo1.demo` and `demo2.demo` you can put them in a folder called `./demos` and put that in your `trainer_config`.

You can record demos in the build but you'll have to edit DemonstrationRecorder.cs. Remove all mentions of `Application.isEditor` in the if statements and the build will record. ",hi put folder name put want instance put folder put record demo build edit remove build record,issue,negative,neutral,neutral,neutral,neutral,neutral
552534411,"This is an issue related to the newest build of PyWin32, which has trouble running with virtual environments: [PyWin32 (226) and virtual environments](https://stackoverflow.com/questions/58805040/pywin32-226-and-virtual-environments)

During the installation of mlagents by PIP, one of the requirements is the xlwings package, which has a requirement of the pywin32 version >= 224, therefore downloading and installing the newest build of pywin32.

Try using:
`pip3 install pywin32==225`

before running:
`pip3 install mlagents`
",issue related build trouble running virtual virtual installation pip one package requirement version therefore build try pip install running pip install,issue,negative,negative,neutral,neutral,negative,negative
552533378,"Hmmm, that's really weird @kevtan. For me the ""BehaviorParameters"" script is under 
""ml-agents\UnitySDK\Assets\ML-Agents\Scripts\Policy\BehaviorScript.cs"" 

There is also a ""BehaviorParametersEditor"" script under the 
""ml-agents\UnitySDK\Assets\ML-Agents\Editor\BehaviorParametersEditor.cs"" path.

 I don't think I'm doing anything different, do you have these scripts in your project?",really weird script also script path think anything different project,issue,negative,negative,negative,negative,negative,negative
552491653,"Another problem, is it able to record demo in build? It seems to be only available in editor.",another problem able record build available editor,issue,negative,positive,positive,positive,positive,positive
552490999,"nvm, already got [a positive answer](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Design-Agents.md#multiple-visual-observations) from the docs. 
",already got positive answer,issue,positive,positive,positive,positive,positive,positive
552378107,"> yes you can. all the python code that creates the TF layers is included. you do not need to touch it for it to work, but if might be a good place to start if you want a custom topology.
> Take a look at https://github.com/Unity-Technologies/ml-agents/blob/master/python/unitytrainers/models.py#L162

This link would be helpful to me, but is broken, any new address to get here? Thanks!",yes python code included need touch work might good place start want custom topology take look link would helpful broken new address get thanks,issue,positive,positive,positive,positive,positive,positive
552291313,"Hi @pranavvaidik,
Visual observations are no longer configured in the behavior parameters, you need to add CameraSensorComponents or RenderTextureSensorComponents to your agent. This is addressed in the [Migration Guide](https://github.com/Unity-Technologies/ml-agents/blob/release-0.11.0/docs/Migrating.md#steps-to-migrate-1):
> If your Agents used visual observations, you must add a CameraSensorComponent corresponding to each old Camera in the Agent's camera list (and similarly for RenderTextures).

You can also check out the GridWorld example scene, which uses Cameras and a RenderTexture to do visual observations.
",hi visual longer behavior need add agent migration guide used visual must add corresponding old camera agent camera list similarly also check example scene visual,issue,negative,positive,neutral,neutral,positive,positive
552268979,"@OliverMathias Thanks for following up, but I still don't think I'm able to replicate what you're doing. I tried it in my own project but also the example project folders found in UnitySDK. I've attached a screenshot for reference. Are you doing anything special that's allowing your Unity to find the ""Behavior Parameters"" component?

<img width=""449"" alt=""Screen Shot 2019-11-10 at 6 30 11 PM"" src=""https://user-images.githubusercontent.com/33914892/68556760-67621a80-03e8-11ea-8b30-76916065cfb3.png"">
",thanks following still think able replicate tried project also example project found attached reference anything special unity find behavior component screen shot,issue,positive,positive,positive,positive,positive,positive
552227032,"@kevtan Sorry for the miscommunication, I was kind of vague when describing the ""auto population"". I meant that if your scene folder is under the ml-agents/Examples folder in the unity project, and you start to type in ""Behavior Parameters"" unity should let you use the default script they already have written for the other examples.
 
To give a better visual, imagine the picture below, but instead of ""Game Setup"" it lets you select a ""Behavior Parameters"" script. 

![50917-unity-problem](https://user-images.githubusercontent.com/44181723/68549646-7c6e8780-03af-11ea-9d5d-40179309f27e.png)

Hope this helps :)",sorry kind vague auto population meant scene folder folder unity project start type behavior unity let use default script already written give better visual imagine picture instead game setup select behavior script hope,issue,positive,negative,neutral,neutral,negative,negative
552224949,"@OliverMathias Wow! That's a much simpler solution. I just tried it though and I don't think I was able to get it working. I created a new script component to the RollerAgent game object called ""BehaviorParameters"" and then I got the following auto-populated script:
```c#
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class BehaviorParameters : MonoBehaviour
{
    // Start is called before the first frame update
    void Start()
    {
        
    }

    // Update is called once per frame
    void Update()
    {
        
    }
}
```
Is this what you were working with? How do I set the different parameters once I'm at this point?",wow much simpler solution tried though think able get working new script component game object got following script public class start first frame update void start update per frame void update working set different point,issue,positive,positive,neutral,neutral,positive,positive
552224262,"Thanks for the instructions @kevtan! I also managed to get it working. All I did was add a default ""Behaviour Parameters"" script to the RollerAgent game object. My scene is in a subfolder of ml-agents so it auto-populated the script. ",thanks also get working add default behaviour script game object scene script,issue,negative,negative,neutral,neutral,negative,negative
552221571,"@OliverMathias So I did some more snooping around and was finally able to figure things out. I think it does have to do with the version of MLAgents that we're using. Essentially, some things have changed about the Agents and Brains since the onboarding readme was last updated (I'm thinking of contributing a new, updated one in the near future!).

Here's what I did to fix my problems regarding the behavior parameters:
1. First right click in the Project window and click Create>ML-Agents>Learning Brain (maybe call it RollerBallLearningBrain) in order to create a new brain to attach to our agent to train. This is the thing that you're going to be setting parameters for.
2. Click on the Learning Brain in the Project window and you should see a bunch of parameters you can set in the Inspector window. Here is where you can specify the `Vector Observation Space Size`, `Vector Action Space Type` and `Vector Action Space Size`.

Here's what I did to fix the issues regarding the `Heuristic()` method:
1. It looks like there isn't a `Heuristic` method in the base class to override. What you actually need to do is the create a ""Player Brain"" in the same manner as creating the Learning Brain (from above).
2. Then click on the newly created Player Brain and set the `Vector Observation Space Size`, `Vector Action Space Type` and `Vector Action Space Size` parameters to match up with how you designed your environment. 
3. Lastly, you need to define some keyboard mappings. Under where you set the parameters, there is a place to ""Edit the [discrete/continuous] inputs for your actions."" Here, change the `Key Continuous Player Actions` parameter to 4 (we want to have 4 movement keys WASD). Then you can specify the `Key`, `Index`, and `Value` for a player action. The index refers to the index inside of the `vectorAction` array that `AgentAction()` receives and the value is just the number at the index inside that array.
4. After setting up these keyboard mappings, I was able to get everything to work!",around finally able figure think version essentially brain since last thinking new one near future fix regarding behavior first right click project window click create learning brain maybe call order create new brain attach agent train thing going setting click learning brain project window see bunch set inspector window specify vector observation space size vector action space type vector action space size fix regarding heuristic method like heuristic method base class override actually need create player brain manner learning brain click newly player brain set vector observation space size vector action space type vector action space size match designed environment lastly need define keyboard set place edit change key continuous player parameter want movement specify key index value player action index index inside array value number index inside array setting keyboard able get everything work,issue,positive,positive,neutral,neutral,positive,positive
552179615,"![image](https://user-images.githubusercontent.com/16376120/68542059-f2c3b780-03b8-11ea-8475-38f959e97648.png)

You can add like script to an agent object.",image add like script agent object,issue,negative,neutral,neutral,neutral,neutral,neutral
552176912,"@surfnerd also when I cloned the ml-agents again, I dont see the Brain scripts under the Scripts folder. Also not able to add new brain scripts to the project. ",also dont see brain folder also able add new brain project,issue,negative,positive,positive,positive,positive,positive
552152301,"I am also facing trouble with current updates on unity. How do we add the behavior parameters?
",also facing trouble current unity add behavior,issue,negative,negative,neutral,neutral,negative,negative
552142738,"@surfnerd , thanks for the quick response. ml-agents version is 0.11
python 3.6.1
conda 4.6.14
tried with Unity 2017.4.34f1 and 2018.4.12f1 but no luck
it is a windows system. ",thanks quick response version python tried unity luck system,issue,positive,positive,positive,positive,positive,positive
552118306,"Hi @bharathk005,
Please make sure that both your python package and UnitySDK are from the same 0.11 release.  You can run `pip list` to see what version of the ML-Agents python package is installed.  

The definition of the gRPC service changed between 0.10 and 0.11 and is not backwards compatible.",hi please make sure python package release run pip list see version python package definition service backwards compatible,issue,positive,positive,positive,positive,positive,positive
552107560,"Hi @droidXrobot, 
This is a known issue if you’ve opened the project with unity 2019.2.x or later.  I’ll try to find the issue to link back to.  In the mean time, the work around is to hook up the brains manually.  We hope to fix this in the coming weeks.  
I will close this issue for now and update it when the fix is available.  Cheers.",hi known issue project unity later try find issue link back mean time work around hook brain manually hope fix coming close issue update fix available,issue,negative,positive,neutral,neutral,positive,positive
552079112,"Thanks ! I was used to use a single instance, given the other github implementations, but that's really good to know ! Thanks a lot ! ((: 
Closing the issue !",thanks used use single instance given really good know thanks lot issue,issue,positive,positive,positive,positive,positive,positive
552034652,"I did a full training run last week using tf 2.0, and ran some of the models for inference; behavior looked good.",full training run last week ran inference behavior good,issue,negative,positive,positive,positive,positive,positive
551996303,"Hi @surfnerd,

I just installed from the repository and followed the instructions to load the python libraries from the ml-agents-envs and ml-agents directories. When I run ml-agents-learn, it works fine and connects to Unity. However, when I try to use a Jupyter notebook, the connection does not occur when I try the getting started notebook. And again, I'm using Ubuntu 16.04. Has this been confirmed to work? If so, any idea where to look for issues? Thank you.",hi repository load python run work fine unity however try use notebook connection occur try getting notebook confirmed work idea look thank,issue,positive,positive,positive,positive,positive,positive
551981621,"Hi @mikeruddy I have the same feeling as you do. If you could open up a new issue on that with the title like ""Why was the docker doc being removed"", and see if the community have the same feeling as you do, that would help a lot for us to gauge the interest in that kind of doc (and potentially bring it back). 

Also I agree with you that in the installation doc we should not have a reference to a removed doc. If you could mention that also in the issue, that would be really helpful. 

Regarding your docker error, I would recommend opening up a separate issue, and give details there. ",hi feeling could open new issue title like docker doc removed see community feeling would help lot u gauge interest kind doc potentially bring back also agree installation doc reference removed doc could mention also issue would really helpful regarding docker error would recommend opening separate issue give,issue,positive,positive,positive,positive,positive,positive
551968884,"Reading the source code for Academy, this line cause the above symptoms:
https://github.com/Unity-Technologies/ml-agents/blob/master/UnitySDK/Assets/ML-Agents/Scripts/Academy.cs#L400

which makes sense given the agents can have visual input which requires rendering. However for setups that don't use visual input there is no need to set the Time.CaptureDeltaTime. Furthermore fixing CaptureDeltaTime at 60 instead of having it as an Academy parameter similar to TargetFrameRate, quality, etc. is questionable. Is MLAgents assuming a screen refresh rate of 60?",reading source code academy line cause sense given visual input rendering however use visual input need set furthermore fixing instead academy parameter similar quality questionable assuming screen refresh rate,issue,negative,negative,negative,negative,negative,negative
551968552,"I have given this a bit more thought:

Basically IMO the Brain should be a low level API that is consumed by the Academy. Observations and Actions should be part of the Brain, not the Agent.

Also, why do brains need to be tied to agents? There are many, many use cases for training and using brains outside the context of an Agent and the FixedUpdate loop.

For example:

- A typed text interpreter (player types some input and the brain tries to understand what the player wants).
- An image categorizer that reads Mamograms

Why do these need to be tied to an Agent and the FixedUpdate loop? As a developer I would like to query these on demand.


- ",given bit thought basically brain low level academy part brain agent also brain need tied many many use training brain outside context agent loop example text interpreter player input brain understand player image need tied agent loop developer would like query demand,issue,negative,positive,positive,positive,positive,positive
551965518,"@xiaomaogy (and Devedse) I appreciate the response. I think I may have things configured properly but that the docker image included isn't working properly. It doesn't seem to connect to my linux host with Unity Editor and errors out when running the headless linux build. (Yes I opened up the ports). 

@xiaomaogy there's currently references to docker sprinkled throughout your docs which makes things very confusing. The readme says `Simplified set-up with Docker`. The installation doc says `If you would like to use our Docker set-up (introduced later)`. There's a dockerfile in the root of the repo. You would think Docker is supported. If anything the maintainers should remove the references until it is supported. 

I'd open a ticket but I'm really just evaluating this for now. But it's a real head scratcher why docker isn't the default platform. I'd much rather run `docker build .` than muck around with python environments. Whoevers calling the shots at Unity, if you'd rather spend less time on support, and more time making progress - then docker should be the default. Build a GUI around it and pretend docker never existed. Cheers",appreciate response think may properly docker image included working properly seem connect host unity editor running headless build yes currently docker sprinkled throughout simplified docker installation doc would like use docker later root would think docker anything remove open ticket really real head scratcher docker default platform much rather run docker build muck around python calling unity rather spend le time support time making progress docker default build around pretend docker never,issue,positive,positive,neutral,neutral,positive,positive
551953216,"@mikeruddy Also we ""removed"" the docker doc page because we wish to have one guide (The venv guide) for all of the platforms (Mac, Windows and Linux). It seems that some people are saying the venv guide doesn't work quite well for them https://github.com/Unity-Technologies/ml-agents/issues/2854 on the conda side, so we are planning to bring the conda page back.  If you feels that you also have some need for the docker page, please open another issue on it, then our team will discuss accordingly. ",also removed docker doc page wish one guide guide mac people saying guide work quite well side bring page back also need docker page please open another issue team discus accordingly,issue,positive,neutral,neutral,neutral,neutral,neutral
551607111,"It seems that i have mlagents 0.11.0 version installed. Yet still can't use tensorboard and have this error
![image](https://user-images.githubusercontent.com/26842884/68472761-7b0f5480-0221-11ea-961d-61aeda1f03aa.png)
",version yet still ca use error image,issue,negative,neutral,neutral,neutral,neutral,neutral
551582201,"Use `pip list` to view the installed version, I had the same problem and after upgrading mlagents I managed to solve it 


Before upgrading
```bash
→ pip list
Package               Version
--------------------- --------
...
mlagents              0.10.1
mlagents-envs         0.10.1
...
```

After upgrading 
```bash
→ pip list
Package               Version
--------------------- --------
...
mlagents              0.11.0
mlagents-envs         0.11.0
...
```",use pip list view version problem solve bash pip list package version bash pip list package version,issue,negative,neutral,neutral,neutral,neutral,neutral
551570955,"> may i ask if you updated the python package with
> 
> ```shell
> pip install --user --upgrade mlagents
> ```

I was using anacond env. I have deleted old env and installed a new one. Should I use ```pip install --user --upgrade mlagents ``` too?",may ask python package shell pip install user upgrade old new one use pip install user upgrade,issue,negative,positive,positive,positive,positive,positive
551514095,"may i ask if you updated the python package with

```bash
pip install --user --upgrade mlagents
```",may ask python package bash pip install user upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
551440751,@mikeruddy The control checkbox was removed as of version 0.11.0 of the ml-agents framework. If you now configure the rest correctly it should be as simple as running your train command from the command line and then pressing the play button.,control removed version framework configure rest correctly simple running train command command line pressing play button,issue,negative,neutral,neutral,neutral,neutral,neutral
551352394,"@Takumi-YPK Thank you. I believe that did it. I still ran into some hiccups here and there, but that may be due to my lack of knowledge of python and the command line. But ultimately I got it working. Thanks. ",thank believe still ran may due lack knowledge python command line ultimately got working thanks,issue,negative,positive,neutral,neutral,positive,positive
551348461,"I'm getting the exact same issue but I'm not seeing the ""control"" checkbox @xiaomaogy 

Is training from docker supported? Why was the docker getting started guide removed? Thanks
",getting exact issue seeing control training docker docker getting guide removed thanks,issue,negative,positive,positive,positive,positive,positive
551305002,"For posterity, here's the script https://gist.github.com/chriselion/3714d05255eea2f9132b96a182fbdcaa (it's not generally usable, so not adding to source control)",posterity script generally usable source control,issue,negative,positive,neutral,neutral,positive,positive
551303330,"I haven't gotten deep into tensorflow to know where it benefits from running on the GPU. I think if you have large vectors of observations it will still speed things up, but not for dozens of observations.",gotten deep know running think large still speed,issue,negative,positive,positive,positive,positive,positive
551280552,"I have been able to get xvfb working with Ubuntu 18.04 with both CPU and GPU.

### Problem 
apt installs xvfb version 1.19.6 which is incompatible with Ubuntu 18.04. 

### Solution
install an older version of xvfb (1.18.4) which works with Ubuntu 18.04. Unfortunately apt repository does not contain this version so it has be installed in a roundabout way using gdebi. Also libxfont is required to install xvfb 1.18.4. Here are the details:

### CPU instructions

1. Install gdebi and wget using the command `apt install gdebi-core wget`

2. Get libxfont and xvfb packages 
`wget http://security.ubuntu.com/ubuntu/pool/main/libx/libxfont/libxfont1_1.5.1-1ubuntu0.16.04.4_amd64.deb`
`wget http://security.ubuntu.com/ubuntu/pool/universe/x/xorg-server/xvfb_1.18.4-0ubuntu0.7_amd64.deb`

3. Install libxfont and xvfb packages
`gdebi libxfont1_1.5.1-1ubuntu0.16.04.4_amd64.deb`
`gdebi xvfb_1.18.4-0ubuntu0.7_amd64.deb`

### GPU Instructions (for Nvidia GPU's only)
1. Follow the steps in CPU instructions

2. Make sure CUDA + Nvidia + CUDNN are setup correctly. These instructions work on Nvidia 410.79, Cuda 10.0, CUDNN 7. 

3. Install the following libraries using apt: xorg, xorg-dev, mesa-utils, libglvnd-dev libgl1-mesa-dev libegl1-mesa-dev libgles2-mesa-dev
`apt install xorg, xorg-dev, mesa-utils, libglvnd-dev libgl1-mesa-dev libegl1-mesa-dev libgles2-mesa-dev`

4. Update the LD\_LIBRARY_PATH variable. The easiest way to do that is to add this line to your .bashrc
`export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH`

5. Ensure the Nvidia binaries are also available in the LD\_LIBRARY_PATH. They are usually installed in `/usr/local/nvidia/bin`.",able get working problem apt version incompatible solution install older version work unfortunately apt repository contain version roundabout way also install install command apt install get install follow make sure setup correctly work install following apt apt install update variable easiest way add line export ensure also available usually,issue,positive,positive,positive,positive,positive,positive
551237916,"Hi @MoMe36, it's less critical for SAC to have many cloned instances, but it still benefits from the increased data throughput, especially if the environment takes a while to step. ",hi mome le critical sac many still data throughput especially environment step,issue,negative,positive,positive,positive,positive,positive
551229855,Looks like you have resolved the issue. Closing it for now. Feel free to re-open if needed. Thanks.,like resolved issue feel free thanks,issue,positive,positive,positive,positive,positive,positive
551206502,I wrote a one-off script to update the demo files. I'll reconfirm that they work though.,wrote script update reconfirm work though,issue,negative,neutral,neutral,neutral,neutral,neutral
551197667,"Oh... sorry I'm downloaded last version on git clone
but this version download to zip file and not set on virtual env",oh sorry last version git clone version zip file set virtual,issue,negative,negative,negative,negative,negative,negative
551192340,"I agree with @Takumi-YPK, it sounds like there's no compatible version of tensorflow that can be installed for your setup. Until you can get that resolved, there's nothing we can do on the ml-agents side of things.",agree like compatible version setup get resolved nothing side,issue,positive,neutral,neutral,neutral,neutral,neutral
551185755,"> Hi, please make sure your ML-Agents python package and UnitySDK are from the 0.11.0 release. The definition of the gRPC service changed and it is not backwards compatible.

How I Can  check that??",hi please make sure python package release definition service backwards compatible check,issue,positive,positive,positive,positive,positive,positive
551184670,"@DVonk either Python 3.6 or 3.7 will work, and Tensorflow 1.7.x - 1.15.x.

The warnings are (mostly) related to the transition to Tensorflow 2.0.  We're currently working on migrating to the new APIs.",either python work mostly related transition currently working new,issue,negative,positive,neutral,neutral,positive,positive
551170635,@unityjeffrey Thank you. Is it still correct to install Python 3.6 and Tensorflow 1.7.1 or should I use a newer version? Because with Tensorflow 1.14 and 1.15 I get a lot of warnings for deprecated TF functions when training.,thank still correct install python use version get lot training,issue,negative,neutral,neutral,neutral,neutral,neutral
551165041,"hi @sonicviz, @DVonk, @devedse - thank you for your feedback, very helpful for us to understand the needs for conda in Windows.  For now, please refer to the previous instructions (it should be the same steps).  https://github.com/Unity-Technologies/ml-agents/blob/0.10.1/docs/Installation-Windows.md

Will keep you posted on next steps on Anaconda instructuions for Windows.",hi thank feedback helpful u understand need please refer previous keep posted next anaconda,issue,positive,negative,neutral,neutral,negative,negative
551113901,"Hi, please make sure your ML-Agents python package and UnitySDK are from the 0.11.0 release.  The definition of the gRPC service changed and it is not backwards compatible.",hi please make sure python package release definition service backwards compatible,issue,positive,positive,positive,positive,positive,positive
551046351,"I have exactly the same issues when trying to train the Example ""3DBall"" environment, or just when trying to access the environment executable I previously built (following the documentation instructions), i.e. by running 

```
from mlagents.envs.environment import UnityEnvironment
env = UnityEnvironment(file_name=<env_name>)
```

Error messages fully match the ones @ghk71 posted above. 

My set-up:
Windows 10
ML-Agents:  0.11.0
TensorFlow: 1.7.1 (GPU)
Unity: I tried both 2017.4.34.f1 and 2019.3.0b7
Python: I tried 3.8.x, 3.7.5 and 3.6.9
Anaconda: latest version",exactly trying train example environment trying access environment executable previously built following documentation running import error fully match posted unity tried python tried anaconda latest version,issue,negative,positive,positive,positive,positive,positive
550594131,"I think this was a bad decision to remove the Anaconda install instructions. 
I just started working with the ML kit and have been scratching my head as to why there were not any instructions for Anaconda, which is a pretty standard way of managing Python environments on Win. ",think bad decision remove anaconda install working kit scratching head anaconda pretty standard way python win,issue,positive,positive,neutral,neutral,positive,positive
550584161,"I have tried it with your version and it works perfectly fine.
Thank you for your help.

I was using 2019.3.0a8 , 2019.3.0b9 which gave no errors but failed to use player brain.
 2017.4.32f1 and 2018.4.7f1 failed to load the setting.

Thank you I will close the issue",tried version work perfectly fine thank help gave use player brain load setting thank close issue,issue,positive,positive,positive,positive,positive,positive
550546054,"I'd be interested in knowing if there were a way to send hyperparameters from Jupyter. I understand the .yaml config files can be used with the ml-agents command line, but I'm invoking a torch library and need to use the Jupyter setup, as far as I can tell. I would like to adjust various parameters within Unity from Jupyter so that it will be easy to change hyperparameters, especially if running headless. An example parameter is adjusting the number of agents on a team in a team-based environment.",interested knowing way send understand used command line torch library need use setup far tell would like adjust various within unity easy change especially running headless example parameter number team environment,issue,positive,positive,positive,positive,positive,positive
550543563,"Hi @kumarict,
Please make sure you've installed the python package and UnitySDK from the 0.11.0 release.  The gRPC service definition changed and is not backwards compatible with previous versions. ",hi please make sure python package release service definition backwards compatible previous,issue,positive,positive,positive,positive,positive,positive
550542437,I'm receiving this same error when running on Ubuntu 16.04. Has v0.11 been confirmed to work on Ubuntu 16.04? I've made sure I'm running the developer code.,error running confirmed work made sure running developer code,issue,negative,positive,positive,positive,positive,positive
550534632,There should be no .nn file because I did not train the learning brain yet. What version of Unity are you using?,file train learning brain yet version unity,issue,negative,neutral,neutral,neutral,neutral,neutral
550534455,It looks like your learning brain isn't setup with the same action/observation space as your player brain.  I'm not sure why it's not working for you.  Very peculiar. ,like learning brain setup space player brain sure working peculiar,issue,positive,positive,positive,positive,positive,positive
550533481,I am using QAWSED to move the robot.  I didn't find a `.nn` file in the project to try running inference.  I haven't tried training it either.  ,move robot find file project try running inference tried training either,issue,negative,neutral,neutral,neutral,neutral,neutral
550532503,"I guess this is a bug in my computers? You can move the robot without QAWSED?

I even tried with another PC and started all over again and it is not moving.
What version of Unity are you using?

What do you think may be the cause of this problem?",guess bug move robot without even tried another moving version unity think may cause problem,issue,negative,neutral,neutral,neutral,neutral,neutral
550529812,"Hi @unityjeffrey. In October 2018 I ran the requirement.txt file with the actual version of the libraries written inside. Using the libraries written in the file today is probably the reason why my code doesn't work. Is the only thing I have re-imported in my project. Unity, python and Ml-Agent versions are the same. In support of my thesis even the examples inside the ML-Agent folder doesn't work with a clear installation following the specifications of the guide written for the version 0.4.0b. Do you have any idea?

NFO:unityagents:Hyperparameters for the PPO Trainer of brain Ball3DBrain:
	batch_size:	64
	beta:	0.001
	buffer_size:	12000
	epsilon:	0.2
	gamma:	0.995
	hidden_units:	128
	lambd:	0.99
	learning_rate:	0.0003
	max_steps:	5.0e4
	normalize:	True
	num_epoch:	3
	num_layers:	2
	time_horizon:	1000
	sequence_length:	64
	summary_freq:	1000
	use_recurrent:	False
	graph_scope:
	summary_path:	./summaries//Users/danielepiergigli/Desktop/test
	memory_size:	256
	use_curiosity:	False
	curiosity_strength:	0.01
	curiosity_enc_size:	128
INFO:unityagents: Ball3DBrain: Step: 1000. Mean Reward: 1.255. Std of Reward: 0.736.
INFO:unityagents: Ball3DBrain: Step: 2000. Mean Reward: 1.389. Std of Reward: 0.751.
INFO:unityagents: Ball3DBrain: Step: 3000. Mean Reward: 1.677. Std of Reward: 1.003.
INFO:unityagents: Ball3DBrain: Step: 4000. Mean Reward: 2.363. Std of Reward: 1.699.
INFO:unityagents: Ball3DBrain: Step: 5000. Mean Reward: 3.512. Std of Reward: 2.837.
INFO:unityagents: Ball3DBrain: Step: 6000. Mean Reward: 5.752. Std of Reward: 5.195.
INFO:unityagents: Ball3DBrain: Step: 7000. Mean Reward: 11.211. Std of Reward: 10.930.
INFO:unityagents: Ball3DBrain: Step: 8000. Mean Reward: 18.083. Std of Reward: 19.186.
INFO:unityagents: Ball3DBrain: Step: 9000. Mean Reward: 23.474. Std of Reward: 22.393.
Traceback (most recent call last):
  File ""learn.py"", line 84, in <module>
    tc.start_learning()
  File ""/Users/danielepiergigli/Downloads/ml-agents-0.4.0b/python/unitytrainers/trainer_controller.py"", line 264, in start_learning
    trainer.update_model()
  File ""/Users/danielepiergigli/Downloads/ml-agents-0.4.0b/python/unitytrainers/ppo/trainer.py"", line 445, in update_model
    self.training_buffer.update_buffer.shuffle()
  File ""/Users/danielepiergigli/Downloads/ml-agents-0.4.0b/python/unitytrainers/buffer.py"", line 166, in shuffle
    raise BufferException(""Unable to shuffle if the fields are not of same length"")
unitytrainers.buffer.BufferException: Unable to shuffle if the fields are not of same length",hi ran file actual version written inside written file today probably reason code work thing project unity python support thesis even inside folder work clear installation following guide written version idea trainer brain beta epsilon gamma normalize true false false step mean reward reward step mean reward reward step mean reward reward step mean reward reward step mean reward reward step mean reward reward step mean reward reward step mean reward reward step mean reward reward recent call last file line module file line file line file line shuffle raise unable shuffle length unable shuffle length,issue,positive,negative,negative,negative,negative,negative
550529759,Ok so in that case how would I train my own model then?,case would train model,issue,negative,neutral,neutral,neutral,neutral,neutral
550527608,"You can find it in the release notes here https://unity3d.com/unity/beta/2019.3#downloads
```Editor: Prevent reloading scripts in playmode when a DPI change occurs (1193597) ```",find release editor prevent change,issue,negative,neutral,neutral,neutral,neutral,neutral
550526880,"Hi, the fix for this will be in the 2019.3 release of unity.  It looks like it may be in 2019.3.0.b10 release if you are following the beta releases.",hi fix release unity like may release following beta,issue,negative,neutral,neutral,neutral,neutral,neutral
550525933,Closing per your last comment.  Thanks for posting. ,per last comment thanks posting,issue,negative,positive,neutral,neutral,positive,positive
550516215,"![Screen Shot 2019-11-06 at 1 48 48 PM](https://user-images.githubusercontent.com/1356616/68340958-40c87a80-009c-11ea-8b9e-85d209adb01f.png)

This is the configuration of the object in the Hierarchy.  The upper script is unchecked, and the robot joints were moving.",screen shot configuration object hierarchy upper script unchecked robot moving,issue,negative,neutral,neutral,neutral,neutral,neutral
550486078,"Maybe you checked the box in the machine setting script? If you uncheck the
upper script it will not move. I mean only with the SDoosan script does it
move?

2019년 11월 6일 (수) 오후 3:02, Chris Goy <notifications@github.com>님이 작성:

> Hey @hongluck <https://github.com/hongluck>,
> I've loaded your project, and I can see the Robot responding to my
> keyboard presses in accordance with your player brain. What issues are you
> running into?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/2803?email_source=notifications&email_token=AMSJ3YSTYP2LCDE6RAQI3PLQSMPGFA5CNFSM4JFJLKTKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDHZYMI#issuecomment-550476849>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AMSJ3YTZW2T7CTIJZ7WNU5DQSMPGFANCNFSM4JFJLKTA>
> .
>
",maybe checked box machine setting script uncheck upper script move mean script move goy hey loaded project see robot keyboard accordance player brain running reply directly view,issue,negative,negative,neutral,neutral,negative,negative
550476849,"Hey @hongluck,
I've loaded your project, and I can see the Robot responding to my keyboard presses in accordance with your player brain.  What issues are you running into?",hey loaded project see robot keyboard accordance player brain running,issue,negative,neutral,neutral,neutral,neutral,neutral
550468690,"Just accepted. Thank you for your help

2019년 11월 6일 (수) 오후 2:06, Chris Goy <notifications@github.com>님이 작성:

> Hi, I've requested access to your project on google drive.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/2803?email_source=notifications&email_token=AMSJ3YRVM3QASAORAA5YGX3QSMISJA5CNFSM4JFJLKTKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDHUOOA#issuecomment-550455096>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AMSJ3YUNHDVSCEK4DXHB6Y3QSMISJANCNFSM4JFJLKTA>
> .
>
",accepted thank help goy hi access project drive reply directly view,issue,positive,positive,neutral,neutral,positive,positive
550455096,"Hi, I've requested access to your project on google drive.  ",hi access project drive,issue,negative,neutral,neutral,neutral,neutral,neutral
550452217,"This is the link to my project
https://drive.google.com/open?id=1fu4ii0XNlpPKA1o_GoRyTx7Rn9ydmUMc

The goal of this project is to make a robot reach a random target(without using IK).
I did not finish the code yet so the resetting and others are still in progress. 
The main problem for me right now is that the robot itself is not controllable with the player brain.

The project consists of a Agent, Goal, Academy with other environmental objects.
Agent object is the robot with a hierarchy. The robot is modeled just using transform(not RigidBody). In the Agent there are two scripts which is Doosanmachinesetting.cs and SDoosanAgent1105.

Doosanmachinesetting.cs is a file that I was testing the robot constraint when I first built the robot.
SDoosanAgent1105 is the file I was going to do ML. 

The Doosanmachinesetting.cs script works fine which you can control the robot joint 1 to 6 with QAWSED keys respectively. But whenever I use the SDoosanAgent1105.cs, it will not be able to control the robot. I do not know why because it is almost the same as the previous mentioned file.

I used ReacherAgent example as a reference without using joints. 
In the player brain I should be able to control the robot but I cannot. I think the AgentAction function is not working. I do not know why and please help me fix the issue.
For your convenience, copied and pasted the previous issue below.









Hello, I am currently working on unity to train robotic manipulators. 

Thanks to the  ML Agent package which has variety of examples including the Reacher example.

I made a virtual robot in the environment, which I can control just by using rotation in the coordinates.

![image](https://user-images.githubusercontent.com/52731362/67604803-9a4eb200-f74a-11e9-8592-f7305ad6146e.png)

I am not using rigidbody component, just rotating it using 

Rotate(Vector3(0f,1f,0f);

It is because the hinge joint, configuration joint has compliance in shear. Also, I could not find a way to have two rigidbodies connected together which has to move depending on each other(without shear compliance)!

I have tested through a simple code in Unity code by just mapping key using

void Update()
{
Input.GetKey(KeyCode.Q)
joint.Rotate(new Vector3(0f,1f, 0f);
}
Using just Unity, made pretty good results so that I can manipulate a robot inside Unity.

However, when it comes to ML Agents, it seems like the AgentAction code is not working.
![image](https://user-images.githubusercontent.com/52731362/67606046-34642980-f74e-11e9-82eb-283515de20c0.png)


I found out that the AgentAction code is not working.
Even though I use Debug.log inside the AgentAction code, there are no response.  
![image](https://user-images.githubusercontent.com/52731362/67605484-7f7d3d00-f74c-11e9-883d-60770f1f8350.png)

I do not have any response with the playerBrain.
I have 7 transform. position VectorObs, resulting in 21.
Also I have connected 6 joints with +1, -1 values so need 12 key continuousPlayerActions Size.
the vectorAction[] should have a size of 6 index.
![image](https://user-images.githubusercontent.com/52731362/67606012-1d253c00-f74e-11e9-9a6d-4dafd32948d8.png)



I do not know what to do since, basic control of the robot with the Brain is not working. 
I was going to use the PlayerBrain reference in the Reacher example but in the ML Agent Package, there are no player Brain in the Reacher example. 

It does not have any compile errors.

Please help me fix the issue.





",link project goal project make robot reach random target without finish code yet still progress main problem right robot controllable player brain project agent goal academy environmental agent object robot hierarchy robot transform agent two file testing robot constraint first built robot file going script work fine control robot joint respectively whenever use able control robot know almost previous file used example reference without player brain able control robot think function working know please help fix issue convenience copied pasted previous issue hello currently working unity train thanks agent package variety reacher example made virtual robot environment control rotation image component rotating rotate vector hinge joint configuration joint compliance shear also could find way two connected together move depending without shear compliance tested simple code unity code key void update new vector unity made pretty good manipulate robot inside unity however come like code working image found code working even though use inside code response image response transform position resulting also connected need key size size index image know since basic control robot brain working going use reference reacher example agent package player brain reacher example compile please help fix issue,issue,positive,positive,positive,positive,positive,positive
550451275,"@harperj Okay. So, have you tried setting up ML-Agents on a clean Windows installation according to the instructions? I'm regularly supervising projects that use ML-Agents at university and it was really easy to set up thank to the step-by-step Anaconda instructions. For the newest project I've had so many questions from students that don't have any experience with Python and I wondered why - until I noticed that the Conda readme was gone.

Think about this from the point of view of someone coming directly from Unity, with no Python experience whatsoever. There is a lot of missing information, or things that are just glanced over that threw my students off. 

There are also some things that flat out didn't work when I tried following the readme, e.g.:

- > In order to use ML-Agents toolkit, you need Python 3.6.1 or higher. Download and install the latest version of Python if you do not already have it.

Yet the newest version (3.8) does not work. I'm getting version mismatches for Tensorflow which prevent installation entirely. 3.7.3 works for some reason.

- > To create a new environment named sample-env execute $ python3 -m venv python-envs\sample-env

Doesn't work. It only works with ""python' instead of ""python3"", otherwise absolutely nothing happens.

- When following with Python 3.6.8 (the newest one I've found an .exe installer for) I get this error, which makes tensorboard unusable:
 tensorboard 1.15.0 has requirement setuptools>=41.0.0, but you'll have setuptools 40.6.2 which is incompatible.


I agree that a universal way of installation is the way to go, but I definitely don't think this is the way to do it.",tried setting clean installation according regularly use university really easy set thank anaconda project many experience python gone think point view someone coming directly unity python experience whatsoever lot missing information threw also flat work tried following order use need python higher install latest version python already yet version work getting version prevent installation entirely work reason create new environment execute python work work python instead python otherwise absolutely nothing following python one found installer get error unusable requirement incompatible agree universal way installation way go definitely think way,issue,positive,positive,positive,positive,positive,positive
550436688,"Hi @emarche, 
Since we've logged this internally, I will close this issue for now.  When this functionality is added, we will post back to this issue to update you.  Thank you for your report. ",hi since logged internally close issue functionality added post back issue update thank report,issue,negative,neutral,neutral,neutral,neutral,neutral
550434392,"Hi @ghk71,
Thank you for posting your issue.  Could you add the following information:
- Unity Version
- ML-Agents Release Version
- Tensorflow Version

As a general practice, please search through the issues before filing a new one.  This question was asked and resolved in #2758.  The fix should be in the 0.11.0 Release of ML-Agents.  I am going to close this issue for now, feel free to reply to it if you still run into issues. ",hi thank posting issue could add following information unity version release version version general practice please search filing new one question resolved fix release going close issue feel free reply still run,issue,positive,positive,positive,positive,positive,positive
550433453,"Hi @devedse / @DVonk -- we decided it would be simpler to help people through the setup process if we only had a single recommended path in our docs.  You're welcome to continue using Anaconda, and we aren't trying to discourage anyone from using the tools that work best for them.",hi decided would simpler help people setup process single path welcome continue anaconda trying discourage anyone work best,issue,positive,positive,positive,positive,positive,positive
550360641,"I would like to know this as well. Am I not supposed to use Anaconda anymore? I find the installation instructions for virtual environments under Windows to be pretty confusing as they assume that you already know how to get a working Python installation and Tensorflow on your machine. 

The Anaconda step-by-step instructions were just right - what happened to them?",would like know well supposed use anaconda find installation virtual pretty assume already know get working python installation machine anaconda right,issue,positive,positive,positive,positive,positive,positive
550350266,"@SavedTheWorld77 could you please share simple project setup that shows the issue?
Thanks!",could please share simple project setup issue thanks,issue,positive,positive,neutral,neutral,positive,positive
550344553,"This issue is on our backlog, but at the moment I can't tell for sure when fix will be available.",issue backlog moment ca tell sure fix available,issue,negative,positive,positive,positive,positive,positive
550203232,"Thank you @harperj, I posted an issue to the tensorflow repo, I hope I'll find a solution :)",thank posted issue hope find solution,issue,positive,neutral,neutral,neutral,neutral,neutral
550161688,"@surfnerd I redownloaded the github root project and now it's working! I think I've used the link provided in the guids & documents previously and allegedly that was an old link.
Thanks It was getting confusing :)",root project working think used link provided previously allegedly old link thanks getting,issue,negative,positive,positive,positive,positive,positive
550119948,"Nice one. Thanks for the fix.
",nice one thanks fix,issue,positive,positive,positive,positive,positive,positive
550095105,"Hi @unnamed7,
The logic around this in the Academy has changed and `ForcedFullReset` is only called from `EnvironmentStep` for the first Academy Reset.  Afterwards, it is then called directly as an event delegate when the python process tells your Unity Environment to reset.  This bug was fixed in #2675 and should be available in the latest release.  ",hi unnamed logic around academy first academy reset afterwards directly event delegate python process unity environment reset bug fixed available latest release,issue,negative,positive,positive,positive,positive,positive
550093957,"Hi @Flavelius,
After some research we've decided that we won't pursue fixing this bug.  If you can narrow it down further we may be able to point you to a 3rd part project we are using (gRPC for instance). A bug report may find a softer landing there.  Thank you for the report.  ",hi research decided wo pursue fixing bug narrow may able point part project instance bug report may find landing thank report,issue,negative,positive,positive,positive,positive,positive
550091362,"Sorry, I don't have an example for you handy.  You can see the [imitation learning docs](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Imitation-Learning.md) for more about how to record demonstrations though.",sorry example handy see imitation learning record though,issue,negative,negative,neutral,neutral,negative,negative
550090790,Hi @wrd90 -- I'm not sure why TFLite wouldn't convert this model.  It sounds like you're right that it has something to do with the unspecified / variable shape for the sequence.  You might try posting an issue for TFLite since they'd have more information about what is and isn't possible to convert.,hi sure would convert model like right something unspecified variable shape sequence might try posting issue since information possible convert,issue,positive,positive,positive,positive,positive,positive
550088618,"Hi @dans-msft,
This was fixed in https://github.com/Unity-Technologies/ml-agents/pull/2783, and the latest version of gym_unity (0.11.0) contains the fix.",hi fixed latest version fix,issue,negative,positive,positive,positive,positive,positive
550085661,"Sorry for the delay on this. This issue was fixed in https://github.com/Unity-Technologies/ml-agents/issues/2738, and the fix is in the 0.11 release.",sorry delay issue fixed fix release,issue,negative,negative,negative,negative,negative,negative
550080753,"Hi, can you make sure the python package and the Unity SDK version are all from the 0.11 release?  The definition of the gRPC service has changed and is not backwards compatible.",hi make sure python package unity version release definition service backwards compatible,issue,negative,positive,positive,positive,positive,positive
550049562,"Does this mean that the .nn models that come with the sdk like for example in pyramids, only work for the pyramids, and not for the visual pyramids? It would be great that a disclaimer is included in the instructions so we don't just assume that the model would work, which is fine and logical",mean come like example work visual would great disclaimer included assume model would work fine logical,issue,positive,positive,positive,positive,positive,positive
550045677,"This is the whole text I'm getting: 
ArgumentOutOfRangeException: Index was out of range. Must be non-negative and less than the size of the collection.
Parameter name: index
System.ThrowHelper.ThrowArgumentOutOfRangeException (System.ExceptionArgument argument, System.ExceptionResource resource) (at <567df3e0919241ba98db88bec4c6696f>:0)
System.ThrowHelper.ThrowArgumentOutOfRangeException () (at <567df3e0919241ba98db88bec4c6696f>:0)
System.Collections.Generic.List`1[T].get_Item (System.Int32 index) (at <567df3e0919241ba98db88bec4c6696f>:0)
MLAgents.InferenceBrain.VectorObservationGenerator.Generate (MLAgents.InferenceBrain.TensorProxy tensorProxy, System.Int32 batchSize,

ArgumentException: An item with the same key has already been added. Key: Agent (PyramidAgent)
System.Collections.Generic.Dictionary`2[TKey,TValue].TryInsert (TKey key, TValue value, System.Collections.Generic.InsertionBehavior behavior) (at <567df3e0919241ba98db88bec4c6696f>:0)
System.Collections.Generic.Dictionary`2[TKey,TValue].Add (TKey key, TValue value) (at <567df3e0919241ba98db88bec4c6696f>:0)
MLAgents.Brain.SendState (MLAgents.Agent agent, MLAgents.AgentInfo info) (at Assets/ML-Agents/Scripts/Brain.cs:56)
MLAgents.Agent.SendInfoToBrain () (at Assets/ML-Agents/Scripts/Agent.cs:653)
MLAgents.Agent.SendInfo () (at Assets/ML-Agents/Scripts/Agent.cs:1039)
MLAgents.Academy.EnvironmentStep () (at Assets/ML-Agents/Scripts/Academy.cs:556)",whole text getting index range must le size collection parameter name index argument resource index item key already added key agent key value behavior key value agent,issue,negative,positive,neutral,neutral,positive,positive
549940278,Hi @HugeKangroo -- I've recorded this issue on our internal tracker as MLA-225 and will review it with the team.,hi issue internal tracker review team,issue,negative,neutral,neutral,neutral,neutral,neutral
549932677,"hi @Lucci93 - i think without knowing what exactly was changed in your setup, it would be difficult to debug.  If nothing was touched or updated, it would run as expected.",hi think without knowing exactly setup would difficult nothing touched would run,issue,negative,negative,negative,negative,negative,negative
549928505,"Gotcha. So in ""real world"" we usually are using cameras to capture data and GPU shines there, but in Unity ray casting (vectors) are used instead, so we don't need to use convolutional layers and images, and thus GPU doesn't shine as much? Or something along those lines? Any idea why OpenAI Five (which I thought used Valve's API (rather than images) for Dota2, and GPU) was different?",real world usually capture data unity ray casting used instead need use convolutional thus shine much something along idea five thought used valve rather different,issue,negative,positive,neutral,neutral,positive,positive
549916649,"Hi @lukemadera,
Unfortunately, I don't think there's a better guideline than ""it depends"". I think the consensus is that GPUs will help with visual observations, but probably not much with vector observations unless you have a lot of them.

Multi-GPU support was added a few releases ago; you can enable it with `--multi-gpu` from the command line.",hi unfortunately think better guideline think consensus help visual probably much vector unless lot support added ago enable command line,issue,positive,positive,neutral,neutral,positive,positive
549852169,"Update: if I disable the RNN by setting `use_recurrent:false` the conversion works, but unfortunately I need a recurrent network.",update disable setting false conversion work unfortunately need recurrent network,issue,negative,negative,negative,negative,negative,negative
549823184,"If I pass the shapes to the command like this: 

```
tflite_convert --output_file=model.tflite --graph_def_file=unity/ml-agents/models/ppo-0/RoverLearning/frozen_graph_def.pb --input_arrays=vector_observation,epsilon,recurrent_in,sequence_length --output_arrays=action --input_shapes=1,7:1,2:7,40:1
```

I get a similar error:

```
2019-11-05 14:25:44.658772: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-05 14:25:44.677569: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fee79ed3960 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-11-05 14:25:44.677591: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-11-05 14:25:44.741840: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-11-05 14:25:44.741948: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-11-05 14:25:44.819310: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2019-11-05 14:25:44.819337: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 304 nodes (-81), 371 edges (-83), time = 56.212ms.
2019-11-05 14:25:44.819342: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 304 nodes (0), 371 edges (0), time = 8.713ms.
Traceback (most recent call last):
  File ""/Users/dwright/anaconda3/envs/mlagents/bin/tflite_convert"", line 8, in <module>
    sys.exit(main())
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 515, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 511, in run_main
    _convert_tf1_model(tflite_flags)
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 199, in _convert_tf1_model
    output_data = converter.convert()
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py"", line 983, in convert
    **converter_kwargs)
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py"", line 449, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py"", line 200, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2019-11-05 14:25:46.811578: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.811623: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.811639: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.811649: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.811675: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.811685: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.811695: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.811705: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.811986: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.812009: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.812022: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.812033: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.812378: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2019-11-05 14:25:46.812411: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-11-05 14:25:46.812433: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2019-11-05 14:25:46.812448: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-11-05 14:25:46.812468: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.812497: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2019-11-05 14:25:46.812507: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-11-05 14:25:46.812525: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.812544: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.812559: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.812567: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-11-05 14:25:46.812580: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.812588: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-11-05 14:25:46.812613: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2019-11-05 14:25:46.812631: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.812640: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-11-05 14:25:46.812661: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2019-11-05 14:25:46.812680: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.812694: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.812708: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.812724: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2019-11-05 14:25:46.812751: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond
2019-11-05 14:25:46.812763: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond
2019-11-05 14:25:46.812864: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2019-11-05 14:25:46.812928: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2019-11-05 14:25:46.812951: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2019-11-05 14:25:46.813037: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2019-11-05 14:25:46.813111: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2019-11-05 14:25:46.813301: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2019-11-05 14:25:46.816417: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 185 operators, 322 arrays (0 quantized)
2019-11-05 14:25:46.818204: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 103 operators, 180 arrays (0 quantized)
2019-11-05 14:25:46.819853: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 103 operators, 180 arrays (0 quantized)
2019-11-05 14:25:46.819987: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1616] Check failed: *packed_shape == shape All input arrays to Pack operators must have the same shape. Input ""sequence_length"" is different.
Fatal Python error: Aborted

Current thread 0x0000000111d5b5c0 (most recent call first):
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52 in execute
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/absl/app.py"", line 250 in _run_main
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/absl/app.py"", line 299 in run
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40 in run
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89 in main
  File ""/Users/dwright/anaconda3/envs/mlagents/bin/toco_from_protos"", line 8 in <module>
```",pas command like epsilon get similar error binary use service platform host guarantee used device host default version number eligible core count compute capability note support starting new session optimization grappler item graph size time graph size time recent call last file line module main file line main file line run file line run main file line main file line file line file line convert file line file line raise see console see console converting unsupported operation enter converting unsupported operation enter converting unsupported operation enter converting unsupported operation enter converting unsupported operation enter converting unsupported operation enter converting unsupported operation enter converting unsupported operation enter converting unsupported operation enter converting unsupported operation enter converting unsupported operation enter converting unsupported operation enter converting unsupported operation unsupported data type converting unsupported operation unsupported data type converting unsupported operation enter converting unsupported operation unsupported data type converting unsupported operation enter converting unsupported operation enter converting unsupported operation enter unsupported data type converting unsupported operation enter unsupported data type converting unsupported operation converting unsupported operation enter unsupported data type converting unsupported operation converting unsupported operation enter converting unsupported operation enter converting unsupported operation enter converting unsupported operation enter converting unsupported operation converting unsupported operation converting unsupported operation exit converting unsupported operation converting unsupported operation converting unsupported operation converting unsupported operation converting unsupported operation removing unused removing unused pas general graph check shape input pack must shape input different fatal python error aborted current thread recent call first file line execute file line file line run file line run file line main file line module,issue,negative,positive,neutral,neutral,positive,positive
549817801,"@harperj Thank you. With an older version of Tensorflow there seems to be the possibility to convert the frozen graph file.

I have been trying to convert `frozen_graph_def.pb` but there seem to be some issues with this.

When I try the following:
`tflite_convert --output_file=model.tflite --graph_def_file=unity/ml-agents/models/ppo-0/RoverLearning/frozen_graph_def.pb --input_arrays=vector_observation,epsilon,recurrent_in,sequence_length --output_arrays=action`

I get the output:
```
2019-11-05 13:59:21.589574: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-05 13:59:21.602134: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe2bf9e7280 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-11-05 13:59:21.602151: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Traceback (most recent call last):
  File ""/Users/dwright/anaconda3/envs/mlagents/bin/tflite_convert"", line 8, in <module>
    sys.exit(main())
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 515, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 511, in run_main
    _convert_tf1_model(tflite_flags)
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 199, in _convert_tf1_model
    output_data = converter.convert()
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py"", line 898, in convert
    self._set_batch_size(batch_size=1)
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py"", line 1032, in _set_batch_size
    shape = tensor.shape.as_list()
  File ""/Users/dwright/anaconda3/envs/mlagents/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_shape.py"", line 1171, in as_list
    raise ValueError(""as_list() is not defined on an unknown TensorShape."")
ValueError: as_list() is not defined on an unknown TensorShape.
```

Using this, I added some debut messages to `lite.py` and found that the issue is with the variable `sequence_length` which is defined in the class `LearningModel` as:

```
        self.sequence_length = tf.placeholder(
            shape=None, dtype=tf.int32, name=""sequence_length""
        )
```

I have been trying to redefine `shape=1` or `shape=[]` but nothing seems to work here.

Do you have an idea how to resolve this?",thank older version possibility convert frozen graph file trying convert seem try following epsilon get output binary use service platform host guarantee used device host default version recent call last file line module main file line main file line run file line run main file line main file line file line file line convert file line shape file line raise defined unknown defined unknown added debut found issue variable defined class trying redefine nothing work idea resolve,issue,negative,positive,neutral,neutral,positive,positive
549668343,"Here is the [Link](https://colab.research.google.com/drive/1mVIuqSCmgWVA84kMsWPKmh8ygbhERWfn) to my updated notebook. It works with the latest beta version of [ml-agents 0.11](https://github.com/Unity-Technologies/ml-agents/releases/tag/0.11.0). I should mention that I am really happy with the changes made in this version.



- Now you can build any scene from the ml-agents repository and train it on COLAB. I used the ``input`` interactive widget and let the user choose which see he or she wants to build.

![Screen Shot 2019-11-05 at 8 54 30 AM](https://user-images.githubusercontent.com/588431/68180701-e4852e00-ffa9-11e9-8f74-5c9f61e70c2d.png)

- You can either train in headless (no graphics) mode or stream the output to Twitch.  You can get your stream key [here](https://www.twitch.tv/taesiri/dashboard/settings).

- I used ``i3`` for managing windows as its super lightweight and easy to work with. You can interact with it through the ``i3ipc`` python package.

![training](https://user-images.githubusercontent.com/588431/68180622-ac7deb00-ffa9-11e9-83f2-f8e4b133afb3.jpg)


Happy Coding!

",link notebook work latest beta version mention really happy made version build scene repository train used input interactive let user choose see build screen shot either train headless graphic mode stream output twitch get stream key used super lightweight easy work interact python package training happy,issue,positive,positive,positive,positive,positive,positive
549614341,"@harperj -- I know that Player Brain only learns at the user's input value. Can i record user input if i just implement the Agent Interface as you say? I'd like to see an example of that, if you don't mind.",know player brain user input value record user input implement agent interface say like see example mind,issue,positive,neutral,neutral,neutral,neutral,neutral
549592564,Hi @WannyWanny -- you'll need to implement the Agent interface even when you just want to use the Player brain to record demonstrations.,hi need implement agent interface even want use player brain record,issue,negative,neutral,neutral,neutral,neutral,neutral
549592280,"Hi @wrd90 -- I can't speak to whether this will work how you expect, but the file output as `frozen_graph_def.pb` in the models folder should be the Tensorflow graph before conversion to the Barracuda `.nn` format.",hi ca speak whether work expect file output folder graph conversion barracuda format,issue,negative,neutral,neutral,neutral,neutral,neutral
549591095,"Hi @HoomanJCode -- we don't currently have an ECS-based solution for ML-Agents, but it's something we're working on.  I can't give you a specific timeline but know that it is on our roadmap.",hi currently solution something working ca give specific know,issue,negative,neutral,neutral,neutral,neutral,neutral
549558813,Hi @firepro20 -- in general this could depend on your environment and training algorithm / hyperparameters.  Without more information it's hard to tell whether this is expected behavior.,hi general could depend environment training algorithm without information hard tell whether behavior,issue,negative,negative,negative,negative,negative,negative
549505501,@Esther2013 an example is generation of single-binary executables for inference on game consoles.  We decided to retire our use of TensorFlowSharp a while ago and are keeping an eye on Tensorflow.NET but haven't decided to adopt it at this time.,example generation inference game decided retire use ago keeping eye decided adopt time,issue,negative,negative,negative,negative,negative,negative
549502182,"That helped a bit, along with dropping the number of steps",bit along dropping number,issue,negative,neutral,neutral,neutral,neutral,neutral
549480898,"Otherwise, this is a thing of beauty, thank you.",otherwise thing beauty thank,issue,positive,neutral,neutral,neutral,neutral,neutral
549480786,looks like the Yamato triggers weren't picked up.  I was assuming the `develop-` branch name style.  Is that more of a soft rule?  Should the trigger just be for `develop` as a target of a PR?,like picked assuming branch name style soft rule trigger develop target,issue,negative,positive,neutral,neutral,positive,positive
549478558,"Ah, great, thanks @chriselion ! As I did more reading on gpu, I saw many mentions that GPU either wasn't implemented for MLAgents, or if it was, it didn't run any faster. But then I saw some other posts saying it was now ready and faster, and that a flag for cpu or gpu was coming? What is the current status of GPU training (e.g. with CUDA 10 and one or more RTX 2070 or RTX 2080 Ti GPU(s))?",ah great thanks reading saw many either run faster saw saying ready faster flag coming current status training one ti,issue,positive,positive,positive,positive,positive,positive
549468604,"Hi @lukemadera,
My recommendation is to wait for the 0.11.0 release of ML Agents, which will be out today. That contains the fix for the conversion to barracuda for tensorflow 1.15.",hi recommendation wait release today fix conversion barracuda,issue,negative,neutral,neutral,neutral,neutral,neutral
549443447,"I will try, but I think to reproduce my experiments I need to re-run the training with the 0.4.0b version of the code. Is it possible that some library is no longer backwards compatible with my project?",try think reproduce need training version code possible library longer backwards compatible project,issue,negative,neutral,neutral,neutral,neutral,neutral
549429892,hi @Lucci93 - we are no longer supporting that version of ML-Agents.  Can you try using the latest version to see if you are still getting these errors?,hi longer supporting version try latest version see still getting,issue,negative,positive,positive,positive,positive,positive
549423968,"Hi, I reopen the issue because I have the same problem. Could you help me?
I used the ML-Agent library for a project in October 2018 with Unity 2018.2.2, Python 3.6.6 and the 0.4.0b version of the library with my custom game. Today I have tried to train the AI of the project with the same configuration of October 2018 but I have got the following error:

escapist 16 ~/Desktop/TOGEXP/run $ python3 ../python/learn.py --run-id=testMC  --train
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])

UNITY LOGO HERE

INFO:unityagents:{'--curriculum': 'None',
 '--docker-target-name': 'Empty',
 '--help': False,
 '--keep-checkpoints': '5',
 '--lesson': '0',
 '--load': False,
 '--no-graphics': False,
 '--run-id': 'testMC',
 '--save-freq': '50000',
 '--seed': '-1',
 '--slow': False,
 '--train': True,
 '--worker-id': '0',
 '<env>': None}
INFO:unityagents:Start training by pressing the Play button in the Unity Editor.
INFO:unityagents:
'Academy' started successfully!
Unity Academy name: Academy
        Number of Brains: 1
        Number of External Brains : 1
        Lesson number : 0
        Reset Parameters :

Unity brain name: SurvivalBrainComp
        Number of Visual Observations (per agent): 0
        Vector Observation space type: continuous
        Vector Observation space size (per agent): 108
        Number of stacked Vector Observation: 25
        Vector Action space type: continuous
        Vector Action space size (per agent): 5
        Vector Action descriptions: Forward, Back, Fire, Left, Right
/Users/dario/Desktop/TOGEXP/python/unitytrainers/trainer_controller.py:194: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  trainer_config = yaml.load(data_file)
2019-10-30 11:13:15.188220: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
INFO:unityagents:Hyperparameters for the PPO Trainer of brain SurvivalBrainComp:
        batch_size:     2048
        beta:   0.001
        buffer_size:    20480
        epsilon:        0.2
        gamma:  0.9
        hidden_units:   128
        lambd:  0.93
        learning_rate:  0.0001
        max_steps:      5.0e5
        normalize:      True
        num_epoch:      5
        num_layers:     2
        time_horizon:   256
        sequence_length:        64
        summary_freq:   2000
        use_recurrent:  True
        graph_scope:
        summary_path:   ./summaries/testMC
        memory_size:    256
        use_curiosity:  False
        curiosity_strength:     0.01
        curiosity_enc_size:     128
Traceback (most recent call last):
  File ""../python/learn.py"", line 71, in <module>
    tc.start_learning()
  File ""/Users/dario/Desktop/TOGEXP/python/unitytrainers/trainer_controller.py"", line 264, in start_learning
    trainer.update_model()
  File ""/Users/dario/Desktop/TOGEXP/python/unitytrainers/ppo/trainer.py"", line 453, in update_model
    self.model.returns_holder: np.array(buffer['discounted_returns'][start:end]).flatten(),
ValueError: could not broadcast input array from shape (64,1,257) into shape (64,1)",hi reopen issue problem could help used library project unity python version library custom game today tried train ai project configuration got following error escapist python train passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type resource unity curriculum help false lesson load false false seed slow false train true none start training pressing play button unity editor successfully unity academy name academy number brain number external brain lesson number reset unity brain name number visual per agent vector observation space type continuous vector observation space size per agent number vector observation vector action space type continuous vector action space size per agent vector action forward back fire left right calling without default loader unsafe please read full binary use trainer brain beta epsilon gamma normalize true true false recent call last file line module file line file line buffer start end could broadcast input array shape shape,issue,positive,positive,neutral,neutral,positive,positive
549144829,"@mbaske @eshvk @unityjeffrey TensorFlow.NET can **build** graph, **train** model and **inferring**. One library for all tensorflow related tasks. There are many [examples](https://github.com/SciSharp/TensorFlow.NET-Examples) you can take.",build graph train model one library related many take,issue,negative,positive,positive,positive,positive,positive
549144427,@JeffBobHans Upgrade tensorflow binary might help you out.,upgrade binary might help,issue,negative,neutral,neutral,neutral,neutral,neutral
549143867,@mantasp @xiaomaogy SciSharp team is working on the [YOLOv3](https://github.com/SciSharp/TensorFlow.NET-Examples/tree/master/src/TensorFlowNET.Examples/ImageProcessing/YOLO) example. You guys might be interested in it.,team working example might interested,issue,negative,positive,positive,positive,positive,positive
549142839,"@harperj As I know, [TensorFlowSharp](https://github.com/migueldeicaza/TensorFlowSharp) is retired by [Miguel](https://github.com/migueldeicaza), he recommends [TensorFlow.NET](https://github.com/SciSharp/TensorFlow.NET) as an alternative. 

TensorFlow.NET is built on top of .net standard 2.0 that is cross-platform ([One library to rule them all](https://docs.microsoft.com/en-us/dotnet/standard/library-guidance/cross-platform-targeting)). So could you tell what platform doesn't support?",know retired alternative built top standard one library rule could tell platform support,issue,positive,positive,positive,positive,positive,positive
549140879,@xiaomaogy Why did you guys give up TensorflowSharp? A substitute called Tensorflow.NET might be your choice too.,give substitute might choice,issue,negative,neutral,neutral,neutral,neutral,neutral
549106240,"I am having the same issue. I did the RollerBall example and it trains to an average reward of 0.99 to 1.0 yet when I run it live it rolls off the platform probably 50% of the time. I was on Python v3.6.9 and I updated to v3.7.4 - no change. Any suggestions or ideas of why the training looks great but the resulting model is far worse?

Update: it was the tensorflow v1.15 known issue. I reverted to v1.14 and it worked. BUT I can't install tensorflow-gpu==1.14; I get a pip install error. I googled and there were some suggestions (e.g. make sure you have python 64 bit, which I already have) but none worked for me. I need the GPU to train fast so I'm still stuck until that hotfix comes out.",issue example average reward yet run live platform probably time python change training great resulting model far worse update known issue worked ca install get pip install error make sure python bit already none worked need train fast still stuck come,issue,positive,positive,positive,positive,positive,positive
549105756,"That's OK, the current version is good enough for me :)",current version good enough,issue,negative,positive,positive,positive,positive,positive
549048080,"Ok, I fixed my problem and in the process realized why I was getting the out-dated exception too.

The problem: I used `pip install mlagents`, which installed from PyPi. This installs the current version v0.10 instead of installing the new version from the 0.11 branch folder. Oops! 

Training started working and the error message is correct when I follow the [Installing for Development instructions](https://github.com/Unity-Technologies/ml-agents/blob/release-0.11.0/docs/Installation.md#installing-for-development).",fixed problem process getting exception problem used pip install current version instead new version branch folder training working error message correct follow development,issue,negative,positive,neutral,neutral,positive,positive
548981351,Cool thanks @harperj I will give a try on this PR as well.,cool thanks give try well,issue,positive,positive,positive,positive,positive,positive
548946893,@harperj @ervteng mind taking a look at this? Currently just adds support for pylint and fixes a few things; we can start enabling other checks later.,mind taking look currently support start later,issue,negative,neutral,neutral,neutral,neutral,neutral
548924974,"Hi @leonardoaraujosantos -- we have a WIP pull request (#2665) to explore support for TF2, but I believe we don't have any specific dates set for when it will be available at this time.",hi pull request explore support believe specific set available time,issue,negative,positive,positive,positive,positive,positive
548924210,"About the inference engine: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Unity-Inference-Engine.md

We wrote this so that we can execute models within Unity.  TensorflowSharp wouldn't allow us to execute the models on all of the platforms we would like to support.  By using the Barracuda inference engine we can execute on all of the platforms Unity supports.

You can see the [basic guide](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md#after-training) for instructions on how to take the model (`.nn` file) after training an environment and use it for inference.",inference engine wrote execute within unity would allow u execute would like support barracuda inference engine execute unity see basic guide take model file training environment use inference,issue,positive,neutral,neutral,neutral,neutral,neutral
548923057,Hi @HugeKangroo -- unfortunately we don't support the ability to reset individual agents from Python at this time.,hi unfortunately support ability reset individual python time,issue,negative,negative,negative,negative,negative,negative
548920909,Thanks @ervteng @chriselion -- I'll merge this on Monday and create the release.,thanks merge create release,issue,positive,positive,positive,positive,positive,positive
548919646,"Sorry, didn't realize I was a reviewer for this.

@ervteng The known issue with Barracuda and tenorflow 1.15 (and 2.0) was fixed on develop, and is present in the release branch and this merge.",sorry realize reviewer known issue barracuda fixed develop present release branch merge,issue,negative,negative,negative,negative,negative,negative
548858510,"Why should we have to create a ML agent in Python? If we had enough AI solutions in .NET, the AI agent ought to be created in .NET, am I right? Microsoft is integrating Tensorflow.NET to solve this problem, I think this should be the future. Unity 3D developers are going to be able to use C# to develop their AI applications without relying on Ml-Agent created in Python.",create agent python enough ai ai agent ought right solve problem think future unity going able use develop ai without python,issue,negative,positive,positive,positive,positive,positive
548710577,"@awjuliani does this also holds true for a static level where an agent has to collect multiple health items, but only gets a goal at the end after collecting all of them? I have just removed the reward for collecting one each time, and just provided a goal/rewards when all are collected. So far it looks good. However since I am using continuous space I cannot make use of memory in order to remember which ones have been collected and the ones that aren't. Any suggestions?",also true static level agent collect multiple health goal end removed reward one time provided collected far good however since continuous space make use memory order remember collected,issue,positive,positive,positive,positive,positive,positive
548632580,"> This sounds like this known issue: #2775 Can you try downgrading tensorflow to 1.14.0?
I've changed the version to 1.14.0, but I still can't solve the problem.
![ai](https://user-images.githubusercontent.com/18605994/67998402-610aca80-fc93-11e9-8015-0ec687b38b48.gif)

",like known issue try version still ca solve problem ai,issue,negative,neutral,neutral,neutral,neutral,neutral
548522086,"What do you mean by inference being done using the built in inference?
Wouldn't we just train our models and then use them?",mean inference done built inference would train use,issue,negative,negative,negative,negative,negative,negative
548128111,"Hi @ervteng , Sorry to bug you again . I have a query in response to your previous comment regarding creating two agents one for moving around and one for guidance.
So my question is how will the guidance agent will be in the same state as the agent which is moving around to provide guidance to it.
The possible answer I am able to think is attaching both the agents to the same gameobject which is moving around. But then two brains will be attached to the same gameobject at the same time.  I am not sure how those two brains will provide actions(one brain should provide make the agent  move , other one which is the trained brain should provide guidance by textual instructions on the screen ).
I hope I am able to describe my problem properly. If any confusion kindly ask. Any suggestion would be helpful.",hi sorry bug query response previous comment regarding two one moving around one guidance question guidance agent state agent moving around provide guidance possible answer able think moving around two brain attached time sure two brain provide one brain provide make agent move one trained brain provide guidance textual screen hope able describe problem properly confusion kindly ask suggestion would helpful,issue,positive,positive,positive,positive,positive,positive
548092252,"You're correct. You can copy the ML-Agents and Gizmos directories into the Assets directory of your project. Be aware that if you delete the Examples folder, you lose some useful scripts, like RayPerception, so you may want to delete the Examples selectively.",correct copy asset directory project aware delete folder lose useful like may want delete selectively,issue,positive,positive,positive,positive,positive,positive
548058217,Thanks @chriselion  for the suggestion. Will be waiting for anymore suggestions from your side.,thanks suggestion waiting side,issue,negative,positive,positive,positive,positive,positive
548034848,"@surfnerd @mantasp 

Did anyone tried to reproduce the bug?
Is it in your plans to reproduce/look at/fix it?

Having no official reply is discouraging as barracuda is *closed source* and we depend on you to look at it.
",anyone tried reproduce bug official reply discouraging barracuda closed source depend look,issue,negative,negative,neutral,neutral,negative,negative
548032242,"In theory you should be able to use `tf.summary.histogram` for the layers you care about. However, I tried this out locally and didn't get any results. I'll ask some other tensorboard experts here and see if they have any suggestions
",theory able use care however tried locally get ask see,issue,negative,positive,positive,positive,positive,positive
548024592,Let us never speak of this again,let u never speak,issue,negative,neutral,neutral,neutral,neutral,neutral
547845094,"@awjuliani  I have the same problem, and my mlagents is v0.10.1

#### my unity side errors:
![image](https://user-images.githubusercontent.com/10903567/67852055-ae2d5600-fb46-11e9-867b-890e8e90a573.png)
![image](https://user-images.githubusercontent.com/10903567/67852151-df0d8b00-fb46-11e9-9620-1fd37b6cd8a8.png)


#### my python terminal side errors:
```Shell
(env-ml-agents) D:\Code\ML-Agents-Demo>mlagents-learn trainer_config.yaml --run-id=RollerBall-1 --train
2019-10-30 18:51:18.501696: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2019-10-30 18:51:18.505223: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.



                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓


INFO:mlagents.trainers:CommandLineOptions(debug=False, num_runs=1, seed=-1, env_path=None, run_id='RollerBall-1', load_model=False, train_model=True, save_freq=50000, keep_checkpoints=5, base_port=5005, num_envs=1, curriculum_folder=None, lesson=0, slow=False, no_graphics=False, multi_gpu=False, trainer_config_path='trainer_config.yaml', sampler_file_path=None, docker_target_name=None, env_args=None)
INFO:mlagents.envs:Start training by pressing the Play button in the Unity Editor.
INFO:mlagents.envs:
'Academy' started successfully!
Unity Academy name: Academy
        Number of Brains: 2
        Number of Training Brains : 1
        Reset Parameters :

Unity brain name: RollerBallBrain
        Number of Visual Observations (per agent): 0
        Vector Observation space size (per agent): 8
        Number of stacked Vector Observation: 1
        Vector Action space type: continuous
        Vector Action space size (per agent): [2]
        Vector Action descriptions: ,
Unity brain name: RollerBallPlayer
        Number of Visual Observations (per agent): 0
        Vector Observation space size (per agent): 8
        Number of stacked Vector Observation: 1
        Vector Action space type: continuous
        Vector Action space size (per agent): [2]
        Vector Action descriptions: ,
WARNING:tensorflow:From d:\github\ml-agents\ml-agents\mlagents\trainers\trainer.py:59: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING:tensorflow:From d:\github\ml-agents\ml-agents\mlagents\trainers\trainer.py:59: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

WARNING:tensorflow:From d:\github\ml-agents\ml-agents\mlagents\trainers\tf_policy.py:64: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From d:\github\ml-agents\ml-agents\mlagents\trainers\tf_policy.py:64: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From d:\github\ml-agents\ml-agents\mlagents\trainers\tf_policy.py:71: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From d:\github\ml-agents\ml-agents\mlagents\trainers\tf_policy.py:71: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-10-30 18:51:30.719320: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-10-30 18:51:30.725398: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-10-30 18:51:30.743335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce 210 major: 1 minor: 2 memoryClockRate(GHz): 1.23
pciBusID: 0000:01:00.0
2019-10-30 18:51:30.750190: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2019-10-30 18:51:30.756172: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_100.dll'; dlerror: cublas64_100.dll not found
2019-10-30 18:51:30.761195: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_100.dll'; dlerror: cufft64_100.dll not found
2019-10-30 18:51:30.766731: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_100.dll'; dlerror: curand64_100.dll not found
2019-10-30 18:51:30.771141: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_100.dll'; dlerror: cusolver64_100.dll not found
2019-10-30 18:51:30.776150: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_100.dll'; dlerror: cusparse64_100.dll not found
2019-10-30 18:51:30.781063: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found
2019-10-30 18:51:30.784305: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2019-10-30 18:51:30.793744: F tensorflow/stream_executor/cuda/cuda_driver.cc:404] Check failed: CUDA_SUCCESS == cuDevicePrimaryCtxGetState(device, &former_primary_context_flags, &former_primary_context_is_active) (0 vs. 303)
Process Process-1:
Traceback (most recent call last):
  File ""C:\Users\DDos\.conda\envs\env-ml-agents\lib\multiprocessing\connection.py"", line 312, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
BrokenPipeError: [WinError 109] 管道已结束。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\DDos\.conda\envs\env-ml-agents\lib\multiprocessing\process.py"", line 297, in _bootstrap
    self.run()
  File ""C:\Users\DDos\.conda\envs\env-ml-agents\lib\multiprocessing\process.py"", line 99, in run
    self._target(*self._args, **self._kwargs)
  File ""d:\github\ml-agents\ml-agents-envs\mlagents\envs\subprocess_env_manager.py"", line 89, in worker
    cmd: EnvironmentCommand = parent_conn.recv()
  File ""C:\Users\DDos\.conda\envs\env-ml-agents\lib\multiprocessing\connection.py"", line 250, in recv
    buf = self._recv_bytes()
  File ""C:\Users\DDos\.conda\envs\env-ml-agents\lib\multiprocessing\connection.py"", line 321, in _recv_bytes
    raise EOFError
EOFError
```",problem unity side image image python terminal side shell train could load dynamic library found ignore set machine warning module included information please see related depend functionality listed please file issue start training pressing play button unity editor successfully unity academy name academy number brain number training brain reset unity brain name number visual per agent vector observation space size per agent number vector observation vector action space type continuous vector action space size per agent vector action unity brain name number visual per agent vector observation space size per agent number vector observation vector action space type continuous vector action space size per agent vector action warning name please use instead warning name please use instead warning name please use instead warning name please use instead warning name please use instead warning name please use instead binary use successfully dynamic library found device name major minor could load dynamic library found could load dynamic library found could load dynamic library found could load dynamic library found could load dynamic library found could load dynamic library found could load dynamic library found please make sure missing properly would like use follow guide setup platform skipping check device process recent call last file line err true handling exception another exception recent call last file line file line run file line worker file line file line raise,issue,positive,positive,neutral,neutral,positive,positive
547704177,"@surfnerd, Thank you for your reply. 
On the second problem, if the decision interval also affects the input interval, there are limitations.

",thank reply second problem decision interval also input interval,issue,negative,neutral,neutral,neutral,neutral,neutral
547689136,"Also this is the current training file, Mean Reward is always in the negative currently.
```

default:
    trainer: ppo
    batch_size: 4096
    beta: 1.0e-2
    buffer_size: 40960
    epsilon: 0.2
    hidden_units: 512
    lambd: 0.95
    learning_rate: 1.0e-5
    learning_rate_schedule: linear
    max_steps: 2.0e6
    memory_size: 512
    normalize: true
    num_epoch: 8
    num_layers: 2
    time_horizon: 2048
    sequence_length: 128
    summary_freq: 1000
    use_recurrent: false
    vis_encode_type: simple
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99
        curiosity:
            strength: 0.02
            gamma: 0.99
            encoding_size: 256
```",also current training file mean reward always negative currently default trainer beta epsilon linear normalize true false simple extrinsic strength gamma curiosity strength gamma,issue,positive,negative,neutral,neutral,negative,negative
547677799,"Hi @chriselion ,
I have another small query which is how can I add a histogram depicting the weights of hidden layers after the training is over.
Any suggestion would be helpful.
",hi another small query add histogram hidden training suggestion would helpful,issue,negative,negative,negative,negative,negative,negative
547677499,"Hi @rajatpaliwal -- 

I agree with your sentiments in issue #2518 and I will definitely look into this. Thanks for the reference!",hi agree issue definitely look thanks reference,issue,positive,positive,neutral,neutral,positive,positive
547671319,Played around with hyperparameters and got a better result for my case scenario,around got better result case scenario,issue,negative,positive,positive,positive,positive,positive
547648065,"Hi @pnut7 ,
The .nn files are the files that basically contain trained neural net . I believe the best way to interpret what's inside .nn files i.e. the neural net architecture, how data is flowing through different layers , data type etc. can be seen by plotting graph of trained neural net in Tensorboard. You can find a lot of insight by going through this issue #2518 . Hope this helps.",hi basically contain trained neural net believe best way interpret inside neural net architecture data flowing different data type seen plotting graph trained neural net find lot insight going issue hope,issue,positive,positive,positive,positive,positive,positive
547587107,Wait for Harper to merge back from release to develop,wait harper merge back release develop,issue,negative,neutral,neutral,neutral,neutral,neutral
547536451,"Hi @emarche,
We have logged this bug in our internal tracking system with ID MLA-230.  We will notify you on this thread when we have found a fix for it. Thank you for your report.",hi logged bug internal system id notify thread found fix thank report,issue,negative,neutral,neutral,neutral,neutral,neutral
547495462,This sounds like this known issue: https://github.com/Unity-Technologies/ml-agents/issues/2775 Can you try downgrading tensorflow to 1.14.0?,like known issue try,issue,negative,neutral,neutral,neutral,neutral,neutral
547338186,I am still experiencing this issue as well on Linux Build. Works when I change the base-port but is there any way to manually force close the previous env?,still issue well build work change way manually force close previous,issue,negative,negative,negative,negative,negative,negative
547335539,have you solved this problem? Could you tell me how you solved it?thanks,problem could tell thanks,issue,negative,positive,positive,positive,positive,positive
547291351,"First of all, thank you very much for considering at least the possibility of the prioritization.

I especially have not done that to not to make the statistics of the brain more complex: I want to provide 100% useful result to an incoming decision. I could add a `DoNothing` method for invalid actions (which should not pass the turn of my board game, but re-request another decision), but then it would learn that those decision would cause a (VALID) DoNothing result where as it should never ever consider making a decision that would cause DoNothing. Yes those may become insignificant with after many decisions but it would still pollute the brain. I understand that this is perfectly usable for various situations; but in my case, I want to provide the correct pairs (action-result) to the brain. If my assumptions are not correct please correct me. I welcome any suggestions and I would be grateful to read about your experience about similar things. Thank you!",first thank much considering least possibility especially done make statistic brain complex want provide useful result incoming decision could add method invalid pas turn board game another decision would learn decision would cause valid result never ever consider making decision would cause yes may become insignificant many would still pollute brain understand perfectly usable various case want provide correct brain correct please correct welcome would grateful read experience similar thank,issue,positive,positive,positive,positive,positive,positive
547288501,"I have not try it yet; but I checked it and yes, as AgentInfo variable of Agent class is now exposed through a getter, this has to work. Thank you!",try yet checked yes variable agent class exposed getter work thank,issue,positive,neutral,neutral,neutral,neutral,neutral
547150579,"Hi @CooperR97,
Does the discussion at #566 answer your question?",hi discussion answer question,issue,negative,neutral,neutral,neutral,neutral,neutral
547136181,"I acknowledge this is not technically supported, however it temporarily solves my problem to fill the observation vector with temporary data at the very start. Feel free to close this issue, thanks for the help!",acknowledge technically however temporarily problem fill observation vector temporary data start feel free close issue thanks help,issue,positive,positive,positive,positive,positive,positive
547124280,"Hey @caioc2,
I think we've narrowed down the issue but I haven't had time to get to it yet.  I can look into this week.",hey think issue time get yet look week,issue,negative,neutral,neutral,neutral,neutral,neutral
547081411,"@surfnerd 
Any news on this issue? It would be amazing to have il2cpp working properly when training with complex environments.",news issue would amazing working properly training complex,issue,positive,positive,positive,positive,positive,positive
547064648,"Hi @jacobson,
I'm not sure about the current prioritization of the bug, but I'll try to get someone to look into it this week.

In the meantime, I'd suggest you add a check for masked actions before taking them, and add some sort of fallback logic in the case it does get selected.",hi sure current bug try get someone look week suggest add check masked taking add sort fallback logic case get selected,issue,negative,positive,positive,positive,positive,positive
547053927,"Hi @jacobson,
This is currently being refactored.  You can check out the latest code on the develop branch.  The HeuristicBrain has ben renamed to HeuristicPolicy, and it now takes an `Agent` instance in the `RequestDecision` method.  Does this address your concerns?",hi currently check latest code develop branch ben agent instance method address,issue,negative,positive,positive,positive,positive,positive
547049082,"Hi @dhiabhr,
While we only support ML-Agents trained networks for inferences, you can try converting your model with the [tensorflow_to_barracuda.py](https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/tensorflow_to_barracuda.py) script.  There is no guarantee that it will work.",hi support trained try converting model script guarantee work,issue,positive,neutral,neutral,neutral,neutral,neutral
547048033,"Hi @xunyiljg,
A step, the way it is implemented in ML-Agents, is one FixedUpdate loop in an environment.  It doesn't matter how many agents you have.

Let me talk to my team about the second question at I am not familiar with all of the details.
",hi step way one loop environment matter many let talk team second question familiar,issue,negative,positive,positive,positive,positive,positive
547045429,"Just an FYI, using `new` to create GameObjects isn't technically supported.  Although it's used as a workaround for you, I'd recommend either pulling a game object from the scene, or instantiating a prefab instead.  You can read about it here:
https://docs.unity3d.com/Manual/CreateDestroyObjects.html
",new create technically although used recommend either game object scene prefab instead read,issue,positive,negative,neutral,neutral,negative,negative
546706191,"Hi @surfnerd,

I'm sorry for bothering you again with this, but I was hoping you could help me with this. 

I was able to plot the tensorflow graphs in tensorboard thanks to your references to issues #2518 and #1992, however, I can't seem to find the structure of the model, including the trained weights. Can you maybe tell me how or where I can find this? is this part of `raw_graph_def.pb` or `frozen_graph_def.pb`?

I can see two subgraphs which contain the layers, but inside the layers I don't see the actual structure or the weights. Is this just my ignorance? I would be very grateful for your help.

The reason why I am asking, is the inference of the model on a rover, and for documentation purposes.

Best,
David

![graph](https://user-images.githubusercontent.com/3441341/67637149-d19e9980-f8d7-11e9-93c7-0f51d801441d.png)
![Hidden layers subgraph](https://user-images.githubusercontent.com/3441341/67637164-f72ba300-f8d7-11e9-9e2d-2b2cc68ecaed.png)
",hi sorry could help able plot thanks however ca seem find structure model trained maybe tell find part see two contain inside see actual structure ignorance would grateful help reason inference model rover documentation best graph hidden,issue,positive,positive,positive,positive,positive,positive
546693789,"Update: I still have some questions but I managed to (somewhat) get it working by introducing an additional penalty.
At the end of each action, the agent will be punished for having very low need levels. This encourages him to take action rather than relaxing.
I'm still not sure why the agent did not learn to take actions that might grant a reward over relaxing in the first place, but this seems to have fixed it.

I also prevent the same actions to be executed twice in a row. **Is this generally a bad practice?**

I seem to have fixed my issue, but **I would still love get some feedback** as to whether or not my whole setup makes sense for a scenario like this.
It seems a bit hard for a beginner like me to decide about things like what hyperparameters to set in the .yaml config and for example whether or not curiosity and memory are needed in a scenario.

Also about this whole Curriculum Learning, **how would I start setting it up?**
Is it possible to start training with a fixed number of decisions and increase that number, so that additional possible actions can be executed?
In the curriculum example it seems very straightforward to adjust the wall height during training, but I'm not sure how to apply that to my situation.

I would like to extend my project and introduce things like currency and resources so that my agent learns that it first has meet some predconditions like gathering some resources before it can execute an action.

Thanks for helping! :)",update still somewhat get working additional penalty end action agent low need take action rather still sure agent learn take might grant reward first place fixed also prevent executed twice row generally bad practice seem fixed issue would still love get feedback whether whole setup sense scenario like bit hard beginner like decide like set example whether curiosity memory scenario also whole curriculum learning would start setting possible start training fixed number increase number additional possible executed curriculum example straightforward adjust wall height training sure apply situation would like extend project introduce like currency agent first meet like gathering execute action thanks helping,issue,positive,positive,positive,positive,positive,positive
546683376,"I think I know what the issue is, I am reloading the whole level when my agent dies. I will try to avoid this by setting it done when agent has no health to start the simulation over again without restarting/loading the whole level.",think know issue whole level agent try avoid setting done agent health start simulation without whole level,issue,negative,positive,positive,positive,positive,positive
546682948,"This has worked. I am initializing at Awake 

```
    private void Awake()
    {
        spikesList = new List<GameObject>();

        GameObject spikeOne = new GameObject();
        GameObject spikeTwo = new GameObject();
        // Initialisation before overwriting on each subsequent frame after first
        spikesList.Insert(0, spikeOne);
        spikesList.Insert(1, spikeTwo);
    } 
```

Now I have a new problem not sure if it's related. I need some help understanding the output as there are no warnings or errors in Unity, however training is ending 2 seconds in with the following Anaconda message -

```
INFO:mlagents.envs:Hyperparameters for the PPOTrainer of brain RLBrain:
        trainer:        ppo
        batch_size:     4096
        beta:   0.005
        buffer_size:    40960
        epsilon:        0.2
        hidden_units:   256
        lambd:  0.95
        learning_rate:  0.0001
        learning_rate_schedule: linear
        max_steps:      5.0e6
        memory_size:    512
        normalize:      False
        num_epoch:      8
        num_layers:     2
        time_horizon:   1024
        sequence_length:        64
        summary_freq:   1000
        use_recurrent:  False
        vis_encode_type:        simple
        reward_signals:
          extrinsic:
            strength:   1.0
            gamma:      0.99
        summary_path:   ./summaries/RLAgent-3_RLBrain
        model_path:     ./models/RLAgent-3-0/RLBrain
        keep_checkpoints:       5
Process Process-1:
Traceback (most recent call last):
  File ""c:\users\dzamm\anaconda3\envs\ml-agents\lib\multiprocessing\process.py"", line 258, in _bootstrap
    self.run()
  File ""c:\users\dzamm\anaconda3\envs\ml-agents\lib\multiprocessing\process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""c:\users\dzamm\anaconda3\envs\ml-agents\lib\site-packages\mlagents\envs\subprocess_env_manager.py"", line 116, in worker
    cmd.payload[0], cmd.payload[1], cmd.payload[2]
  File ""c:\users\dzamm\anaconda3\envs\ml-agents\lib\site-packages\mlagents\envs\environment.py"", line 352, in reset
    self._n_agents[_b] = len(s[_b].agents)
KeyError: 'RLBrain'
INFO:mlagents.envs:Learning was interrupted. Please wait while the graph is generated.
INFO:mlagents.envs:Saved Model
INFO:mlagents.trainers:List of nodes to export for brain :RLBrain
INFO:mlagents.trainers: is_continuous_control
INFO:mlagents.trainers: version_number
INFO:mlagents.trainers: memory_size
INFO:mlagents.trainers: action_output_shape
INFO:mlagents.trainers: action
INFO:mlagents.trainers: action_probs
INFO:tensorflow:Froze 11 variables.
INFO:tensorflow:Froze 11 variables.
Converted 11 variables to const ops.
Converting ./models/RLAgent-3-0/RLBrain/frozen_graph_def.pb to ./models/RLAgent-3-0/RLBrain.nn
IGNORED: StopGradient unknown layer
GLOBALS: 'is_continuous_control', 'version_number', 'memory_size', 'action_output_shape'
IN: 'vector_observation': [-1, 1, 1, 14] => 'main_graph_0/hidden_0/BiasAdd'
IN: 'epsilon': [-1, 1, 1, 2] => 'mul'
OUT: 'action', 'action_probs'
DONE: wrote ./models/RLAgent-3-0/RLBrain.nn file.
INFO:mlagents.trainers:Exported ./models/RLAgent-3-0/RLBrain.nn file
```

Not sure if this appropriate place to post the above, but it seems to be related to previous error.",worked awake private void awake new list new new subsequent frame first new problem sure related need help understanding output unity however training ending following anaconda message brain trainer beta epsilon linear normalize false false simple extrinsic strength gamma process recent call last file line file line run file line worker file line reset learning interrupted please wait graph saved model list export brain action froze froze converted converting unknown layer done wrote file file sure appropriate place post related previous error,issue,positive,positive,neutral,neutral,positive,positive
546677542,"(I am commenting on this because I opened the mentioned #2769 bug. That bug happens even if you use CPU. I do not think that these 2 issues are related. I changed EPSILON to a lower value and it worked **only** up to a certain degree. It still **must** be fixed. My change was inside models.py but it was for discrete vector space type. This issue may be related to nvidia graphics card or cuda versions, etc.. I am not sure, I did not follow the steps to reproduce this (#2784) issue.)",bug bug even use think related epsilon lower value worked certain degree still must fixed change inside discrete vector space type issue may related graphic card sure follow reproduce issue,issue,positive,positive,positive,positive,positive,positive
546674181,"(Even 2.220446049250313e-16 does not work good enough after 1.5 million steps. This workaround is no longer helpful enough. I am looking forward to the fix you have mentioned. Thank you for your concern, time and efforts. I do not want to dig into python code as you can modify it anytime, so I really appreciate if that happens to be possible to increase the priority of this bug internally, as it directly affects training with many actions while using discrete vector space type. I am not able to resume the training too with --load under these circumstances.)",even work good enough million longer helpful enough looking forward fix thank concern time want dig python code modify really appreciate possible increase priority bug internally directly training many discrete vector space type able resume training load,issue,positive,positive,positive,positive,positive,positive
546663220,"Hi @daqulazhang,
There is a work in progress branch for tf2.0 but it isn’t ready yet.  You should be able to install the latest Nvidia driver and it will work with earlier versions of tensorflow.  Please try installing the latest Nvidia driver for your card, and the CUDA version that is compatible with it.  Older versions of CUDA are *mostly* forwards compatible.  ",hi work progress branch ready yet able install latest driver work please try latest driver card version compatible older mostly forward compatible,issue,positive,positive,positive,positive,positive,positive
546663056,"Hi @Hongsungchan,
Thank you for reporting your issue.  I’m not sure I completely understand what problem it is you are having.  Is this custom code you have?  From the inspector I might guess that WS and AD return 3 since they are set to three, but I don’t have enough information to know what’s happening in your code.  Would you be willing to create a small reproduction project that highlights this bug?  Thanks for your time and interest in ML-Agents.",hi thank issue sure completely understand problem custom code inspector might guess ad return since set three enough information know happening code would willing create small reproduction project bug thanks time interest,issue,positive,positive,positive,positive,positive,positive
546662662,"Hi @firepro20,
Indeed CollectObservations is being called before OnTriggerXXX as laid out by [this event timeline of MonoBehaviour.](https://docs.unity3d.com/Manual/ExecutionOrder.html)  CollectObservations is called from FixedUpdate in this diagram.


It seems like you are using a trigger to let your Agent know when it’s intersecting an objects collider on the first frame (and maybe subsequent frames.). You could instead let your agent have a reference to this first object so it can initialize itself on Awake.  This way, when CollectObservations is called on the first frame, your Agent will be properly initialized.  And you can then use your OnTriggerEnter function to update it as you already are.  

Does this make sense? 
",hi indeed laid event diagram like trigger let agent know first frame maybe subsequent could instead let agent reference first object initialize awake way first frame agent properly use function update already make sense,issue,negative,positive,positive,positive,positive,positive
546635283,"Thanks for providing that link @jacobson ! I will take a look at Curriculum Learning and see if I can somehow apply it to my problem. I can also provide some code:

In my agent's update functions the needs of the agent are reduced using `Time.deltaTime` and the function
```
    public void updateNeeds(float deltaTime)
    {
        float needReductionPerSecond = 1f / FULL_NEED_CYCLE_SECONDS;  // A full cycle is 200 seconds.
        float currentNeedReduction = needReductionPerSecond * deltaTime;

        Hunger = Mathf.Max(0f, Hunger - currentNeedReduction);
        Thirst = Mathf.Max(0f, Thirst - currentNeedReduction);
        // same with a few more needs ...
    }
```

The agent is only actually moved to a location during inference. When training, the reduction of the needs is simulated by calculating the duration it would take the agent to move by using the length of the full path and the agents speed. Otherwise the `NavMeshAgent` will get stuck and training will take forever. Basically like this:
```
private void _handleMoveDuringTraining()
{
    float duration = _getPathLength() / _npcAgentMono.navAgent.speed;
    _npcAgentMono.learningAgent.updateNeeds(duration);
    _npcAgentMono.transform.position = _destination;
}
```

When the ""moving"" or rather teleporting is done, a reward is added based on the needs of the agent.
So what this does is the following:
If the need value of the agent exceeds the satuation threshold (in this case 0.7f), the part of the gained need that exceeds that threshold will be a penalty. So if the the current hunger value is 0.5f and the action grants 0.5f, we will be 0.3f above the threshold, so there will be a penalty, as the agent ""ate too much"" now.
```
_getEatingReward()
{
    const float SATUATION_THRESHOLD = 0.7f;
    const float NEED_GAINED = 0.5f;
    float currentNeed = _npcAgent.getHunger();
    if (currentNeed + NEED_GAINED > SATUATION_THRESHOLD ) {
        // Calculate the reward or penalty based on how much we stepped over the threshold
        // If we ate only a little too much, there might still be a reward.
        // If we ate 3 hamburgers while already stuffed, there will be a large penalty.
        return someRewardOrPentalty;
    } else {
        return NEED_GAINED;
    }
}
```

Now for the relaxing function, the only thing the agent does it wait for 10 seconds (meaning he will calculate the decrease of needs for that 10 seconds) and then get a penalty:
```
_getRelaxingReward()
{
    return -0.5f;
}
```

So the agent will get a penalty when it over-satisfies its needs. Relaxing will always result in a penalty.
All of my needs have a `_getReward` function and work the same way.

The observations I make:
```
public override void CollectObservations()
{
    AddVectorObs(NeedHunger);
    AddVectorObs(NeedThirst);
    AddVectorObs(NeedSleep);
    AddVectorObs(NeedEducation);
    AddVectorObs(NeedHobby);
    AddVectorObs(NeedCommunication);
    AddVectorObs(Money);

    _maskTooExpensiveActions();
}
```

I also have a money value. One of my 18 actions is `working` which will not give a reward but reset the money value to the maximum of 1.0f. Some actions require money to be executed. If there is not enough money for an action I will mask them and stop them from being executed. Most of the actions do not require money.

I am incrementing a counter each time after an action was executed and after a few hundred actions I reset the agent. If the limit was not reached I request a new decision. So after an `AgentAction` was called and the action was performed (meaning the agent was moved and the reward was given etc. etc.), then this is called:
```
if (_numActions >= MAX_ACTIONS && !IsInference)
{
    Done();
} else
{
    _numActions++;
    RequestDecision();
}
```
This is what happens on reset:
```
public override void AgentReset()
{
    _setRandomAgentValues(); // Resets needs, money etc to `Random.value`
    _numActions = 0;
}
``` 

I really hope this somewhat understandable.
I just can't understand why it always ends up with a policy where relaxing is almost always chosen over rewarding actions.",thanks providing link take look curriculum learning see somehow apply problem also provide code agent update need agent reduced function public void float float full cycle float hunger hunger thirst thirst need agent actually location inference training reduction need calculating duration would take agent move length full path speed otherwise get stuck training take forever basically like private void float duration duration moving rather done reward added based need agent following need value agent threshold case part need threshold penalty current hunger value action threshold penalty agent ate much float float float calculate reward penalty based much stepped threshold ate little much might still reward ate already stuffed large penalty return else return function thing agent wait meaning calculate decrease need get penalty return agent get penalty need always result penalty need function work way make public override void money also money value one working give reward reset money value maximum require money executed enough money action mask stop executed require money counter time action executed hundred reset agent limit request new decision action meaning agent reward given done else reset public override void need money really hope somewhat understandable ca understand always policy almost always chosen rewarding,issue,positive,positive,positive,positive,positive,positive
546545649,"Sorry for not responding to your questions faster, @surfnerd I will post my android logs later, @mantasp Sorry, I don't know the answer to that. I don't know where I can find the CPU or GPU option. can you point me where?
",sorry faster post android later sorry know answer know find option point,issue,negative,negative,negative,negative,negative,negative
546525695,"Hi @hongluck, 
Would you be able to share your project?  Without knowing how your scene and objects are setup, it's hard to know what the issue is. ",hi would able share project without knowing scene setup hard know issue,issue,negative,positive,positive,positive,positive,positive
546498436,"Hi @wrd90,
It looks like this was requested previously in #2518 and #1992.  We have logged it internally as a request and will bump up its priority in our internal tracker.  Internally we are tracking it with ID MLA-19.  I am going to close this issue for now, and will update this thread when we have added support for this. ",hi like previously logged internally request bump priority internal tracker internally id going close issue update thread added support,issue,positive,negative,neutral,neutral,negative,negative
546469586,"May that be possible for you to share some pseudo code for your actions and rewards, we can think more about it. Otherwise, you may want to try teaching your AI slowly, 1 by 1, each time one thing and make it complex and include everything in the final stage. ([https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Curriculum-Learning.md](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Curriculum-Learning.md))",may possible share pseudo code think otherwise may want try teaching ai slowly time one thing make complex include everything final stage,issue,negative,negative,negative,negative,negative,negative
546449189,Looks like bair links are down again. OK to merge with that test failing.,like link merge test failing,issue,negative,neutral,neutral,neutral,neutral,neutral
546181475,"@niskander 

I'm not sure if they are related. Here I use only a single visual observation and continuous action space  without any mask. It only happens when using inference with an already trained model inside Unity with GPU inference select (using CPU inference works fine).
If it is really related could you elaborate how and which work around I could use?",sure related use single visual observation continuous action space without mask inference already trained model inside unity inference select inference work fine really related could elaborate work around could use,issue,negative,positive,positive,positive,positive,positive
546067165,"@xunyiljg I'm glad you liked it 😊.

Just a quick update:

- I was able to stream Unity's graphical output to Twitch directly from a COLAB notebook. In my early attempts  ``xvbf`` gave me a headache so I switched to Xorg virtual desktop. My current notebook is a mess but I will share it after I cleaned it up.
- I also wrote a script to alter a scene before building the game. It comes handy to change few things like mark a ``Brain`` as trainable in the ``BroadcastHub`` directly in COLAB.
- I saw other people have some struggle running Unity in headless server with graphical output (**without** ``--no-graphics`` flag). For example #1786 marked as closed without any solution or workaround. I will write a tutorial for this later.
- The choice of Twitch is totally optional. I used ``ffmpeg`` to capture the screen. You can just save the video on Google Drive instead.
- The current setup is super inefficient! I will do some benchmark to find good parameters for broadcasting. 



![Screen Shot 2019-10-24 at 10 41 30 PM](https://user-images.githubusercontent.com/588431/67517467-899d2e00-f6af-11e9-85ee-f6f36df4833a.png)",glad quick update able stream unity graphical output twitch directly notebook early gave headache switched virtual current notebook mess share also wrote script alter scene building game come handy change like mark brain trainable directly saw people struggle running unity headless server graphical output without flag example marked closed without solution write tutorial later choice twitch totally optional used capture screen save video drive instead current setup super inefficient find good screen shot,issue,positive,positive,positive,positive,positive,positive
546029278,"@jacobson Glad the workaround works. Using any sort of epsilon here still feels bad because there's a chance it can pick an invalid action; we're looking at rewriting this using `tfp.distributions.Multinomial` which takes probabilities instead of log-probabilities, so zeroing out will be cleaner.",glad work sort epsilon still bad chance pick invalid action looking instead cleaner,issue,negative,negative,neutral,neutral,negative,negative
546025452,"Turns out this whole section got rewritten as part of a larger refactor, so I'm not going to try to merge it into the develop branch",turn whole section got part going try merge develop branch,issue,negative,positive,positive,positive,positive,positive
546025073,This whole section was written with the Brain refactor.,whole section written brain,issue,negative,positive,positive,positive,positive,positive
545954478,"@chriselion Thank you. I checked it and yes it affects the stability.

I changed `EPSILON` from `1e-7` to `1e-10` and it seems like it is working: `AgentAction` was NOT called with an invalid action during `630.000 steps (210 total actions)`. (**I** stopped it after 630.000 steps). (`1e-10` also worked for `99.999 total actions` and I stopped it after 42.000 steps.)

I understand that `1e-7` was defined as `EPSILON` thinking about 32bit in mind; but as I tested, `EPSILON` definition in `models.py` is already working as 64bit (`np.finfo(EPSILON).eps`) by default (on most modern computers) as it was not defined specifically as `float32`, unlike many places in the codebase. On top of that, I see that `1e-10` **is already used** in `models.py` (inside `create_learning_rate` function), so it looks like it is not banned. **For now**, it looks like this is an acceptable workaround? (Probably even `2.220446049250313e-16` can be used as `EPSILON` ""**for now**"")

What do you think?",thank checked yes stability epsilon like working invalid action total stopped also worked total stopped understand defined epsilon thinking bit mind tested epsilon definition already working bit epsilon default modern defined specifically float unlike many top see already used inside function like like acceptable probably even used epsilon think,issue,positive,positive,positive,positive,positive,positive
545826163,"Hi @chriselion, thanks, that's great! Maybe you could run the check every 100 steps or something similar? How much of an improvement do you get if you disable it entirely? ",hi thanks great maybe could run check every something similar much improvement get disable entirely,issue,positive,positive,positive,positive,positive,positive
545811908,"> Hi @Hongsungchan,
> I'm not sure I understand your question. RayCasts are one way to observe, and depending on how you use them, easier to get them to train in some cases.
> 
> Can you elaborate on what you are trying to achieve and what specific problems you are having?


I'm learning using curiosity now, but is it better not to use curiosity?",hi sure understand question one way observe depending use easier get train elaborate trying achieve specific learning curiosity better use curiosity,issue,positive,positive,positive,positive,positive,positive
545805925,"![Desktop Screenshot 2019 10 21 - 21 17 28 20](https://user-images.githubusercontent.com/43347736/67466680-6f992680-f682-11e9-9136-9a1027801d18.png)
![Desktop Screenshot 2019 10 24 - 17 19 08 66](https://user-images.githubusercontent.com/43347736/67466681-6f992680-f682-11e9-940b-d7fe3465c412.png)

I am experimenting with whether agents can compete and collaborate.

You win if you destroy all your opponent's buildings.

The reward is the greatest if you break the last building.

Attacks, skill use, stats, etc., you get a small reward.

Agents team up by three.

1 attack agent

2 agents that stun and heal

First, I tried training as an agent that only attacked

Do not attack and move only near your building.

Agents are scared to die, so they eliminated the penalty for dying, but they never attacked.",whether compete collaborate win destroy opponent reward break last building skill use get small reward team three attack agent stun heal first tried training agent attack move near building die penalty dying never,issue,negative,positive,positive,positive,positive,positive
545772527,"I just noticed that in the console when I begin training, there are a lot of messages about deprecated tensorflow commands. I'm going to verify I have the correct version of tensorflow.",console begin training lot going verify correct version,issue,negative,neutral,neutral,neutral,neutral,neutral
545766363,"Thank you very much. I have totally missed this setting! 
I will close this now.",thank much totally setting close,issue,negative,neutral,neutral,neutral,neutral,neutral
545685013,"@nikola-j Hard to say that this is ""fixed"", but we sped up one part of the code that slowed down a lot with large numbers of vector observations: https://github.com/Unity-Technologies/ml-agents/pull/2717",hard say fixed sped one part code lot large vector,issue,negative,positive,neutral,neutral,positive,positive
545672471,"
![Screen Shot 2019-10-23 at 4 05 21 PM](https://user-images.githubusercontent.com/1356616/67440760-13b19d80-f5af-11e9-870c-b8c099819778.png)
If you got into Edit -> Project Settings -> Physics2D, uncheck the `Queries start in colliders` checkbox and see if that helps.
",screen shot got edit project uncheck start see,issue,negative,neutral,neutral,neutral,neutral,neutral
545671790,"It appears that the different physics systems handle raycasts which start inside a collider differently:

For 2D: https://docs.unity3d.com/ScriptReference/Physics2D.Raycast.html
> Additionally, this will also detect Collider(s) at the start of the ray. In this case, the ray starts inside the Collider and doesn't intersect the Collider surface. This means that the collision normal cannot be calculated, in which case the returned collision normal is set to the inverse of the ray vector being tested. This can easily be detected because such results are always at a RaycastHit2D fraction of zero.

For 3D: https://docs.unity3d.com/ScriptReference/Physics.Raycast.html
> **Note**: Raycasts will not detect Colliders for which the Raycast origin is inside the Collider.

This is a property of the physics systems, so there's not much that the RayPerception code can do about it right now.",different physic handle start inside differently additionally also detect start ray case ray inside intersect surface collision normal calculated case returned collision normal set inverse ray vector tested easily always fraction zero note detect origin inside property physic much code right,issue,negative,positive,positive,positive,positive,positive
545652192,"I am having the same issue, I've been trying to figure out why my machine learning samples do not work until I found out the same problem and wanted to report.",issue trying figure machine learning work found problem report,issue,negative,neutral,neutral,neutral,neutral,neutral
545627131,"Hi @zhaijinfeng ,

I haven't used them much, but I think you can achieve your first goal using [curriculum learning](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Curriculum-Learning.md). Basically, you split your training in multiple ""levels"" of increasing difficulties (or even, totally different task that gradually reach your goal) and you define a condition to get from one level to another. (eg. if my averaged reward over my last 100 episodes is more than 5, go to the next level. You can see which lesson (level) is being player in Tensorboard.

For your second point, yes you could but you'd be best using [generalized training](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Generalized-Reinforcement-Learning-Agents.md). It should play really well with curriculum learning (it also seems to use academy parameters, like the size of a ball, gravity force, ...).

Good luck!",hi used much think achieve first goal curriculum learning basically split training multiple increasing even totally different task gradually reach goal define condition get one level another reward last go next level see lesson level player second point yes could best generalized training play really well curriculum learning also use academy like size ball gravity force good luck,issue,positive,positive,positive,positive,positive,positive
545610102,"Hi,
I was able to reproduce this behavior with the steps you provided. Using a branch size of 210, this happened 6 times in the 150000 training steps.

I think the cause of this is an epsilon that we add to the probabilities before taking the log here: https://github.com/Unity-Technologies/ml-agents/blob/1a1919a2cff14582da06c89df7eccd14673b78d9/ml-agents/mlagents/trainers/models.py#L445-L449
(to avoid taking log(0)).

I don't have a good workaround at the moment, but I'll see if we can get a simple fix for this. I've also logged it in our internal tracker as MLA-218.",hi able reproduce behavior provided branch size time training think cause epsilon add taking log avoid taking log good moment see get simple fix also logged internal tracker,issue,negative,positive,positive,positive,positive,positive
545534304,"@devedse,
There has been an issue filed against Unity with the ID 1193597.  I am going to close this for now, and I'll update it when the fix goes into a Unity release.  Until then, feel free to open another issue if you run into trouble.
Cheers,
Chris and the ML-Agents Team",issue unity id going close update fix go unity release feel free open another issue run trouble team,issue,negative,positive,neutral,neutral,positive,positive
545529421,"Hey @devedse,
I spoke with our engine team and it seems like this might be a bug in the editor.  It's causing a script reload when you drag between windows with different DPIs at the moment, which seems like a bit of overkill.  This pull request will indeed fix the errors from showing up in the console, but your entire scene is being reloaded when you drag between those different DPI screens.  In other words, if you are in play mode on windows, don't drag the editor between screens if you don't want to reload your entire scene.😢 

I am filing a bug against the editor internally and will post back here when it is fixed.  ",hey spoke engine team like might bug editor causing script reload drag different moment like bit pull request indeed fix showing console entire scene drag different play mode drag editor want reload entire scene filing bug editor internally post back fixed,issue,negative,negative,neutral,neutral,negative,negative
545521123,"Hey @devedse, 
There is a pull request open #2782 that potentially fixes this issue.  When you get a chance to try it out please let us know.",hey pull request open potentially issue get chance try please let u know,issue,positive,neutral,neutral,neutral,neutral,neutral
545516561,"Hi @Hongsungchan,
I'm not sure I understand your question.  RayCasts are one way to observe, and depending on how you use them, easier to get them to train in some cases.  

Can you elaborate on what you are trying to achieve and what specific problems you are having?",hi sure understand question one way observe depending use easier get train elaborate trying achieve specific,issue,positive,positive,positive,positive,positive,positive
545259360,"Hi @jacobson,
Sorry for the delay on this - I'm going to look into it in more detail tomorrow.",hi sorry delay going look detail tomorrow,issue,negative,negative,negative,negative,negative,negative
545252023,"Hi @dans-msft ,
Sorry for the delay on this.

I confirmed today that the results for `env.observation_space.low` and `high` where what you described.

It looks like we currently only use uint8_visual for scaling the observations here:
https://github.com/Unity-Technologies/ml-agents/blob/36433174a9cf20559be5a44eb60d2c8eaf071429/gym-unity/gym_unity/envs/__init__.py#L242-L268

but we should probably be using it when setting up the observation_space here too https://github.com/Unity-Technologies/ml-agents/blob/36433174a9cf20559be5a44eb60d2c8eaf071429/gym-unity/gym_unity/envs/__init__.py#L139-L147

I'll follow up tomorrow with some folks that are more familiar with this part of the code than I am...",hi sorry delay confirmed today high like currently use scaling probably setting follow tomorrow familiar part code,issue,negative,positive,neutral,neutral,positive,positive
545242976,@mantasp are there any know limitations for running inference on Android?,know running inference android,issue,negative,neutral,neutral,neutral,neutral,neutral
545242640,Do you see any errors in the android logs? Any information you can give us will help us diagnose the problem. ,see android information give u help u diagnose problem,issue,negative,neutral,neutral,neutral,neutral,neutral
545223804,"sorry if I can't explain my self well, What I meant was I was using a pre-trained .nn file. The pre-trained file works fine in the unity editor but everytime I try to build it to android the pre-trained file doesn't seem to work. ",sorry ca explain self well meant file file work fine unity editor try build android file seem work,issue,negative,negative,neutral,neutral,negative,negative
545212759,can you merge the latest develop in to get the Yamato pipelines to run?,merge latest develop get run,issue,negative,positive,positive,positive,positive,positive
545206589,"> Would this be causing problems on develop now?

I think this was broken by [this change](https://github.com/Unity-Technologies/ml-agents/commit/4a564606d53b4e611ca2e5162480e6ebab2c6603). The checks for visual obs resolution was ignored no?",would causing develop think broken change visual resolution,issue,negative,negative,negative,negative,negative,negative
545182671,@c43892 This question is over a year old; the repo structure has changed a lot since then. You should follow the instructions here https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md#install-python-and-mlagents-package or here https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md#installing-for-development and open a new issue if you have further problems.,question year old structure lot since follow open new issue,issue,negative,positive,neutral,neutral,positive,positive
545155094,"Tested my setup with a copied and simplified version of my agent script, which worked fine.
Then went back to using my old Agent script and suddenly no more issues. I am very confident that all settings for the agent are exactly the same now as they were before. I am baffled as to why this is working now.",tested setup copied simplified version agent script worked fine went back old agent script suddenly confident agent exactly working,issue,positive,positive,positive,positive,positive,positive
545131762,"it might because of your python version is not 3.6, it's 3.7 ?
my question is I didn't find any 'python' subfolder in the repository....",might python version question find repository,issue,negative,neutral,neutral,neutral,neutral,neutral
545092011,"Hey everyone, I have the same issue.
I am using 18 discrete on demand actions, however the decision is ALWAYS 0.
",hey everyone issue discrete demand however decision always,issue,negative,neutral,neutral,neutral,neutral,neutral
545082169,Looks like marking the classes as public gets rid of the warnings (and is probably something we want anyway): https://stackoverflow.com/a/12618676/224264,like marking class public rid probably something want anyway,issue,negative,neutral,neutral,neutral,neutral,neutral
545079241,"> Re: the warning: the field is assigned in the UI

There has to be a way to suppress this warning, we have a lot of fields that are only assigned in the inspector. I think there is a way around it.",warning field assigned way suppress warning lot assigned inspector think way around,issue,negative,neutral,neutral,neutral,neutral,neutral
545061232,"Sorry that changing tensorflow didn't fix it.

Can you please explain what you mean by ""doesn't show the training""? Do you mean that the python trainer doesn't connect? If so, that's expected; we don't support training on mobile devices currently.",sorry fix please explain mean show training mean python trainer connect support training mobile currently,issue,positive,negative,negative,negative,negative,negative
544989138,It seems that the tensorflow I installed is already at 1.14. I'm trying to make the 3dball in ARCore. It really doesn't show the training like it did in the unity editor. ,already trying make really show training like unity editor,issue,negative,positive,positive,positive,positive,positive
544984583,"Thanks, I'm gonna install it now. ",thanks gon na install,issue,negative,positive,positive,positive,positive,positive
544963961,"I have now started training on our local cluster. I've only had a couple of sessions of 10 million steps with 20 instances, but all have completed without issue. No modifications were made since my last post, other than to propagate the timeout param in case the cluster takes a few moments to allocate. The issue was repeatable with AWS, so I expect it is to do with AWS itself or some strangeness over the public internet. I can't connect to our cluster from outside to test, but I might find some spare machines to see if I can re-create separately.",training local cluster couple session million without issue made since last post propagate param case cluster allocate issue repeatable expect strangeness public ca connect cluster outside test might find spare see separately,issue,negative,neutral,neutral,neutral,neutral,neutral
544890303,Thank you for sharing. I have also successfully used colab.,thank also successfully used,issue,positive,positive,positive,positive,positive,positive
544751802,@vincentpierre Updated scenes - everything uses CameraSensor Component (except for the RenderTexture one on GridWorld),everything component except one,issue,negative,neutral,neutral,neutral,neutral,neutral
544730110,"The AreaRemderTexture Agent in the gridworld scene is using a Camera sensor but should be using a RenderTextureSensor.
You should also add the CameraSensor to all of the other Visual Environments such as Pyramids and Pushblock",agent scene camera sensor also add visual,issue,negative,neutral,neutral,neutral,neutral,neutral
544723154,"Hi,
This is pretty similar to the setup in the Basic scene (with slightly different reward values):
https://github.com/Unity-Technologies/ml-agents/blob/a6e8a64fd80ccd7a8040252695ff6575a88eb8bb/UnitySDK/Assets/ML-Agents/Examples/Basic/Scripts/BasicAgent.cs#L50-L62
So you might want to check that you're doing the same thing there.

You might also want to check the tensorboard results https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Using-Tensorboard.md to track that the reward gradually increases over time.",hi pretty similar setup basic scene slightly different reward might want check thing might also want check track reward gradually time,issue,positive,positive,neutral,neutral,positive,positive
544718752,"> I would like to second this request. It would be even better if ML-Agents was available as a package through Unity's new package manager.

Yes please!!!",would like second request would even better available package unity new package manager yes please,issue,positive,positive,positive,positive,positive,positive
544718283,"You should look through some of the beginning documentation. I think https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Create-New.md is a good place to start, since it covers what you need to add to an existing ""game"". In particular, you'll need to add an Agent class and get it to collection observations and respond to actions; see the ""Implement an Agent"" section for more information.",look beginning documentation think good place start since need add game particular need add agent class get collection respond see implement agent section information,issue,negative,positive,positive,positive,positive,positive
544709153,"Hi,
What version of tensorflow are you training with? We've seen some problems with tensorflow==1.15 generating graphs that tensorflow2barracuda doesn't know how to process; this might explain what you're seeing.

Can you please try installing 1.14 instead? We hope to have a better fix soon, but that's the best solution for now.",hi version training seen generating know process might explain seeing please try instead hope better fix soon best solution,issue,positive,positive,positive,positive,positive,positive
544698733,"Next time, please copy the logs and paste them (using ```) instead of taking screenshots. This makes them easier for us to read, and makes it easier for other people to find related issues in the future.

For the warnings regarding `cudart`, this is something to do with your tensorflow setup and not coming from ml-agents. We don't provide support for this here.

The `KeyError: ""default""`is because we expect a ""default"" section in the config yaml (for example https://github.com/Unity-Technologies/ml-agents/blob/9314f243493ef65709b738bca51ff26634edc37f/config/trainer_config.yaml#L1-L3). You can fix this error by adding that in. I'll log a feature request on our side to make this error clearer.

For the last set of errors, it's hard to tell what's going on because some of the text is cut off. One possibility is that you start the trainer but didn't start training from the Unity editor within the timeout of 30 seconds.",next time please copy paste instead taking easier u read easier people find related future regarding something setup coming provide support default expect default section example fix error log feature request side make error clearer last set hard tell going text cut one possibility start trainer start training unity editor within,issue,negative,negative,neutral,neutral,negative,negative
544687889,"Hi @Hongsungchan,
This isn't supported right now. The list of observations is cleared right before Agent.CollectObservations() is called:
https://github.com/Unity-Technologies/ml-agents/blob/9314f243493ef65709b738bca51ff26634edc37f/UnitySDK/Assets/ML-Agents/Scripts/Agent.cs#L562-L567

One option would be to accumulate the observations in another list inside the coroutines, and then use `AddVectorObs(IEnumerable<float> observation)` inside Agent.CollectObservations()",hi right list right one option would accumulate another list inside use float observation inside,issue,negative,positive,positive,positive,positive,positive
544614528,"Hey @ashwalcs,
We are going to close this issue since it seems like it has been resolved.  Feel free to open another if you run into issues.",hey going close issue since like resolved feel free open another run,issue,positive,positive,positive,positive,positive,positive
544367087,I managed to find the problem. It appears I do have two google.protobuf both in ml-agents and ARCore SDK plugin. Now I don't know if I should remove it or not. ,find problem two know remove,issue,negative,neutral,neutral,neutral,neutral,neutral
544361062,"![image](https://user-images.githubusercontent.com/46953162/67179963-a8c76180-f40a-11e9-8d9b-75414cde7e1b.png)

I've managed to remove the pre-compiled assembly errors by removing the ads in the package manager. But the 'google.protobuf' persists. ",image remove assembly removing package manager,issue,negative,neutral,neutral,neutral,neutral,neutral
544271699,"@chriselion Hi that's right my path was wrong!
but I have new problem that I reported as a new bug

![image](https://user-images.githubusercontent.com/16706911/67163287-ffd12600-f379-11e9-9425-8fe76c1d63b7.png)
",hi right path wrong new problem new bug image,issue,negative,positive,neutral,neutral,positive,positive
544230813,"Thanks! Yes, I can confirm Tensorflow 1.14 works fine. I had installed 1.15 but just because that is the dependency your pip package installs automatically (in setup.py you have something like <2, >1.7). 

Good to know it is now fixed! ",thanks yes confirm work fine dependency pip package automatically something like good know fixed,issue,positive,positive,positive,positive,positive,positive
544200880,"What version of tensorflow were you using initially? I see a message about `AddV2` in the logs; so far I've only seen that with tensorflow 2.0, but it's possible that 1.15 produces it too.

Anyway, the fix is here https://github.com/Unity-Technologies/ml-agents/pull/2703",version initially see message far seen possible anyway fix,issue,negative,positive,neutral,neutral,positive,positive
544200632,"Hi,
That version of tensorflow should be OK (although 1.14 is the highest that we've tested with currently).

The main problem right now is this:
```
FileNotFoundError: [Errno 2] No such file or directory: 'config/trainer_config.yaml'
```
You need specify a valid file in the commandline. If you cloned ml-agents from github, the file should be in the config directory; otherwise you can get it from here: https://github.com/Unity-Technologies/ml-agents/blob/master/config/trainer_config.yaml",hi version although highest tested currently main problem right file directory need specify valid file file directory otherwise get,issue,negative,positive,positive,positive,positive,positive
544176565,Thank you very much for your answer. I now understand how RayPerception3d works. A super helpful explanation. Thanks Hanseoup! ,thank much answer understand work super helpful explanation thanks,issue,positive,positive,positive,positive,positive,positive
544117368,"should I install cuda?

![image](https://user-images.githubusercontent.com/16706911/67140897-b8f50a80-f26b-11e9-88db-0294ba5df27b.png)

```mlagents-learn mlagents/trainers/learn.py --run-id=3dball_generalize --train```



![image](https://user-images.githubusercontent.com/16706911/67140954-fa85b580-f26b-11e9-9d12-d87308d3d2bf.png)


Please help me",install image train image please help,issue,positive,neutral,neutral,neutral,neutral,neutral
544103995,"fruit face please help me 

I installed tensorflow==1.15rc2 but it seems didn't work :(

```
(base) C:\Users\Smkgames>mlagents-learn config/trainer_config.yaml --run-id=firs
tRun --train
2019-10-19 08:55:22.995702: W tensorflow/stream_executor/platform/default/dso_lo
ader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64
_100.dll not found
2019-10-19 08:55:22.995922: I tensorflow/stream_executor/cuda/cudart_stub.cc:29]
 Ignore above cudart dlerror if you do not have a GPU set up on your machine.
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-su
nset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.



                        ___¦¦¦¦
                   +¦¦¦¦¦¦¦¦¦¦¦¦
              ,___m¯¯¯'  ,¦¦¦¯¦¦_                           ¦¦¦  ¦¦¦
            _¦¦¦¯'      _¦¦¯  ¦¦¦      __     __ ,__ ____   ,__ _¦¦¦_ ___    ,__

          _¦¦¦¯        _¦¦¯   ¦¦¦¦     ¦¦¦   ¦¦¦ ¦¦¦¦¯¯¯¦¦¦ ¦¦¦ ¯¦¦¦¯ ^¦¦¦  +¦¦¦

        _¦¦¦¦¦________¦¦¦      ¦¯      ¦¦¦   ¦¦¦ ¦¦¦    ¦¦¦ ¦¦¦  ¦¦¦   ¦¦¦_ ¦¦¦
        ¯¦¦¦¦¯¯¯¯¯¯¯¯¯¯¦¦_     ¦¦      ¦¦¦   ¦¦¦ ¦¦¦    ¦¦¦ ¦¦¦  ¦¦¦    ¦¦¦¦¦¦
          ^¦¦¦¦        ¯¦¦_   ¦¦¦¦     ¦¦¦¦_¦¦¦¦ ¦¦¦    ¦¦¦ ¦¦¦  ¦¦¦_    ¦¦¦¦`
            '¯¦¦¦_      ^¦¦¦  ¦¦¦       +¯¯¯¯ ¯¯ ^¯¯    `¯¯ `¯¯   '¯¯    ¦¦¦¦
               ¯¯¯¯¦___   ¦¦¦¦¦¦,                                      ¦¦¦¦¯
                   `¯¦¦¦¦¦¦¦¦¦¦¦
                        ¬`¯¯¯¦¦


INFO:mlagents.trainers:CommandLineOptions(debug=False, num_runs=1, seed=-1, env_
path=None, run_id='firstRun', load_model=False, train_model=True, save_freq=5000
0, keep_checkpoints=5, base_port=5005, num_envs=1, curriculum_folder=None, lesso
n=0, slow=False, no_graphics=False, multi_gpu=False, trainer_config_path='config
/trainer_config.yaml', sampler_file_path=None, docker_target_name=None, env_args
=None)
Traceback (most recent call last):
  File ""d:\programfiles\anaconda3\lib\site-packages\mlagents\trainers\trainer_ut
il.py"", line 116, in load_config
    with open(config_path) as data_file:
FileNotFoundError: [Errno 2] No such file or directory: 'config/trainer_config.y
aml'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""d:\programfiles\anaconda3\lib\runpy.py"", line 193, in _run_module_as_mai
n
    ""__main__"", mod_spec)
  File ""d:\programfiles\anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""D:\ProgramFiles\Anaconda3\Scripts\mlagents-learn.exe\__main__.py"", line
7, in <module>
  File ""d:\programfiles\anaconda3\lib\site-packages\mlagents\trainers\learn.py"",
 line 417, in main
    run_training(0, run_seed, options, Queue())
  File ""d:\programfiles\anaconda3\lib\site-packages\mlagents\trainers\learn.py"",
 line 205, in run_training
    trainer_config = load_config(trainer_config_path)
  File ""d:\programfiles\anaconda3\lib\site-packages\mlagents\trainers\trainer_ut
il.py"", line 120, in load_config
    f""Config file could not be found at {config_path}.""
mlagents.envs.exception.UnityEnvironmentException: Config file could not be foun
d at config/trainer_config.yaml.
```",fruit face please help work base trun train could load dynamic library found ignore set machine warning module included information please see related depend functionality listed please file issue recent call last file line open file directory handling exception another exception recent call last file line file line code file line module file line main queue file line file line file could found file could foun,issue,positive,negative,neutral,neutral,negative,negative
544096432,"I just had been trying to do this myself and I have had to go download cuda 9 to try to run this MLAgents mess, as it says that Tensorflow itself can't find the right assets since ML Agents requires the 1`7.1 tensorflow, so you have to have 9.0 cuda to use it, as it can't find things it needs in  10.0 cuda X_X ",trying go try run mess ca find right asset since use ca find need,issue,negative,positive,neutral,neutral,positive,positive
544095191,"![image](https://user-images.githubusercontent.com/16706911/67139511-f3f04180-f25d-11e9-9a14-33c540a88930.png)


@chriselion  I use tensorflow version is 2.0
what version should I install
I should try again",image use version version install try,issue,negative,neutral,neutral,neutral,neutral,neutral
543981774,"Update: I've retrained the same exact environment using tensorflow 1.14.0 instead of 1.15.0 (which is the one ml-agents installs automatically). This results in much less warnings and in the model to work correctly in inference mode. Also, I stopped training early to test this out, so it could be related to that. I will do further test during the week to confirm what this is about (if there is no official response from the devs).

Cheers!",update exact environment instead one automatically much le model work correctly inference mode also stopped training early test could related test week confirm official response,issue,negative,positive,positive,positive,positive,positive
543953612,What version of tensorflow are you using? We don't support 2.0 yet (but hope to soon).,version support yet hope soon,issue,positive,neutral,neutral,neutral,neutral,neutral
543950977,@AkhilRaja could you please share your model?,could please share model,issue,positive,neutral,neutral,neutral,neutral,neutral
543922894,`git cherry-pick` might be too heavy-weight. It's a one-line change so you could probably just do it manually. But I can't guarantee that there won't be some other tensorflow 2.0 bugs (I still need to run more tests on my branch) so downgrading is probably best for now...,git might change could probably manually ca guarantee wo still need run branch probably best,issue,positive,positive,positive,positive,positive,positive
543917029,"I'm sorry, cherrypicking didn't help (or I failed in doing it). I think I'll downgrade TensorFlow. Thanks for your effort, this is becoming an amazing piece of software!",sorry help think downgrade thanks effort becoming amazing piece,issue,positive,positive,positive,positive,positive,positive
543891356,"I can change the target branch to develop, but I cant change the branch ""from"". This would cause some conflicts. Then I think it may be better to close this one and open a new PR.",change target branch develop cant change branch would cause think may better close one open new,issue,negative,positive,positive,positive,positive,positive
543887478,"Hello,

i'd like to add additional summary information like a successrate or a total success count but i cannot seem to find any information on how to do this. Was there any progress in this matter or did i miss some documentation?
Thank's in advance.",hello like add additional summary information like total success count seem find information progress matter miss documentation thank advance,issue,positive,positive,positive,positive,positive,positive
543817875,"Yeah, things have changed a lot in the last week weeks. The [migration guide](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Migrating.md#important-changes) explains some of the changes, but it might be easiest to just cherry-pick that fix into a checkout of the 0.10.2 release branch instead.

",yeah lot last week migration guide might easiest fix release branch instead,issue,positive,neutral,neutral,neutral,neutral,neutral
543764302,"I could not run with `CUDA_VISIBLE_DEVICES=-1 mlagents-learn` but probably one can set in another way, the only drawback is this variable may not be known by many people (I didn't know about it until now), while the flag is easier and would be in the documentation about training.

Anyway, I made the pull request.",could run probably one set another way drawback variable may known many people know flag easier would documentation training anyway made pull request,issue,negative,positive,positive,positive,positive,positive
543696000,"That was quick, thanks! Now in this version, the academy doesn't have the ""Control"" checkbox, that was very intuitive in the recent version. I don't know how tell Python that I want to train. (I do research about it later but don't know it for now)",quick thanks version academy control intuitive recent version know tell python want train research later know,issue,negative,positive,positive,positive,positive,positive
543574150,"I have the similar issue while I don't use ML-Agent, I just build a simple unity standalone application for Linux. And I run it with xvfb but get the same log. While my glxinfo shows that my OpenGL core profile is version 3.3. But Unity still said it can not find a supported OpenGL and render.
```
Desktop is 640 x 480 @ 0 Hz
Unable to find a supported OpenGL core profile
Failed to create valid graphics context: please ensure you meet the minimum requirements
E.g. OpenGL core profile 3.2 or later for OpenGL Core renderer
Vulkan detection: 0
No supported renderers found, exiting
```


My glxinfo is 
```
OpenGL vendor string: VMware, Inc.
OpenGL renderer string: llvmpipe (LLVM 8.0, 256 bits)
OpenGL core profile version string: 3.3 (Core Profile) Mesa 19.0.8
OpenGL core profile shading language version string: 3.30
OpenGL core profile context flags: (none)
OpenGL core profile profile mask: core profile
OpenGL core profile extensions:
OpenGL version string: 3.1 Mesa 19.0.8
OpenGL shading language version string: 1.40
OpenGL context flags: (none)
OpenGL extensions:
OpenGL ES profile version string: OpenGL ES 3.0 Mesa 19.0.8
OpenGL ES profile shading language version string: OpenGL ES GLSL ES 3.00
OpenGL ES profile extensions:
```",similar issue use build simple unity application run get log core profile version unity still said find render unable find core profile create valid graphic context please ensure meet minimum core profile later core renderer detection found vendor string renderer string core profile version string core profile mesa core profile shading language version string core profile context none core profile profile mask core profile core profile version string mesa shading language version string context none e profile version string e mesa e profile shading language version string e e e profile,issue,positive,negative,neutral,neutral,negative,negative
543504603,"@mantasp, thanks for your help, but I see that the ML-Agents won't supply my needs. So I decided to create my own Unity library for Deep Learning training. Soon I will post the code to the community.",thanks help see wo supply need decided create unity library deep learning training soon post code community,issue,positive,positive,neutral,neutral,positive,positive
543469576,"It's possible that the permissions are incorrect on the file, although I would expect a different exception in that case. Can you make sure the config directory and the yaml file are readable? More details [here](https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2008-R2-and-2008/cc754344(v=ws.11)?redirectedfrom=MSDN)

If that doesn't work, you could start doing things like editing learn.py to print out `os.getcwd()` (https://docs.python.org/2/library/os.html#os.getcwd) to make sure you're actually running in the directory that you think you are...",possible incorrect file although would expect different exception case make sure directory file readable work could start like print make sure actually running directory think,issue,positive,positive,positive,positive,positive,positive
543464111,"Hi @caioc2,
Thanks for the feedback. I'll log it as a feature request, but if you want to submit a pull request with the change, I can try to get it merged in.

As a side note, does running with `CUDA_VISIBLE_DEVICES=-1 mlagents-learn ...` work too?",hi thanks feedback log feature request want submit pull request change try get side note running work,issue,negative,positive,positive,positive,positive,positive
543461668,"Hi @mistermotomax,
We're still in the process of enabling support for tensorflow 2.0. But we do have a fix for this particular conversion error; the fix was made on the `develop` branch here: https://github.com/Unity-Technologies/ml-agents/pull/2703

Can you try that out and let me know if it fixes things?",hi still process support fix particular conversion error fix made develop branch try let know,issue,negative,positive,positive,positive,positive,positive
543461419,"Hi, @chriselion 
Thanks for the comment. Is there any reason v0.10 multi environment should be slow?
Looking at the[ trainer_controller.py](https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/trainer_controller.py) line 202, I thought synchronous update in v0.8 changed to asynchronous, and multi environment speed should be faster.

However, toy training on v0.10 took 
11723 sec total
128 env
1.0e6 max steps

Previous training on v0.8 took
6914 sec total
128 env
1.0e4 max steps.

I just want to be sure if my testing codes were wrong or is jt correct that it should be slower.",hi thanks comment reason environment slow looking line thought synchronous update asynchronous environment speed faster however toy training took sec total previous training took sec total want sure testing wrong correct,issue,negative,negative,neutral,neutral,negative,negative
543459423,"Hi @DuckLord25,
That's definitely strange. What version of ml-agents and ml-agents envs are you using? I'm pretty sure it's an older version based on how the imports are happening (we got rid of the `import *`'s before version 0.10.1). Can you try uninstalling both packages and reinstalling the latest versions?",hi definitely strange version pretty sure older version based happening got rid import version try latest,issue,positive,positive,positive,positive,positive,positive
543457242,"Hi @LibraryAI,
Yes, those should be roughly equivalent.",hi yes roughly equivalent,issue,negative,negative,neutral,neutral,negative,negative
543330925,Tried but it also didnt work :( I am not sure if it has to do with something wrong with the installation? ,tried also didnt work sure something wrong installation,issue,negative,neutral,neutral,neutral,neutral,neutral
543290355,"nit: Could we move all of the Concrete Policies to a 'Policy' folder.  
![Screen Shot 2019-10-17 at 10 55 50 AM](https://user-images.githubusercontent.com/1356616/67034550-b7411080-f0cc-11e9-9c02-ad0d97ed4023.png)
",nit could move concrete folder screen shot,issue,negative,positive,positive,positive,positive,positive
543281162,"Hey @devedse,
Thanks for the report.  We will take look at this.",hey thanks report take look,issue,negative,positive,positive,positive,positive,positive
543144591,"Hello @mantasp. Thanks for the response.
Done, and same error. Including Graphics Emulation set to No Emulation.

Also if I switch to Standalone or Windows Universal Platform I get 
![image](https://user-images.githubusercontent.com/35074359/67007470-7558a000-f0b5-11e9-921a-c7b38c576f1e.png)

Does that mean that my model is not supported by barracuda?
",hello thanks response done error graphic emulation set emulation also switch universal platform get image mean model barracuda,issue,negative,negative,neutral,neutral,negative,negative
543086471,"@RobertoE90 this error usually means that GPU compute is disabled on your platform. If you see this happening on Android device, please check your Player Settings and make sure that either Vulkan on GLES 3 is picked as preferred GFX API. In case it's GLES 3, please also make sure that ""Requires GLES 3.1"" checkbox is also checked.
If you see this error in Editor while Android platform is selected make sure that Edit->Graphics Emulation is set to ""No Emulation"" (sometimes editor restart might be needed after changing it)",error usually compute disabled platform see happening android device please check player make sure either picked preferred case please also make sure also checked see error editor android platform selected make sure graphic emulation set emulation sometimes editor restart might,issue,positive,positive,positive,positive,positive,positive
543056444,I would like to second this request. It would be even better if ML-Agents was available as a package through Unity's new package manager.,would like second request would even better available package unity new package manager,issue,positive,positive,positive,positive,positive,positive
542917876,"Overall it looks good.  I'd like to discuss the instantiation of Policies more, but that shouldn't block this from going in.  ",overall good like discus block going,issue,negative,positive,positive,positive,positive,positive
542897653,"FWIW this conflicts with some changes on the develop branch. I'll merge this to master, then handle the conflicts on develop.",develop branch merge master handle develop,issue,negative,neutral,neutral,neutral,neutral,neutral
542894987,"No worries, glad you got it figured out. For what it's worth, this should get simpler in the next release.",glad got figured worth get simpler next release,issue,positive,positive,positive,positive,positive,positive
542893596,I tested PPO and it doesn't break (though there's no test). In fact it's more correct than the previous implementation. Let me add one though.,tested break though test fact correct previous implementation let add one though,issue,negative,negative,negative,negative,negative,negative
542890803,I'm so sorry for even creating this issue. I just forgot to check the control beside the brain. I'm really sorry. It's working fine now. ,sorry even issue forgot check control beside brain really sorry working fine,issue,negative,negative,negative,negative,negative,negative
542884934,Do we have anything that tests PPO+curiosity+continuous? Just want to make sure that doesn't break too.,anything want make sure break,issue,negative,positive,positive,positive,positive,positive
542877947,"Hi,
The tennis example that we have is actually cooperative - the ""players"" get rewards for keeping the ball in the air as long as possible. The soccer example might be a better example, but in this case both the goalies (on different teams) share a brain, and do the strikers.

We're planning on expanding support for team and competitive play soon, but there's nothing officially available right now. But you can try out the ""ghost trainer"" here https://github.com/Unity-Technologies/ml-agents/pull/1975 which some other people have gotten to work with their games.

Hope that helps...",hi tennis example actually get keeping ball air long possible soccer example might better example case different share brain expanding support team competitive play soon nothing officially available right try ghost trainer people gotten work hope,issue,positive,positive,positive,positive,positive,positive
542870282,Thanks @albertoxamin - can you please sign the Contributor License Agreement?,thanks please sign contributor license agreement,issue,positive,positive,positive,positive,positive,positive
542860048,"same problem here, building for android platform
",problem building android platform,issue,negative,neutral,neutral,neutral,neutral,neutral
542859915,"Wow, interesting.  Thanks for the update @albertoxamin.  ",wow interesting thanks update,issue,positive,positive,positive,positive,positive,positive
542848512,"UPDATE, I was able to make it work.
Apparently downloading the repository as zip marks the bundle as untrusted, but cloning the repo with git doesn't.

For anyone in the future that encounters this problem, just do a git clone.

If you don't want to download again the repo you could also run this on the terminal (from the Plugins/OSX folder): `xattr -d com.apple.quarantine macblas.bundle`",update able make work apparently repository zip bundle untrusted git anyone future problem git clone want could also run terminal folder,issue,negative,positive,positive,positive,positive,positive
542827245,"Apple just pushed an update on Catalina, I'm downloading it right now and I will report if anything changes.

Edit: No luck I'm still encountering this problem in build 19A602",apple update catalina right report anything edit luck still problem build,issue,negative,positive,positive,positive,positive,positive
542817471,"Hi,
Not sure how else to debug this. mlagents-learn just takes the argument and passes it to `open()`, but it's possible that anaconda is doing something weird with the working directory. Can you try passing the absolute path, like ""C:\users\alber\anaconda3\envs\ml-agents\config\ trainer_config.yaml"" instead?",hi sure else argument open possible anaconda something weird working directory try passing absolute path like instead,issue,negative,positive,neutral,neutral,positive,positive
542804523,"@andymule Thanks, I can reproduce the error here; it appears to be related to the combination of continuous action space, SAC, and curiosity. We'll try to get a fix soon, but in the meantime, you probably don't want to enable curiosity on the Walker scene.",thanks reproduce error related combination continuous action space sac curiosity try get fix soon probably want enable curiosity walker scene,issue,negative,positive,positive,positive,positive,positive
542796863,The only major difference that I see potentially is that I'm on a beta version of Catalina.  I can try to partition my drive tonight or tomorrow and install the public release in order to get closer to your setup.  Thanks for the extra info. ,major difference see potentially beta version catalina try partition drive tonight tomorrow install public release order get closer setup thanks extra,issue,negative,positive,neutral,neutral,positive,positive
542786585,"I just tried with unity 2019.2.3f1 and got the same error.



I even tried to do this and got your same results
> Hey @albertoxamin,
> After a bit of inspection, it seems like macOS Catalina may be a bit more strict about checking that a `.bundle` code signature is valid.
> 
> ```
> codesign --verify -v ""UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle""
> UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle: a sealed resource is missing or invalid
> file added: UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle/Contents/MacOS.meta
> file added: UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle/Contents/MacOS/macblas.meta
> file added: UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle/Contents/Info.plist.meta
> file added: UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle/Contents/_CodeSignature.meta
> ```
> 
> Since a `.bundle` is actually just a directory, Unity adds `.meta` files where, maybe, it shouldn't. After removing the `.meta` files from the bundle I see:
> 
> ```
> codesign --verify -v ""UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle""
> UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle: valid on disk
> UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle: satisfies its Designated Requirement
> ```
> 
> I will test later tonight on my personal laptop which has Catalina installed to gather more information. Thanks for the report!

",tried unity got error even tried got hey bit inspection like catalina may bit strict code signature valid verify sealed resource missing invalid file added file added file added file added since actually directory unity maybe removing bundle see verify valid disk requirement test later tonight personal catalina gather information thanks report,issue,negative,neutral,neutral,neutral,neutral,neutral
542785669,"HI @andymule - this might be one of the bugs that was fixed in 0.10.1, but I'll try to reproduce it and confirm.",hi might one fixed try reproduce confirm,issue,negative,positive,neutral,neutral,positive,positive
542783295,"I'm using Unity 2017.4.32f1 downloaded from Unity Hub
PATH
```
/usr/local/opt/qt/bin:/Library/Frameworks/Python.framework/Versions/3.6/bin:/Users/alberto/Library/Android//tools:/Users/alberto/Library/Android//platform-tools:/opt/local/bin:/opt/local/sbin:/usr/local/opt/libxml2/bin:/Users/alberto/.pyenv/shims:/Users/alberto/.nvm/versions/node/v10.10.0/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/alberto/Development/flutter/bin:/Library/TeX/texbin:/usr/local/share/dotnet:/opt/X11/bin:~/.dotnet/tools:/Applications/Wireshark.app/Contents/MacOS:/Users/alberto/.antigen/bundles/ael-code/zsh-colored-man-pages:/Users/alberto/gocode/bin:/usr/local/bin:/usr/local/opt/fzf/bin
```
PYTHON
```
λ alberto [Barracuda/Plugins/OSX] → python --version
Python 3.7.0

λ alberto [Barracuda/Plugins/OSX] → python -m pip list
Package               Version
--------------------- --------
absl-py               0.8.1
appnope               0.1.0
asciinema             2.0.2
astor                 0.8.0
astroid               2.3.1
attrs                 19.3.0
autopep8              1.4.4
backcall              0.1.0
bleach                3.1.0
cloudpickle           1.2.2
cycler                0.10.0
decorator             4.4.0
defusedxml            0.6.0
entrypoints           0.3
gast                  0.3.2
google-pasta          0.1.7
grpcio                1.24.1
h5py                  2.10.0
importlib-metadata    0.23
ipykernel             5.1.2
ipython               7.8.0
ipython-genutils      0.2.0
ipywidgets            7.5.1
isort                 4.3.21
jedi                  0.15.1
Jinja2                2.10.3
jsonschema            3.1.1
jupyter               1.0.0
jupyter-client        5.3.4
jupyter-console       6.0.0
jupyter-core          4.6.0
Keras-Applications    1.0.8
Keras-Preprocessing   1.1.0
kiwisolver            1.1.0
lazy-object-proxy     1.4.2
Markdown              3.1.1
MarkupSafe            1.1.1
matplotlib            3.1.1
mccabe                0.6.1
mistune               0.8.4
mlagents              0.10.1
mlagents-envs         0.10.1
more-itertools        7.2.0
nbconvert             5.6.0
nbformat              4.4.0
notebook              6.0.1
numpy                 1.17.2
opencv-contrib-python 4.1.1.26
opencv-python         4.1.1.26
pandocfilters         1.4.2
parso                 0.5.1
pexpect               4.7.0
pickleshare           0.7.5
Pillow                6.2.0
pip                   19.3
prometheus-client     0.7.1
prompt-toolkit        2.0.10
protobuf              3.10.0
psycopg2              2.7.6.1
psycopg2-binary       2.7.6.1
ptyprocess            0.6.0
pycodestyle           2.5.0
Pygments              2.4.2
pylint                2.4.2
pyparsing             2.4.2
pyrsistent            0.15.4
python-dateutil       2.8.0
PyYAML                5.1.2
pyzmq                 18.1.0
qtconsole             4.5.5
Send2Trash            1.5.0
setuptools            39.0.1
six                   1.12.0
tensorboard           1.14.0
tensorflow            1.14.0
tensorflow-estimator  1.14.0
termcolor             1.1.0
terminado             0.8.2
testpath              0.4.2
tornado               6.0.3
traitlets             4.3.3
typed-ast             1.4.0
wcwidth               0.1.7
webencodings          0.5.1
Werkzeug              0.16.0
wheel                 0.33.6
widgetsnbextension    3.5.1
wrapt                 1.11.2
zipp                  0.6.0
```
SYSTEM REPORT
```
Hardware Overview:

  Model Name:	MacBook Pro
  Model Identifier:	MacBookPro14,2
  Processor Name:	Dual-Core Intel Core i5
  Processor Speed:	3,1 GHz
  Number of Processors:	1
  Total Number of Cores:	2
  L2 Cache (per Core):	256 KB
  L3 Cache:	4 MB
  Hyper-Threading Technology:	Enabled
  Memory:	16 GB
  Boot ROM Version:	202.0.0.0.0
  SMC Version (system):	2.44f2

```
",unity unity hub path python python version python python pip list package version astor astroid bleach cycler decorator gast jinja markdown notebook pillow pip six tornado wheel system report hardware overview model name pro model identifier processor name core processor speed number total number cache per core cache technology memory boot version version system,issue,negative,neutral,neutral,neutral,neutral,neutral
542760705,"Hi @albertoxamin,
I tried to reproduce your issue on Catalina (19B68f) and the previous version, with unity versions 2017.4.33f and 2019.3.b06.  I was able to run training on both with no issues on ML-Agents 0.10.1.  Can you give us any more information about your personal setup?  ",hi tried reproduce issue catalina previous version unity able run training give u information personal setup,issue,negative,positive,positive,positive,positive,positive
542597346,+) I thick above one is still not working...,thick one still working,issue,negative,negative,negative,negative,negative,negative
542588886,"I would like some clarification on this as well.

In the past I've had straight up bugs with constant decision rates (cross wiring between agents being run in the same environment), and recently ive seen some strange behavior when using variable decision rate.

When i use the same looping timer to trigger decisions for all the agents in a scene (which share a brain),meaning they all make a decision at the same time,  agent steps will accumulate at a familiar rate in the  cmd prompt summaries. But, when i allowed agents to set their own next decision delay, agent steps started accumulating about 20 times faster (and there were about 20 agents in the scene) (given the rage they could set their decision delay to,  decision steps should have accumulated more slowly).

This implies that when a brain makes a decision for multiple agents in a single update, it only counts it as a single decision. I need to do some testing to confirm this theory, but I have long suspected it. 

The in-game results of training -appear- to be unaffected by this issue, so it might just be a bug in the diagnostic outputs, but it may also point toward a deeper root cause. This might also explain why using recurrence with multiple unique agents in the same scene tends to throw the value-key error (i've never tried running recurrent agents on different fixed updates to see if it fixes the error, but that would definitely be confirmation)....",would like clarification well past straight constant decision cross wiring run environment recently seen strange behavior variable decision rate use looping timer trigger scene share brain meaning make decision time agent accumulate familiar rate prompt set next decision delay agent time faster scene given rage could set decision delay decision slowly brain decision multiple single update single decision need testing confirm theory long suspected training unaffected issue might bug diagnostic may also point toward root cause might also explain recurrence multiple unique scene throw error never tried running recurrent different fixed see error would definitely confirmation,issue,negative,positive,neutral,neutral,positive,positive
542527491,"directory in the command is: (ml-agents) C:\Users\alber\ml-agents>

Which the folder config is inside ml-agents folder",directory command folder inside folder,issue,negative,neutral,neutral,neutral,neutral,neutral
542500319,"Hi @albertodcm,
What directory are you running the command from? The examples generally assume that you're in the root directory of the ml-agents repo, in which case this file https://github.com/Unity-Technologies/ml-agents/blob/master/config/trainer_config.yaml would be at `config/trainer_config.yaml` . If you're running from a different directory, you'll have to point it to a different path, or copy the file near where you're running from.",hi directory running command generally assume root directory case file would running different directory point different path copy file near running,issue,negative,positive,neutral,neutral,positive,positive
542489889,Thank you for your reply! even after closing the issue :) ,thank reply even issue,issue,negative,neutral,neutral,neutral,neutral,neutral
542488728,"My partner loaded my learned model(frozen_graph_def.pb) in C# script. 
And checked loaded model get `m_InferenceInputs` of 1) as input 
and make `vectorAction` of 2) as output. 

And It seems correct! :) So I Close this issue.

+) But I'm still confused with `m_InferenceOutputs`. What is the meaning of this value?
could you explain it? :(",partner loaded learned model script checked loaded model get input make output correct close issue still confused meaning value could explain,issue,negative,negative,negative,negative,negative,negative
542474276,"As you can see ""rayobs.Count"" value is 56. 
It represent 7(one ray info size: size of detectable objects + 2) * 8 rays = 56 size float array.

But you just see the value rayobs[0]~rayobs[7]. 
It maybe only contain ray detail info that attached with ""0""angle. (rayobs[0]~rayobs[6]) 

One ray info made up with size of ( size of detectable objects + 2 ).
For example, if you detect ""firstRed"" at 6.0f depart from agent center.(with ""angle 0f"" attached ray) 
rayobs[0] : 0 (wall)
rayobs[1] : 0 (agent)
rayobs[2] : 1 (firstRed)
rayobs[3] : 0 (secondRed)
rayobs[4] : 0 (human)
rayobs[5] : 0 (it turns on 1, when ray detect nothing.) 
rayobs[6] : 0.5 ( 6.0f / 12.0f) 

So if you want see ""angle 90f"" attached ray, you have to see the value of
rayobs[14]~ rayobs[20].   
",see value represent one ray size size detectable size float array see value maybe contain ray detail attached angle one ray made size size detectable example detect depart agent center angle attached ray wall agent human turn ray detect nothing want see angle attached ray see value,issue,positive,negative,neutral,neutral,negative,negative
542453715,"Hi @wardVD, sorry for the delay on this.

The original author of the tennis scene is currently on leave, so it's hard to get a first-hand answer about it. However, the consensus from the rest of the team is that in general it's a hard scene to train, so some of the values might have been hacks to get it to train the first time, and haven't been updated since then.",hi sorry delay original author tennis scene currently leave hard get answer however consensus rest team general hard scene train might get train first time since,issue,negative,negative,neutral,neutral,negative,negative
542446849,"Hey @albertoxamin,
After a bit of inspection, it seems like macOS Catalina may be a bit more strict about checking that a `.bundle` code signature is valid.
```
codesign --verify -v ""UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle""
UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle: a sealed resource is missing or invalid
file added: UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle/Contents/MacOS.meta
file added: UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle/Contents/MacOS/macblas.meta
file added: UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle/Contents/Info.plist.meta
file added: UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle/Contents/_CodeSignature.meta
```

Since a `.bundle` is actually just a directory, Unity adds `.meta` files where, maybe, it shouldn't.  After removing the `.meta` files from the bundle I see:
```
codesign --verify -v ""UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle""
UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle: valid on disk
UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle: satisfies its Designated Requirement
```

I will test later tonight on my personal laptop which has Catalina installed to gather more information.  Thanks for the report!",hey bit inspection like catalina may bit strict code signature valid verify sealed resource missing invalid file added file added file added file added since actually directory unity maybe removing bundle see verify valid disk requirement test later tonight personal catalina gather information thanks report,issue,negative,neutral,neutral,neutral,neutral,neutral
542446096,"Hi @spaggi,
Looks like we will need to code sign these binaries for use on macOS Catalina.  We'll update you when we have done so.  Thanks for the report!",hi like need code sign use catalina update done thanks report,issue,positive,positive,positive,positive,positive,positive
542434820,"Hi @albertoxamin - we'll look into this. I don't think 32-bit is the problem; if you run

```
$ file UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle/Contents/MacOS/macblas
UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda/Plugins/OSX/macblas.bundle/Contents/MacOS/macblas: Mach-O 64-bit bundle x86_64
```
but there might be a problem with the code signing.",hi look think problem run file bundle might problem code,issue,negative,neutral,neutral,neutral,neutral,neutral
542400850,Could it be that this problem is caused by macOS Catalina that dropped 32 bit support and the plugin was not compiled for 64bit? @mantasp  ,could problem catalina bit support bit,issue,negative,neutral,neutral,neutral,neutral,neutral
542399652,"Hi @hanseoup, thanks for sending the params and glad 10.1 fixed the issue. We did fix a bug with sequence length in 10.1, so perhaps the issue was related to that. ",hi thanks sending glad fixed issue fix bug sequence length perhaps issue related,issue,positive,positive,positive,positive,positive,positive
542397151,"Yep! Our example in the blog ran fairly slowly, so parallelizing it produced many more samples per second. But you're still going to end with one policy. ",yep example ran fairly slowly produced many per second still going end one policy,issue,negative,positive,neutral,neutral,positive,positive
542350526,Will put this on hold until #2731 is good to go,put hold good go,issue,negative,positive,positive,positive,positive,positive
542129015,"@xiaomaogy , I think I used this guide:
https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md

After reading through it it seems that I simply missed it here as well. Maybe we can add a picture to this guide as well or simply close this issue.",think used guide reading simply well maybe add picture guide well simply close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
542121625,"I've found that, inside `DecideAction()` in ""LearningBrain.cs"" ,
variable `m_InferenceInputs` has [observation, action mask] value.
So, I think `m_InferenceInputs` is would be what I want.

But there is  also `m_InferenceOutputs` with same size of `m_InferenceInputs`, so i'm confused with this.

To make certain this, I've run Debug mode. And set 2 break points in 
1) LeaningBrain.cs : DecideAction() for watching `m_InferenceInputs`
2) Agent.cs: AgentAction(float[] vectorAction, string textAction) for watching `vectorAction`

In result, debug mode stopped at 1) -> 2) ->1) ->2) .... sequencially. 

So, I think `m_InferenceInputs` of 1) & `vectorAction` of 2) is correct input-output pair.
Am I correct?
",found inside variable observation action mask value think would want also size confused make certain run mode set break watching float string watching result mode stopped think correct pair correct,issue,negative,negative,neutral,neutral,negative,negative
542079770,"@xiaomaogy 
I am glad you liked it. I prepared this notebook very quickly, and surely, we can add a better explanation for each cell to reduce confusion.

I also had some thoughts about visualizing Unity's output. I am pretty sure we can add it either through some sort of ``xvbf`` hack or a minor modification in the ml-agents.

About the time-saving part,  I like to modify/create different environments, and training agents on them can be time-consuming (I only have two computers). With this setup, I can use COLAB to train more agents simultaneously. The training power part aside, I think it is really convenient to push your C# code changes to the GitHub and let the scripts do the compiling job for you. (After installing Unity in one COLAB session, you can reuse it until the session expires.) Surely one can make a Linux build locally and upload the binaries in COLAB (or in any other Linux servers), but it's time consuming compared to automatic builds. (I also have bad internet connection which makes upload process painful)",glad prepared notebook quickly surely add better explanation cell reduce confusion also unity output pretty sure add either sort hack minor modification part like different training two setup use train simultaneously training power part aside think really convenient push code let job unity one session reuse session surely one make build locally time consuming automatic also bad connection process painful,issue,positive,positive,positive,positive,positive,positive
542077315,"Thanks for the answer! I think I will go for just recording what the agent sees in this case, which will not be too tricky. However, I would be very interested in a Python video recorder if that is something you are considering to add in the future :)

Tanks again!",thanks answer think go recording agent case tricky however would interested python video recorder something considering add future,issue,positive,positive,positive,positive,positive,positive
542066777,"I have already done tagging and right now the environment works like with these 2 tags.  Though i would like to take a step further and make it more realistic, so that the rayperceptions work like lasers giving information only about the distance of objects. Therefore i put a camera on my agent but  so that it should find its goal object with the camera input.  The rayperceptions would only give distance information. Also I would use grayscaling to filter the input image. The goal object is quite unique from the obstacles regarding its color.
Im just not sure If its possible because this makes the task too complex for the PPO.",already done right environment work like though would like take step make realistic work like giving information distance therefore put camera agent find goal object camera input would give distance information also would use filter input image goal object quite unique regarding color sure possible task complex,issue,positive,positive,positive,positive,positive,positive
542047351,"We found that hallway learning is stable after I made version up from 0.10.0 to 0.10.1
also, my environment learning comes stable too. 

I saw the quite a lot of hot-fix in 0.10.1 about 5 days ago. 
I think there were problem solving fix in its hot-fix. 

and here is my config option. 

simpleBattleLearning:
    trainer: ppo
    batch_size: 256
    beta: 5.0e-3
    buffer_size: 4096
    epsilon: 0.2
    gamma: 0.99
    hidden_units: 128
    lambd: 0.95
    learning_rate: 3.0e-4
    max_steps: 1.0e7
    memory_size: 64
    normalize: true
    num_epoch: 3
    num_layers: 3
    time_horizon: 64
    sequence_length: 4
    summary_freq: 1000
    use_curiosity: false
    curiosity_strength: 0.01
    curiosity_enc_size: 128
    use_recurrent: false

",found hallway learning stable made version also environment learning come stable saw quite lot day ago think problem fix option trainer beta epsilon gamma normalize true false false,issue,positive,negative,negative,negative,negative,negative
542015709,"Yes, your understanding is correct, we have 16 simulations of the exact same scenes with one policy being updated by the samples generated from these simulations. ",yes understanding correct exact one policy,issue,negative,positive,positive,positive,positive,positive
542010751,"@xiaomaogy On your official blog website, it mentioned that ""Today, we are introducing the ability to train faster by having multiple concurrent instances of Unity on a multi-core machine"". and "" The changes we provide in v0.8 enable a training speedup of 5.5x on easy levels and up to 7.5x on harder levels by leveraging **16 Unity simulations**. Generally speaking, the gains of utilizing multiple Unity simulations are greater for more complex levels and games."" Does it mean that ML agents train 16 simulations in one scenario concurrently and finally get one policy to make agent be able to solve many levels? ",official today ability train faster multiple concurrent unity machine provide enable training easy harder unity generally speaking gain multiple unity greater complex mean train one scenario concurrently finally get one policy make agent able solve many,issue,positive,positive,positive,positive,positive,positive
541991326,"@taesiri
I was able to go through the whole process and I really like it. I was especially impressed by the convenience of this solution, everything is laid out nicely in just one place. The tensorboard, training console and the files can all be managed in one place (if we can somehow visualize the unity, that would be even better). 

Regarding the setup part, aside from a little bit confusion in the license activation part(and the video helped a lot), all of the other steps are really smooth. Nice job!

One thing I'm curious about is you mentioned that this setup helps you save time. Could you please elaborate more on this if you don't mind?



",able go whole process really like especially convenience solution everything laid nicely one place training console one place somehow visualize unity would even better regarding setup part aside little bit confusion license activation part video lot really smooth nice job one thing curious setup save time could please elaborate mind,issue,positive,positive,positive,positive,positive,positive
541983269,"> Yeah, I think the overall approach makes sense. It's slightly counterintuitive that the ModelRunner or Communicator 's DecideBatch() get called (# of agents) times and only the first one does any work, but I don't have a better way to do it.

Feels weird too and I realize I did not explicitly call this out in the PR comment. Hopefully, this will be temporary and after the Observation and Action refactor, we will be able to return actual action values rather than void in DecideAction.",yeah think overall approach sense slightly communicator get time first one work better way weird realize explicitly call comment hopefully temporary observation action able return actual action rather void,issue,positive,positive,neutral,neutral,positive,positive
541982253,"Yeah, I think the overall approach makes sense. It's slightly counterintuitive that the ModelRunner or Communicator 's DecideBatch() get called (# of agents) times and only the first one does any work, but I don't have a better way to do it.",yeah think overall approach sense slightly communicator get time first one work better way,issue,positive,positive,positive,positive,positive,positive
541981197,"@taesiri Thanks for the video, I will try again. I was really confused by ""Upload ulf file in colab using the left side-bar."", because I wan't able to see the Files tab and switch to it, and the video helped me find it. ",thanks video try really confused file left wa able see tab switch video find,issue,negative,positive,neutral,neutral,positive,positive
541959404,"@xiaomaogy, I think you haven't installed Unity yet. Either ``$UNITY_EXEC`` is pointing to the wrong path or you've forgotten to accept the License Agreement.

![Screen Shot 2019-10-15 at 1 52 15 AM](https://user-images.githubusercontent.com/588431/66786956-f6b20600-eeee-11e9-879b-51becf7e545d.png)

After executing the above cell, you have to accept the License Agreement by pressing ``y`` in the textbox which appeared bellow the cell.

That being said, It is better to activate Unity using the second method. For some reason passing the password in the command line is problematic.",think unity yet either pointing wrong path forgotten accept license agreement screen shot cell accept license agreement pressing bellow cell said better activate unity second method reason passing password command line problematic,issue,positive,neutral,neutral,neutral,neutral,neutral
541944573,"Hi @hanseoup, @nomoreid, I did some runs of Hallway on my end. They all roughly train in similar time - though, as I mentioned before, there is a bit of randomness in the runs. Green is 0.8, pink is 0.9, and red is 0.10 with the 0.8 Hallway env. 

Note that I do expect the 0.10 training run to be different as the Hallway env (black theme) was changed in 0.10. But the trainer should behave the same way across all the versions. 

![image](https://user-images.githubusercontent.com/5085265/66785481-3e567480-ee92-11e9-94a7-0a9a51d7fc75.png)

Could you by any chance share the hyperparameters used in your custom environment? The one thing that changed a lot between 0.8 and 0.9.3 was the curiosity module. 
",hi hallway end roughly train similar time though bit randomness green pink red hallway note expect training run different hallway black theme trainer behave way across image could chance share used custom environment one thing lot curiosity module,issue,negative,negative,neutral,neutral,negative,negative
541930561,"Hi @taesiri, this looks really nice! I've tried to walk through your notebook, but on the step of ""Activate Unity"" I got a few error messages saying ""invalid option -- o"" etc, so I wasn't able to proceed. 

![image](https://user-images.githubusercontent.com/5985076/66784099-53c99f80-ee8e-11e9-8ebc-76b3a45cdc4a.png)
",hi really nice tried walk notebook step activate unity got error saying invalid option able proceed image,issue,negative,positive,positive,positive,positive,positive
541888720,"Why don't you just use ray perception and give two tags to the ""obstacle"" and the ""goal"" object? ",use ray perception give two obstacle goal object,issue,negative,neutral,neutral,neutral,neutral,neutral
541887102,"Hi @kaiolae, we have video recorder (here)[https://github.com/Unity-Technologies/video-recorder]which is pretty easy to use. You can't use it in python with the gym interface though. 

For experience reply, I think you will have to store everything yourself and replay it in a customized way. We don't offer any solution right now. ",hi video recorder pretty easy use ca use python gym interface though experience reply think store everything replay way offer solution right,issue,positive,positive,positive,positive,positive,positive
541852723,"@yjcnbnbnb200 We've deprecated the usage of TensorFlowSharp, and now we are using Barracuda, which is already included in the repo. ",usage barracuda already included,issue,negative,neutral,neutral,neutral,neutral,neutral
541851984,"Hi @devedse, I believe this is already included in the documentation. For example https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md#setting-up-the-environment-for-training",hi believe already included documentation example,issue,negative,neutral,neutral,neutral,neutral,neutral
541850311,"Hi @sunirisgrace, you can set one brain or multiple brain in a certain scene, and that will correspond to one or multiple policies. This is not relevant to whether you are using parallel training. 

When we use the parallel environment feature, the only purpose here is to collect more samples from the simulation, and we will only allow one kind env and launch multiple of them to speed up the collection speed. Two scenarios are not allowed. 

",hi set one brain multiple brain certain scene correspond one multiple relevant whether parallel training use parallel environment feature purpose collect simulation allow one kind launch multiple speed collection speed two,issue,positive,positive,positive,positive,positive,positive
541792332,"1) One brain. Parallel training is the same (in effect) of having multiple training areas/agents in one scene.
2) One policy per brain. If there's only one brain in the scene, there will only be one policy as well. 
3) Parallel training is for multiples of the same env, so you only have to pass the --env once, with the --num-envs. Currently we don't support parallel training with different environments.
4) All scenarios need to be identical, and you'll see the average reward across all of the environments. ",one brain parallel training effect multiple training one one policy per brain one brain scene one policy parallel training pas currently support parallel training different need identical see average reward across,issue,positive,negative,neutral,neutral,negative,negative
541620134,"> It might be a bug in pylint. This page could help.
https://medium.com/@haominnn/how-to-deal-with-python-not-finding-tensor-flow-module-under-anaconda-3aefa8ffae11

This is NOT a solution. Simply disabling a tool because of an error doesn't fix the error.

Now to the bug:

Why: Then TF package is just a proxy that redirects stuff to the internal package called `tensorflow_core` as can be verified by importing that. The problem: You are not allowed to do that as it is an internal package.

Solution: Tell pylint about this. This can be done via pylint plugins although I'm currently not aware if there are already some that do that.

Note: Yes everything *does* work even though there are **linter** errors. You may ignore them or switch to another linter as described in the link. But it would be great if someone finds a solution for pylint as that is way more powerful than pep8, which is mostly a style-checker.",might bug page could help solution simply tool error fix error bug package proxy stuff internal package problem internal package solution tell done via although currently aware already note yes everything work even though linter may ignore switch another linter link would great someone solution way powerful pep mostly,issue,positive,positive,positive,positive,positive,positive
541570197,"@chriselion , cool  thanks :). Keep me posted",cool thanks keep posted,issue,positive,positive,positive,positive,positive,positive
541569641,"I found the issue, I simply had to check the ""Control"" checkbox in the Academy. Is this something we'd want to add to the documentation?",found issue simply check control academy something want add documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
541480370,"It seems experimentally that this effect can be achieved without using LSTM, but there is still the problem of convergence instability between v0.9.x and v0.10.x.",experimentally effect without still problem convergence instability,issue,negative,positive,neutral,neutral,positive,positive
541327789,"Hi, I have some questions about parrel training in different scenarios:
1) I wonder whether there is one brain or there are multiple brains after parrel training is finished.
2) Whether there is one policy or there are multiple policies during the process of parrel training.
3) How to implement parrel training in different scenarios. If I have two scenarios, should I write one line of command for training or I need to write two commands for training. such as: mlagents-learn < trainer- config-file > --num-envs=<2> --env = < env_name1 > --run-id = <run-identifier1>  --env = < env_name2 > --run-id = <run-identifier2> --train. 
4) During the process of the training for one scenario, we can see the value of the reward. How to differentiate the rewards for two scenarios if I train two scenarios concurrently.",hi parrel training different wonder whether one brain multiple brain parrel training whether one policy multiple process parrel implement parrel training different two write one line command training need write two training process training one scenario see value reward differentiate two train two concurrently,issue,positive,neutral,neutral,neutral,neutral,neutral
541227200,"> Also, how do we test it?

Make a test that runs in inference mode, and assert that the buffer is empty after stepping a few times?",also test make test inference mode assert buffer empty stepping time,issue,negative,negative,neutral,neutral,negative,negative
541143830,"Hi @devedse,
Thanks for the bug report. We're currently investigating the missing models on import (originally reported in https://github.com/Unity-Technologies/ml-agents/issues/2633) - this only appears to affect Unity versions 2019.x.

For the SendState fix, great minds think alike - I already made the change in https://github.com/Unity-Technologies/ml-agents/pull/2629 a few weeks ago 😄  Unfortunately it was too late for the 0.10.0 release.",hi thanks bug report currently investigating missing import originally affect unity fix great think alike already made change ago unfortunately late release,issue,positive,positive,positive,positive,positive,positive
540909182,Thank you. I've checked that `--env-args` option worked well !,thank checked option worked well,issue,positive,neutral,neutral,neutral,neutral,neutral
540860862,"Yes. we just used `trainer_config.yaml` file included in ""config"" folder.
And we just used specific version of `trainer_config.yaml` file. 
So, It could be slightly different by its version. But when I checked ""hallway"" hyper-params,
It seems almost same.",yes used file included folder used specific version file could slightly different version checked hallway almost,issue,negative,neutral,neutral,neutral,neutral,neutral
540778136,"we tested multiple times. multiple computer. multiple version. default setting. (3 times on 0.8 vs 0.10)
version 0.8 (default setting) always learning very well on hallway example. (10-20 min on unity editor)
but version 0.10 is not so good(more then 2 hour on unity editor)
we use default hyper-params. it's not luck.  
may be some problem on hallway env or training code.",tested multiple time multiple computer multiple version default setting time version default setting always learning well hallway example min unity editor version good hour unity editor use default luck may problem hallway training code,issue,positive,positive,positive,positive,positive,positive
540693017,"HI @hanseoup, are you using multiple environments by any chance? We changed the way the steps are logged in 0.9 - previously, each step is across all environments (e.g. 1 step with 8 environments would actually be 8 steps), which is now corrected. 

Also, your curve in orange is much steeper than I've seen for Hallway in any version. There's a bit of luck involved - training hallway multiple times can result in different curves. Can you post your hyperparams as well? Thanks!",hi multiple chance way logged previously step across step would actually corrected also curve orange much steeper seen hallway version bit luck involved training hallway multiple time result different post well thanks,issue,positive,positive,neutral,neutral,positive,positive
540678585,"@DooblyNoobly, this issue was fixed in the latest Hotfix 0.10.1 - thanks for reporting and closing the issue for now. ",issue fixed latest thanks issue,issue,negative,positive,positive,positive,positive,positive
540671265,"HI @nikola-j 
A small update on this. While doing some profiling yesterday, I noticed this this block was especially slow for large observations:
https://github.com/Unity-Technologies/ml-agents/blob/8f2aff493c81c0287b2815ebec577869e4dab98b/ml-agents-envs/mlagents/envs/brain.py#L232-L235

Some alternate approaches are discussed here: https://stackoverflow.com/questions/6736590/fast-check-for-nan-in-numpy . I'm probably going to switch to the `dot` approach soon. I'm reluctant to drop the check entirely, but maybe we can add an option to disable it...",hi small update yesterday block especially slow large alternate probably going switch dot approach soon reluctant drop check entirely maybe add option disable,issue,negative,negative,negative,negative,negative,negative
540661683,"@xiaomaogy Yeah, that's the only downside, but I think we can mention it in the release notes.",yeah downside think mention release,issue,negative,neutral,neutral,neutral,neutral,neutral
540357178,"![image](https://user-images.githubusercontent.com/53593292/66539819-7ee98180-eb64-11e9-806b-bd27ce6d8a9d.png)

I've retrained hallway example with `0.10.0` about 200K (Blue)
It seems to start learning, but it take much longer time than `0.8.0` (about x10)
Is there any big change on 0.10.0 that can affect its Training conditions(learning speed, pattern)?",image hallway example blue start learning take much longer time big change affect training learning speed pattern,issue,negative,positive,neutral,neutral,positive,positive
540241377,"Hi @MentalGear, we're refactoring how vector observations are being done completely - but no plans for names. We'll keep you posted!",hi vector done completely keep posted,issue,negative,positive,neutral,neutral,positive,positive
540207024,"unit tests failing with
```        LOGGER.debug(
            "" Policy Update Training Metrics for {}: ""
            ""\n\t\tTime to update Policy: {:0.3f} s \n""
            ""\t\tTime elapsed since training: {:0.3f} s \n""
            ""\t\tTime for experience collection: {:0.3f} s \n""
            ""\t\tBuffer Length: {} \n""
            ""\t\tReturns : {:0.3f}\n"".format(
                self.brain_name,
                self.delta_policy_update,
                delta_train_start,
                self.delta_last_experience_collection,
                self.last_buffer_length,
>               self.last_mean_return,
            )
        )
E       TypeError: unsupported format string passed to NoneType.__format__
```
The link check is ignorable if you want, or cherrypick https://github.com/Unity-Technologies/ml-agents/pull/2702",unit failing policy update training metric update policy since training experience collection length unsupported format string link check ignorable want,issue,negative,neutral,neutral,neutral,neutral,neutral
540136225,markdown_link_check is a known problem that's fixed on develop.,known problem fixed develop,issue,negative,positive,neutral,neutral,positive,positive
540133219,"@ashwalcs 
- if you need to run network through many pictures, pass them in batches instead of processing one by one. Basically when you allocate tensor specify `batch size` as first tensor shape parameter `var shape = new TensorShape(batch_size, 416, 416, 3);`
- your example shows that you are not disposing all inputs, you have to dispose any tensor you did allocate via `inputs[BarracudaModel.inputs[0].name] = new Tensor(shape, imgData);`. So these tensors are causing errors you see in the logs.
- almost always it is best to call `worker.Execute()` instead of `worker.ExecuteAndWaitForCompletion(inputs);` your code will block anyway when you access output tensor data for the first time. Also note that if you Peek data you don't have to dispose it, but it's valid only until worker is alive and before second call to Execute() on the same worker instance.",need run network many pas instead one one basically allocate tensor specify batch size first tensor shape parameter shape new example dispose tensor allocate via new tensor shape causing see almost always best call instead code block anyway access output tensor data first time also note peek data dispose valid worker alive second call execute worker instance,issue,positive,positive,positive,positive,positive,positive
540114701,"The failing link right now is a unity one, rerunning.
",failing link right unity one,issue,negative,positive,positive,positive,positive,positive
540113639,I think someone made a force push or something to a branch that was merged to develop because I had to redo a lot of changes I had already made.,think someone made force push something branch develop redo lot already made,issue,negative,neutral,neutral,neutral,neutral,neutral
540101900,"Cool, they can all wait until after this is merged, but would be good to get rid of them soon after.",cool wait would good get rid soon,issue,positive,positive,positive,positive,positive,positive
540100851,"
> And a few stale references to `Batcher` too:
> 
> ```
> git grep -i batcher
> UnitySDK/Assets/ML-Agents/Scripts/Grpc/RpcCommunicator.cs:        /// Keeps track of which brains queried the batcher on the current step
> UnitySDK/Assets/ML-Agents/Scripts/Grpc/RpcCommunicator.cs:        /// The current UnityRLOutput to be sent when all the brains queried the batcher
> UnitySDK/Assets/ML-Agents/Scripts/ICommunicator.cs:        /// sent once all the brains that subscribed to the batcher have tried
> UnitySDK/Assets/ML-Agents/Scripts/LearningBrain.cs:        /// <param name=""communicator""> The Batcher the brain will use for the current session</param>```
> ```


That's my fault. I can clean those up in another PR",stale batcher git batcher track brain batcher current step current sent brain batcher sent brain batcher tried param communicator batcher brain use current session fault clean another,issue,negative,negative,neutral,neutral,negative,negative
540099631,Just ran into this also with setuptools 28.8.0. Upgrading (to 41.4.0) seems to fix it. I suspect anything past [40.1.0 when find_namespace_packages() was introduced](https://setuptools.readthedocs.io/en/latest/history.html#v40-1-0) would work.,ran also fix suspect anything past would work,issue,negative,negative,negative,negative,negative,negative
540089947,"And a few stale references to `Batcher` too: 
```
git grep -i batcher
UnitySDK/Assets/ML-Agents/Scripts/Grpc/RpcCommunicator.cs:        /// Keeps track of which brains queried the batcher on the current step
UnitySDK/Assets/ML-Agents/Scripts/Grpc/RpcCommunicator.cs:        /// The current UnityRLOutput to be sent when all the brains queried the batcher
UnitySDK/Assets/ML-Agents/Scripts/ICommunicator.cs:        /// sent once all the brains that subscribed to the batcher have tried
UnitySDK/Assets/ML-Agents/Scripts/LearningBrain.cs:        /// <param name=""communicator""> The Batcher the brain will use for the current session</param>```",stale batcher git batcher track brain batcher current step current sent brain batcher sent brain batcher tried param communicator batcher brain use current session,issue,negative,negative,negative,negative,negative,negative
540089625,"For doc cleanup, you might want to run `git grep  -i hub | grep -vi github` - there are still some hub references in comments and markdown
```
UnitySDK/Assets/ML-Agents/Scripts/Grpc/RpcCommunicator.cs:        /// <param name=""broadcastHub"">The BroadcastHub to get the controlled brains.</param>
UnitySDK/Assets/ML-Agents/Scripts/ICommunicator.cs:        /// <param name=""broadcastHub"">The BroadcastHub to get the controlled brains.</param>
UnitySDK/Assets/ML-Agents/Scripts/LearningBrain.cs:    /// least one LearningBrain in the BroadcastHub.
...
docs/Basic-Guide.md:3. In the **Project** window, go to `Assets/ML-Agents/Examples/3DBall/Brains` folder and drag the **3DBallLearning** Brain to the `Brains` property under `Braodcast Hub` in the `Ball3DAcademy` object in the **Inspector** window.
docs/ML-Agents-Overview.md:  Academy's `Broadcast Hub` with the `Control` checkbox checked. This is helpful
```",doc cleanup might want run git hub still hub markdown param get param get least one project window go folder drag brain brain property hub object inspector window academy broadcast hub control checked helpful,issue,negative,negative,negative,negative,negative,negative
540082468,I'm also interested in tracking how MLAgents works with ECS.  Is there page or thread I should be watching?  Thanks!,also interested work page thread watching thanks,issue,positive,positive,positive,positive,positive,positive
539752593,"@unityjeffrey Updated the PR title to ""Allow usage with tensorflow 2.0.0 (via tf.compat.v1)"", so hopefully that's less ambiguous about the goals.",title allow usage via hopefully le ambiguous,issue,positive,neutral,neutral,neutral,neutral,neutral
539744051,"@mantasp I am not running them concurrently. I am running them in series, one after the other. The model is loaded only once and the worker is created on each scan/detection. After completion of detection, worker is disposed off. Only after disposing the worker is another scan/detection initiated.

Here's a snippet :
 ```
    public IEnumerator LoadModel() // Called once in Start
    {
        Debug.Log(""Model loaded"");
        labels = label.text.Split(new char[] { '\n' }, StringSplitOptions.RemoveEmptyEntries);
        BarracudaModel = ModelLoader.Load(modelfile);
        yield return null;
    }

    public void sendImage() // Onclick of button
    {
        StartCoroutine(RunTF(m_Texture));
    }

    public IEnumerator RunTF(Texture2D m_Texture)
    {
        StartCoroutine(DecodeTexture(m_Texture,` 416, 416, 0, Flip.NONE, (result) =>
         { // Converts Texture2D to floatArray
             StartCoroutine(detector.Detect(result, (FinalResult) => { }));
         }));
    }

    public IEnumerator Detect(float[] imgData, System.Action<List<BoundingBox>> callback, int numResultsPerClass = 1, float threshold = 0f)
    {
          Debug.Log(""Create Worker"");
        worker = BarracudaWorkerFactory.CreateWorker(BarracudaWorkerFactory.Type.Compute, BarracudaModel, true);
        yield return null;
        var inputs = new Dictionary<string, Tensor>();
        var shape = new TensorShape(1, 416, 416, 3);
        inputs[BarracudaModel.inputs[0].name] = new Tensor(shape, imgData);
        worker.ExecuteAndWaitForCompletion(inputs);
        runner = worker.Peek(BarracudaModel.outputs[0]);
        runner.Dispose();
        worker?.Dispose();
        yield return null;
        callback(results);
    }


```
",running concurrently running series one model loaded worker completion detection worker disposed worker another snippet public start model loaded new char yield return null public void button public result result public detect float list float threshold create worker worker true yield return null new dictionary string tensor shape new new tensor shape runner worker yield return null,issue,positive,positive,neutral,neutral,positive,positive
539730709,"@DooblyNoobly 
Thank you! I finally got it working!",thank finally got working,issue,negative,neutral,neutral,neutral,neutral,neutral
539725738,"hmm not sure why you are trying to set up for development. Are you using windows? Just remove mlagents from your conda environment i.e. `pip uninstall package name` and install the mlagents version you want to replace it: `pip install mlagents==0.9.2` The only thing you need from the github repo (if you are not actively contributing to the development of mlagents) is the sdk and the config folder (if you are using the examples found in the sdk). You install mlagents itself from pip.  

So basic rule if you want to upgrade or downgrade your project:

- Remove sdk from your project
- Download the release you want from here
- Copy sdk over to your project
- Create a new conda environment and install the version of mlagents you need. Or use an existing conda environment and just pip uninstall the packages you want to replace and reinstall the version you need",sure trying set development remove environment pip package name install version want replace pip install thing need actively development folder found install pip basic rule want upgrade downgrade project remove project release want copy project create new environment install version need use environment pip want replace reinstall version need,issue,positive,positive,positive,positive,positive,positive
539705690,"Hey @DooblyNoobly, we've logged this issue with internal tracking number MLA-106. We're working on a fix, expect a patch soon. ",hey logged issue internal number working fix expect patch soon,issue,negative,neutral,neutral,neutral,neutral,neutral
539701658,"@FromZeus We are now using the Unity Inference Engine to do inference within Unity Games. For this reason, we deprecated our support for TF#.",unity inference engine inference within unity reason support,issue,negative,neutral,neutral,neutral,neutral,neutral
539691816,"@vincentpierre @chriselion Guys, I'm just getting into this stuff but can I ask you why are you getting rid off of TF#? I just wanted to research TF# and use it in Unity.",getting stuff ask getting rid research use unity,issue,negative,neutral,neutral,neutral,neutral,neutral
539685315,"@DooblyNoobly 
Or when I try to update the envs setting by doing 
`cd ml-agents-envs
pip install -e .`
It shows that: ERROR: mlagents 0.9.2 has requirement mlagents-envs==0.9.2, but you'll have mlagents-envs 0.10.0 which is incompatible.
I don't know how to change the envs into 0.9.2",try update setting pip install error requirement incompatible know change,issue,negative,neutral,neutral,neutral,neutral,neutral
539634733,"Hi,
Thank you for your information. However, if I try to downgrade from 0.10.0 to 0.9.2, when I use git clone https://github.com/Unity-Technologies/ml-agents.git, I think it will automatically download ml-agents==0.10.0, what is the write git clone for version 0.9.2?",hi thank information however try downgrade use git clone think automatically write git clone version,issue,negative,neutral,neutral,neutral,neutral,neutral
539623006,"@chriselion I think I was using tab instead of space in the trainer_config.yaml. Maybe if you could make a comment in the file, next time the user will know that they should use space in the yaml file",think tab instead space maybe could make comment file next time user know use space file,issue,negative,neutral,neutral,neutral,neutral,neutral
539615314,"Oh sorry, I should have mentioned that it was added for the 0.10 release.",oh sorry added release,issue,negative,negative,negative,negative,negative,negative
539612354,Glad you got it working. Can I ask what the problem was? Anything we can to do make it easier to detect and fix in the future?,glad got working ask problem anything make easier detect fix future,issue,negative,positive,positive,positive,positive,positive
539570456,"moving from 
`from mlagents.envs import UnityEnvironment`
to
`from mlagents.envs.environment import UnityEnvironment`
worked for me, on Windows, using Python 3.6.9
Thx (Might want to fiw the notebooks ""geting-started"")",moving import import worked python might want,issue,negative,neutral,neutral,neutral,neutral,neutral
539536193,"@ashwalcs could you please share code how are you calling Barracuda worker? 
P.S. If you have to run several concurrent executions then you have to create multiple workers.",could please share code calling barracuda worker run several concurrent create multiple,issue,positive,neutral,neutral,neutral,neutral,neutral
539535103,"@Unknown-Technologies unfortunately this graph include too many nodes that we do not support. It seems that during the freezing you requested to save all the nodes:
```
target_nodes = "","".join([node.name for node in model.sess.graph.as_graph_def().node])
```
could you try freezing graph only with output nodes that are actually needed as output of your model?",unfortunately graph include many support freezing save node could try freezing graph output actually output model,issue,negative,neutral,neutral,neutral,neutral,neutral
539457125,"**Editor Environment** : 
- Unity 2019.1.14f - Windows 10 - 16Gb Ram.

**Testing Environment** : 
- One Plus 7 Pro - Android 9 Pie - 8Gb Ram
- Moto G6 - Android 9 Pie - 3Gb Ram.
                                      
This the Neural Network file - trained for 2 Objects : https://drive.google.com/file/d/1oDPiWjUgvLylk7nIZWK9s5noGThDXjKL/view?usp=sharing

@surfnerd Yes i am disposing the data structures properly and also disposing the worker after every scan. Basically it crashes after every 3 concurrent detections/ scan.

",editor environment unity ram testing environment one plus pro android pie ram android pie ram neural network file trained yes data properly also worker every scan basically every concurrent scan,issue,negative,neutral,neutral,neutral,neutral,neutral
539394718,"Oh.. I checked `--env-args` option is newly updated after 0.10.0
Thank you. I will add a comment, if there any problem after version up!",oh checked option newly thank add comment problem version,issue,negative,positive,positive,positive,positive,positive
539387642,"And I use ml-agents version 0.9.3
Is it still possible to use `--env-args` option on my version?",use version still possible use option version,issue,negative,neutral,neutral,neutral,neutral,neutral
539381754,"OH!!! @chriselion Thanks a lot!!!
But I can't find `--env-args` option in ML Agent official documents... where I can find the
more detail option like `--env-args`?

+) I tried the command you showed above, but it didn't work. could you check it again? 
I though that it should contain ""="" right after `--env-args` 
so i change `--env-args 1` to `--env-args=1` and `--env-args={1}`
but all above is not working... :( 
Please give your advice.",oh thanks lot ca find option agent official find detail option like tried command work could check though contain right change working please give advice,issue,positive,positive,positive,positive,positive,positive
539366330,"If you pass `--env-args`, everything after that will be passed to the environment (https://github.com/Unity-Technologies/ml-agents/blob/a7c1fccd9a86045dece3206604fa93064aac58ea/docs/Training-ML-Agents.md#command-line-training-options)

So in your example,  `mlagents-learn config/trainer_config.yaml --run-id=argTest --train --env=C:\simpleBattleEnvironment\simpleBattle.exe --env-args 1` should do what you need.",pas everything environment example train need,issue,negative,neutral,neutral,neutral,neutral,neutral
539366065,If she is using windows just get her to create a new conda environment with the latest ml-agents installed. Or you can downgrade by removing ml-agents and installing an earlier version of ml-agents from pip: https://pypi.org/project/mlagents/#history,get create new environment latest downgrade removing version pip,issue,negative,positive,positive,positive,positive,positive
539325064,"Thanks a lot Chris for the solution , it works.
Closing issue.
 ",thanks lot solution work issue,issue,positive,positive,positive,positive,positive,positive
539286845,"For the ones who has same issue, please put --train in the end.",issue please put train end,issue,negative,neutral,neutral,neutral,neutral,neutral
539149447,"Thank you @ervteng, I will develop my game further and when I get to a point where I can implement that I will try your solution and contact you if necessary.",thank develop game get point implement try solution contact necessary,issue,positive,negative,negative,negative,negative,negative
539137747,"We don't currently support this, though it should be possible using Soft Actor-Critic. You can save the agent's interactions using the demonstration recorder (Modify DemonstrationRecorder.cs and remove all mention of `Application.isEditor`, and your game build will record `.demo` files) - then, what you'd have to do is load these interactions into the training buffer during training. 

If you'd like to give it a go I can provide some pointers to where in the code you could do this. I started something similar on the `develop-humanil` branch but it's pretty out of date. ",currently support though possible soft save agent demonstration recorder modify remove mention game build record load training buffer training like give go provide code could something similar branch pretty date,issue,positive,negative,neutral,neutral,negative,negative
539136207,@vincentpierre fixes are in the latest commit aae9487d.  Let me know if those work for you.,latest commit let know work,issue,negative,positive,positive,positive,positive,positive
539135493,"Hi @ervteng , more like a PC game. Would it be possible to somehow save the iterations with players and send it to an online server, where the result would be processed by ml and the agent could be updated this way?",hi like game would possible somehow save send server result would agent could way,issue,positive,negative,negative,negative,negative,negative
539119785,"I tested this on Gridworld, so it had 1 80x80x3 visual obs and no other float obs. When running for 20000 steps:
```
Gridworld, 20000 steps
""old"" visual observations
INFO:mlagents.trainers: gridhack-off: GridWorldLearning: Step: 20000. Time Elapsed: 171.397 s Mean Reward: -0.725. Std of Reward: 0.817. Training.

""float"" visual observations
INFO:mlagents.trainers: gridhack-on: GridWorldLearning: Step: 20000. Time Elapsed: 321.175 s Mean Reward: -0.830. Std of Reward: 0.578. Training.
```

Timers (from a shorter run) as also added here.

I ran these commands to actually do the test runs (and make json readable):
```
mlagents-learn config/trainer_config.yaml --run-id=gridhack-off --train --env=./UnitySDK/gridhack --env-args --hack-vis-obs false --timer-path ./gridhack-off_timers.json && python -mjson.tool UnitySDK/gridhack-off_timers.json UnitySDK/gridhack-off_timers.pretty.json
mlagents-learn config/trainer_config.yaml --run-id=gridhack-on --train --env=./UnitySDK/gridhack --env-args --hack-vis-obs true --timer-path ./gridhack-on_timers.json && python -mjson.tool UnitySDK/gridhack-on_timers.json UnitySDK/gridhack-on_timers.pretty.json 

```",tested visual float running old visual step time mean reward reward training float visual step time mean reward reward training shorter run also added ran actually test make readable train false python train true python,issue,positive,negative,neutral,neutral,negative,negative
539112043,"We don't currently have native support for federated learning, but since we're using Tensorflow under the hood, you could look at existing frameworks such as TF distributed or Horovod to achieve this. ",currently native support learning since hood could look distributed achieve,issue,negative,neutral,neutral,neutral,neutral,neutral
539111920,"Thanks @chriselion. 

I have integrated the latest develop with those changes, however the behaviour re-occurs. 

This time, I noticed that all the UnityEnvWorker child processes were running, as well as *all* actual environment processes on the other machine, but when I terminated the training, the UnityEnvWorker processes exited but the environment processes continued even though the shutdown messages were supposedly sent. 

This suggests the connection between one or more of the worker child processes and the environments has been lost, perhaps.

Is such a thing possible? The RpcCommunicator created by the UnityEnvironment created by the env_factory uses grpc I see, but I don't know much about this (e.g. what protocol it uses underneath).

EDIT: Looks like its over http2, which is over tcp. I will create a physical training machine on the local network to try it, as it may be an AWS specific problem. Also, for record, I am initiating training from an RDP session on a Win10 machine - closing the rdp session may do something to the rcp servers created within it, though its never been a problem with local environment processes.",thanks latest develop however behaviour time child running well actual environment machine training environment continued even though shutdown supposedly sent connection one worker child lost perhaps thing possible see know much protocol underneath edit like create physical training machine local network try may specific problem also record training session win machine session may something within though never problem local environment,issue,positive,positive,positive,positive,positive,positive
539111748,"Shutdown works on develop but not on this branch
walljump is a different issue that I think is due to multi-brain training",shutdown work develop branch different issue think due training,issue,negative,negative,neutral,neutral,negative,negative
539111206,You can put a different demo file under each brain name in the config file. The config file is organized by brain name (e.g. 3DBallLearning) and you can put two entries with different names and options. ,put different file brain name file file organized brain name put two different,issue,negative,neutral,neutral,neutral,neutral,neutral
539109211,"Thank you @ervteng for replying. So, if I give agents different brains, how can I map each demonstration to a brain in the config file?",thank give different brain map demonstration brain file,issue,negative,neutral,neutral,neutral,neutral,neutral
539107900,"Hi @kamicazer, do you want to do training in a deployed game (e.g. on a phone)? If so, that isn't currently supported.

You can interact with the game during training on your local machine, however, just as you would any Unity game. ",hi want training game phone currently interact game training local machine however would unity game,issue,negative,negative,negative,negative,negative,negative
539107252,"Hi @taesiri, thank you for sharing! This will be useful for others for sure. @unityjeffrey perhaps we can look into linking the tutorial in some way?",hi thank useful sure perhaps look linking tutorial way,issue,positive,positive,positive,positive,positive,positive
539106441,"You can assign different Brains for the different levels of the hierarchy. However, there's no special treatment for HRL (all brains will train at the same time) in the training, so you'll have to be clever about it to get it to train stably. ",assign different brain different hierarchy however special treatment brain train time training clever get train stably,issue,positive,positive,positive,positive,positive,positive
539105900,"It really depends on your setup. Generally GPU is faster, but for small networks you'll see little improvement. For large networks (e.g. visual observations, lots of vector obs) GPU will give a big gain, and especially on SAC. ",really setup generally faster small see little improvement large visual lot vector give big gain especially sac,issue,positive,positive,neutral,neutral,positive,positive
539105345,"Hi @AdhamAlHarazi, if the agents share the same brain, then they learn the same thing regardless of how many agents are there (they behave the same way). So it doesn't make sense to map demonstrations to multiple agents with the same brain. You can put multiple demo files in a directory and pass a directory in your trainer_config.yaml if you want to use multiple recordings. 

you can give the agents different brains if you want them to learn different behaviors. ",hi share brain learn thing regardless many behave way make sense map multiple brain put multiple directory pas directory want use multiple give different brain want learn different,issue,negative,positive,neutral,neutral,positive,positive
539105310,"> It seems walljump is broken
> The editor does no longer exit playmode when ctrl-C on the terminal


@vincentpierre 
That could be related to #2620.  I didn't change any of the shutdown code related to this.  Is the wall jump issue separate from the ctrl-C issue?",broken editor longer exit terminal could related change shutdown code related wall jump issue separate issue,issue,negative,negative,negative,negative,negative,negative
539072864,"Hi @graybob,
It looks like you need to upgrade your UnitySDK folder in your unity project based on this line:
```
Getting Error -> mlagents.envs.exception.UnityEnvironmentException: The API number is not compatible between Unity and python. Python API : API-10, Unity API : API-9.
```

Please replace your UnitySDK folder with the one in the 0.10.0 release.  This should fix your issue. ",hi like need upgrade folder unity project based line getting error number compatible unity python python unity please replace folder one release fix issue,issue,negative,neutral,neutral,neutral,neutral,neutral
538811382,"I also get NullReferenceException: Object reference not set to an instance of an object when trying to run the hallway example.
",also get object reference set instance object trying run hallway example,issue,negative,neutral,neutral,neutral,neutral,neutral
538754080,Thanks for sorting it out! I'll check out the `develop` branch and consider using the proposed method. ,thanks check develop branch consider method,issue,negative,positive,positive,positive,positive,positive
538572030,"As @smshehryar suggested, this usually means something in your config file changed, or something in the environment changed, between the initial training and the load. ",usually something file something environment initial training load,issue,negative,negative,negative,negative,negative,negative
538571702,"This issue has been fixed in our `develop` branch - closing the issue for now. Thanks for reporting it!

P.S. We do consider BC a ""legacy"" feature that will be deprecated soon. PPO+GAIL and PreTraining can achieve the same results - and we suggest using that moving forward. ",issue fixed develop branch issue thanks consider legacy feature soon pretraining achieve suggest moving forward,issue,negative,positive,positive,positive,positive,positive
538557092,"@vincentpierre Going to merge this now. Agreed it doesn't feel like it belongs with the  ""core"" ML-Agents code, but we can easily move it later if you have a suggestion of where it should go.",going merge agreed feel like core code easily move later suggestion go,issue,positive,positive,positive,positive,positive,positive
538554535,I'd really like to get #2675 before this as there were already a number of merge conflicts from #2669 in that PR. ,really like get already number merge,issue,negative,positive,positive,positive,positive,positive
538552531,"Hi @ashwalcs,
Are you sure that you are disposing off your data structures properly when the scene is closed?  Can you share your environment with us?",hi sure data properly scene closed share environment u,issue,positive,positive,positive,positive,positive,positive
538548150,Did you try the link I sent? Can you attach (don't copy and paste because it loses whitespace) your yaml file here?,try link sent attach copy paste file,issue,negative,neutral,neutral,neutral,neutral,neutral
538546404,for this error seems like you need to check your trainer_config.yaml file for any problems with unfimiliar syntax or format. ,error like need check file syntax format,issue,negative,neutral,neutral,neutral,neutral,neutral
538544074,"@chriselion Hi,
I tried another demo and it still has the same problem:

```
INFO:mlagents.trainers:CommandLineOptions(debug=False, num_runs=1, seed=-1, env_path=None, run_id='pig_tutorial_01', load_model=False, train_model=True, save_freq=50000, keep_checkpoints=5, base_port=5005, num_envs=1, curriculum_folder='config/curricula/pig/', lesson=0, slow=False, no_graphics=False, multi_gpu=False, trainer_config_path='config/trainer_config.yaml', sampler_file_path=None, docker_target_name=None, env_args=None)
Traceback (most recent call last):
  File ""C:\Users\chenchet\AppData\Local\Continuum\anaconda3\envs\ml-agents\Scripts\mlagents-learn-script.py"", line 11, in <module>
    load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')()
  File ""c:\users\chenchet\ml-agents\ml-agents\mlagents\trainers\learn.py"", line 417, in main
    run_training(0, run_seed, options, Queue())
  File ""c:\users\chenchet\ml-agents\ml-agents\mlagents\trainers\learn.py"", line 205, in run_training
    trainer_config = load_config(trainer_config_path)
  File ""c:\users\chenchet\ml-agents\ml-agents\mlagents\trainers\trainer_util.py"", line 117, in load_config
    return _load_config(data_file)
  File ""c:\users\chenchet\ml-agents\ml-agents\mlagents\trainers\trainer_util.py"", line 134, in _load_config
    return yaml.safe_load(fp)
  File ""C:\Users\chenchet\AppData\Local\Continuum\anaconda3\envs\ml-agents\lib\site-packages\yaml\__init__.py"", line 162, in safe_load
    return load(stream, SafeLoader)
  File ""C:\Users\chenchet\AppData\Local\Continuum\anaconda3\envs\ml-agents\lib\site-packages\yaml\__init__.py"", line 114, in load
    return loader.get_single_data()
  File ""C:\Users\chenchet\AppData\Local\Continuum\anaconda3\envs\ml-agents\lib\site-packages\yaml\constructor.py"", line 41, in get_single_data
    node = self.get_single_node()
  File ""C:\Users\chenchet\AppData\Local\Continuum\anaconda3\envs\ml-agents\lib\site-packages\yaml\composer.py"", line 36, in get_single_node
    document = self.compose_document()
  File ""C:\Users\chenchet\AppData\Local\Continuum\anaconda3\envs\ml-agents\lib\site-packages\yaml\composer.py"", line 55, in compose_document
    node = self.compose_node(None, None)
  File ""C:\Users\chenchet\AppData\Local\Continuum\anaconda3\envs\ml-agents\lib\site-packages\yaml\composer.py"", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
  File ""C:\Users\chenchet\AppData\Local\Continuum\anaconda3\envs\ml-agents\lib\site-packages\yaml\composer.py"", line 133, in compose_mapping_node
    item_value = self.compose_node(node, item_key)
  File ""C:\Users\chenchet\AppData\Local\Continuum\anaconda3\envs\ml-agents\lib\site-packages\yaml\composer.py"", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
  File ""C:\Users\chenchet\AppData\Local\Continuum\anaconda3\envs\ml-agents\lib\site-packages\yaml\composer.py"", line 127, in compose_mapping_node
    while not self.check_event(MappingEndEvent):
  File ""C:\Users\chenchet\AppData\Local\Continuum\anaconda3\envs\ml-agents\lib\site-packages\yaml\parser.py"", line 98, in check_event
    self.current_event = self.state()
  File ""C:\Users\chenchet\AppData\Local\Continuum\anaconda3\envs\ml-agents\lib\site-packages\yaml\parser.py"", line 428, in parse_block_mapping_key
    if self.check_token(KeyToken):
  File ""C:\Users\chenchet\AppData\Local\Continuum\anaconda3\envs\ml-agents\lib\site-packages\yaml\scanner.py"", line 116, in check_token
    self.fetch_more_tokens()
  File ""C:\Users\chenchet\AppData\Local\Continuum\anaconda3\envs\ml-agents\lib\site-packages\yaml\scanner.py"", line 260, in fetch_more_tokens
    self.get_mark())
yaml.scanner.ScannerError: while scanning for the next token
found character '\t' that cannot start any token
  in ""config/trainer_config.yaml"", line 292, column 1
```",hi tried another still problem recent call last file line module file line main queue file line file line return file line return file line return load stream file line load return file line node file line document file line node none none file line node anchor file line node file line node anchor file line file line file line file line file line scanning next token found character start token line column,issue,negative,positive,neutral,neutral,positive,positive
538525199,did you go through the installation of the mlagents properly and check that if it is installed? ,go installation properly check,issue,negative,neutral,neutral,neutral,neutral,neutral
538524402,is the run id correct? and what about the environment?,run id correct environment,issue,negative,neutral,neutral,neutral,neutral,neutral
538456347,"The finalizer doesn’t run on the main thread, so it looks like that’s why Unity isn’t happy.  Thanks for the extra info!  Very helpful. ",run main thread like unity happy thanks extra helpful,issue,positive,positive,positive,positive,positive,positive
538401318,"@LeSphax 

We are still quite interested in the contribution! We are planning on taking a look at is as part of a wider look at multi-agent, which we will be taking soon.",still quite interested contribution taking look part look taking soon,issue,negative,positive,positive,positive,positive,positive
538345359,"Hello everyone, 

Glad to see this PR was useful to you, I agree your environments look really cool :)

I have got time to work on this again in the next few weeks.
So I am planning to update the PR to the latest develop branch and then look into the multi-agents team problem.
I think something based on elo rating could work, since competitive video games often use that.
I am not sure if trueskill would be better. It feels like it just adds a parameter to tell us how uncertain we are about a player skill level. Since we don't regularly add agents with unknown skill level into the game, I feel like this uncertainty is always the same.

@AcelisWeaven You mentioned removing the rating. At the moment the rating doesn't affect behavior at all, it's just there to indicate if the agents are improving or not. You should be able to disable it by removing use_elo_rating: true from the brain config.
I would guess that openAI used a rating system there as well even if they mostly talk about the behaviors. I don't really see a better indicator of skill in a zero sum game.
Though adding a way to track specific behaviors with heuristics would be helpful instead of just watching the agents play. For example, you could imagine a tensorboard graph telling you how often the items are picked up in your game.
Maybe it's possible to do that currently with custom protobuf messages.

@awjuliani It would be helpful to hear your thoughts on this feature as well. Do you feel like this is something that could be integrated into the repo? What improvements would we need to do that?",hello everyone glad see useful agree look really cool got time work next update latest develop branch look team problem think something based rating could work since competitive video often use sure would better like parameter tell u uncertain player skill level since regularly add unknown skill level game feel like uncertainty always removing rating moment rating affect behavior indicate improving able disable removing true brain would guess used rating system well even mostly talk really see better indicator skill zero sum game though way track specific would helpful instead watching play example could imagine graph telling often picked game maybe possible currently custom would helpful hear feature well feel like something could would need,issue,positive,positive,positive,positive,positive,positive
538328137,"Hey @LeSphax !

I've been running it for 24h+ and everything works flawlessly.

Thanks!

Edit: The longer sessions allowed me to find some issues in my environment, and since I fixed those, the training process is a lot better. I still have a little issue after 48h+ training (without crash!), were nothing happen anymore. It may be coming from my environment so I'll take a closer look. Edit 2: Definitely come from my environment (the ball can sometime be stuck ""inside"" the wall).",hey running everything work flawlessly thanks edit longer session find environment since fixed training process lot better still little issue training without crash nothing happen may coming environment take closer look edit definitely come environment ball sometime stuck inside wall,issue,positive,positive,positive,positive,positive,positive
538297664,"@batu @ervteng Thanks a lot for your feedback!

Would you be willing to show us an example you've made, @batu ?",thanks lot feedback would willing show u example made,issue,negative,positive,positive,positive,positive,positive
538273484,"Hi @chriselion ,
This is the callstack :
```
UnityException: DestroyBuffer can only be called from the main thread.
10-04 12:25:04.783 4320 4416 E Unity : Constructors and field initializers will be executed from the loading thread when loading a scene.
10-04 12:25:04.783 4320 4416 E Unity : Don't use this function in the constructor or field initializers, instead move initialization code to the Awake or Start function.
10-04 12:25:04.783 4320 4416 E Unity : at UnityEngine.ComputeBuffer.Dispose (System.Boolean disposing) [0x00000] in <00000000000000000000000000000000>:0
10-04 12:25:04.783 4320 4416 E Unity : at UnityEngine.ComputeBuffer.Dispose () [0x00000] in <00000000000000000000000000000000>:0
10-04 12:25:04.783 4320 4416 E Unity : at Barracuda.ComputeTensorData.Dispose () [0x00000] in <00000000000000000000000000000000>:0
10-04 12:25:04.783 4320 4416 E Unity : at Barracuda.Tensor.Dispose () [0x00000] in <00000000000000000000000000000000>:0
10-04 12:25:04.783 4320 4416 E Unity : at Barracuda.Tensor.Finalize () [0x00000] in <00000000000000000000000000000000>:0
10-04 12:25:04.783 4320 4416 E Unity : UnityEngine.DebugLogHandler:LogException(Exception, Object)
10-04 12:25:04.783 4320 4416 E Unity : UnityEngine.Logger:LogException(Exception, Object)
10-04 12:25:04.783 4320 4416 E Unity : UnityEngine.Debug:LogException(Exception)
10-04 12:25:04.783 4320 4416 E Unity : UnityEngine.UnhandledExceptionHandler:<RegisterUECatcher>m__0(Object, UnhandledExceptionEventAr
10-04 12:25:04.785 4320 4416 E Unity : UnityException: GetStackTraceLogType can only be called from the main thread.10-04 12:25:04.785 4320 4416 E Unity : Constructors and field initializers will be executed from the loading thread when loading a scene.
10-04 12:25:04.785 4320 4416 E Unity : Don't use this function in the constructor or field initializers, instead move initialization code to the Awake or Start function.
10-04 12:25:04.785 4320 4416 E Unity : at Barracuda.D.LogWarning (System.Object message) [0x00000] in <00000000000000000000000000000000>:0
10-04 12:25:04.785 4320 4416 E Unity : at Barracuda.ComputeTensorData.Finalize () [0x00000] in <00000000000000000000000000000000>:0
10-04 12:25:04.785 4320 4416 E Unity : UnityEngine.DebugLogHandler:LogException(Exception, Object)
10-04 12:25:04.785 4320 4416 E Unity : UnityEngine.Logger:LogException(Exception, Object)
10-04 12:25:04.785 4320 4416 E Unity : UnityEngine.Debug:LogException(Exception)
10-04 12:25:04.785 4320 4416 E Unity : UnityEngine.UnhandledExceptionHandler:<RegisterUECatcher>m__0(Object, UnhandledExceptionEventArgs)
10-04 12:25:04.785 4320 4416 E Unity : System.UnhandledExceptionEventHandler:Invoke(Object, UnhandledExceptionEventArgs)
10-04 12:25:04.785 4320 4416 E Unity :
10-04 12:25:04.785 4320 4416 E Unity : (Filename: currently not available on il2cpp Line: -1)
10-04 12:25:04.785 4320 4416 E Unity :
10-04 12:25:04.787 4320 4416 W Unity : GarbageCollector disposing of ComputeBuffer. Please use ComputeBuffer.Release() or .Dispose() to manually release the buffer.
10-04 12:25:04.787 4320 4416 W Unity : UnityEngine.DebugLogHandler:LogFormat(LogType, Object, String, Object[])
10-04 12:25:04.787 4320 4416 W Unity : UnityEngine.Logger:Log(LogType, Object)
10-04 12:25:04.787 4320 4416 W Unity : UnityEngine.Debug:LogWarning(Object)
10-04 12:25:04.787 4320 4416 W Unity : UnityEngine.ComputeBuffer:Dispose(Boolean)
10-04 12:25:04.787 4320 4416 W Unity : UnityEngine.ComputeBuffer:Finalize()
10-04 12:25:04.787 4320 4416 W Unity :
10-04 12:25:04.787 4320 4416 W Unity : (Filename: ./Runtime/Export/Debug/Debug.bindings.h Line: 48)
10-04 12:25:04.787 4320 4416 W Unity :
```",hi main thread unity field executed loading thread loading scene unity use function constructor field instead move code awake start function unity unity unity unity unity unity exception object unity exception object unity exception unity object unity main unity field executed loading thread loading scene unity use function constructor field instead move code awake start function unity message unity unity exception object unity exception object unity exception unity object unity invoke object unity unity currently available line unity unity please use manually release buffer unity object string object unity log object unity object unity dispose unity finalize unity unity line unity,issue,negative,positive,positive,positive,positive,positive
538185574,"Hi @hanseoup, decision interval has nothing to do with masked actions, or at least it shouldn't.

If only the loaded model gives masked actions, then it is indeed a bug. ",hi decision interval nothing masked least loaded model masked indeed bug,issue,negative,negative,negative,negative,negative,negative
538181809,"![image](https://user-images.githubusercontent.com/53593292/66173162-e6499200-e689-11e9-97a2-043121faee24.png)
I used above command, and it seems to mask action correctly. 
Then, is it fine that loaded model(.nn file) give masked action to agent?
Eventually agent will be given only unmasked action regardless of its ""decision Interval""?",image used command mask action correctly fine loaded model file give masked action agent eventually agent given unmasked action regardless decision interval,issue,negative,positive,positive,positive,positive,positive
538180648,"@wardVD 
You can consider using HER with the Soft-Actor Critic. As ervteng said, it is very environment-specific, yet, in certain navigation tasks, I had significant performance increases with my HER implementation. One benefit is that it reduces the dependence on reward-shaping.",consider critic said yet certain navigation significant performance implementation one benefit dependence,issue,positive,positive,positive,positive,positive,positive
538132919,"> ah i see. makes sense. would it be suffice to say that we upgrade to TF 2.0 now (but do not make it 2.0 ready). But this will enable us later in the future to take on more work to make things cleaner and more modular (when we are ready)?

Yes",ah see sense would suffice say upgrade make ready enable u later future take work make cleaner modular ready yes,issue,positive,positive,neutral,neutral,positive,positive
538128536,ah i see.  makes sense.  would it be suffice to say that we upgrade to TF 2.0 now (but do not make it 2.0 ready).  But this will enable us later in the future to take on more work to make things cleaner and more modular (when we are ready)?,ah see sense would suffice say upgrade make ready enable u later future take work make cleaner modular ready,issue,positive,positive,neutral,neutral,positive,positive
538119183,"This bug has been fixed in the `develop` branch of ML-Agents, and will be fixed in the next release. Thanks for reporting - closing the issue for now. ",bug fixed develop branch fixed next release thanks issue,issue,negative,positive,neutral,neutral,positive,positive
538115146,glad it worked for you @kamicazer! I'm going to close this issue.  Please reopen it if you run into this issue again.,glad worked going close issue please reopen run issue,issue,positive,positive,positive,positive,positive,positive
538111920,"spot on @surfnerd , that did the trick for me, sorry about taking up time. Have a nice one.",spot trick sorry taking time nice one,issue,negative,positive,neutral,neutral,positive,positive
538094265,"`The API number is not compatible between Unity and python. Python API : API-10, Unity API`
Looks like you may need to update your UnitySDK.",number compatible unity python python unity like may need update,issue,negative,neutral,neutral,neutral,neutral,neutral
538082250,"Hi @ervteng , thank you for taking the time to answer.

Yes, I believe so
![image](https://user-images.githubusercontent.com/12896365/66156152-0856fc00-e619-11e9-814f-a885245b7d90.png)

this is the Ball3DAcademy gameobject inside the 3DBall scene.
",hi thank taking time answer yes believe image inside scene,issue,positive,neutral,neutral,neutral,neutral,neutral
538061332,This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.,issue automatically marked stale recent activity closed activity thank,issue,negative,negative,negative,negative,negative,negative
538052588,I've logged this bug in our internal tracking system as ID MLA-94.  We will update this issue as we gather more information.  Thank you for reporting this!,logged bug internal system id update issue gather information thank,issue,negative,neutral,neutral,neutral,neutral,neutral
538048451,Thank you for the quick reply @ervteng! I'll look into doing the PPO+LSTM+GAIL for now!  ,thank quick reply look,issue,negative,positive,positive,positive,positive,positive
538047531,"@unityjeffrey IMO TF 2.0 will make our trainer codebase a lot cleaner and more modular. However, as @chriselion says, it will completely break 1.0 compatibility, and it will take a substantial effort to change over. It's probably worth the effort, but there's no immediate need right now. ",make trainer lot cleaner modular however completely break compatibility take substantial effort change probably worth effort immediate need right,issue,positive,positive,positive,positive,positive,positive
538046451,"Hi @hanseoup, does the agent still take masked actions when running in Python? (e.g. running mlagents-learn without the `--train` flag). ",hi agent still take masked running python running without train flag,issue,negative,neutral,neutral,neutral,neutral,neutral
538045861,"Yes, that looks fine! We've actually been able to reproduce your issue and are looking into it - it's been logged with internal tracking number MLA-93. 

In the meantime, you can achieve something similar by using PPO+LSTM+GAIL. It will train slower but should achieve a better result. See: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Imitation-Learning.md",yes fine actually able reproduce issue looking logged internal number achieve something similar train achieve better result see,issue,positive,positive,positive,positive,positive,positive
538042825,"> Hi @ervteng! I've cloned the repo and installed the packages using:
> 
> ```
> cd ml-agents-envs
> pip install -e .
> cd ..
> cd ml-agents
> pip install -e .
> ```
> 
> Is this what you mean?

I've also created another virtual environment and followed [the steps from here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation-Windows.md) . Unfortunately I still get that error. ",hi pip install pip install mean also another virtual environment unfortunately still get error,issue,negative,negative,negative,negative,negative,negative
538038784,"Hi @ervteng! I've cloned the repo and installed the packages using: 
```
cd ml-agents-envs
pip install -e .
cd ..
cd ml-agents
pip install -e .
```

Is this what you mean? ",hi pip install pip install mean,issue,negative,negative,negative,negative,negative,negative
538038126,"Yeah, sorry, it was changed on the develop branch a few weeks ago, but the first published package that has the change is 0.10.0.",yeah sorry develop branch ago first package change,issue,negative,negative,negative,negative,negative,negative
538035694,"Hi @cristinadobre, we fixed this bug back in 0.9.1. Have you upgraded the Python package to the latest version?",hi fixed bug back python package latest version,issue,negative,positive,positive,positive,positive,positive
538033731,"HI @wardVD, HER is definitely on our radar. The main change you'll have to make is that you need to output a goal state as well as a reward from your Unity environment. This isn't very generalizable as in many cases the goal state isn't defined or is a set of states (e.g. the agent should be close to an item). Welcome to give it a go and submit a PR!",hi definitely radar main change make need output goal state well reward unity environment generalizable many goal state defined set agent close item welcome give go submit,issue,positive,positive,positive,positive,positive,positive
538025252,Unable to change the target branch due to a bug on Github. Opening a separate PR targeting develop : #2669 ,unable change target branch due bug opening separate develop,issue,negative,negative,negative,negative,negative,negative
538013473,"Hi @luizbalestreri,
We renamed the Communicator.cs file to ICommunicator.cs to be more closely aligned with Unity's coding conventions.  If you've copied the UnitySDK folder over, and didn't delete the old files this will happen.  Please delete the old UnitySDK folder and then copy the new one over.  This should solve your issue.",hi file closely unity copied folder delete old happen please delete old folder copy new one solve issue,issue,negative,positive,positive,positive,positive,positive
537971839,"Ok I see, that requirement was changed in 0.10.0, 3 days ago? For 0.9.3 it says
`dist=tensorflow<1.8,>=1.7 parent=mlagents==0.9.3`",see requirement day ago,issue,negative,neutral,neutral,neutral,neutral,neutral
537955050,"@simonbogh 
We currently require `tensorflow>=1.7,<2.0`
https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/setup.py#L43

We're working on supporting tensorflow 2.0.0 now that it's available.",currently require working supporting available,issue,negative,positive,positive,positive,positive,positive
537915922,"@chriselion That is really great to hear.
Now we might be actually able to use this with students.",really great hear might actually able use,issue,positive,positive,positive,positive,positive,positive
537817924,"Hi @chriselion ,

I am happy to hear that, because it is quite annoying to get your TF package downgraded when installing mlagents. When you say relaxes the restrictions on TF, is it still for a specific version or just any version?",hi happy hear quite annoying get package say still specific version version,issue,negative,neutral,neutral,neutral,neutral,neutral
537806462,"@unityjeffrey - correct, the goal of this PR is not to take advantage of any 2.0 features. Currently if someone has tensorflow==2.0.0 installed, ml-agents will break (and it will complain at `pip install` time first). After this PR, we'll be compatible with 2.0.0

As for ""2.0 native"", I'll have to let other tf experts chime in. I doubt that there's a way to do that without breaking 1.x compat, so we'll have to decide if/when the new features are compelling enough to drop support for users on the old version.",correct goal take advantage currently someone break complain pip install time first compatible native let chime doubt way without breaking decide new compelling enough drop support old version,issue,negative,positive,positive,positive,positive,positive
537789098,"Hi @chriselion, had a question.  I believe 2.0 does offer some backward compatibility to 1.x versions.  However, my understanding is, without making the traininer code 2.0 native, you don't get the major advantages of 2.0.  With that, when we say ""2.0.0 support"", does that mean we are going to (or need to) also make our code 2.0 native?",hi question believe offer backward compatibility however understanding without making code native get major say support mean going need also make code native,issue,negative,negative,negative,negative,negative,negative
537728120,Thanks for the explanation. Everything makes sense now.,thanks explanation everything sense,issue,negative,positive,positive,positive,positive,positive
537724043,"Hi @kamicazer, have you added your Learning Brain into the Broadcast Hub (in Unity, under the Academy) and checked the Control checkbox? This occasionally happens if that's not checked. ",hi added learning brain broadcast hub unity academy checked control occasionally checked,issue,negative,neutral,neutral,neutral,neutral,neutral
537713319,"First let's talk about the format of the demo files. The relevant source for writing them is here:
https://github.com/Unity-Technologies/ml-agents/blob/8f4e6038c5e548061601f2323a3951a71c9ca2b7/UnitySDK/Assets/ML-Agents/Scripts/DemonstrationStore.cs#L58-L104

and for reading them at training time:

https://github.com/Unity-Technologies/ml-agents/blob/8f4e6038c5e548061601f2323a3951a71c9ca2b7/ml-agents/mlagents/trainers/demo_loader.py#L105-L124

So the format is
1) a metadata protobuf
2) the BrainParameters protobuf
3) several AgentInfo protobufs

The AgentInfo protobufs contain the observations that the agent made when the demo was recorded. So if these observations include the speed and orientation (or anything else) they will be accounted for in GAIL and BC.",first let talk format relevant source writing reading training time format several contain agent made include speed orientation anything else,issue,negative,positive,positive,positive,positive,positive
537711636,"Hi,
I think it's a reasonable request for something we should provide (with the caveats that it might perform sub-optimally). Besides the issue I linked to before, it has come up a few other times in the past.

I added the request to our internal tracker with the ID MLA-88. I’ll ping back with any updates (but no timeline yet)...",hi think reasonable request something provide might perform besides issue linked come time past added request internal tracker id ping back yet,issue,negative,negative,neutral,neutral,negative,negative
537706769,"I believe same process will take place while using GAIL approach for imitation learning. Also, does the factors like speed and orientation of agent are also learned using .demo files?
 If yes, then how does the input of factors like speed and orientation is extracted from .demo files?",believe process take place approach imitation learning also like speed orientation agent also learned yes input like speed orientation extracted,issue,positive,negative,negative,negative,negative,negative
537698850,"Hi @AcelisWeaven,
This issue has been fixed by #2662, which was just merged to the develop branch.  I am closing this issue, please reopen it if you still run into problems with code stripping in the future. ",hi issue fixed develop branch issue please reopen still run code stripping future,issue,negative,positive,neutral,neutral,positive,positive
537691048,@mmattar - looks like your requested change has been addressed.  can you approve and get it shipped?,like change approve get shipped,issue,negative,neutral,neutral,neutral,neutral,neutral
537688052,"I've added a request to update the documentation around this to our internal tracker with the ID MLA-18. I’m going to close this issue for now, but we’ll ping back with any updates",added request update documentation around internal tracker id going close issue ping back,issue,negative,neutral,neutral,neutral,neutral,neutral
537670798,"Thanks for submitting this request. I’ve added it to our internal tracker with the ID MLA-86. I’m going to close this issue for now, but we’ll ping back with any updates",thanks request added internal tracker id going close issue ping back,issue,negative,positive,neutral,neutral,positive,positive
537630735,"Hi @chriselion  
If I am saying it correctly the trainer goes over demonstration buffer again and again. Updates itself of the action that are taking place , creates mini batches to train the neural net and calculates the policy loss.
Kindly correct me if I am wrong",hi saying correctly trainer go demonstration buffer action taking place train neural net policy loss kindly correct wrong,issue,negative,positive,neutral,neutral,positive,positive
537625928,"Hi @DooblyNoobly, this is because your batch size is not a multiple of and greater than your sequence length. I've logged the bug with internal tracking id MLA-85, and will keep you posted. In 0.9, it would have added a batch of size `sequence_length` regardless of your `batch_size`. 

In the meantime, if you increase your batch size to larger than the sequence length, it should continue to train. ",hi batch size multiple greater sequence length logged bug internal id keep posted would added batch size regardless increase batch size sequence length continue train,issue,positive,positive,positive,positive,positive,positive
537611768,"Hello, thank you for this discussion.  We are closing it now due to inactivity.  Please reopen it if you'd like to further discuss this topic.",hello thank discussion due inactivity please reopen like discus topic,issue,positive,negative,negative,negative,negative,negative
537611424,Hello.  Thank you for your discussion.  We are closing it now due to inactivity. Please reopen it if you want to further discuss this topic. ,hello thank discussion due inactivity please reopen want discus topic,issue,positive,negative,negative,negative,negative,negative
537611056,Thank you for the discussion.  We are closing this issue now due to inactivity.  Please feel free to open it back up if you want to further discuss this proposal. ,thank discussion issue due inactivity please feel free open back want discus proposal,issue,positive,positive,neutral,neutral,positive,positive
537609726,"Hi @tjad,
Thanks for you interest and feedback to ML-Agents.  This issue has been inactive for a while and we will close this issue.  Please reopen it if you have more information to share or a PR related to your request. ",hi thanks interest feedback issue inactive close issue please reopen information share related request,issue,positive,positive,neutral,neutral,positive,positive
537608885,"Hi @nikola-j, 
Thanks for reporting this issue.  We have logged the bug internally as MLA-84 and will update this issue once we have more information. ",hi thanks issue logged bug internally update issue information,issue,negative,positive,neutral,neutral,positive,positive
537607994,"Hi all,
We area currently figuring out our roadmap for DOTS compatibility.  We will share it once we have nailed down a plan.  We have logged this request internally as MLA-83.  We will close this issue and update it when we can answer your questions. ",hi area currently compatibility share plan logged request internally close issue update answer,issue,negative,neutral,neutral,neutral,neutral,neutral
537605872,"Hi @ScriptBono,
Thanks for your feedback.  We have logged this request internally as ID MLA-82.  We will be closing this issue for now, and will update this issue when we have any updates to share.
",hi thanks feedback logged request internally id issue update issue share,issue,positive,positive,neutral,neutral,positive,positive
537604194,We probably need to define a link.xml to make sure these symbols aren't stripped.  Docs [here](https://docs.unity3d.com/Manual/ManagedCodeStripping.html),probably need define make sure stripped,issue,negative,positive,positive,positive,positive,positive
537603800,"This issue has been inactive for some time, so I'm going to close it due to inactivity.  Feel free to reopen it if there's more to discuss.",issue inactive time going close due inactivity feel free reopen discus,issue,positive,positive,positive,positive,positive,positive
537603581,"Hi,
Thank you for the discussion.  This conversation seems to be idle so we will close it for now.  Feel free to reopen it if you have further questions related to this topic. ",hi thank discussion conversation idle close feel free reopen related topic,issue,positive,positive,positive,positive,positive,positive
537602855,"Hi, we have relaxed the restrictions on pip version constraints.  Please try the 0.10 release and let us know if this solved your issue. ",hi relaxed pip version please try release let u know issue,issue,positive,neutral,neutral,neutral,neutral,neutral
537602490,"Thank you for submitting this request. I’ve added it to our internal tracker with the ID MLA-81. I’m going to close this issue for now, but we’ll ping back with any updates",thank request added internal tracker id going close issue ping back,issue,negative,neutral,neutral,neutral,neutral,neutral
537602162,"Hello all,
Thank you for your questions.  This discussion has been idle for a few weeks and we are closing the issue.  Please reopen it if there are further questions. ",hello thank discussion idle issue please reopen,issue,positive,neutral,neutral,neutral,neutral,neutral
537599592,"Hi @unnamed7,
Thank you for your input.  We have logged this bug internally as MLA-80.  We will update this issue when a fix is merged into the develop branch.",hi unnamed thank input logged bug internally update issue fix develop branch,issue,negative,neutral,neutral,neutral,neutral,neutral
537598958,"Hi @WannyWanny,
This issue has been idle for several days so we are going to close it.  Please feel free to reopen it if you have further questions. ",hi issue idle several day going close please feel free reopen,issue,positive,positive,positive,positive,positive,positive
537597973,"Hi @laarroyo,
Since you haven't update this issue with any information, we will close it for now. Please reopen it if you can clarify your problem and give us additional information in order to help you. ",hi since update issue information close please reopen clarify problem give u additional information order help,issue,negative,neutral,neutral,neutral,neutral,neutral
537593841,"Hi @avatcc,
Thank you for your question.  We hope the advice from @awjuliani has solved your problem.  We will close this issue for now, but if you have further questions about this issue, please feel free to reopen it.",hi thank question hope advice problem close issue issue please feel free reopen,issue,positive,positive,positive,positive,positive,positive
537593206,"Hi all, thank you for your discussion.  It seems like the question has been answered.  We have shared this feedback with the team and will close this issue for now.  Feel free to re-open it if more discussion on this topic is needed.",hi thank discussion like question feedback team close issue feel free discussion topic,issue,positive,positive,positive,positive,positive,positive
537591975,"Hi @Flavelius,
Thanks for your feedback.  We have logged this bug internally as ID MLA-79, and will update this issue when a fix is merged into the `develop` branch.  ",hi thanks feedback logged bug internally id update issue fix develop branch,issue,negative,positive,neutral,neutral,positive,positive
537589815,"Hi @wardVD,
Thanks for your feedback.  We've logged this internally with ID MLA-78.  We will close this issue for now and ping back when we have any updates. ",hi thanks feedback logged internally id close issue ping back,issue,negative,positive,neutral,neutral,positive,positive
537574755,Should the online food collector scene and BCHelper also be removed?,food collector scene also removed,issue,negative,neutral,neutral,neutral,neutral,neutral
537412773,"Hey @AcelisWeaven ,

I finally spent some time investigating this memory leak (actually there were two). I believe my two latest commits should fix it.

There is also a third memory leak that happens when you run only in inference mode but it seems like this one was already in the framework in that version. To fix it I will need to update the PR to the latest develop branch.",hey finally spent time investigating memory leak actually two believe two latest fix also third memory leak run inference mode like one already framework version fix need update latest develop branch,issue,negative,positive,positive,positive,positive,positive
537322268,Ok 0.9.0 is confirmed working. The error isn't there. I have not tried 0.9.1 or 0.9.2 but it is definitely in 0.9.3 and 0.10,confirmed working error tried definitely,issue,negative,positive,positive,positive,positive,positive
537308635,rolled back to 0.9.3 and have the same error there. I'll go back to 0.9.0 and see if that is that was the working version I was on before.,rolled back error go back see working version,issue,negative,neutral,neutral,neutral,neutral,neutral
537280823,"Thank you for submitting this request. This should be achievable through the Sampling feature released in v0.9. I’m going to close this issue for now, but feel free to resubmit an issue if you're still having problems.",thank request achievable sampling feature going close issue feel free resubmit issue still,issue,positive,positive,positive,positive,positive,positive
537280240,"Thank you for submitting this request. We’ve added it to our internal tracker. I’m going to close this issue for now, but we’ll ping back with any updates.

Also note that parallel environments were improved in 0.9, they no longer block each other when training. Give it a go. Also, if environments are the bottleneck, the SAC trainer in v0.10 should help quite a bit even in single machine training.",thank request added internal tracker going close issue ping back also note parallel longer block training give go also bottleneck sac trainer help quite bit even single machine training,issue,negative,negative,neutral,neutral,negative,negative
537280145,"@mbaske This was fixed on the develop branch a few weeks ago, and the fix is in version 0.10.0. Closing this issue.",fixed develop branch ago fix version issue,issue,negative,positive,neutral,neutral,positive,positive
537279168,Closing this issue since it's fixed on develop and in version 0.10.0,issue since fixed develop version,issue,negative,positive,neutral,neutral,positive,positive
537279027,"Hi @avatcc,
Any update on this? We should have some better reporting of errors like this in version 0.10.0. If you're still having problems with your yaml file, I'd suggest a tool like http://www.yamllint.com/ to validate the file.",hi update better like version still file suggest tool like validate file,issue,positive,positive,positive,positive,positive,positive
537278308,"As of v0.9, you can give a directory to the YAML file to read in multiple demo files.  

Closing this issue for now - thanks!",give directory file read multiple issue thanks,issue,negative,positive,neutral,neutral,positive,positive
537277926,"An answer to the previous question: You'd add 28 as the sequence length, as each agent would know its own actions and the state (which is determined by the opposing agent). 

Closing this issue due to inactivity - we've added the documentation request to our internal tracker. ",answer previous question add sequence length agent would know state determined opposing agent issue due inactivity added documentation request internal tracker,issue,negative,negative,neutral,neutral,negative,negative
537275222,"I've added the request for stacking visual observations to our internal tracker with the ID MLA-52. I’m going to close this issue for now, but we’ll ping back with any updates.",added request visual internal tracker id going close issue ping back,issue,negative,neutral,neutral,neutral,neutral,neutral
537273130,"@SestoAle - the  [tensorflow_to_barracuda](https://github.com/Unity-Technologies/ml-agents/blob/develop/ml-agents/mlagents/trainers/tensorflow_to_barracuda.py) script is used to convert .pb files to .nn files. You can see where this is used here: https://github.com/Unity-Technologies/ml-agents/blob/3a189a00afe170bab1c630a101cff2f3294f286d/ml-agents/mlagents/trainers/tf_policy.py#L230

However, we can only guarantee support for models that were generated by ML-Agents. Since you created you model on your own, the script may or may not work. ",script used convert see used however guarantee support since model script may may work,issue,positive,neutral,neutral,neutral,neutral,neutral
537270364,It looks like this issue has been resolved by #265 (thanks for the contribution @DanAmador).  I'm going to close the issue for now but if this doesn't resolve the problem feel free to reopen it.,like issue resolved thanks contribution going close issue resolve problem feel free reopen,issue,positive,positive,positive,positive,positive,positive
537270072,"Thanks for submitting this request.  I’ve shared it with the team, so I’m going to close this issue for now.",thanks request team going close issue,issue,negative,positive,positive,positive,positive,positive
537269672,"Thanks for submitting your request.  We've logged this request to our internal tracker with the ID MLA-74.  I'm going to close this issue for now, but we'll get back to you if there are any updates. ",thanks request logged request internal tracker id going close issue get back,issue,negative,positive,neutral,neutral,positive,positive
537268384,"I'm going to close this issue since it's captured in our internal tracker, but we’ll ping back with any updates.",going close issue since internal tracker ping back,issue,negative,neutral,neutral,neutral,neutral,neutral
537267978,"Thanks for the suggestion. I've added it to our internal tracker with the ID MLA-73. I’m going to close this issue for now, but we’ll ping back with any updates.",thanks suggestion added internal tracker id going close issue ping back,issue,negative,positive,neutral,neutral,positive,positive
537259242,"Now that TF2.0 is officially released, we plan to add support for it soon. Our internal tracker ID for this is MLA-51.",officially plan add support soon internal tracker id,issue,negative,neutral,neutral,neutral,neutral,neutral
537237995,"Hi @chriselion,

even though i do not really like it. Yes that basically answers the question why there is randomness :/
Sorry for missing that issue. Should have come up when i researched this.

The last thing that results as a question for me now: Is there a way to manipulate how often a random action is chosen instead of an optimal one? Like is that directly related to the policies entropy given on tensorboard while training? 

Does this belong here or would you rather have me reopen issue 2112 for this?

EDIT:
Found this [article from OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html) which contains the sentence:
""PPO trains a stochastic policy[...]"" 
Well that basically says it all. I should probably try some other Algorithm for my problem if i want absolute deterministic behaviour.

Thanks again for your help and support. ",hi even though really like yes basically question randomness sorry missing issue come last thing question way manipulate often random action chosen instead optimal one like directly related entropy given training belong would rather reopen issue edit found article sentence stochastic policy well basically probably try algorithm problem want absolute deterministic behaviour thanks help support,issue,positive,negative,neutral,neutral,negative,negative
537232152,This issue should be fixed in the new Crawler environment in v0.10. Closing this bug. ,issue fixed new crawler environment bug,issue,negative,positive,positive,positive,positive,positive
537221749,"Got it, I think I know what's going on. We'll look at the issue in more depth, but for now you should be able to import it by using `from mlagents.envs.environment import UnityEnvironment`.",got think know going look issue depth able import import,issue,negative,positive,positive,positive,positive,positive
537219398,"I installed it using Anaconda on Windows, Python 3.6.1. I followed the installation guide in the repo. The error appears when I ran the getting_started notebook in Jupyter. I ran some of the demos in Unity and things seemed to work fine. ",anaconda python installation guide error ran notebook ran demo unity work fine,issue,negative,positive,positive,positive,positive,positive
537218860,"Hi @JMFox, which platform are you on (Windows, Linux, Mac), and which Python version? I've tried a fresh pip install on Mac with Python 3.6.5 and haven't been able to replicate",hi platform mac python version tried fresh pip install mac python able replicate,issue,negative,positive,positive,positive,positive,positive
537212616,I got the same error installing via pip.,got error via pip,issue,negative,neutral,neutral,neutral,neutral,neutral
537160273,"I'm going to close this due to inactivity. 

That particular error log is fixed on the development branch right now. It is generally caused by another error, so if you're hitting problems, you should look earlier in the output logs.",going close due inactivity particular error log fixed development branch right generally another error look output,issue,negative,positive,neutral,neutral,positive,positive
537159543,"Thank you for the discussion. We are closing this issue due to inactivity. Feel free to reopen it if you’d like to continue the discussion.
",thank discussion issue due inactivity feel free reopen like continue discussion,issue,positive,positive,positive,positive,positive,positive
537157532,I logged this as MLA-62 in our internal tracker. Are you OK with working around the problem in the meantime?,logged internal tracker working around problem,issue,negative,neutral,neutral,neutral,neutral,neutral
537149215,"Hi @CaptainPineapple,
Here's an answer to a very similar question about randomness (from someone who's better at RL than I am): https://github.com/Unity-Technologies/ml-agents/issues/2112

Does that help address your concerns?",hi answer similar question randomness someone better help address,issue,positive,positive,positive,positive,positive,positive
537148586,"As a side note, https://github.com/Unity-Technologies/ml-agents/pull/2601 hopefully made these issues easier to debug.",side note hopefully made easier,issue,positive,neutral,neutral,neutral,neutral,neutral
537139341,"Thanks @DanAmador - I changed the target to our develop branch, but otherwise this looks good!",thanks target develop branch otherwise good,issue,positive,positive,positive,positive,positive,positive
537129374,"Hi @BlueFisher, @StepNeverStop, which install method are you using? Are you installing from pip or from source? We haven't changed the imports from 0.9.2 in 0.10.0.",hi install method pip source,issue,negative,neutral,neutral,neutral,neutral,neutral
537094163,"a lot of problems about file references, i.e. no `.py` file named tensorflow_to_barracuda in trainers folder.",lot file file folder,issue,negative,neutral,neutral,neutral,neutral,neutral
537020415,"For anyone that stumbles on this with a line 1 column 1 (char 0) error, this may help you #2651",anyone line column char error may help,issue,negative,neutral,neutral,neutral,neutral,neutral
537017955,"As @DanAmador referenced, please use my fix #2651 mine used to fail but this will sort it.",please use fix mine used fail sort,issue,negative,negative,negative,negative,negative,negative
537017593,"@DanAmador Yes it is, I had not seen this issue. I was wracking my head with it last week but realised it was simply trying to json.load the Unity .meta files.

With this modification mine works fine where it previously did not.",yes seen issue head last week simply trying unity modification mine work fine previously,issue,positive,positive,neutral,neutral,positive,positive
537006185,"we are working on faster matrix multiplication across CPU platforms, for present moment GPU inference is the best choice for bigger models / more agents in single batch",working faster matrix multiplication across present moment inference best choice bigger single batch,issue,positive,positive,positive,positive,positive,positive
536999800,Fixed through tedious and careful tuning of hyperparameters. Very sensitive: small changes result in huge difference in performance,fixed tedious careful tuning sensitive small result huge difference performance,issue,negative,negative,neutral,neutral,negative,negative
536925354,"Hey @chriselion,

thank you for your support on this. 

So by forcing the same random seed i can ensure the same behaviour for parallel running agents.
What i do not understand so far is why there even is random behaviour at all. From my understanding an Agent in reinforcement learning chooses the best action for it's current state based on its observations. So shouldn't the exact same observation vector always lead to the same action beeing chosen in inference? If there is still random behaviour in there this basically means i cannot ensure optimal behaviour for an agent.

Or does the random behaviour only come into action if there are multiple actions whos expected future reward is almost identical?",hey thank support forcing random seed ensure behaviour parallel running understand far even random behaviour understanding agent reinforcement learning best action current state based exact observation vector always lead action chosen inference still random behaviour basically ensure optimal behaviour agent random behaviour come action multiple future reward almost identical,issue,positive,negative,neutral,neutral,negative,negative
536919740,"HI @chriselion,

Thank you for the response. Could it also be possible to report on the logic about the rewards, as I believe it is the core behind the RL.

Thank you.",hi thank response could also possible report logic believe core behind thank,issue,positive,negative,negative,negative,negative,negative
536917910,"So the issue with your workaround is that the fake stubs for `dlopen` and `dlsym` should only be provided on windows (I understood that providing them on windows fixes the build). On Linux, they would mask the real dlopen and dlsym, which obviously breaks things.",issue fake provided understood providing build would mask real obviously,issue,negative,negative,negative,negative,negative,negative
536793493,Will fix conflict after merge to `master`. Thanks @surfnerd for the fix!,fix conflict merge master thanks fix,issue,negative,positive,positive,positive,positive,positive
536778750,"There's no support for stacking right now. You'd probably need to do some hacking around how we set up the inputs here
https://github.com/Unity-Technologies/ml-agents/blob/9370b635cb52320e0d3b73829d239f1273929021/ml-agents/mlagents/trainers/models.py#L146
and something similar when we read the protobuf and convert to np arrays here: https://github.com/Unity-Technologies/ml-agents/blob/9370b635cb52320e0d3b73829d239f1273929021/ml-agents-envs/mlagents/envs/brain.py#L188-L194
",support right probably need hacking around set something similar read convert,issue,negative,positive,positive,positive,positive,positive
536758896,"@chriselion as we mentioned in our chat offline there are tests in place for loading demonstration files here:
https://github.com/Unity-Technologies/ml-agents/blob/758e2c66c8e4b0da4f7fb23bc0d6cbdab97e7791/ml-agents/mlagents/trainers/tests/test_demo_loader.py#L6

I think this should be sufficient enough for us to know if we've broken backwards compatibility. ",chat place loading demonstration think sufficient enough u know broken backwards compatibility,issue,negative,negative,negative,negative,negative,negative
536753633,"Hi @wardVD,
I can't promise when we'll get to this, but I logged this as a feature request in our internal tracker (MLA-47).",hi ca promise get logged feature request internal tracker,issue,negative,neutral,neutral,neutral,neutral,neutral
536751885,"@chriselion , Thanks for your reply. Actually, I do have multiple cameras that generate a list of images (one for segmentation, one for depth etc...) is there anyway to stack them during the training? (considering that I'm able to get my results into a RenderTexture)",thanks reply actually multiple generate list one segmentation one depth anyway stack training considering able get,issue,negative,positive,positive,positive,positive,positive
536748927,"Hi @maystroh,
There hasn't been any additional work on this since the previous requests. Someone else asked something similar the other day: https://github.com/Unity-Technologies/ml-agents/issues/2634 (I found the same project that you linked to). As mentioned there, if you can get your results into a RenderTexture, ML Agents will convert it to a bitmap for visual observations.",hi additional work since previous someone else something similar day found project linked get convert visual,issue,negative,negative,neutral,neutral,negative,negative
536730852,"> Does this break recorded demos, or are the old files compatible? Wasn't sure if it used name matching or something else.

Good question.  I am not sure.  Let me dig into this.",break demo old compatible sure used name matching something else good question sure let dig,issue,positive,positive,positive,positive,positive,positive
536729692,"Does this break recorded demos, or are the old files compatible? Wasn't sure if it used name matching or something else.",break demo old compatible sure used name matching something else,issue,negative,positive,positive,positive,positive,positive
536702650,"Hi @CaptainPineapple,
This sounds like the expected behavior for multiple agents using the same brain. All the ""random"" decisions at inference time should come from
https://github.com/Unity-Technologies/ml-agents/blob/99e997c811176b163908f7c619b8aa9947e7e9b7/UnitySDK/Assets/ML-Agents/Scripts/InferenceBrain/Utils/Multinomial.cs#L23 and
https://github.com/Unity-Technologies/ml-agents/blob/99e997c811176b163908f7c619b8aa9947e7e9b7/UnitySDK/Assets/ML-Agents/Scripts/InferenceBrain/Utils/RandomNormal.cs#L21 - each has its own (pseudo) random number generator which is seeded by the LearningBrain initialization: https://github.com/Unity-Technologies/ml-agents/blob/99e997c811176b163908f7c619b8aa9947e7e9b7/UnitySDK/Assets/ML-Agents/Scripts/LearningBrain.cs#L76

However, since inference is run for all the agents corresponding corresponding to the brain in a batch, we'll make sequential calls (once for each agent) to these random number generators, providing different results for each agent. 

If you really want identical behavior for each agent, I think you can accomplish this by creating a different brain (using the same model) for each one.

Note that since the generators are seeded identically, I would expect the agents (as a group) to behave identically across multiple executions (assuming everything else in the environment is deterministic also). At the moment we don't have any tests to enforce this, though.

> I also can't seem to find any information if a learnt NN-Model is tweakable or modifiable (especially regarding random actions) after the learning process has been finished.

There's nothing supported out of the box for this.",hi like behavior multiple brain random inference time come pseudo random number generator seeded however since inference run corresponding corresponding brain batch make sequential agent random number providing different agent really want identical behavior agent think accomplish different brain model one note since seeded identically would expect group behave identically across multiple assuming everything else environment deterministic also moment enforce though also ca seem find information learnt modifiable especially regarding random learning process finished nothing box,issue,positive,negative,negative,negative,negative,negative
536695586,@chriselion [Here](https://github.com/Unity-Technologies/ml-agents/issues/1786) is much bigger discussion regarding the issue with xvfb on ubuntu 16/18.,much bigger discussion regarding issue,issue,negative,neutral,neutral,neutral,neutral,neutral
536684717,"Hi @enikon,
Sorry, but we don't support training of arbitrary networks, and we can only support conversion to barracuda for models that were created by ML-Agents.

If you want to try adding dropout to the mode, I'd suggest looking at where we create the graph and making modifications there. For example, here's the code where we create the convolutional layers for visual observations:
https://github.com/Unity-Technologies/ml-agents/blob/99e997c811176b163908f7c619b8aa9947e7e9b7/ml-agents/mlagents/trainers/models.py#L500-L532",hi sorry support training arbitrary support conversion barracuda want try dropout mode suggest looking create graph making example code create convolutional visual,issue,positive,negative,negative,negative,negative,negative
536679440,"Hi @sebjf ,
I made a fix recently around environment shutdown that might be helpful:
https://github.com/Unity-Technologies/ml-agents/pull/2600
It sounds like it might be related to your situation; python processes would hang on shutdown until any pending step date was popped from the queue. Calling `cancel_join_thread() from the worker fixed this.

https://github.com/Unity-Technologies/ml-agents/pull/2620 is something else to be aware of (although probably not directly related). I changed `subprocess.Popen` to pass `start_new_session`, so that signals to the parent wouldn't be sent to the child process (the goal being to stop SIGINT from killing the environment before it had a chance to shutdown). If that's not desirable behavior, let me know and we can add command line switch to toggle it.

Can you let me know if the `cancel_join_thread()` fixes your problem; if not we can try to dig into it more.",hi made fix recently around environment shutdown might helpful like might related situation python would shutdown pending step date queue calling worker fixed something else aware although probably directly related pas parent would sent child process goal stop killing environment chance shutdown desirable behavior let know add command line switch toggle let know problem try dig,issue,negative,positive,neutral,neutral,positive,positive
536673130,"Hi @wardVD,
For https://github.com/Unity-Technologies/ml-agents/blob/493c75bf683d35d512ae6fb57d4a1a332116df15/UnitySDK/Assets/ML-Agents/Examples/Tennis/Scripts/HitWall.cs#L34 - agreed, that's weird. I'll try to find more historical info on it.

For https://github.com/Unity-Technologies/ml-agents/blob/493c75bf683d35d512ae6fb57d4a1a332116df15/UnitySDK/Assets/ML-Agents/Examples/Tennis/Scripts/HitWall.cs#L75-L87 - If you look at the `floorB` block below, that also repeats itself. I think in both cases the `lastAgentHit` logic can just be removed.

I'll do some more digging on the 0 reward and get back to you.",hi agreed weird try find historical look block also think logic removed digging reward get back,issue,negative,negative,negative,negative,negative,negative
536657350,"Hi @simonbogh ,
I think that's just a quirk of how pip handles things. In my experience, sometime `mlagents-learn` will complain about the versions when you run it, even if you managed to make pip happy.

The good news is that the latest development version of ml-agents relaxes the restrictions on tensorflow (and numpy and python) versions. We're releasing v0.10 this week, which will contain these changes.",hi think quirk pip experience sometime complain run even make pip happy good news latest development version python week contain,issue,positive,positive,positive,positive,positive,positive
536650536,"Glad you were able to get it working.
Do you have any more information on the problems between xvfb and ubuntu 18? If so we can add it to the documentation.",glad able get working information add documentation,issue,negative,positive,positive,positive,positive,positive
536641272,"@awjuliani, where we stand on this? is it done? I mean to allow the user to use depth image instead of RGB for visual observations.",stand done mean allow user use depth image instead visual,issue,negative,negative,negative,negative,negative,negative
536625020,"> > So why the slowdown issues? It's ok for a few seconds then starts slowing down, sounds like a memory leak problem. Consuming more and more resource somewhere?. Pre-recorded demonstrations are good but real-time learning is also very important please look into this issue!
> 
> Hi, Have you finished your VR tennis? Is it possible to share your code?

any luck with your research?",slowdown like memory leak problem consuming resource somewhere good learning also important please look issue hi finished tennis possible share code luck research,issue,positive,positive,positive,positive,positive,positive
536595114,@Unknown-Technologies any chance you could share your frozen_graph.pb file?,chance could share file,issue,negative,neutral,neutral,neutral,neutral,neutral
536525337,"Hi Ward,

The Value loss shows how well the model is able to predict the value of each state. From your graph, it look as if the **value loss goes to 0** around 40k steps. Both the cumulative reward & episode length seem to deteriorate from then on - it somehow seems to forget what it learned in the beginning? 

Any RL/Mlagents expert reading this has an idea how the model is able to ""forget"" what it learned in the beginning, and how to solve it?  
 

   ",hi ward value loss well model able predict value state graph look value loss go around cumulative reward episode length seem deteriorate somehow forget learned beginning expert reading idea model able forget learned beginning solve,issue,positive,positive,positive,positive,positive,positive
536512981,"Hi @vincentpierre ,
I was searching for the Action-Value function. Are they open to access ? 
I was wondering about these, to get the Q values for any action input at a given state, in Unity.
q= getActionValue(GameState, Action);

Thank you,
Zilch123

(P.S., Sorry for the trouble, I am still learning RL, just exploring, Hopefully someone else will find them helpful too)",hi searching function open access wondering get action input given state unity action thank sorry trouble still learning exploring hopefully someone else find helpful,issue,positive,negative,neutral,neutral,negative,negative
536189776,"Trying to understand what is happening here, because I am experiencing the same.

Clean virtenv with python 3.6.7. I run the following.
`pip install mlagents==0.9.3`
It will install tensorflow 1.7.1

Hereafter I can install tensorflow 1.12.3 if I want to without any problems.

If I install tensorflow 1.12.x first, and then mlagents, then it will downgrade tensorflow to 1.7.1.",trying understand happening clean python run following pip install install hereafter install want without install first downgrade,issue,negative,positive,positive,positive,positive,positive
536161248,@chriselion Thank you very much -- both resources have great info!  It's a huge help!,thank much great huge help,issue,positive,positive,positive,positive,positive,positive
536139346,"@chriselion I was able to make it run on ubuntu **16.04**, apparently **xvfb** tool does not work well with ubuntu **18**.
In case anyone have difficulties with running **MLAgents** with visual observations inside docker  container, I have created a [branch](https://github.com/koryakinp/mlagents-issue/tree/mlagents-issue-fixed) with the working setup for Docker.",able make run apparently tool work well case anyone running visual inside docker container branch working setup docker,issue,negative,positive,positive,positive,positive,positive
536127536,"Hi @zako42,
This is definitely something we don't support out of the box; I can't give you much guidance on the rendering side of things. But if you can get your results to a RenderTexture, you can pass that onto an Agent as a VisualObservation.

Based on quick Google search for ""Unity image segmentation"", this might give you some pointers 
https://bitbucket.org/Unity-Technologies/ml-imagesynthesis/src/master/ 
http://www.immersivelimit.com/tutorials/unity-image-segmentation

Hope that's helpful...",hi definitely something support box ca give much guidance rendering side get pas onto agent based quick search unity image segmentation might give hope helpful,issue,positive,positive,positive,positive,positive,positive
536086753,"I ended up replacing the Library Folder with the sample project and that seemed to fix the issue, personally I think the problem was that I think I didnt drag the ML-Agent folder but copy/pasted it.

So at the very least if someone else is having this problem, they might try to do that,

Thanks a lot for the help.",ended library folder sample project fix issue personally think problem think didnt drag folder least someone else problem might try thanks lot help,issue,negative,negative,neutral,neutral,negative,negative
536081915,"Agreed, we should make this clearer. I'll log this as a task; hopefully we'll get to it in our next sprint...",agreed make clearer log task hopefully get next sprint,issue,positive,neutral,neutral,neutral,neutral,neutral
536079852,"This solution in the post works for building Windows IL2CPP, however if you build for Linux (Mono) it'll compile but the library will say it can't communicate with Unity.

```
UnityAgentsException: The Communicator was unable to connect. Please make sure the External process is ready to accept communication with Unity.
  at MLAgents.Batcher.SendAcademyParameters (MLAgents.CommunicatorObjects.UnityRLInitializationOutput academyParameters) [0x0002d] in <82ac4562db4940c9b5f1b06d4a9be1a1>:0                  at MLAgents.Academy.InitializeEnvironment () [0x00228] in <82ac4562db4940c9b5f1b06d4a9be1a1>:0
  at MLAgents.Academy.Awake () [0x00000] in <82ac4562db4940c9b5f1b06d4a9be1a1>:0
```

Rolling back the changes makes Linux Mono work again.",solution post work building however build mono compile library say ca communicate unity communicator unable connect please make sure external process ready accept communication unity rolling back mono work,issue,positive,positive,neutral,neutral,positive,positive
536060062,"Hi,
This
> No supported renderers found, exiting

makes it sound like Unity is quitting before it can start trying to communicate with the python trainer. I think you need to resolve that problem first.",hi found sound like unity start trying communicate python trainer think need resolve problem first,issue,negative,positive,positive,positive,positive,positive
536046624,"@chriselion, I am not asking to help me with debugging the environment. In fact, there is nothing wrong with the environment as I was able to run it on Windows. I just need some help with configuring to run it inside Docker container. 

5005 port is enabled in Docker configuration: 

> EXPOSE 5005 

Here is a full stack trace of the exception:

> Found path: /mlagents-issue/environments/mldriver-discrete-steering.x86_64
> Mono path[0] = '/mlagents-issue/environments/mldriver-discrete-steering_Data/Managed'
> Mono config path = '/mlagents-issue/environments/mldriver-discrete-steering_Data/MonoBleedingEdge/etc'
> Preloaded 'libgrpc_csharp_ext.x64.so'
> Unable to preload the following plugins:
> 	ScreenSelector.so
> Display 0 '0': 128x128 (primary device).
> PlayerPrefs - Creating folder: /root/.config/unity3d/DefaultCompany
> PlayerPrefs - Creating folder: /root/.config/unity3d/DefaultCompany/Cars
> Logging to /root/.config/unity3d/Editor/Player.log
>   File ""main.py"", line 4, in <module>
>     file_name=""environments/mldriver-discrete-steering"", worker_id=1)
>   File ""/usr/local/lib/python3.6/dist-packages/mlagents/envs/environment.py"", line 101, in __init__
>     aca_params = self.send_academy_parameters(rl_init_parameters_in)
>   File ""/usr/local/lib/python3.6/dist-packages/mlagents/envs/environment.py"", line 676, in send_academy_parameters
>     return self.communicator.initialize(inputs).rl_initialization_output
>   File ""/usr/local/lib/python3.6/dist-packages/mlagents/envs/rpc_communicator.py"", line 84, in initialize
>     ""The Unity environment took too long to respond. Make sure that :\n""
> mlagents.envs.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that :
> 	 The environment does not need user interaction to launch
> 	 The Academy's Broadcast Hub is configured correctly
> 	 The Agents are linked to the appropriate Brains
> 	 The environment and the Python interface have compatible versions.

And here what I have in **/root/.config/unity3d/Editor/Player.log:**

> Error loading /mlagents-issue/environments/mldriver-discrete-steering_Data/Plugins/x86_64/ScreenSelector.so: libgtk-x11-2.0.so.0: cannot open shared object file: No such file or directory
> Desktop is 128 x 128 @ 0 Hz
> Unable to find a supported OpenGL core profile
> Failed to create valid graphics context: please ensure you meet the minimum requirements
> E.g. OpenGL core profile 3.2 or later for OpenGL Core renderer
> Vulkan detection: 0
> No supported renderers found, exiting
>  #0 ???
>  #1 ???
>  #2 ???
>  #3 __libc_start_main
>  #4 ???
>  
> (Filename: ./PlatformDependent/LinuxStandalone/main.cpp Line: 639)",help environment fact nothing wrong environment able run need help run inside docker container port docker configuration expose full stack trace exception found path mono path mono path unable following display primary device folder folder logging file line module file line file line return file line initialize unity environment took long respond make sure unity environment took long respond make sure environment need user interaction launch academy broadcast hub correctly linked appropriate brain environment python interface compatible error loading open object file file directory unable find core profile create valid graphic context please ensure meet minimum core profile later core renderer detection found line,issue,positive,positive,neutral,neutral,positive,positive
536039773,Our internal tracking for this feature is MLA-16. I'll close this issue for now...,internal feature close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
536033546,"HI @albertbuchard,
I can reproduce something similar. Can you scroll up in the logs a bit? I think you'll see something like
```
No model was present for the Brain 3DBallLearning.
UnityEngine.Debug:LogError(Object)
MLAgents.LearningBrain:DecideAction() (at Assets/ML-Agents/Scripts/LearningBrain.cs:143)
MLAgents.Brain:BrainDecideAction() (at Assets/ML-Agents/Scripts/Brain.cs:98)
MLAgents.Academy:EnvironmentStep() (at Assets/ML-Agents/Scripts/Academy.cs:558)
MLAgents.Academy:FixedUpdate() (at Assets/ML-Agents/Scripts/Academy.cs:583)
```
(the `An item with the same key has already been added` error should no longer appear after https://github.com/Unity-Technologies/ml-agents/pull/2629)

You can resolve this for now by setting the model for the 3DBallLearning brain:
![image](https://user-images.githubusercontent.com/6877802/65789886-2a311a00-e113-11e9-9322-43e3fa771fa3.png)

But I'll look into why this isn't working in newer versions.",hi reproduce something similar scroll bit think see something like model present brain object item key already added error longer appear resolve setting model brain image look working,issue,negative,neutral,neutral,neutral,neutral,neutral
536031875,"If you can provide the source neural network file, that would be even better.",provide source neural network file would even better,issue,negative,positive,positive,positive,positive,positive
536022652,"Hi @ashwalcs,
Can you please provide more information about the crash, such as the callstack? Any other information like the device type and Unity version would probably be helpful too.",hi please provide information crash information like device type unity version would probably helpful,issue,positive,neutral,neutral,neutral,neutral,neutral
536021259,"Hi @koryakinp,
Unfortunately we're not able to help debug custom environments.

Can you provide more logs from the trainer and environment? That might help diagnosing the issue. I doubt that visual observations are the problem here, but it might be worth disable them to simplify the problem.

I'm not that familiar with Docker, but can you make sure that the trainer and the environment can communicate over the right port? This defaults to 5005, but you might need to configure Docker to enable this.",hi unfortunately able help custom provide trainer environment might help issue doubt visual problem might worth disable simplify problem familiar docker make sure trainer environment communicate right port might need configure docker enable,issue,negative,positive,positive,positive,positive,positive
535897913,"The issue seems to be around gRPC's declaring of an [DllImport(""__Internal"")] native function. This is elaborated on this forum post: 
https://forum.unity.com/threads/solved-ml-agents-break-when-using-il2cpp-as-scripting-backend.577075/

The issue seems to be somewhat anknowledged by the gRPC team. I believe that it was adressed for Android on these PRs:
https://github.com/grpc/grpc/pull/18039
https://github.com/grpc/grpc/pull/18327

I'm not sure that GRPC is set to be IL2CPP compiled for windows, and it's possible that it will require additional stubs. Perhaps @jtattermusch can comment more on this. I'll see if this issue happens on Android as well. ",issue around native function forum post issue somewhat team believe android sure set possible require additional perhaps comment see issue android well,issue,positive,positive,positive,positive,positive,positive
535897010,"Well, I can't seem to setup the environment even with anaconda due to package version conflicts. I know little about Python and not sure where do I get the ancient Tensorflow 1.7.1 nowadays, since pip says it doesn't have it.
Is there some way to just create some shared Anaconda environment config or a Docker image?
",well ca seem setup environment even anaconda due package version know little python sure get ancient nowadays since pip way create anaconda environment docker image,issue,positive,positive,neutral,neutral,positive,positive
535851209,"Hi @chriselion,

Thank you for the reply. We encountered the same error, yes.

Still, I believe it should be made more clear that in general lTS versions and later are supported. Can this be added more clearly in the documentation?",hi thank reply error yes still believe made clear general later added clearly documentation,issue,positive,positive,neutral,neutral,positive,positive
535774888,"On certain instances, the agent is able to find the goal. On other runs, the agent either moves very small distances or keeps repeating the same poor policy. Each run is totally different. 

While I understand that each run will be different, it should keep exploring the environment which is not the case. 

The environment is 500 x 500. I successfully learnt in smaller environments but made the Environments larger for sparse rewards and hard exploration (Research). 

I have tested with a shaped reward, but I want agent to learn with only a terminal reward and curiosity as a signal. The curiosity losses drop very quickly as well which is a concern. ",certain agent able find goal agent either small poor policy run totally different understand run different keep exploring environment case environment successfully learnt smaller made sparse hard exploration research tested shaped reward want agent learn terminal reward curiosity signal curiosity drop quickly well concern,issue,positive,positive,neutral,neutral,positive,positive
535719298,"Hi,
The BehavioralCloning trainer will repeatedly use demo data to improve the model. You can see how the demonstration_buffer (which contains the loaded demonstration data) is used here: https://github.com/Unity-Technologies/ml-agents/blob/99e997c811176b163908f7c619b8aa9947e7e9b7/ml-agents/mlagents/trainers/bc/trainer.py#L126-L131

As has been mentioned in other issues, GAIL is now the recommended approach for imitation learning; I would suggest you switch to that if possible.",hi trainer repeatedly use data improve model see loaded demonstration data used approach imitation learning would suggest switch possible,issue,positive,negative,neutral,neutral,negative,negative
535718568,"Also, it looks like you may be missing the Barracuda plugin.  I don't see the NNModel script in your editor screen shot.  Can you verify that your Barracuda plugin is in the right place?",also like may missing barracuda see script editor screen shot verify barracuda right place,issue,negative,positive,neutral,neutral,positive,positive
535712444,"Hi @maxirejlis,
In order to help us diagnose your issue could you follow these steps:

- Close unity.
- Reopen Unity.
- Do the same steps you are currently doing to attach the NN to the brain.
- Find the editor log and attach it to this issue.  (You can find your editor log by looking at [this page](https://docs.unity3d.com/Manual/LogFiles.html)).  

Based on the information you've given I can't say exactly what the issue is.  Have you by any chance modified the `NNModelImporter.cs` file?  Could you check that file and make sure that the file name matches the class name **exactly**.   Thank you.",hi order help u diagnose issue could follow close unity reopen unity currently attach brain find editor log attach issue find editor log looking page based information given ca say exactly issue chance file could check file make sure file name class name exactly thank,issue,positive,positive,positive,positive,positive,positive
535660509,"Hi @chriselion,

Thanks for your answer. Is there any way to do the training with Baselines and use the Tensor Model in Unity?

For curiosity, what the best algorithm Baselines PPO2 or Unity PPO? I'm undecided about which one to use.

Thanks in advance!",hi thanks answer way training use tensor model unity curiosity best algorithm unity undecided one use thanks advance,issue,positive,positive,positive,positive,positive,positive
535625677,"Not as far as I can tell; the current implementation hasn't been touched (aside from your formatting change) in over a year https://github.com/Unity-Technologies/ml-agents/pull/1250/files#diff-105e3a9c5ea8f20088b50e764b7e51d5R56

Under normal circumstances the dictionary gets emptied here:
https://github.com/Unity-Technologies/ml-agents/blob/f3042b5bc8dc7ad19a65227d9788968d615ca088/UnitySDK/Assets/ML-Agents/Scripts/LearningBrain.cs#L170",far tell current implementation touched aside change year normal dictionary,issue,negative,positive,neutral,neutral,positive,positive
535623776,"Hi @mbaske! Yes I still have that memory leak, and I'm not versed enough in Python/ML to fix it myself :)
What I did was to wrap my training command in a infinite bash loop. I keep Tensorboard running in the background and until it crashes (about every 7 days), the graphs are fine. I mentionned it in a comment on the PR but the brain order is important to have more consistent graphs.

Anyway, I've been training many models since I started (almost non stop) but I found it hard to have good results. In my game, if my rewards are sparse, the AI can score but can't figure how to use the items on the map (and that was with 20+ days of training) You may have the same problem with your passes. The solution I've found is not satisfying, but it works fine for my game: I also reward item usage. For example, if an enemy is trapped I give +1/-1 reward to the players. If a 'magnet' item is used, you get a reward depending on the ball velocity relative to the other team goal. Etc...
Here's a video of a model that has been trained in a few days: https://youtu.be/MJwEhCY01kw
Here's another one with a 12h-old model (~400k steps): https://youtu.be/RSEL708Inbc

In these models, I only have an hidden layer of size 180. Training is a lot faster and give better results that a two/three layers model in the same timeframe.

Also a downside of the ELO rating is that it only works on 1v1. But I think this rating is not mandatory and that removing it could help with crafting multi-agents teams. OpenAI used self-play on an hide-and-seek game and got awesome results, and I'm not sure they actually use a rating system at all: https://openai.com/blog/emergent-tool-use/ ([Paper](https://arxiv.org/abs/1909.07528))

Sorry for the long comment, I'm just sharing some thoughts! @LeSphax's fork actually get the job done and I'd be glad to see more development regarding self-play.",hi yes still memory leak versed enough fix wrap training command infinite bash loop keep running background every day fine comment brain order important consistent anyway training many since almost non stop found hard good game sparse ai score ca figure use map day training may problem solution found satisfying work fine game also reward item usage example enemy trapped give reward item used get reward depending ball velocity relative team goal video model trained day another one model hidden layer size training lot faster give better model also downside rating work think rating mandatory removing could help used game got awesome sure actually use rating system paper sorry long comment fork actually get job done glad see development regarding,issue,positive,positive,positive,positive,positive,positive
535608974,"Hi @wardVD and @dietervh,
As a general rule, we support the Long Term Support (LTS) versions here https://unity3d.com/unity/qa/lts-releases and newer versions.

I do know of one issue that affected some newer versions, which was fixed here: https://github.com/Unity-Technologies/ml-agents/pull/2580

Is that the same issue you both encountered, or was there another issue? Can you provide more details?",hi general rule support long term support know one issue affected fixed issue another issue provide,issue,positive,positive,neutral,neutral,positive,positive
535477230,I would love to see documentation that shows compatible versions of Unity for each ml-agents version. Unity 2019.2 broke my ml-agents projects so had to revert them to Unity 2019.1.14 ,would love see documentation compatible unity version unity broke revert unity,issue,negative,positive,positive,positive,positive,positive
535447519,"Hi,

Thanks, this is my first contrib to anything that I don't own and I was
sure I did something wrong. Change away!

Best,
Doug Lawrence

On Thu, 26 Sep 2019, 06:12 Chris Elion, <notifications@github.com> wrote:

> Hi @Vaspra <https://github.com/Vaspra>
> Hope you don't mind, I changed the target branch from master to develop.
> I'll take a closer look tomorrow..
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/pull/2627?email_source=notifications&email_token=AIBUEWV6QK3HZIDDKBT5JQLQLRAEPA5CNFSM4I2UG4BKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD7UKB6A#issuecomment-535339256>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AIBUEWTQVEL5H3JQ6HGLKB3QLRAEPANCNFSM4I2UG4BA>
> .
>
",hi thanks first anything sure something wrong change away best wrote hi hope mind target branch master develop take closer look tomorrow reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
535373023,"@MentalGear isn't GetReward() current reward of the game?
I was also searching for anticipated (future) reward. Yet to figure out. 
Essentially the value function output, given the current state of the game and the action. 
Is it possible to get the value function output in unity? @awjuliani ",current reward game also searching future reward yet figure essentially value function output given current state game action possible get value function output unity,issue,positive,negative,neutral,neutral,negative,negative
535339256,"Hi @Vaspra 
Hope you don't mind, I changed the target branch from `master` to `develop`. I'll take a closer look tomorrow..",hi hope mind target branch master develop take closer look tomorrow,issue,negative,neutral,neutral,neutral,neutral,neutral
535330691,"Hi, we just merged some protobuf generation changes to develop if you’d like to give it a try.  Do you have a local version of protoc installed?  If so, what version?  I tried regenerating the protobufs myself and ran into compile issues because my local protoc was version 3.9.1 while the NuGet package one was 3.51.  Maybe something similar is happening to you? ",hi generation develop like give try local version version tried ran compile local version package one maybe something similar happening,issue,negative,neutral,neutral,neutral,neutral,neutral
535293723,"I cannot particularly find where is the compile error, I'm not super experienced with Unity, if there's a specific log folder I can find or anything I'll add any additional data.
(See attached images)

![no error](https://user-images.githubusercontent.com/8903187/65651049-96acfb80-dfe3-11e9-958d-e42669859cae.png)


![no error vs](https://user-images.githubusercontent.com/8903187/65651050-96acfb80-dfe3-11e9-9265-5511f3720396.png)

I'll add additional information about the setup to see if you see something weird too.
I followed Windows install documentation but skipped step 4:
https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation-Windows.md

Also, I installed Anaconda at this location: C:\Users\Maxi\Anaconda3
And use the following line to train the model (I dont think here's the problem as I seem to be able to create the nn file without problems...)
`C:\Users\Maxi\Documents\Unity ML\ML-Project\ML>mlagents-learn trainer_config.yaml --run-id=test1 --train`",particularly find compile error super experienced unity specific log folder find anything add additional data see attached error error add additional information setup see see something weird install documentation step also anaconda location use following line train model dont think problem seem able create file without train,issue,negative,positive,positive,positive,positive,positive
535271952,"Hi @maxirejlis 
> in the lower section it has in script (None) and a compile error.

What's the compiler error that you're seeing? It's probably important to fix that first.

> `2019-09-25 19:02:11.235799: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2`

This is a warning from tensorflow but it's safe to ignore. There's some more info here https://stackoverflow.com/q/47068709/224264. Definitely don't spend any time on it until you get the basics working 😄 

> `INFO:mlagents.envs:Hyperparameters for the PPOTrainer of brain RollerBallBrain`

This is just some diagnostic information output by ml-agents. It's safe to ignore too.

",hi lower section script none compile error compiler error seeing probably important fix first binary use warning safe ignore definitely spend time get working brain diagnostic information output safe ignore,issue,positive,positive,positive,positive,positive,positive
535271547,"@Hunter-Unity I discussed with @mmattar and we decided that we should leave Soccer in as an example env, but not provide any trained models until we have a good behavior. ",decided leave soccer example provide trained good behavior,issue,negative,positive,positive,positive,positive,positive
535266761,"Tested on windows - CTRL-C still gets sent to the executable, but it's no worse (and not sure we can do better)",tested still sent executable worse sure better,issue,negative,positive,positive,positive,positive,positive
535232567,I'd recommend at least giving it a test on a Windows machine before merging.  Otherwise 🚢 .,recommend least giving test machine otherwise ship,issue,negative,negative,negative,negative,negative,negative
535230692,"@harperj Haven't tried on Windows. The start_new_session parameter only applies to POSIX systems. From https://docs.python.org/3/library/subprocess.html#popen-constructor
> If start_new_session is true the setsid() system call will be made in the child process prior to the execution of the subprocess. (POSIX only)

So it at least shouldn't make anything worse on Windows (I'm not sure if `KeyboardInterrupt` will propagate there anyway)

I think the ""wait, else kill"" approach is generally better.",tried parameter true system call made child process prior execution least make anything worse sure propagate anyway think wait else kill approach generally better,issue,negative,positive,positive,positive,positive,positive
535147702,"Hi @Unknown-Technologies,
We can only support graphs that were generated by ML-Agents. @mantasp from the Barracuda team might be able to help you with this though.",hi support barracuda team might able help though,issue,positive,positive,positive,positive,positive,positive
535126063,"Hi @TiNovTec, did you install using pip from the directory? Try installing again from the directory in that case. Maybe uninstall ml-agents using pip, and reinstall?",hi install pip directory try directory case maybe pip reinstall,issue,negative,neutral,neutral,neutral,neutral,neutral
535111722,"Thanks @chenmingxiang110 and @TheMartianLife. Glad you were able to resolve your issue.

If you can make a PR with your change, that would be great (you'll probably need to pass the mask as a parameter and make it default to `DefaultRaycastLayers`). Otherwise, we'll try to get to it soon, but we've already made the next release branch (for 0.10.0) so it won't make it into that one.",thanks glad able resolve issue make change would great probably need pas mask parameter make default otherwise try get soon already made next release branch wo make one,issue,positive,positive,positive,positive,positive,positive
534930760,"@ervteng When I tried to use your command in Anaconda with environment set to ml-agents, I got this error:
 Could not find a version that satisfies the requirement ml-agents (from versions: )
No matching distribution found for ml-agents
You are using pip version 9.0.1, however version 19.2.3 is available.
You should consider upgrading via the 'python -m pip install --upgrade pip' command.

Then I updated the pip to 19.2.3 but still get this error:
ERROR: Could not find a version that satisfies the requirement ml-agents (from versions: none)
ERROR: No matching distribution found for ml-agents",tried use command anaconda environment set got error could find version requirement matching distribution found pip version however version available consider via pip install upgrade pip command pip still get error error could find version requirement none error matching distribution found,issue,negative,positive,positive,positive,positive,positive
534924086,@Sohojoe Thank you! I didn't realise but I was installing the patches instead of the base installer!,thank instead base installer,issue,negative,negative,negative,negative,negative,negative
534907010,"@awjuliani This seems to clear up most issues. It takes a lot more time to learn this way though. It still seems to have some issues with navigation around obstacles. In general, how much steps would it need to train for to learn something as complex as a game like i described above? Because i have seen machine learning run into millions of steps before anything even started learning properly.

Part of the navigation issues is also that it doesn't get what line of sight means (he can't differentiate enemy from wall, because the raycast only tells him the length of the ray) How would you recommend raycasts to be send to the NN most efficiently? 
In your examples you say ""convert world positions to agents relative local positions"" does something similar apply to raycasts and their hit/distance combination, or does ""forward ray == 5f/10f long this frame"" normalized, tell it enough?

Also an interesting observation is the following:
![nolearning](https://user-images.githubusercontent.com/3433068/65587387-12785b00-df86-11e9-8ec8-842ffcce7083.PNG)
Because levels only get harder as it goes on completing levels, and the reward can only be 1 maximum at the end of a level. Completion time can be optimized for less punishment, but in the end it does not get a better reward for reaching the 9th level in a run, than it gets for reaching the 2nd level in a run. The reward graph looks nearly the same either way once it starts completing randomized levels semi reliably. What would you suggest against this? Lowering the per-frame punishment as levels go on perhaps?

Also to restate the thing from before, would my usecase be relevant to the request of ""machine learning used in games""? that i saw in the issue list?",clear lot time learn way though still navigation around general much would need train learn something complex game like seen machine learning run million anything even learning properly part navigation also get line sight ca differentiate enemy wall length ray would recommend send efficiently say convert world relative local something similar apply combination forward ray long frame tell enough also interesting observation following get harder go reward maximum end level completion time le punishment end get better reward reaching th level run reaching level run reward graph nearly either way semi reliably would suggest lowering punishment go perhaps also restate thing would relevant request machine learning used saw issue list,issue,positive,positive,neutral,neutral,positive,positive
534890536,"> Hi @chenmingxiang110,
> I think you're correct that they rays hit another object besides the ones that you're looking for.
> 
> I'm not an expert on Unity's Physics system, but I think the best fix would be to change where we perform the raycast (actually a sphere cast) here:
> https://github.com/Unity-Technologies/ml-agents/blob/035045e75c04287a8ba6a3cdf65100995db7c283/UnitySDK/Assets/ML-Agents/Examples/SharedAssets/Scripts/RayPerception3D.cs#L61-L63
> 
> 
> and pass a layerMask to the `SphereCast` call:
> https://docs.unity3d.com/ScriptReference/Physics.SphereCast.html
> More information on forming the LayerMask is here https://docs.unity3d.com/Manual/Layers.html
> 
> Can you give that a try and see if it helps? If so, I'll log a request to add it to a future version of the SDK.

Thank you @chriselion and @TheMartianLife . Very sorry for the late reply. The good news is layer mask solved the problem. The code here
https://github.com/Unity-Technologies/ml-agents/blob/035045e75c04287a8ba6a3cdf65100995db7c283/UnitySDK/Assets/ML-Agents/Examples/SharedAssets/Scripts/RayPerception3D.cs#L61-L63
is now as follows:

` int layerMask = 1 << 8;`
` if (Physics.SphereCast(transform.position +`
`     new Vector3(0f, startOffset, 0f), 0.5f,`
`     endPosition, out hit, rayDistance, layerMask))`

where the cargo layer is layer 8. Now the ray can correctly find the cargo (I tested every other 5 degrees from 0 to 360). If the layer mask feature can be added to the latest version of ml-agent that would be great.",hi think correct hit another object besides looking expert unity physic system think best fix would change perform actually sphere cast pas call information forming give try see log request add future version thank sorry late reply good news layer mask problem code new vector hit cargo layer layer ray correctly find cargo tested every layer mask feature added latest version would great,issue,positive,positive,positive,positive,positive,positive
534856275,@ewan15 that doesn't sound right - I have a 2080Ti and it works ok with ml-agents. you need to install the right version of CUDA that Tensorflow was built against. ,sound right ti work need install right version built,issue,negative,positive,positive,positive,positive,positive
534798869,"Recently in my case, I changed the learning rate smaller and that helped. It's not a solution, so I'm just adding a comment. :)",recently case learning rate smaller solution comment,issue,negative,neutral,neutral,neutral,neutral,neutral
534768399,"I logged a feature request in our internal tracker to automatically save the graph for Tensorboard.
Closing this issue for now, but please reopen (or make a new one) if you have more problems.",logged feature request internal tracker automatically save graph issue please reopen make new one,issue,positive,positive,neutral,neutral,positive,positive
534768320,"Hi @rajatpaliwal,
I logged a feature request in our internal tracker to automatically save the graph for Tensorboard.
Closing this issue for now, but please reopen (or make a new one) if you have more problems.",hi logged feature request internal tracker automatically save graph issue please reopen make new one,issue,positive,positive,neutral,neutral,positive,positive
534755509,"The fixes for this are now merged into develop and should come soon in v0.10, so I'm going to close this issue.",develop come soon going close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
534751869,This is fixed on develop and will be available in the next release. See https://github.com/Unity-Technologies/ml-agents/pull/2594,fixed develop available next release see,issue,negative,positive,positive,positive,positive,positive
534728511,"Please share if you find a good fix for this :-)

In the meantime, one suggestion I can make is to create a binary build of your game and pass to `--env`.  Then you can set the `--base-port` to change the port used on each run rather than waiting around.",please share find good fix one suggestion make create binary build game pas set change port used run rather waiting around,issue,positive,positive,positive,positive,positive,positive
534726626,"It looks like this issue has been solved, so I'm going to close it.  Thanks, and feel free to reopen if needed.",like issue going close thanks feel free reopen,issue,positive,positive,positive,positive,positive,positive
534726189,We are closing this issue due to inactivity. Feel free to reopen it if you’d like to continue the discussion.,issue due inactivity feel free reopen like continue discussion,issue,positive,positive,positive,positive,positive,positive
534725580,Thanks for the discussion. We are closing this issue due to inactivity. Feel free to reopen it if you’d like to continue the discussion.,thanks discussion issue due inactivity feel free reopen like continue discussion,issue,positive,positive,positive,positive,positive,positive
534723533,"@ewan15 we don't have any explicit plans around supporting certain CUDA versions, but if you explain what problems you're running into in a separate issue we may be able to suggest workarounds or plan for how to support it in the future.",explicit around supporting certain explain running separate issue may able suggest plan support future,issue,positive,positive,positive,positive,positive,positive
534722857,"Hi @TiNovTec, request noted to share with the team.  In the meantime you can create your build with a different name (without the space) or escape the space in a different way.  I think my suggestion of the forward slash was incorrect, on Windows I believe [you can escape with ^](https://stackoverflow.com/questions/32457007/how-to-escape-space-in-file-path-in-windows-run).",hi request noted share team create build different name without space escape space different way think suggestion forward slash incorrect believe escape,issue,negative,neutral,neutral,neutral,neutral,neutral
534653042,"Hi @TiNovTec, as far as I know this bug was fixed back in 0.9.1. You might have to do a `pip install --upgrade ml-agents` to get the latest build installed in your virtual environment. ",hi far know bug fixed back might pip install upgrade get latest build virtual environment,issue,negative,positive,positive,positive,positive,positive
534439514,"Yes, @harperj. It's the white space. And no, escaping doesn't work and wouldn't be user-friendly at all. So please just change the name to contain no white space! xD

[Here is what I see.](https://imgur.com/a/KUAGqqX)",yes white space work would please change name contain white space see,issue,positive,neutral,neutral,neutral,neutral,neutral
534426258,"> 
> 
> GPU training is generally better (depending on hyperparameters) but never worse in our tests. If you use the SAC trainer you should see substantial improvement using GPU for training.

@harperj When will ml-agents support a new version of CUDA as my 2080 Ti is not compatible with the current version of CUDA?",training generally better depending never worse use sac trainer see substantial improvement training support new version ti compatible current version,issue,positive,positive,positive,positive,positive,positive
534417767,That's unfortunate. I've tried to find anwers to this and read that reusing the socket in question may be a way to overcome it. I think in this case the sideeffects are controlled/limited so maybe that could be worth to investigate.,unfortunate tried find read socket question may way overcome think case maybe could worth investigate,issue,negative,negative,neutral,neutral,negative,negative
534410781,"Hi, it succeeded in IL2CPP but not .NET. i really should switch my Project scripting backend to IL2CPP. ",hi really switch project,issue,negative,positive,positive,positive,positive,positive
534330125,Hi @weizhuang-93 -- I can't seem to reproduce this with our example environments.  If you try just opening the `UnitySDK` project and building one of the example environments does it work?,hi ca seem reproduce example try opening project building one example work,issue,negative,neutral,neutral,neutral,neutral,neutral
534303460,@Phantomb with non-visual observations you'll likely see less of a benefit (possibly no benefit).  You'll start to see more of a benefit as the batch/buffer size increase or if you use a larger neural network.  That's the reason SAC usually benefits sooner than PPO -- the network is larger.,likely see le benefit possibly benefit start see benefit size increase use neural network reason sac usually sooner network,issue,positive,negative,neutral,neutral,negative,negative
534301586,"@harperj is that only the case when using visual (camera) observations, or also in general with non-visual states?",case visual camera also general,issue,negative,positive,neutral,neutral,positive,positive
534288434,Hi @Flavelius -- this is a known issue on Linux.  After closing the training session the OS seems to hold onto the ports used by the editor (or builds) for about a minute and not allow new connections.  Since this isn't an issue with ML-Agents but the networking I don't believe there's anything we can do to fix it.,hi known issue training session o hold onto used editor minute allow new since issue believe anything fix,issue,negative,positive,positive,positive,positive,positive
534280206,Hi @Fuseques -- unfortunately that is the best option for setting the reset parameters from `mlagents-learn` at this time.  I'll relate the feature request to the team though; this would be a useful addition.,hi unfortunately best option setting reset time relate feature request team though would useful addition,issue,positive,positive,positive,positive,positive,positive
534232998,"Hi @mhadji05 -- thanks for reporting this to us.  Could you update `setuptools` and try again?

```
pip install -U setuptools
```",hi thanks u could update try pip install,issue,negative,positive,positive,positive,positive,positive
534231070,GPU training is generally better (depending on hyperparameters) but never worse in our tests.  If you use the SAC trainer you should see substantial improvement using GPU for training.,training generally better depending never worse use sac trainer see substantial improvement training,issue,positive,positive,positive,positive,positive,positive
534166211,"Yes, I would recommend randomizing the level order.",yes would recommend level order,issue,positive,neutral,neutral,neutral,neutral,neutral
533975101,"Hi @SestoAle ,

i tried using tensorflow_to_barracuda to convert to .nn but to no success. maybe u can try:
[https://github.com/Unity-Technologies/ml-agents/blob/master/UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda.md](url)

incase i posted my issue here.. would be glad if you could tell me if u succeeded
[#2483](https://github.com/Unity-Technologies/ml-agents/issues/2483)",hi tried convert success maybe try incase posted issue would glad could tell,issue,positive,positive,positive,positive,positive,positive
533964870,"Same error using axis(-1):

`(ml-agents) C:\Users\Wei Zhuang>python ./ml-agents/ml-agents/mlagents/trainers/tensorflow_to_barracuda.py D:\MasterArbeit\05_Programm_und_Code\model\CNN_8k_2Conv2Dense_64_16.pb CNN_8k_2Conv2Dense_64_16.nn
Converting D:\MasterArbeit\05_Programm_und_Code\model\CNN_8k_2Conv2Dense_64_16.pb to CNN_8k_2Conv2Dense_64_16.nn
Sorting model, may take a while... Done!
IGNORED: PlaceholderWithDefault unknown layer
IGNORED: Switch unknown layer
WARNING: rank unknown for tensor batch_normalization/cond/Switch:1 while processing node batch_normalization/cond/switch_t
Traceback (most recent call last):
  File ""./ml-agents/ml-agents/mlagents/trainers/tensorflow_to_barracuda.py"", line 26, in <module>
    tf2bc.convert(args.source_file, args.target_file, args.trim_unused_by_output, args)
  File ""C:\Users\Wei Zhuang\ml-agents\ml-agents\mlagents\trainers\tensorflow_to_barracuda.py"", line 1552, in convert
    i_model, args
  File ""C:\Users\Wei Zhuang\ml-agents\ml-agents\mlagents\trainers\tensorflow_to_barracuda.py"", line 1397, in process_model
    process_layer(node, o_context, args)
  File ""C:\Users\Wei Zhuang\ml-agents\ml-agents\mlagents\trainers\tensorflow_to_barracuda.py"", line 1150, in process_layer
    for x in tensor_names
  File ""C:\Users\Wei Zhuang\ml-agents\ml-agents\mlagents\trainers\tensorflow_to_barracuda.py"", line 1150, in <listcomp>
    for x in tensor_names
  File ""C:\Users\Wei Zhuang\ml-agents\ml-agents\mlagents\trainers\tensorflow_to_barracuda.py"", line 697, in get_tensor_data
    return np.array(data).reshape(dims)
UnboundLocalError: local variable 'data' referenced before assignment`

in case it could help, i tried the keras to Barracuda conversiona and get the following error:
`Converting .\CNN_8k_2Conv2Dense_64_16\CNN_8k_2Conv2Dense_64_16.h5 to CNN_8k_2Conv2Dense_64_16.bytes
IN: '': [None, 40, 65, 1] => 'state'
OUT: 'action'
Traceback (most recent call last):
  File "".\barracuda-release-release-0.2.4\Tools\keras_to_barracuda.py"", line 21, in <module>
    keras2bc.convert(args.source_file, args.target_file, args.trim_unused_by_output, args)
  File ""C:\Users\Wei Zhuang\barracuda-release-release-0.2.4\Tools\keras_to_barracuda.py"", line 421, in convert
    barracuda.write(o_model, target_file)
  File ""C:\Users\Wei Zhuang\barracuda-release-release-0.2.4\Tools\barracuda.py"", line 499, in write
    w.write_int32(l.axis)
  File ""C:\Users\Wei Zhuang\barracuda-release-release-0.2.4\Tools\barracuda.py"", line 447, in write_int32
    self.f.write(struct.pack('<i', d))
struct.error: required argument is not an integer`",error axis python converting model may take done unknown layer switch unknown layer warning rank unknown tensor node recent call last file line module file line convert file line node file line file line file line return data local variable assignment case could help tried barracuda get following error converting none recent call last file line module file line convert file line write file line argument integer,issue,negative,negative,negative,negative,negative,negative
533963232,"sorry for the very late reply, i was busy with other things. i changed the model to axis = -1 and the problem still exists

below are my files, .pb, .h5, .bytes(converted from tf, instead of .pb i use .bytes) and .pbtxt
[CNN_8k_2Conv2Dense_64_16.zip](https://github.com/Unity-Technologies/ml-agents/files/3640857/CNN_8k_2Conv2Dense_64_16.zip)
",sorry late reply busy model axis problem still converted instead use,issue,negative,negative,negative,negative,negative,negative
533950565,Sorry about that. I am closing that issue. I was using 'player brain' instead of 'learning brain'. ,sorry issue brain instead brain,issue,negative,negative,negative,negative,negative,negative
533937996,"Thank you @surfnerd ! I checked my memory issue solved, also in my custom environment!
Thanks a lot!  ",thank checked memory issue also custom environment thanks lot,issue,positive,positive,positive,positive,positive,positive
533888242,If concurrent isn't possible (i can possibly run 4 instances max) what would be the best fix after that? Would randomizing the levels in a way that all levels are seen an equal amount of times over the total learning time work?,concurrent possible possibly run would best fix would way seen equal amount time total learning time work,issue,positive,positive,positive,positive,positive,positive
533882536,"One thing to try is concurrently training on instances of all the levels, to prevent the issue you've brought up.",one thing try concurrently training prevent issue brought,issue,negative,neutral,neutral,neutral,neutral,neutral
533882453,"@MrGitGo,

I would point you to the PPO paper for a description of how the reward is used to influence the network parameters. https://arxiv.org/abs/1707.06347",would point paper description reward used influence network,issue,positive,neutral,neutral,neutral,neutral,neutral
533856139,"@banish09 I switched to a Xbox controller and used the right analog stick's axis input to rotate my AI, you should be able to map this directly in your player brain. Unfortunately my memory is a bit hazy since it's been almost a year, but I hope this helps!",banish switched controller used right stick axis input rotate ai able map directly player brain unfortunately memory bit hazy since almost year hope,issue,negative,positive,neutral,neutral,positive,positive
533855985,@michaelzlatkin Can you please expand a little bit about the workaround you figured out?,please expand little bit figured,issue,positive,negative,negative,negative,negative,negative
533855891,@banish09 unfortunately I didn't find a solution for this issue and used a workaround instead. ,banish unfortunately find solution issue used instead,issue,negative,negative,negative,negative,negative,negative
533855036,@michaelzlatkin Can you please explain how to implement the Input.mousePosition in the PlayerBrain Asset. I am struggling with the same problem.,please explain implement asset struggling problem,issue,negative,neutral,neutral,neutral,neutral,neutral
533830842,"Thanks @mbaske, you just saved me a few hours of retraining after pyton threw a wobbly. 👍 ",thanks saved threw wobbly,issue,positive,positive,positive,positive,positive,positive
533776045,"I tried several training with a large `batch_size`to avoid   the error but the training is not learning. 
It seems that the policy is never updated.
In fact on Tensorboard the graph under the policy section is empty",tried several training large avoid error training learning policy never fact graph policy section empty,issue,negative,positive,neutral,neutral,positive,positive
533767185,"@TSTsankov  - I did some research around creating a controller for a 2d ml character. I found that discrete input trained better / has a better feel that continuous input (the discrete input drives an analog-style controller simular to 8bit/NES Mario)

you can see my code and experiments here - https://github.com/Sohojoe/ActiveRagdollControllers",research around controller character found discrete input trained better better feel continuous input discrete input controller simular see code,issue,positive,positive,positive,positive,positive,positive
533749667,"textAction seems OK for this scenario. You won't be able to do Barracuda inference with that either, though. I'd give it a discrete action and pick from a list of texts",scenario wo able barracuda inference either though give discrete action pick list,issue,negative,positive,positive,positive,positive,positive
533746476,"@awjuliani 
Making it that way, i figure it would take quite a while for it to randomly get to the goal and learn, seeing how it could take like 20 seconds for 1 reward to happen (ingame time)

What is a good way to ""hand hold"" it through the steps?
Could it also be that the fact that the level progression is static (always start at level 1, and proceed until dead) actually makes it next to impossible for it to properly train all the way?
Considering that level 6 or 7 and later level 12 still introduce new mechanics to the actor?",making way figure would take quite randomly get goal learn seeing could take like reward happen time good way hand hold could also fact level progression static always start level proceed dead actually next impossible properly train way considering level later level still introduce new mechanic actor,issue,positive,negative,neutral,neutral,negative,negative
533716239,I want this textual instructions to be displayed by agent after the training is done when I am using an already trained brain to control the agent. I don't think I will need the trainer to perform this action.. Do you think textAction is the best way to go ahead for this problem or would you suggest any other approach.,want textual displayed agent training done already trained brain control agent think need trainer perform action think best way go ahead problem would suggest approach,issue,negative,positive,positive,positive,positive,positive
533714035,"You can use text actions, though our trainer won't support it. If you're writing your own using the Python API, it should work. ",use text though trainer wo support writing python work,issue,negative,neutral,neutral,neutral,neutral,neutral
533708185,"We can test this using the training automation, I actually saw this happening since last week when we started to run daily job on parallel envs. But I wasn't sure if this is specific to Linux or not. ",test training actually saw happening since last week run daily job parallel sure specific,issue,negative,positive,neutral,neutral,positive,positive
533631084,"Just a note on the lazy loading. Here's timings from the same run of GridWorld with and without explicitly calling Image.load():

`without load()`
``` json
{
  ""name"": ""BrainInfo.process_pixels"",
  ""total"": 3.584165224000029,
  ""count"": 10002,
  ""is_parallel"": true,
  ""self"": 2.7557900760001024,
  ""children"": [
    {
      ""name"": ""image_decompress"",
      ""total"": 0.8283751479999264,
      ""count"": 10002,
      ""is_parallel"": true,
      ""self"": 0.8283751479999264
    }
  ]
}
```

`with load()`
``` json
{
  ""name"": ""BrainInfo.process_pixels"",
  ""total"": 3.646127201000195,
  ""count"": 10001,
  ""is_parallel"": true,
  ""self"": 0.9565701650002723,
  ""children"": [
    {
      ""name"": ""image_decompress"",
      ""total"": 2.689557035999923,
      ""count"": 10001,
      ""is_parallel"": true,
      ""self"": 2.689557035999923
    }
  ]
}
```
So total time is about the same (within error bars) but forcing the load shifts the time into the `image_decompress` block (where it belongs).",note lazy loading run without explicitly calling without load name total count true self name total count true self load name total count true self name total count true self total time within error forcing load time block,issue,negative,positive,positive,positive,positive,positive
533602586,"When it comes to attempting to balance multiple objectives, the simpler the reward function, the better. So if the agent has to kill enemies to get to a goal, then just rewarding the goal *should* eventually be enough to have the agent learn that it needs to also kill the enemies in order to achieve that goal. Of course the more steps you put in between the start of the episode and the sparse reward at the end the harder it will become for the agent to learn from just that signal. ",come balance multiple simpler reward function better agent kill get goal rewarding goal eventually enough agent learn need also kill order achieve goal course put start episode sparse reward end harder become agent learn signal,issue,positive,positive,positive,positive,positive,positive
533441131,"After some more study on what the behavior is, it seems that when the agent starts to progress through rooms with more enemies and obstacles in them. That the agent learns about killing enemies and the reward that gets them. But promptly stars forgetting/not caring about actually going through to the end of the room (even though that is the biggest reward they will get in their sessions.

Is there any good tips to make multiple objectives work? Is it just as simple as ""only reward the end goal, and let it figure it out even if it takes 100 million attempts"" or is there a way to get them to not forget about their original goals?
Or is the fact that i am spoonfeeding it little intermediate rewards/punishments actually just confusing to the agent?
Kind of at a loss on what is best to do here in order to not confuse the agent on what i want it to do. Any advice?
",study behavior agent progress agent killing reward promptly actually going end room even though biggest reward get session good make multiple work simple reward end goal let figure even million way get forget original fact little intermediate actually agent kind loss best order confuse agent want advice,issue,positive,positive,positive,positive,positive,positive
533397224,"Well the `CarController` I'm familiar with from the UnityStandardAssets the `Move()` function takes four parameters and looks like this

```csharp
Move(float steering, float accel, float footbrake, float handbrake)
```

So while our example--which prioritised being as simple as possible for Unity first-timers--employs a constant forward motion without braking and allows Agent control of steering only, you could just add VectorAction array positions so the Agent can control all four of these parameters at once (though it would be more prone to nonsense moves like accelerating and braking at once), else I would need to see your `CarController` convenience method that makes the final call to `Move()` to know for sure.

If your implementation is just ultimately applying `carController.Move(steer, gas, brake, 0);` then it should be working just fine as is 👍",well familiar move function four like move float steering float float float example simple possible unity constant forward motion without agent control steering could add array agent control four though would prone nonsense like else would need see convenience method final call move know sure implementation ultimately steer gas brake working fine,issue,positive,positive,positive,positive,positive,positive
533363504,Also maybe you could sign the CLA @Hunter-Unity ?,also maybe could sign,issue,negative,neutral,neutral,neutral,neutral,neutral
533340912,@sterlingcrispin this change is merged into our `develop` branch and should make it into the next release.  Feel free to give it a try.,change develop branch make next release feel free give try,issue,positive,positive,positive,positive,positive,positive
533339917,"@chriselion VAIL is a superset of the functionality in GAIL, so I changed the test config to use VAIL. ",vail functionality test use vail,issue,negative,neutral,neutral,neutral,neutral,neutral
533330671,"In order to make sure we got this in the next release (branching today), I made a new branch off of develop, merged your change into it, and fixed things up to use argparse. 
https://github.com/Unity-Technologies/ml-agents/pull/2594
Hope that's OK.

Closing this since the work is done.",order make sure got next release branching today made new branch develop change fixed use hope since work done,issue,positive,positive,positive,positive,positive,positive
533329862,Please add a unit test that would have exposed this (or at least make a follow-up item to do it later),please add unit test would exposed least make item later,issue,negative,negative,negative,negative,negative,negative
533327436,"Hi @TiNovTec -- we regularly train builds with that product name, so I think it's unlikely that is what's causing the issue.  Does anything at all come up in the console after you enter your command?

One issue I see as a potential problem is that your `.exe` file is two words.  You may need to escape the space in your path to `BuildFolder/Unity\ Environment.exe`",hi regularly train product name think unlikely causing issue anything come console enter command one issue see potential problem file two may need escape space path,issue,negative,negative,negative,negative,negative,negative
533325297,Hi @TiNovTec -- glad you were able to figure out the problem.  We're looking into better ways of surfacing white space issues in our config files.  I'm going to close this Github issue since you've figured it out,hi glad able figure problem looking better way surfacing white space going close issue since figured,issue,negative,positive,positive,positive,positive,positive
533318370,"Relevant `--help` output:
```
$ mlagents-learn --help

usage: mlagents-learn [-h] [--env ENV_PATH] [--curriculum CURRICULUM_FOLDER]
                      [--sampler SAMPLER_FILE_PATH]
                      [--keep-checkpoints KEEP_CHECKPOINTS] [--lesson LESSON]
                      [--load] [--run-id RUN_ID] [--num-runs NUM_RUNS]
                      [--save-freq SAVE_FREQ] [--seed SEED] [--slow] [--train]
                      [--base-port BASE_PORT] [--num-envs NUM_ENVS]
                      [--docker-target-name DOCKER_TARGET_NAME]
                      [--no-graphics] [--debug] [--multi-gpu] [--env-args ...]
                      trainer_config_path

<...snip...> 

  --env-args ...        Arguments passed to the Unity executable. (default:
                        None)
```",relevant help output help usage curriculum sampler lesson lesson load seed seed slow train snip unity executable default none,issue,positive,positive,neutral,neutral,positive,positive
533273057,"Yeah, I don't think we should refer to removed features in the current FAQ",yeah think refer removed current,issue,negative,neutral,neutral,neutral,neutral,neutral
533258071,I left them on purpose since those docs are more migration things. Is the FAQ linked only to the current version ?,left purpose since migration linked current version,issue,negative,neutral,neutral,neutral,neutral,neutral
533169062,"I think it would be nice to have this is this section of the documentation:
https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-ML-Agents.md#training-config-file

Most developers use C# and are not familiar with Phyton's bitchiness.",think would nice section documentation use familiar phyton,issue,negative,positive,positive,positive,positive,positive
533120234,"OK. My fault.
Visual Studio adds invisible tabs to the text document.
With Edit -> Advanced -> View White Space this can be seen.
Just use Spacebar key several times instead of Tab key.
xD",fault visual studio invisible text document edit advanced view white space seen use key several time instead tab key,issue,negative,positive,neutral,neutral,positive,positive
533060046,"> Myself + the [Secret Lab](https://secretlab.institute/2019/07/11/installing-unity-ml-agents/) team just ran a tutorial teaching people new to Unity how to do a minimal working example of this at AIConf in San Jose. All the code and assets are up [on GitHub](https://github.com/parisba/ORM-AI-SJC-2019-Unity-ML-Agents) with [a full running sheet](https://github.com/parisba/ORM-AI-SJC-2019-Unity-ML-Agents/blob/master/Notes/RunningSheet.asciidoc) if you wanted something similar to look at?
> 
> Definitely an overfit problem--that agent would probably suck at any track that requires turning the other way--and the reward scheme could definitely be improved (say, by making completing a single lap the goal state) but the gist is there 🙂

I think my problem is similar to your work. Now I pass the image by visual_observation in a function to get the edges and then the output as new visual_input.
But I still need to understand how to manage the throttle and brake.
I thought something like this:

    public override void AgentAction(float[] vectorAction, string textAction) {
        
        speed = carController.speed;

            if(isCollided) {
                AgentReset();
                Done();
            } else {
                steer = Mathf.Clamp(vectorAction[0], -1f, 1f);
                throttle = Mathf.Clamp(vectorAction[1], -1, 1f);
                
               carController.Move(steer, throttle);
                velocityReward = carController.speed / carController.maxspeed * 0.05f;
                AddReward(velocityReward);
            }
        }

In CarController I check:

    if(throttle >= 0)
        gas = throttle
    else
        brake = throttle

I don't know if this method is correct",secret lab team ran tutorial teaching people new unity minimal working example san code asset full running sheet something similar look definitely overfit problem agent would probably suck track turning way reward scheme could definitely say making single lap goal state gist think problem similar work pas image function get output new still need understand manage throttle brake thought something like public override void float string speed done else steer throttle steer throttle check throttle gas throttle else brake throttle know method correct,issue,positive,positive,neutral,neutral,positive,positive
533052907,"Hi I have more or less the same problem but with KeyError: 'default'.
Its my first time so im not sure the issue, im using mlagents 0.9.2
Traceback (most recent call last):
  File ""d:\users\danin\anaconda3\envs\ml-agents\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""d:\users\danin\anaconda3\envs\ml-agents\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""D:\Users\danin\Anaconda3\envs\ml-agents\Scripts\mlagents-learn.exe\__main__.py"", line 9, in <module>
  File ""d:\users\danin\anaconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\learn.py"", line 337, in main
    run_training(0, run_seed, options, Queue())
  File ""d:\users\danin\anaconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\learn.py"", line 110, in run_training
    multi_gpu,
  File ""d:\users\danin\anaconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\trainer_util.py"", line 45, in initialize_trainers
    trainer_parameters = trainer_config[""default""].copy()
KeyError: 'default'

",hi le problem first time sure issue recent call last file line file line code file line module file line main queue file line file line default,issue,negative,positive,positive,positive,positive,positive
533044127,"@ActiveNick Hello, I met the same problem. How did you solve it in the end? Thank you.",hello met problem solve end thank,issue,negative,neutral,neutral,neutral,neutral,neutral
532964248,"Sorry, this took longer than I expected.
https://github.com/Unity-Technologies/ml-agents/pull/2586 converts arg parsing from docopt to argparse
https://github.com/Unity-Technologies/ml-agents/pull/2587/files shows how to add this feature to the command line with the new system.

I'll get the first one approved tomorrow, and then combine the second one with this PR. We should be able to get the whole thing merged before we branch for the next release...",sorry took longer add feature command line new system get first one tomorrow combine second one able get whole thing branch next release,issue,negative,positive,neutral,neutral,positive,positive
532960361,"This is a prerequisite for doing https://github.com/Unity-Technologies/ml-agents/pull/2491 the ""right"" way, but doesn't accomplish that here.",prerequisite right way accomplish,issue,negative,positive,positive,positive,positive,positive
532960182,"This looks like a big change but it was pretty well tested along the way. The basic process was
* define the CommandLineOptions namedtuple and from that from the docopt code
* add unit tests that the CommandLineOptions parse as expected
* write the argparse implementation and get it passing the same tests
* remove docopt implementation and clean up",like big change pretty well tested along way basic process define code add unit parse write implementation get passing remove implementation clean,issue,positive,positive,positive,positive,positive,positive
532922903,"@mmattar @ervteng According our previous discussion, I've moved the demo files under their corresponding Example folders and renamed they accordingly. It now looks cleaner and more organized. ",according previous discussion corresponding example accordingly cleaner organized,issue,negative,negative,negative,negative,negative,negative
532917466,"Myself + the [Secret Lab](https://secretlab.institute/2019/07/11/installing-unity-ml-agents/) team just ran a tutorial teaching people new to Unity how to do a minimal working example of this at AIConf in San Jose. All the code and assets are up [on GitHub](https://github.com/parisba/ORM-AI-SJC-2019-Unity-ML-Agents) with [a full running sheet](https://github.com/parisba/ORM-AI-SJC-2019-Unity-ML-Agents/blob/master/Notes/RunningSheet.asciidoc) if you wanted something similar to look at?

Definitely an overfit problem--that agent would probably suck at any track that requires turning the other way--and the reward scheme could definitely be improved (say, by making completing a single lap the goal state) but the gist is there 🙂",secret lab team ran tutorial teaching people new unity minimal working example san code asset full running sheet something similar look definitely overfit problem agent would probably suck track turning way reward scheme could definitely say making single lap goal state gist,issue,positive,negative,neutral,neutral,negative,negative
532915619,"Weirdly, it actually doesn't look like that font's character width is too inconsistent, but it is likely still the cause.

![64942170-f9dbb700-d89a-11e9-8b0a-36ef9150a3aa](https://user-images.githubusercontent.com/25126921/65198697-6fc46600-dac7-11e9-8d52-4888a4af4663.PNG)

To change the font in Windows 10 CMD:

1. search ""Command Prompt"" in the start bar
2. right click the result > 'Open File Location'
3. right click the Command Prompt executable (now shown in the File Explorer) > 'Properties'
4. click the 'Fonts' tab and select something monospaced (generally anything with Mono or Console in the name, or if there's a default selection)

Hope that helps! + Either way the logo is not going to affect the framework performance 🙃",weirdly actually look like font character width inconsistent likely still cause change font search command prompt start bar right click result file right click command prompt executable shown file explorer click tab select something generally anything mono console name default selection hope either way going affect framework performance,issue,negative,positive,positive,positive,positive,positive
532843895,Can we ship this as it is? Seems like it still fixes a legit bug.,ship like still legit bug,issue,negative,neutral,neutral,neutral,neutral,neutral
532842411,looks like there are bunch of files with `~blah` at the end of them.  Were those meant to be checked in?,like bunch end meant checked,issue,negative,neutral,neutral,neutral,neutral,neutral
532799257,I find the new component confusing and not sure why that behavior was split from the Agent since it's requiring that the `Agent` component is also on the GameObject.  ,find new component sure behavior split agent since agent component also,issue,negative,positive,positive,positive,positive,positive
532796526,I feel like people are just going to throw `RequestDecision` in their `Agent` `FixedUpdate`.  ,feel like people going throw agent,issue,negative,neutral,neutral,neutral,neutral,neutral
532789582,opening new PR based on develop,opening new based develop,issue,negative,positive,positive,positive,positive,positive
532703513,"I spent a couple of days trying to figure out what is happening without success. It's really annoying to have such problem when working with visual observation. 

I've modified `Agent.cs` in MLAgent library to get the image from the environment:
```

public static void ObservationToTexture(Camera obsCamera, int width, int height, ref Texture2D texture2D)
        {
            //print(""************ Observation To Texture ******************"");
            Rect oldRec = obsCamera.rect;
            obsCamera.rect = new Rect(0f, 0f, 1f, 1f);
            var depth = 24;
            var format = RenderTextureFormat.Default;
            var readWrite = RenderTextureReadWrite.Default;

            var tempRT =
                RenderTexture.GetTemporary(width, height, depth, format, readWrite);

            if (width != texture2D.width || height != texture2D.height)
            {
                texture2D.Resize(width, height);
            }

            var prevActiveRT = RenderTexture.active;
            var prevCameraRT = obsCamera.targetTexture;

            // render to offscreen texture (readonly from CPU side)
            RenderTexture.active = tempRT;
            obsCamera.targetTexture = tempRT;

            obsCamera.Render();

            texture2D.ReadPixels(new Rect(0, 0, texture2D.width, texture2D.height), 0, 0);
            texture2D.Apply();

            SaveTextureAsPNG(texture2D, ""images/"" + image_index + "".png""); image_index++;
            obsCamera.targetTexture = prevCameraRT;
            obsCamera.rect = oldRec;
            RenderTexture.active = prevActiveRT;
            RenderTexture.ReleaseTemporary(tempRT);
        }

        public static void SaveTextureAsPNG(Texture2D _texture, string _fullPath)
        {
            byte[] _bytes = _texture.EncodeToPNG();
            System.IO.File.WriteAllBytes(_fullPath, _bytes);
            //Debug.Log(_bytes.Length / 1024 + ""Kb was saved as: "" + _fullPath);
        }

```
The images show black textures as well, thus the problem is most likely related to how xvfb dump the images into the memory. I've added more materials with images in my envs and they don't show up properly. So, the problem is mostly related to any material linked to an image as texture. Not sure how to proceed. Any help would be appreciated. ",spent couple day trying figure happening without success really annoying problem working visual observation library get image environment public static void camera width height ref observation texture rect new rect depth format width height depth format width height width height render texture side new rect public static void string saved show black well thus problem likely related dump memory added show properly problem mostly related material linked image texture sure proceed help would,issue,negative,positive,neutral,neutral,positive,positive
532667292,"@djsdvg @awjuliani 
Same for me , the error disappear with a batch size of 1000000.",error disappear batch size,issue,negative,neutral,neutral,neutral,neutral,neutral
532655692,"@leobol96  I had a very similar issue where the error was :
`InvalidArgumentError (see above for traceback): Incompatible shapes: [64,11] vs. [80,11]`

I found out that **64** was my `batch_size` in the `offline_bc_config.yaml`
So i tried to increase it till the error has disappear
Now my `batch_size` is 10000",similar issue error see incompatible found tried increase till error disappear,issue,negative,neutral,neutral,neutral,neutral,neutral
532541433,"> Does that sound OK?

Yes, that sounds like the best of both worlds 😄. I already tried something similar with docopt but could not figure out a working syntax definition. It had problems with `--` and split strings at spaces. 

It looks like `argparse.REMAINDER` is designed for argument forwarding, so I guess that should work.",sound yes like best already tried something similar could figure working syntax definition split like designed argument forwarding guess work,issue,positive,positive,positive,positive,positive,positive
532530318,Updated this PR to remove changes to `tensorflow_to_barracuda.py` by simply writing out the frozen graph definition using the same approach as the `freeze_graph` function from Tensorflow.  I've tested that Barracuda inference on a saved `.nn` model and also restarting training using `--load` still work as expected.,remove simply writing frozen graph definition approach function tested barracuda inference saved model also training load still work,issue,negative,neutral,neutral,neutral,neutral,neutral
532463224,"Yes, I get that. I am wondering if it is better to try to separate the Agent into multiple classes (requesting decisions does not seem like it should be merged into the agent). I am trying to separate concerns. ",yes get wondering better try separate agent multiple class seem like agent trying separate,issue,positive,positive,positive,positive,positive,positive
532419394,"Sorry for the back-and-forth on this. I think what I'd like to do is replace docopt with python's built-in argparse, and use [`argparse.REMAINDER`](https://docs.python.org/3/library/argparse.html#nargs) to handle the environment arguments. That keeps the usage concise, doesn't need extra parsing, and gets rid of docopt (my main motivation 😄). Does that sound OK? I'll try to get to it today or tomorrow...",sorry think like replace python use handle environment usage concise need extra rid main motivation sound try get today tomorrow,issue,positive,positive,neutral,neutral,positive,positive
532394448,"I have copied them below. The interesting thing is I don't even have line 292 in my trainer_config.yaml
`default:
    trainer: ppo
    batch_size: 1024
    beta: 5.0e-3
    buffer_size: 10240
    epsilon: 0.2
    gamma: 0.99
    hidden_units: 128
    lambd: 0.95
    learning_rate: 3.0e-4
    max_steps: 5.0e4
    memory_size: 256
    normalize: false
    num_epoch: 3
    num_layers: 2
    time_horizon: 64
    sequence_length: 64
    summary_freq: 1000
    use_recurrent: false
    use_curiosity: false
    curiosity_strength: 0.01
    curiosity_enc_size: 128

BananaLearning:
    normalize: false
    batch_size: 1024
    beta: 5.0e-3
    buffer_size: 10240
    max_steps: 1.0e5

BouncerLearning:
    normalize: true
    max_steps: 5.0e5
    num_layers: 2
    hidden_units: 64

PushBlockLearning:
    max_steps: 5.0e4
    batch_size: 128
    buffer_size: 2048
    beta: 1.0e-2
    hidden_units: 256
    summary_freq: 2000
    time_horizon: 64
    num_layers: 2

SmallWallJumpLearning: 
    max_steps: 1.0e6
    batch_size: 128
    buffer_size: 2048
    beta: 5.0e-3
    hidden_units: 256
    summary_freq: 2000
    time_horizon: 128
    num_layers: 2
    normalize: false

BigWallJumpLearning: 
    max_steps: 1.0e6
    batch_size: 128
    buffer_size: 2048
    beta: 5.0e-3
    hidden_units: 256
    summary_freq: 2000
    time_horizon: 128
    num_layers: 2
    normalize: false

StrikerLearning:
    max_steps: 5.0e5
    learning_rate: 1e-3
    batch_size: 128
    num_epoch: 3
    buffer_size: 2000
    beta: 1.0e-2
    hidden_units: 256
    summary_freq: 2000
    time_horizon: 128
    num_layers: 2
    normalize: false

GoalieLearning:
    max_steps: 5.0e5
    learning_rate: 1e-3
    batch_size: 320
    num_epoch: 3
    buffer_size: 2000
    beta: 1.0e-2
    hidden_units: 256
    summary_freq: 2000
    time_horizon: 128
    num_layers: 2
    normalize: false

PyramidsLearning:
    use_curiosity: true
    summary_freq: 2000
    curiosity_strength: 0.01
    curiosity_enc_size: 256
    time_horizon: 128
    batch_size: 128
    buffer_size: 2048
    hidden_units: 512
    num_layers: 2
    beta: 1.0e-2
    max_steps: 5.0e5
    num_epoch: 3

VisualPyramidsLearning:
    use_curiosity: true
    curiosity_strength: 0.01
    curiosity_enc_size: 256
    time_horizon: 128
    batch_size: 64
    buffer_size: 2024
    hidden_units: 256
    num_layers: 1
    beta: 1.0e-2
    max_steps: 5.0e5
    num_epoch: 3

3DBallLearning:
    normalize: true
    batch_size: 64
    buffer_size: 12000
    summary_freq: 1000
    time_horizon: 1000
    lambd: 0.99
    gamma: 0.995
    beta: 0.001

3DBallHardLearning:
    normalize: true
    batch_size: 1200
    buffer_size: 12000
    summary_freq: 1000
    time_horizon: 1000
    max_steps: 5.0e5
    gamma: 0.995
    beta: 0.001

TennisLearning:
    normalize: true
    max_steps: 2e5

CrawlerStaticLearning:
    normalize: true
    num_epoch: 3
    time_horizon: 1000
    batch_size: 2024
    buffer_size: 20240
    gamma: 0.995
    max_steps: 1e6
    summary_freq: 3000
    num_layers: 3
    hidden_units: 512

CrawlerDynamicLearning:
    normalize: true
    num_epoch: 3
    time_horizon: 1000
    batch_size: 2024
    buffer_size: 20240
    gamma: 0.995
    max_steps: 1e6
    summary_freq: 3000
    num_layers: 3
    hidden_units: 512

WalkerLearning:
    normalize: true
    num_epoch: 3
    time_horizon: 1000
    batch_size: 2048
    buffer_size: 20480
    gamma: 0.995
    max_steps: 2e6
    summary_freq: 3000
    num_layers: 3
    hidden_units: 512

ReacherLearning:
    normalize: true
    num_epoch: 3
    time_horizon: 1000
    batch_size: 2024
    buffer_size: 20240
    gamma: 0.995
    max_steps: 1e6
    summary_freq: 3000

HallwayLearning:
    use_recurrent: true
    sequence_length: 64
    num_layers: 2
    hidden_units: 128
    memory_size: 256
    beta: 1.0e-2
    gamma: 0.99
    num_epoch: 3
    buffer_size: 1024
    batch_size: 128
    max_steps: 5.0e5
    summary_freq: 1000
    time_horizon: 64

VisualHallwayLearning:
    use_recurrent: true
    sequence_length: 64
    num_layers: 1
    hidden_units: 128
    memory_size: 256
    beta: 1.0e-2
    gamma: 0.99
    num_epoch: 3
    buffer_size: 1024
    batch_size: 64
    max_steps: 5.0e5
    summary_freq: 1000
    time_horizon: 64

VisualPushBlockLearning:
    use_recurrent: true
    sequence_length: 32
    num_layers: 1
    hidden_units: 128
    memory_size: 256
    beta: 1.0e-2
    gamma: 0.99
    num_epoch: 3
    buffer_size: 1024
    batch_size: 64
    max_steps: 5.0e5
    summary_freq: 1000
    time_horizon: 64

GridWorldLearning:
    batch_size: 32
    normalize: false
    num_layers: 1
    hidden_units: 256
    beta: 5.0e-3
    gamma: 0.9
    buffer_size: 256
    max_steps: 5.0e5
    summary_freq: 2000
    time_horizon: 5

BasicLearning:
    batch_size: 32
    normalize: false
    num_layers: 1
    hidden_units: 20
    beta: 5.0e-3
    gamma: 0.9
    buffer_size: 256
    max_steps: 5.0e5
    summary_freq: 2000
    time_horizon: 3

PenguinLearning:
    use_curiosity: true
	summary_freq: 5000
	time_horizon: 128
	batch_size: 128
	buffer_size: 2048
	hidden_units: 256
	beta: 1.0e-2
	max_steps: 1.0e6
`",copied interesting thing even line default trainer beta epsilon gamma normalize false false false normalize false beta normalize true beta beta normalize false beta normalize false beta normalize false beta normalize false true beta true beta normalize true gamma beta normalize true gamma beta normalize true normalize true gamma normalize true gamma normalize true gamma normalize true gamma true beta gamma true beta gamma true beta gamma normalize false beta gamma normalize false beta gamma true beta,issue,positive,positive,neutral,neutral,positive,positive
532392244,"Hi @avatcc 

Can you share your trainer config file? It seems that there was an error reading it.",hi share trainer file error reading,issue,negative,neutral,neutral,neutral,neutral,neutral
532387415,"""Solved"" ground color being the same as the goal doesn't really work, since you can't see the goal...

<img width=""610"" alt=""Screen Shot 2019-09-17 at 1 25 06 PM"" src=""https://user-images.githubusercontent.com/9065325/65076813-93b06c00-d94e-11e9-9820-4426d4cefb0f.png"">
",ground color goal really work since ca see goal screen shot,issue,negative,positive,positive,positive,positive,positive
532386420,"Pyramid with the cube and the other pyramids should look differently...

<img width=""855"" alt=""Screen Shot 2019-09-17 at 1 22 28 PM"" src=""https://user-images.githubusercontent.com/9065325/65076614-36b4b600-d94e-11e9-954b-cdc0813a49d2.png"">
",pyramid cube look differently screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
532385874,"This `redGoal` prefab seems to be stray? 

<img width=""584"" alt=""Screen Shot 2019-09-17 at 1 20 58 PM"" src=""https://user-images.githubusercontent.com/9065325/65076523-0240fa00-d94e-11e9-95de-b92e98b317cd.png"">
",prefab stray screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
532383747,"@awjuliani

I checked and the brains have: 
- Same action spaces.
- Same environment with 28 obs.

![Hbrain](https://user-images.githubusercontent.com/36326039/65075792-e06c6500-d997-11e9-87c5-b47fe120b9c0.PNG)

![lbrain](https://user-images.githubusercontent.com/36326039/65075801-e5311900-d997-11e9-8767-100454b61eb6.PNG)
",checked brain action environment,issue,negative,positive,neutral,neutral,positive,positive
532347411,"The area on `Basic` is still deceptively shaped given the actual state space. Also the logo is not aligned here at all.

<img width=""1198"" alt=""Screen Shot 2019-09-17 at 11 35 44 AM"" src=""https://user-images.githubusercontent.com/9065325/65069332-4f699f80-d93f-11e9-9da0-b74d78136aba.png"">
",area basic still deceptively shaped given actual state space also screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
532334790,"Unity logo seems to be floating randomly out in space... 
<img width=""1418"" alt=""Screen Shot 2019-09-17 at 11 03 13 AM"" src=""https://user-images.githubusercontent.com/9065325/65067215-d5cfb280-d93a-11e9-8e5d-3136b76b210f.png"">
",unity floating randomly space screen shot,issue,negative,negative,negative,negative,negative,negative
532328916,"Hi @avatcc 

Please uninstall your old version of `ml-agents-envs`, and then try to reinstall `ml-agents`. ",hi please old version try reinstall,issue,negative,positive,neutral,neutral,positive,positive
532328678,"Hi @leobol96 

It seems that your teacher and student brains have different action spaces. ",hi teacher student brain different action,issue,negative,positive,neutral,neutral,positive,positive
532206380,"I have so far not made any significant improvements. I let my ML play out over the weekend to see where it would end up.

It seems to have significantly worse than before the tweaks i have been doing that should be all documentation and advice i have read online, have improved the behavior of the neural net. (less over-fitting etc.)

Of course this could imply that before it was over-fitting to such a degree that it somehow remembered it's surroundings and just fit level specific knowledge in that memory instead of actually learning the tasks proper?

Sadly it does not help me improve the neural net in any way currently.

Here is the longer run data:
![weekend](https://user-images.githubusercontent.com/3433068/65043135-9d3fd100-d95a-11e9-97b7-fc0bbcd27359.PNG)

PS: I saw a thread here about games with ML agents, would this apply to that? I am doing this as a worker for a company after all.",far made significant let play weekend see would end significantly worse documentation advice read behavior neural net le course could imply degree somehow surroundings fit level specific knowledge memory instead actually learning proper sadly help improve neural net way currently longer run data weekend saw thread would apply worker company,issue,positive,negative,neutral,neutral,negative,negative
532199274,"Does ml-agents support TF 2.0? I also want to migrate my code to TF2.0, need I modify some file in ml-agents/envs or ml-agents/trainers??",support also want migrate code need modify file,issue,negative,neutral,neutral,neutral,neutral,neutral
532143508,"Although using `--args` instead of  `--env-args` makes repeated arguments less verbose, I still think that splitting all env arguments into separate `mlagents-learn` arguments is not that user friendly. 

It is more verbose than the non-repeated `--env-args` approach and potentially increases the syntactic distance between argument names and values. You will need to write `--args=--arg1 --args=2` instead of `--env-args=""--arg1 2""` which makes it harder to recognize that this is an argument-value pair. To overcome this problem, you could always use  `--args=--arg1=2` instead. However, this requires additional parsing in Unity.

I would prefer the `shlex.split` approach as it has less syntactic overhead and allows you to define all env arguments in one place.",although instead repeated le verbose still think splitting separate user friendly verbose approach potentially syntactic distance argument need write instead harder recognize pair overcome problem could always use instead however additional unity would prefer approach le syntactic overhead define one place,issue,negative,positive,neutral,neutral,positive,positive
532122892,"Thanks @awjuliani. It looks like two agents' ELO ratings converge over time. I need to interrupt and resume training (CPU) every couple of hours though, because of the python memory leak @AcelisWeaven is describing in the PR thread. After doing so, the rewards and ELO ratings can change quite a bit. Sometimes the checkpoint file will keep previous model checkpoint paths, sometimes they seem to disappear and the trainer then isn't sampling past checkpoints as it did before interrupting. I'm also getting some tensorboard glitches, so I can't always see the exact training progress.

Overall, the ghost trainer works much better than my manual approach to self-play though. I initially started with training an agent against a simple heuristic, then duplicated the model and pitted the two copies against each other. Whenever they diverged enough, I would pause, discard the weaker model, duplicate the stronger one and repeat.

So far I haven't seen more complex behaviour like passes, but that's hopefully a matter of training time. I'm currently at around 12M steps with my most promising looking ghost trained model, and will keep training it for a while longer. Also, I'm really just guessing with regard to the network size. Since the number of observations and actions is small (58 -> 8), I used 2x256 hidden units.",thanks like two converge time need interrupt resume training every couple though python memory leak thread change quite bit sometimes file keep previous model sometimes seem disappear trainer sampling past interrupting also getting ca always see exact training progress overall ghost trainer work much better manual approach though initially training agent simple heuristic model pitted two whenever enough would pause discard model duplicate one repeat far seen complex behaviour like hopefully matter training time currently around promising looking ghost trained model keep training longer also really guessing regard network size since number small used hidden,issue,positive,positive,neutral,neutral,positive,positive
532108214,"Someones have already figured it out [#2027](https://github.com/Unity-Technologies/ml-agents/issues/2027)
Have to implement import the .nn file (of a game) in C# and implement something similar to [this](https://www.dropbox.com/s/i983x9fydjtm0yd/TestTensorflow.cs?dl=0).
The[ Barracuda documentation](https://github.com/Unity-Technologies/ml-agents/blob/master/UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda.md) was what I was looking for. 
For more of how to use Barracuda Inference Engine look into the[ Inference Brain ](https://github.com/Unity-Technologies/ml-agents/tree/master/UnitySDK/Assets/ML-Agents/Scripts/InferenceBrain).
Thank you, 
Zilch",already figured implement import file game implement something similar barracuda documentation looking use barracuda inference engine look inference brain thank,issue,negative,negative,negative,negative,negative,negative
532020527,"Root of problem seems to be that Unity Editor runs just fine on about anything, while Unity Player has dependencies that don't match -- libgtk2 and OpenGL 3.3 are required for the player, nonsensically.  Why?
",root problem unity editor fine anything unity player match player nonsensically,issue,negative,positive,positive,positive,positive,positive
532010945,"@awjuliani I tested this on Snoopy Pop (finetune from a levels 1-10 bot to level 25) and it was considerably worse than training from scratch. 

Won't be merging this PR as-is, but you're right, we should have a deeper discussion about giving such a feature to our users. Could also tie into generalization of agents across games. ",tested snoopy pop bot level considerably worse training scratch wo right discussion giving feature could also tie generalization across,issue,negative,negative,neutral,neutral,negative,negative
531995745,"@mmattar In the ml-agents-cloud, currently we don't download the while UnitySDK folder, just the demos folder to save the download time. If we put the demo files within their corresponding folder, it would be messy to download the demo files from each of the corresponding folder. ",currently folder demo folder save time put within corresponding folder would messy corresponding folder,issue,negative,negative,neutral,neutral,negative,negative
531994871,@mmattar Would be kind of messy to put all the files everywhere. I'd rather have them in the same folder so a user knows exactly which demos are available,would kind messy put everywhere rather folder user exactly demo available,issue,negative,positive,positive,positive,positive,positive
531976084,"@xiaomaogy / @ervteng - one proposal is to include the .demo files for the sample environments in their respective folders. This way, each sample environment is self-contained within its folder. Thoughts?",one proposal include sample respective way sample environment within folder,issue,negative,neutral,neutral,neutral,neutral,neutral
531899562,"Hi @awjuliani . I absolutely agree that .nn file is the final product of learning. That is why I was confused how student agent which is controlled by the learning brain continues to learn from data collected in the past.
I think what you mean by that statement is using RNN's to retain previous learning or  using command line option --load for loading the parameters of previously trained brain.
Kindly correct if I am wrong.",hi absolutely agree file final product learning confused student agent learning brain learn data collected past think mean statement retain previous learning command line option load loading previously trained brain kindly correct wrong,issue,negative,negative,negative,negative,negative,negative
531892845,Hi all. Closing this issue due to inactivity. Feel free to re-open if you feel necessary.,hi issue due inactivity feel free feel necessary,issue,positive,positive,neutral,neutral,positive,positive
531892770,"Hi @ervteng , shall I use textAction option inside AgentAction for the trained brain to provide textual instructions?",hi shall use option inside trained brain provide textual,issue,negative,neutral,neutral,neutral,neutral,neutral
531892733,"Hi @rajatpaliwal 

The `.nn` file is the final product of learning. It does not change once created. ",hi file final product learning change,issue,negative,neutral,neutral,neutral,neutral,neutral
531891093,Hi all. Hopefully you were able to get the Monitor working as desired. Closing this issue due to inactivity. Feel free to re-open if you feel necessary.,hi hopefully able get monitor working desired issue due inactivity feel free feel necessary,issue,positive,positive,positive,positive,positive,positive
531890803,Hi @rajatpaliwal you are correct it's not like online BC. In online BC the two agents perform the same task (hence behavioral cloning) but in your task they do different things. ,hi correct like two perform task hence behavioral task different,issue,negative,neutral,neutral,neutral,neutral,neutral
531890191,"Hi @gzrjzcx 

We are working on a solution for a future release that will allow for continued training as you have described. Currently we recommend using a very large max step, and then stopping early. ",hi working solution future release allow continued training currently recommend large step stopping early,issue,positive,positive,neutral,neutral,positive,positive
531889836,"Hi @AidinD 

We use the concept of discounted returns (sums of rewards over time) to determine which actions are best. If you are interested, I would recommend taking a look at David Silver's lectures on RL available on YouTube. ",hi use concept time determine best interested would recommend taking look silver available,issue,positive,positive,positive,positive,positive,positive
531888437,"Hi all. I am closing this issue as it is something we don't expect to be adding anytime in the near future, as Unity is moving to be 100% C# based.",hi issue something expect near future unity moving based,issue,negative,positive,neutral,neutral,positive,positive
531883381,"Hi all,

The simulation in Offline BC trainer exists to provide a test-time measure of performance. 

We also have a new imitation learning method, GAIL, which performs much better when there are only small amounts of demonstration data. I would recommend taking a look at that.

Closing this issue for now. ",hi simulation trainer provide measure performance also new imitation learning method much better small demonstration data would recommend taking look issue,issue,positive,positive,neutral,neutral,positive,positive
531882779,"Hi all. For the next release of ML-Agents we are cleaning up some of the logic in Academy, and this issue should be resolved. As such, I am closing this issue. ",hi next release cleaning logic academy issue resolved issue,issue,negative,neutral,neutral,neutral,neutral,neutral
531882523,Hi all. I am closing this issue due to inactivity. Please feel free to open a new issue if you'd like to continue the discussion. ,hi issue due inactivity please feel free open new issue like continue discussion,issue,positive,positive,positive,positive,positive,positive
531882455,"@awjuliani I am not familiar with this problem.  This seems like the key issue:

```You need to supply the name of a node to --output_node_names.```",familiar problem like key issue need supply name node,issue,negative,positive,positive,positive,positive,positive
531881015,"Hi @atapley,

In the past few months we have made a number of fixes to training to address lingering memory leaks. Please try the latest version of ML-Agents, and let us know if the issue is persisting. (Closing this issue for now)",hi past made number training address memory please try latest version let u know issue persisting issue,issue,negative,positive,positive,positive,positive,positive
531879881,"Hi @haoxiangli6 

This seems to be an issue caused by the font being used in your console. The logo is designed to display with certain fixed-width fonts, which yours seems to not be one of. ",hi issue font used console designed display certain one,issue,negative,positive,positive,positive,positive,positive
531879796,"Looks like the `UnitySDK.log` is expected and read from the Python side here: https://github.com/Unity-Technologies/ml-agents/blob/3ee09631089e7e25425e1a5c1795bf995eb76ac0/ml-agents-envs/mlagents/envs/exception.py#L51

You'll need to figure out whether to remove that error case / surface it in a different way.  Also make sure to get the CLA issue worked out (see bot message above).  You may need to re-sign and push the commit.",like read python side need figure whether remove error case surface different way also make sure get issue worked see bot message may need push commit,issue,positive,positive,positive,positive,positive,positive
531879512,"Hi @jsalli 

We don't have an example of an agent learning multiple goals at once, but we do have the `Hallway` example, where the correct goal location changes based on a marker in the environment.",hi example agent learning multiple hallway example correct goal location based marker environment,issue,negative,neutral,neutral,neutral,neutral,neutral
531878962,Thanks for bringing this issue to our attention. @harper-u3d do you know why this might be happening? ,thanks issue attention know might happening,issue,negative,positive,positive,positive,positive,positive
531878537,"Hi @mnsmuts 

Thanks for this feedback. Many of us on the development team for ML-Agents actually just use direct (or virtual env) python installations and environments. We choose Anaconda instructions to help game developer users who may be less familiar with python tools. We will take this feedback into consideration as we think through documentation improvements in the future.",hi thanks feedback many u development team actually use direct virtual python choose anaconda help game developer may le familiar python take feedback consideration think documentation future,issue,positive,positive,positive,positive,positive,positive
531877760,"Hi @SimonNordon 

Differences in performance between training and testing can be due to differences in the training and inference settings in the academy. I would recommend looking there to ensure that things such as visual quality aren't being changed (In the case that your agent is learning from visual observations). ",hi performance training testing due training inference academy would recommend looking ensure visual quality case agent learning visual,issue,positive,negative,neutral,neutral,negative,negative
531876970,Thanks for sharing @mbaske! This is a really cool environment. Can you share a little about your experience using this feature? Did the two sets of agents reach an equilibrium you were happy with? ,thanks really cool environment share little experience feature two reach equilibrium happy,issue,positive,positive,positive,positive,positive,positive
531876279,"Hi @ervteng ,
I am trying to provide real time guidance using the trained brain. I believe by your previous comment you mean to create two agents in the same environment where one agent controlled by the player brain is controlled by the user , while other agent being controlled by the trained brain will provide instructions for guidance. It is not like online behavioral cloning where we have two agents in two different environment.
Kindly correct me if I am saying this wrong.",hi trying provide real time guidance trained brain believe previous comment mean create two environment one agent player brain user agent trained brain provide guidance like behavioral two two different environment kindly correct saying wrong,issue,positive,negative,neutral,neutral,negative,negative
531684014,"Yes that's it. Actually I punish him if he'll going off the track or collide against a barrier. Then I gain a little reward if he is still driving.
The car gain even a reward if the time of a sector or the entire track is new record.",yes actually punish going track collide barrier gain little reward still driving car gain even reward time sector entire track new record,issue,positive,negative,neutral,neutral,negative,negative
531653014,"Hi @awjuliani 
and thanks for the idea to change the rewarding. I made two branches in which I modified the rewarding system.
1) ""dev/random-goal-type-gives-reward"" in which the rewarding goal type is randomized on every agent reset and the rewarding goal gives +1 points and non-rewarding goal 0 points for correct goals. Scoring the block to the wrong goal resets the environment. I've also tested the game to end 1) if all scoring blocks are gone, 2) all blocks of both types are gone. In every setup the agents still choose randomly to prefer one type of block over the other.

2) ""dev/last-goal-rewards-most"". In this branch the agent is given a small rewards +0.1 points for all right goals and +1 point for the last right goal. Scoring the block to the wrong goal resets the environment. Still the agents choose to prefer randomly one type of block.

Are there any examples of agent having multiple objectives at the same time?

Is this issue most likely the rewarding system which leads the agent to find some unwanted way to maximize the reward?",hi thanks idea change rewarding made two rewarding rewarding goal type every agent reset rewarding goal goal correct scoring block wrong goal environment also tested game end scoring gone gone every setup still choose randomly prefer one type block branch agent given small right point last right goal scoring block wrong goal environment still choose prefer randomly one type block agent multiple time issue likely rewarding system agent find unwanted way maximize reward,issue,positive,positive,neutral,neutral,positive,positive
531610455,Actually it seems like multi-gpu is failing on my system closing this to open a clearer issue,actually like failing system open clearer issue,issue,negative,neutral,neutral,neutral,neutral,neutral
531606365,"@the-zer07 what is your question? 

sounds like if you just reward it for forward motion on the track and punish it for going off the track it would learn to drive it",question like reward forward motion track punish going track would learn drive,issue,negative,neutral,neutral,neutral,neutral,neutral
531361429,"Hi @chriselion , Thanks for guiding me to this file. This was really helpful. I believe I have quite a lot of insight into the neural net architecture we are using and the way to configure it according to me.
",hi thanks file really helpful believe quite lot insight neural net architecture way configure according,issue,positive,positive,positive,positive,positive,positive
531333762,"@harperj Agreed, I think it's pretty clunky, and also something we don't need to handle right now.

Are you OK with the rest of this change?",agreed think pretty also something need handle right rest change,issue,positive,positive,positive,positive,positive,positive
531332674,"@chriselion the only reasonable solution I can think of for preventing the additional error is to handle the exception in the children and report them back to the parent process (which would then need to display the error message and exit).  I'm not sure whether that's something we want to do, though.",reasonable solution think additional error handle exception report back parent process would need display error message exit sure whether something want though,issue,negative,positive,positive,positive,positive,positive
531330638,@xiaomaogy we can add a guide to use GAIL + PreTraining to speed up training (in the same way as Getting-Started-3D-Ball). Basically to replicate the plots on the Imitation Learning page,add guide use pretraining speed training way basically replicate imitation learning page,issue,negative,negative,negative,negative,negative,negative
531330160,"> I am wondering if we should add an IL example to the basic guide to help onboard a new user quickly to IL and the use of demo files.

Yes that would be really helpful! However since we already have GAIL, should we add GAIL to the basic guide instead and suggest that as the default way of doing IL, since it achieves everything offline BC has, and more? @ervteng @mmattar 

",wondering add example basic guide help new user quickly use yes would really helpful however since already add basic guide instead suggest default way since everything,issue,positive,positive,positive,positive,positive,positive
531328986,"@tecniuf - i was able to get AirSim Unity working - (note: commit i used https://github.com/microsoft/AirSim/issues/2190)

to integrate into one can hook into the python - they api is simple - https://github.com/Microsoft/AirSim/blob/master/PythonClient/multirotor/hello_drone.py ",able get unity working note commit used integrate one hook python simple,issue,negative,positive,positive,positive,positive,positive
531327143,"The `.nn` file (and some other files) are produced here:
https://github.com/Unity-Technologies/ml-agents/blob/fba79899eba43442af00524acab8ee220d800087/ml-agents/mlagents/trainers/tf_policy.py#L213-L235

The .nn file is a Unity-specific format for the Barracuda inference engine. But the `frozen_graph_def.pb` is a more standard tensorflow format that you should be able to visualize with standard tools.",file produced file format barracuda inference engine standard format able visualize standard,issue,negative,positive,positive,positive,positive,positive
531322306,"I've found more about this problem. 

I thought this issue would be solved in version 9 and decided to retry it.

At first I got excited as the trainer loaded the curriculum and said how a parameter was not present in the environment. I edited the json file and tried to reload it and still had the same JsonDecode

This is the trace when it reads the extra param:

`Traceback (most recent call last):
  File ""c:\users\anton\anaconda3\envs\unity9\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\anton\anaconda3\envs\unity9\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\anton\Anaconda3\envs\unity9\Scripts\mlagents-learn.exe\__main__.py"", line 9, in <module>
  File ""c:\users\anton\anaconda3\envs\unity9\lib\site-packages\mlagents\trainers\learn.py"", line 337, in main
    run_training(0, run_seed, options, Queue())
  File ""c:\users\anton\anaconda3\envs\unity9\lib\site-packages\mlagents\trainers\learn.py"", line 94, in run_training
    maybe_meta_curriculum = try_create_meta_curriculum(curriculum_folder, env, lesson)
  File ""c:\users\anton\anaconda3\envs\unity9\lib\site-packages\mlagents\trainers\learn.py"", line 163, in try_create_meta_curriculum
    meta_curriculum = MetaCurriculum(curriculum_folder, env.reset_parameters)
  File ""c:\users\anton\anaconda3\envs\unity9\lib\site-packages\mlagents\trainers\meta_curriculum.py"", line 37, in __init__
    curriculum = Curriculum(curriculum_filepath, default_reset_parameters)
  File ""c:\users\anton\anaconda3\envs\unity9\lib\site-packages\mlagents\trainers\curriculum.py"", line 56, in __init__
    ""the Environment"".format(key, location)
mlagents.trainers.exception.CurriculumError: The parameter extra_param in Curriculum ML-Agents/Config/curricula/Run\WalkerLearning.json is not present in the Environment`


And without it:



`Traceback (most recent call last):
  File ""c:\users\anton\anaconda3\envs\unity9\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\anton\anaconda3\envs\unity9\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\anton\Anaconda3\envs\unity9\Scripts\mlagents-learn.exe\__main__.py"", line 9, in <module>
  File ""c:\users\anton\anaconda3\envs\unity9\lib\site-packages\mlagents\trainers\learn.py"", line 337, in main
    run_training(0, run_seed, options, Queue())
  File ""c:\users\anton\anaconda3\envs\unity9\lib\site-packages\mlagents\trainers\learn.py"", line 94, in run_training
    maybe_meta_curriculum = try_create_meta_curriculum(curriculum_folder, env, lesson)
  File ""c:\users\anton\anaconda3\envs\unity9\lib\site-packages\mlagents\trainers\learn.py"", line 163, in try_create_meta_curriculum
    meta_curriculum = MetaCurriculum(curriculum_folder, env.reset_parameters)
  File ""c:\users\anton\anaconda3\envs\unity9\lib\site-packages\mlagents\trainers\meta_curriculum.py"", line 37, in __init__
    curriculum = Curriculum(curriculum_filepath, default_reset_parameters)
  File ""c:\users\anton\anaconda3\envs\unity9\lib\site-packages\mlagents\trainers\curriculum.py"", line 29, in __init__
    self.data = json.load(data_file)
  File ""c:\users\anton\anaconda3\envs\unity9\lib\json\__init__.py"", line 299, in load
    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
  File ""c:\users\anton\anaconda3\envs\unity9\lib\json\__init__.py"", line 354, in loads
    return _default_decoder.decode(s)
  File ""c:\users\anton\anaconda3\envs\unity9\lib\json\decoder.py"", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""c:\users\anton\anaconda3\envs\unity9\lib\json\decoder.py"", line 357, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)`",found problem thought issue would version decided retry first got excited trainer loaded curriculum said parameter present environment file tried reload still trace extra param recent call last file line file line code file line module file line main queue file line lesson file line file line curriculum curriculum file line environment key location parameter curriculum present environment without recent call last file line file line code file line module file line main queue file line lesson file line file line curriculum curriculum file line file line load file line return file line decode end file line raise value none value line column char,issue,positive,positive,neutral,neutral,positive,positive
531319173,"@xiaomaogy @mmattar Hmm, the overcrowding in the directory is pretty much the reason for moving it into the `demos` directory and not the `Demonstrations` directory. On more thought I don't particularly like having the examples in the root `Demonstrations` folder, it's just too cluttered. My vote is for a subdirectory called `Examples` and renaming the files from `ExpertX.demo` to just `X.demo`. I don't think it's a stretch for the user to see the word ""Example"" in a path and know that you have to remove it - less so for `Expert`.  

Just my two cents, and apologize for changing my mind again :P",directory pretty much reason moving demo directory directory thought particularly like root folder vote think stretch user see word example path know remove le expert two apologize mind,issue,positive,positive,positive,positive,positive,positive
531304840,"~~DO NOT MERGE~~
~~Once the PR is approved, I will make  the commits for documentation changes.~~
Documentation changes made",make documentation documentation made,issue,negative,neutral,neutral,neutral,neutral,neutral
531293406,"I totally agree with your point that the Expert demo files we provided won't be confused with the user recorded demo files when we put all them under Demonstrations folder, because 1) the files are based on scene name 2) user can choose their custom name. 

My concern here comes from a user workflow perspective. When a user try to record some demo file, they don't just try once, they will try a lot of times until they get a satisfying demo file. Imagine you have a Demonstration folder with 20+ demo files you just recorded, along with all of the Expert demo files officially provided, and now you want to clean this folder up without removing the Expert demo files. It would be pretty painful for you having to select out your own demo files and delete them when all of these officially provided Expert demo files sit in the same folder. 

@ervteng @mmattar 
",totally agree point expert provided wo confused user put folder based scene name user choose custom name concern come user perspective user try record file try try lot time get satisfying file imagine demonstration folder along expert officially provided want clean folder without removing expert would pretty painful select delete officially provided expert sit folder,issue,positive,positive,neutral,neutral,positive,positive
531264180,"@bjg2 , in this [link](http://tiny.cc/764wbz) you can find a singularity image that works with xvbf",link find singularity image work,issue,negative,neutral,neutral,neutral,neutral,neutral
531185690,"Thanks a lot @chriselion, I wasn't expecting you to answer me. Thank you!
The above will be useful, if I'm to use python to control unity, I was hoping to do it in unity itself through c#.
I want to get the raw output of a trained RL agents, just today I came to know that the inference engine is doing this job.
https://github.com/Unity-Technologies/ml-agents/tree/master/UnitySDK/Assets/ML-Agents/Scripts/InferenceBrain
I will figure out by reading all these Scripts. In case there is any documents of the SDK and inference engine that would be great, please point me. Thank you again!
",thanks lot answer thank useful use python control unity unity want get raw output trained today came know inference engine job figure reading case inference engine would great please point thank,issue,positive,positive,positive,positive,positive,positive
531148268,"yes actually i had to freeze the model and save it as .pb not h5 , then
convert it from .pb to bytes :)

On Thu, 12 Sep 2019 at 22:23, AnitaSP <notifications@github.com> wrote:

> did you try to store the weights and then set them as
> pre_model(set_weights)?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/2053?email_source=notifications&email_token=AMAINKTC32F4Q6IFR2SPYQTQJKXLHA5CNFSM4HNTS2X2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD6TJTEA#issuecomment-531011984>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AMAINKXN7CMOVMTBPYXMQIDQJKXLHANCNFSM4HNTS2XQ>
> .
>
",yes actually freeze model save convert wrote try store set thread reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
531115879,"So, I have seen that Unity's AirSim does not have real computer vision, so little can be experienced. And no one seems to have used AirSim with ML-Agents, so this post makes misleading proposals:

https://blogs.unity3d.com/2018/11/14/airsim-on-unity-experiment-with-autonomous-vehicle-simulation/",seen unity real computer vision little experienced one used post misleading,issue,negative,positive,positive,positive,positive,positive
531108835,"I think it needs to be consistent. Same folder for both. The use of the word `Expert` is overloaded as there are no other types of Demonstrations. Since the files we provide are named based on scene name, the developer won't confuse with their own scenes. And if they record demonstrations for the same scenes we provide they still have the option to select their own, custom name.

If we wanted to extra careful, we could rename our files to be something like SampleHallway, to indicate that this is a sample demo file, but I don't necessarily think it's necessary. I am wondering if we should add an IL example to the basic guide to help onboard a new user quickly to IL and the use of demo files.",think need consistent folder use word expert since provide based scene name developer wo confuse record provide still option select custom name extra careful could rename something like indicate sample file necessarily think necessary wondering add example basic guide help new user quickly use,issue,positive,positive,neutral,neutral,positive,positive
531061631,"I know it's quite an old issue, but I do have an update on this. `construct_curr_info` seems to only be called when agents are being constructed/destroyed, and in one timestep there is no agent. We recently fixed a bug with `construct_curr_info` that caused different issues with visual observations, and this one seems to be in the same vein. The fix proposed here should be OK, we'll roll it in to the repo. Thanks!",know quite old issue update one agent recently fixed bug different visual one vein fix roll thanks,issue,negative,positive,neutral,neutral,positive,positive
531058190,"@harperj Is there a specific changelog doc, or do you mean https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Migrating.md? Definitely think we need to update that, but can be in another PR.",specific doc mean definitely think need update another,issue,negative,negative,negative,negative,negative,negative
531040784,"Yup, I agree that's pretty verbose. Would using `--args` instead be concise enough for you? If not, I'm OK with the `shlex.split` approach.",agree pretty verbose would instead concise enough approach,issue,positive,positive,positive,positive,positive,positive
531038157,"If you mean where the actions are decided, this happens in this part of the code:
https://github.com/Unity-Technologies/ml-agents/blob/fba79899eba43442af00524acab8ee220d800087/ml-agents/mlagents/trainers/tf_policy.py#L113-L130

For PPO, the implementation is here:
https://github.com/Unity-Technologies/ml-agents/blob/34300b906c4dcca4ab0161f8eafd8158ca060fd6/ml-agents/mlagents/trainers/ppo/policy.py#L139-L169

So the actions, memory, and value will be contained in the run_out dictionary.",mean decided part code implementation memory value dictionary,issue,negative,negative,negative,negative,negative,negative
531026495,"If you want more control over the network, I'd recommend modifying the code that we use to construct the graph instead of trying to modify the graph itself. For example if you wanted to add dropout to the convolutional layers, you could start from one of the options here
https://github.com/Unity-Technologies/ml-agents/blob/339781594317d48a87ff75b5ef1c77b979ebc420/ml-agents/mlagents/trainers/models.py#L474-L505
",want control network recommend code use construct graph instead trying modify graph example add dropout convolutional could start one,issue,negative,neutral,neutral,neutral,neutral,neutral
531023444,"@xiaomaogy OK, that makes more sense. I did a search on the documentation and cleaned up anything that referred to a specific python version (might need to try the condas doc and make sure it works in 3.7 though)",sense search documentation anything specific python version might need try doc make sure work though,issue,negative,positive,positive,positive,positive,positive
531020533,Here is the link to the experimental version on Airsim's site - https://github.com/Microsoft/AirSim/tree/master/Unity,link experimental version site,issue,negative,positive,neutral,neutral,positive,positive
531018187,"> Yeah, we might need to update some documentation on versions. But this PR won't affect the python version that they have installed, it will allow them to use either python3.6 or 3.7.

If they don't have a python preinstalled (which is most of our game dev users), they will install python 3.7 if they follow step by step our Installation guide, and the doc saying they have python 3.6 will confuse them. ",yeah might need update documentation wo affect python version allow use either python python game dev install python follow step step installation guide doc saying python confuse,issue,negative,negative,negative,negative,negative,negative
531015674,"Yeah, we might need to update some documentation on versions. But this PR won't affect the python version that they have installed, it will allow them to use either python3.6 or 3.7.",yeah might need update documentation wo affect python version allow use either python,issue,negative,neutral,neutral,neutral,neutral,neutral
531013250,"I will try to get a python 3.7 image working for cloud builds. Regarding the PR, I think all of the changes make sense. Maybe we also need to do a follow up PR on the doc. Currently we tell our users we are always using python 3.6. With this PR, by default they will install python 3.7. ",try get python image working cloud regarding think make sense maybe also need follow doc currently tell always python default install python,issue,negative,neutral,neutral,neutral,neutral,neutral
531002286,"I think we should have a larger discussion about this. I'm not sure what versions of python our users have; if we can reasonably continue to test 3.6.x I think we should do that instead of forcing them to upgrade.

I'll do a little more research to see if we can make the CI tests do
* py3.6.1 with the minimum version of the dependencies (i.e. numpy==1.13.3 tensorflow==1.7)
* py3.7.x with whatever version of the dependencies that pip selects

For cloud builds, I think just using 3.7.x is fine.",think discussion sure python reasonably continue test think instead forcing upgrade little research see make minimum version whatever version pip cloud think fine,issue,negative,positive,positive,positive,positive,positive
530981949,"Shall we simply upgrade our python all to 3.7 instead of maintaining both 3.6 and 3.7?  Having two version simultaneously would increase the possibility of having issues related to different python versions (for example, python 3.6 and python 3.7 have different compatible tensorflow versions).  ",shall simply upgrade python instead two version simultaneously would increase possibility related different python example python python different compatible,issue,negative,neutral,neutral,neutral,neutral,neutral
530950735,"@chriselion Currently we define the version of the python in the docker image here: https://github.com/Unity-Technologies/ml-agents-cloud/blob/master/docker-files/runner/Dockerfile-GCP.txt, that's where we change it. 

On Ubuntu 16.04 installing python 3.6 was really painful (line 12 - 120). I guess python 3.7 should be much easier since it seems to come with Ubuntu 16.04 by default. ",currently define version python docker image change python really painful line guess python much easier since come default,issue,negative,negative,negative,negative,negative,negative
530945223,"@ervteng I was also hesitating about that. 

The pros for placing it in a subfolder within Demonstrations folder is the main folder can become very messy, and I guess users would want to remove all of the files within that folder pretty frequently, so keeping all of the expert demos provided by us helps separate that user recorded vs what we provided. 

The cons is as you mentioned. 

@mmattar What do you think?",also hesitating within folder main folder become messy guess would want remove within folder pretty frequently keeping expert demo provided u separate user provided think,issue,negative,positive,neutral,neutral,positive,positive
530932947,@xiaomaogy How can we change the version of python that the cloud training uses?,change version python cloud training,issue,negative,neutral,neutral,neutral,neutral,neutral
530923652,@ervteng that solved the problem. We were using an older config file. Many thanks for the quick support!,problem older file many thanks quick support,issue,negative,positive,positive,positive,positive,positive
530923271,"My case is I am trying to teach my student agent to move through a path while maintaining a particular orientation and speed . I am using teacher agent controlled by me to show the required speed and orientation. I thought for this case BC would be a good approach, but I am not seeing much improvement in the learning of my agent. I will try GAIL imitation learning which works on PPO  trainer but I wonder how the student agent will learn more effectively using this approach.",case trying teach student agent move path particular orientation speed teacher agent show speed orientation thought case would good approach seeing much improvement learning agent try imitation learning work trainer wonder student agent learn effectively approach,issue,positive,positive,positive,positive,positive,positive
530919753,"Edit: Just saw your edit. Glad to hear that you've already connected with Mantas on the issue 😄 

Sorry to hear you are having an issue with this, @FlimFlamm. Ideally they should both be equally as fast. I am cc'ing @mantasp who works on the Barracuda project, and can perhaps provide additional support. ",edit saw edit glad hear already connected issue sorry hear issue ideally equally fast work barracuda project perhaps provide additional support,issue,positive,positive,neutral,neutral,positive,positive
530918282,"Have you tried the new GAIL imitation learning algorithm we added in v0.9? Behavioral Cloning is actually one of the least effective methods, and will likely be removed in a future release. https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Imitation-Learning.md",tried new imitation learning algorithm added behavioral actually one least effective likely removed future release,issue,negative,positive,neutral,neutral,positive,positive
530918153,"@xiaomaogy should we place it in the root `Demonstrations` folder? I guess this way we avoid confusion as to what is user created and what isn't, so pros/cons for both. ",place root folder guess way avoid confusion user,issue,negative,neutral,neutral,neutral,neutral,neutral
530917551,"Hi @joobei, you need to define at least one reward signal in your `trainer_config.yaml` file. Something like:

```
reward_signal:
    extrinsic:
            strength: 1.0
            gamma: 0.99
```

In the examples, it works because there is a default config provided at the top of the YAML. You could also copy/paste this default config to the top of your yaml. ",hi need define least one reward signal file something like extrinsic strength gamma work default provided top could also default top,issue,positive,positive,positive,positive,positive,positive
530849697,"Hello,
I think that i have the same issue using learning by demostration.
Brains have the same parameters.

This is the error message:

INFO:mlagents.envs:Hyperparameters for the OfflineBCTrainer of brain RightBrain:
        trainer:        offline_bc
        batch_size:     256
        summary_freq:   1000
        max_steps:      5.0e6
        batches_per_epoch:      10
        use_recurrent:  True
        hidden_units:   256
        learning_rate:  0.0003
        num_layers:     3
        sequence_length:        32
        memory_size:    256
        demo_path:      nodescription_1.demo
        summary_path:   ./summaries/ppo_RightBrain
        model_path:     ./models/ppo-0/RightBrain
        keep_checkpoints:       5
Traceback (most recent call last):
  File ""c:\users\leonardo\anaconda3\envs\ml-agents\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\leonardo\anaconda3\envs\ml-agents\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Leonardo\Anaconda3\envs\ml-agents\Scripts\mlagents-learn.exe\__main__.py"", line 9, in <module>
  File ""c:\users\leonardo\anaconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\learn.py"", line 337, in main
    run_training(0, run_seed, options, Queue())
  File ""c:\users\leonardo\anaconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\learn.py"", line 132, in run_training
    tc.start_learning(env)
  File ""c:\users\leonardo\anaconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\trainer_controller.py"", line 202, in start_learning
    n_steps = self.advance(env_manager)
  File ""c:\users\leonardo\anaconda3\envs\ml-agents\lib\site-packages\mlagents\envs\timers.py"", line 261, in wrapped
    return func(*args, **kwargs)
  File ""c:\users\leonardo\anaconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\trainer_controller.py"", line 292, in advance
    trainer.update_policy()
  File ""c:\users\leonardo\anaconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\bc\trainer.py"", line 138, in update_policy
    run_out = self.policy.update(mini_batch, self.n_sequences)
  File ""c:\users\leonardo\anaconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\bc\policy.py"", line 95, in update
    run_out = self._execute_model(feed_dict, self.update_dict)
  File ""c:\users\leonardo\anaconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\tf_policy.py"", line 148, in _execute_model
    network_out = self.sess.run(list(out_dict.values()), feed_dict=feed_dict)
  File ""c:\users\leonardo\anaconda3\envs\ml-agents\lib\site-packages\tensorflow\python\client\session.py"", line 905, in run
    run_metadata_ptr)
  File ""c:\users\leonardo\anaconda3\envs\ml-agents\lib\site-packages\tensorflow\python\client\session.py"", line 1116, in _run
    str(subfeed_t.get_shape())))
ValueError: Cannot feed value of shape (8, 5) for Tensor 'teacher_action:0', which has shape '(?, 4)'",hello think issue learning brain error message brain trainer true recent call last file line file line code file line module file line main queue file line file line file line wrapped return file line advance file line file line update file line list file line run file line feed value shape tensor shape,issue,negative,positive,positive,positive,positive,positive
530806554,"Hi @chriselion. I agree that it is cleaner without `shlex`. But the downside of this approach is the bloated argument syntax.

For example, 

`--env-args=""--arg1 2 --arg2 'hello world' --arg3=bar --enable-something""` 

becomes 

`--env-args=--arg1 --env-args=2 --env-args=--arg2 --env-args='hello world' --env-args=--arg3=bar --env-args=--enable-something`.

Personally, I think this is hard to read and not very user friendly. I would therefore prefer the approach with `shlex`. What is your opinion on that?
",hi agree cleaner without downside approach bloated argument syntax example world becomes world personally think hard read user friendly would therefore prefer approach opinion,issue,positive,positive,neutral,neutral,positive,positive
530772720,"I am also having a problem with KeyError: 'reward_signals'. This is Ubuntu 18.04 with Unity 2019.1.14f1 on anaconda python 3.6 

pip list and conda info shows:
mlagents           0.9.3     /home/jubei/.local/lib/python3.6/site-packages
mlagents-envs      0.9.3     /home/jubei/local/lib/python3.6/site-packages

Here is my log:

INFO:mlagents.trainers:{'--base-port': '5005',
 '--curriculum': 'None',
 '--debug': False,
 '--docker-target-name': 'None',
 '--env': 'None',
 '--help': False,
 '--keep-checkpoints': '5',
 '--lesson': '0',
 '--load': False,
 '--multi-gpu': False,
 '--no-graphics': False,
 '--num-envs': '1',
 '--num-runs': '1',
 '--run-id': 'test99',
 '--sampler': 'None',
 '--save-freq': '50000',
 '--seed': '-1',
 '--slow': False,
 '--train': True,
 '<trainer-config-path>': 'configs/maze_config.yaml'}
INFO:mlagents.envs:Start training by pressing the Play button in the Unity Editor.
INFO:mlagents.envs:
'Academy' started successfully!
Unity Academy name: Academy
        Number of Brains: 1
        Number of Training Brains : 1
        Reset Parameters :
		
Unity brain name: MazeLearningBrain
        Number of Visual Observations (per agent): 0
        Vector Observation space size (per agent): 80
        Number of stacked Vector Observation: 1
        Vector Action space type: continuous
        Vector Action space size (per agent): [5]
        Vector Action descriptions: , , , , 
Traceback (most recent call last):
  File ""/home/jubei/.local/bin/mlagents-learn"", line 11, in <module>
    load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')()
  File ""/home/jubei/coding/ml-agents/ml-agents/mlagents/trainers/learn.py"", line 337, in main
    run_training(0, run_seed, options, Queue())
  File ""/home/jubei/coding/ml-agents/ml-agents/mlagents/trainers/learn.py"", line 110, in run_training
    multi_gpu,
  File ""/home/jubei/coding/ml-agents/ml-agents/mlagents/trainers/trainer_util.py"", line 89, in initialize_trainers
    multi_gpu,
  File ""/home/jubei/coding/ml-agents/ml-agents/mlagents/trainers/ppo/trainer.py"", line 47, in __init__
    brain, trainer_parameters, training, run_id, reward_buff_cap
  File ""/home/jubei/coding/ml-agents/ml-agents/mlagents/trainers/rl_trainer.py"", line 41, in __init__
    if not self.trainer_parameters[""reward_signals""]:
KeyError: 'reward_signals'

This project of ours was made to run with an older branch ""barracuda-test-0.2.0"". The error above showed up when we tried to migrate that project to the latest version of mlagents. So essentially we moved that project from barracuda-test-0.2.0 to the latest stable mlagents (0.9.3)

Could it be that the brains we created as ""assets"" inside the unity project were old? From that older version?",also problem unity anaconda python pip list log curriculum false help false lesson load false false false sampler seed slow false train true start training pressing play button unity editor successfully unity academy name academy number brain number training brain reset unity brain name number visual per agent vector observation space size per agent number vector observation vector action space type continuous vector action space size per agent vector action recent call last file line module file line main queue file line file line file line brain training file line project made run older branch error tried migrate project latest version essentially project latest stable could brain asset inside unity project old older version,issue,positive,positive,neutral,neutral,positive,positive
530674441,"Did some quick testing and it looks like layer width is the main culprit (not sure if the number of layers will cause the delay to scale).

At 1024 width the lag from the dense layers is about 51 ms, which is hard to notice, but with a layer size of 2048 the lag from Barracuda.Execute is 170 ms, which is game-ruining.",quick testing like layer width main culprit sure number cause delay scale width lag dense hard notice layer size lag,issue,negative,positive,positive,positive,positive,positive
530668518,"![demspikes](https://user-images.githubusercontent.com/24987395/64755993-81c06a80-d503-11e9-92cf-24e1d15a1c27.png)

Admittedly, 2048 hidden units might be a little on the fat side, but it's not unheard of for PPO, and optimizing the inference engine for large scale models would probably be beneficial to delay at lower scales (it would allow inference mode to be run in higher time scales without causing too much delay).

Rich data environments are one of the main selling points of machine learning in unity, and very high dimensional observation vectors are more or less how we would take advantage of that. In any case, i cant fathom why if training can be run at X speed, that inferencing cannot be run at X speed, or faster.",admittedly hidden might little fat side unheard inference engine large scale would probably beneficial delay lower scale would allow inference mode run higher time scale without causing much delay rich data one main selling machine learning unity high dimensional observation le would take advantage case cant fathom training run speed run speed faster,issue,positive,positive,positive,positive,positive,positive
530654598,"> Sorry for the delay, glad you were able to find the answers.

No Problem @chriselion, I've a solution of implementing an RL algorithm in GYM and use that to get all the output from a NN.
I was wondering if you can point me a better way, I mean directly getting them in Unity by tinkering the of output function or something? ",sorry delay glad able find problem solution algorithm gym use get output wondering point better way mean directly getting unity output function something,issue,negative,positive,positive,positive,positive,positive
530629969,@tecniuf  - would love to know if you get this working,would love know get working,issue,positive,positive,positive,positive,positive,positive
530623973,"This PR is to resolve the issue raise under https://github.com/Unity-Technologies/ml-agents/pull/2526#issuecomment-530612998. 

To summarize, this PR moves all of the .demo files under /demo to /UnitySDK/Assets/Demonstrations/ExpertDemos in order to have a consistent place for users to place and manage their .demo files. ",resolve issue raise summarize order consistent place place manage,issue,negative,positive,positive,positive,positive,positive
530620174,"Trello card here: https://trello.com/c/My3kIfVq/803-move-example-demo-files-to-outside-of-unitysdk-package-when-we-shift-to-packman

Will need to be really careful such that the configs still work when we move to separate 📦 's",card need really careful still work move separate package,issue,negative,negative,neutral,neutral,negative,negative
530617277,"@vincentpierre if we replace the load feature, how do we resume from an existing checkpoint? (not finetune)? 

I would almost support loading by default, and requiring a different `run-id` to retrain. The number of times I've accidentally overwrote the old model is quite high ",replace load feature resume would almost support loading default different retrain number time accidentally old model quite high,issue,negative,positive,neutral,neutral,positive,positive
530612998,"Agree @ervteng - a lot will change when we move to package. Can you please create an internal card to capture this? Should be something we fix as we move towards packman.

@xiaomaogy - thanks for doing a new PR. Please add me as a reviewer.",agree lot change move package please create internal card capture something fix move towards packman thanks new please add reviewer,issue,positive,positive,positive,positive,positive,positive
530609617,"I am attempting to install ML Agents on an Nvidia Jetson Nano with CUDA 10.

Whenever I run `pip3 install mlagents` I get the error:
```
Could not find a version that satisfies the requirement tensorflow<1.8,>=1.7 (from mlagents) (from versions: )
No matching distribution found for tensorflow<1.8,>=1.7 (from mlagents)
```

Would that be related to this issue in anyway?",install whenever run pip install get error could find version requirement matching distribution found would related issue anyway,issue,negative,neutral,neutral,neutral,neutral,neutral
530590228,"To the above: layer masking is pretty simply done by passing the tags of objects you want your agent to see.

e.g. something like (incomplete)
```csharp
public class BeepoAgent : Agent
{
    public float rayDistance = 12f;
    public float rayHeight = 1.5f;
    RayPerception rayPer;

    public override void InitializeAgent()
    {
        base.InitializeAgent();
        rayPer = GetComponent<RayPerception>();
    }

    public override void CollectObservations()
    {
        float[] rayAngles = { 0f, 45f, 70f, 90f, 110f, 135f, 180f };
        string[] detectableObjects = { ""crate"", ""goal"", ""wall"" };

        AddVectorObs(rayPer.Perceive(rayDistance, rayAngles, detectableObjects, 0f, 0f));
        AddVectorObs(rayPer.Perceive(rayDistance, rayAngles, detectableObjects, rayHeight, 0f));
    }
}
```

Hope that helps 👍

EDIT: if this is being caused by self-collision then it likely relates to #2402 ",layer pretty simply done passing want agent see something like incomplete public class agent public float public float public override void public override void float string crate goal wall hope edit likely,issue,positive,positive,neutral,neutral,positive,positive
530586922,"I think we should actually replace the load feature entirely. It seems it is just a worst version of what you are proposing.
",think actually replace load feature entirely worst version,issue,negative,negative,negative,negative,negative,negative
530581956,"Yes, slowly it starts doing imitation . But earlier it used to show the similar behavior with training steps being completed faster. What might have changed now?",yes slowly imitation used show similar behavior training faster might,issue,negative,negative,neutral,neutral,negative,negative
530581855,"Hi @hanseoup, 
Thank you for providing feedback on this issue.  #2541 should fix the memory leak.  Please let us know if you have further issues.",hi thank providing feedback issue fix memory leak please let u know,issue,negative,neutral,neutral,neutral,neutral,neutral
530580269,"Hi @rajatpaliwal 

This is expected behavior, as there is a large amount of training which takes place between steps of the simulation during behavioral cloning. Does your agent improve? ",hi behavior large amount training place simulation behavioral agent improve,issue,negative,positive,positive,positive,positive,positive
530558535,"Note that if this happens, the error is reported for the spawned process; there's another error output in the main process slightly later.
```
Traceback (most recent call last):
  File ""/Users/chris.elion/code/ml-agents/ml-agents-envs/mlagents/envs/subprocess_env_manager.py"", line 55, in recv
    response: EnvironmentResponse = self.conn.recv()
  File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py"", line 250, in recv
    buf = self._recv_bytes()
  File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py"", line 407, in _recv_bytes
    buf = self._recv(4)
  File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py"", line 383, in _recv
    raise EOFError
EOFError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/chris.elion/code/ml-agents/venv/bin/mlagents-learn"", line 11, in <module>
    load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')()
  File ""/Users/chris.elion/code/ml-agents/ml-agents/mlagents/trainers/learn.py"", line 337, in main
    run_training(0, run_seed, options, Queue())
  File ""/Users/chris.elion/code/ml-agents/ml-agents/mlagents/trainers/learn.py"", line 96, in run_training
    sampler_file_path, env.reset_parameters, run_seed
  File ""/Users/chris.elion/code/ml-agents/ml-agents-envs/mlagents/envs/subprocess_env_manager.py"", line 214, in reset_parameters
    return self.env_workers[0].recv().payload
  File ""/Users/chris.elion/code/ml-agents/ml-agents-envs/mlagents/envs/subprocess_env_manager.py"", line 58, in recv
    raise UnityCommunicationException(""UnityEnvironment worker: recv failed."")
mlagents.envs.exception.UnityCommunicationException: UnityEnvironment worker: recv failed.
```
I tried briefly to clean this up but didn't make any progress.",note error process another error output main process slightly later recent call last file line response file line file line file line raise handling exception another exception recent call last file line module file line main queue file line file line return file line raise worker worker tried briefly clean make progress,issue,negative,positive,neutral,neutral,positive,positive
530543239,"Thanks @awjuliani, I added some of your comments to the disclaimer. Let me know what you think.",thanks added disclaimer let know think,issue,negative,positive,positive,positive,positive,positive
530541947,"@awjuliani 
Here is my output from running ./make.bat
<img width=""653"" alt=""Screen Shot 2019-09-11 at 12 59 28 PM"" src=""https://user-images.githubusercontent.com/2358507/64730383-11c6cb80-d494-11e9-8c24-b79b486bece5.png"">
",output running screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
530518213,Glad you figured out the problem. I'll close this issue.,glad figured problem close issue,issue,negative,positive,positive,positive,positive,positive
530517897,Still investigating - at the moment it appears there might be a bug with our reuse (or lack thereof) of tensors in Barracuda.,still investigating moment might bug reuse lack thereof barracuda,issue,negative,neutral,neutral,neutral,neutral,neutral
530508468,"Many thanks for the reply! I'll just take things back to my previous setup and play around with the recurrent neural network parameters.

```
bool moveFwd = vectorAction[0] > 0;
bool moveBack = vectorAction[0] < 0;
bool moveLeft = vectorAction[1] > 0;
bool moveRight = vectorAction[1] < 0;
float xAxis = vectorAction[2];
float yAxis = vectorAction[3];
bool jump = vectorAction[4] > 0.5f;		
surfComp.UpdateMoveData (moveFwd, moveBack, moveLeft, moveRight, jump);
surfComp.UpdateRotation (xAxis, yAxis);
```",many thanks reply take back previous setup play around recurrent neural network bool bool bool bool float float bool jump jump,issue,positive,positive,positive,positive,positive,positive
530507011,Thank you for the discussion. We are closing this issue due to inactivity. Feel free to reopen it if you’d like to continue the discussion though.,thank discussion issue due inactivity feel free reopen like continue discussion though,issue,positive,positive,positive,positive,positive,positive
530506522,"Its just that in order to achieve better training results I believe apart from setting hyperparameters in config files, configuring the architecture of the neural network by deciding upon the loss function being used, dropout techniques etc. can help in achieving better results.",order achieve better training believe apart setting architecture neural network upon loss function used dropout help better,issue,positive,positive,positive,positive,positive,positive
530505803,Closing this issue since it's fixed in the most recent release.,issue since fixed recent release,issue,negative,positive,neutral,neutral,positive,positive
530504161,"Code looks good to me aside from fixing the minor linting issue from CircleCI.  You should add a reference to this change in the changelog.  It would also be nice to see a test that the ""reset on first step"" works correctly.",code good aside fixing minor issue add reference change would also nice see test reset first step work correctly,issue,positive,positive,positive,positive,positive,positive
530502912,"Sorry for the delay, glad you were able to find the answers.",sorry delay glad able find,issue,negative,positive,positive,positive,positive,positive
530501264,"Hi,
Sorry for the delay on this.

I would strongly discourage you from using CustomActions. Just updating the protobuf definitions isn't enough; you would also need to make several customizations to the Trainer in order to tell it how to produce the CustomActions. The documentation is misleading on the amount of effort required, and I'm going to update this soon.

In your case, I would recommend breaking down the direction and jumping actions into 3 components, for example forwardAmount (to handle FWD/BACKWD), turnAmount (to handle LEFT/RIGHT) and jumpAmount (to handle JUMP). Then, for example, use JUMPBACKWDLEFT if `(jumpAmount > .5) && (forwardAmount < -.5) && (turnAmount > .5)`",hi sorry delay would strongly discourage enough would also need make several trainer order tell produce documentation misleading amount effort going update soon case would recommend breaking direction example handle handle handle jump example use,issue,negative,negative,neutral,neutral,negative,negative
530500822,"@ervteng I also agree with the latter approach to move the example demos to the UnitySDK/Assets/Demonstrations folder.  Another reason why the latter approach is better is we can't write to the demos folder path from Unity, since the demos folder path is not under UnitySDK folder. 

@mmattar That's a good point, we should have a consistent place for users to place and manage their demo files. I will make a PR to change it as suggested by Ervin. 

",also agree latter approach move example demo folder another reason latter approach better ca write demo folder path unity since demo folder path folder good point consistent place place manage make change,issue,positive,positive,positive,positive,positive,positive
530500504,"It seems there is indeed something wrong with your build process. As @TSTsankov asked, what is your output in the terminal from running the `.bat` ?",indeed something wrong build process output terminal running,issue,negative,negative,negative,negative,negative,negative
530490318,"To add to what @xiaomaogy says - this change also makes it so that a user could just run the example BC code as-is and have it work. 

As for the consistency issue - the basic solution seems to be just to change either the DemonstrationRecorder to save in the `demos` folder, or put the example demos in the `UnitySDK/Assets/Demonstrations` folder. I'm inclined to go with the latter. The main reason they're not in there today is that we didn't want the user to have to import these demos when they open the editor. 

But I think this is also coupled with where we put our example scenes. e.g. today, they are part of the UnitySDK folder. But ideally, if you only want to use ML-Agents as a library, you're not importing the example environments (and the demo files that go with it) into Unity. Furthermore, if you're using ML-Agents in your own project, it's unlikely that your project will be in the same directory as the Python scripts. If we were to separate these at all, I'd have the demos with the examples and not the SDK. ",add change also user could run example code work consistency issue basic solution change either save demo folder put example demo folder go latter main reason today want user import demo open editor think also coupled put example today part folder ideally want use library example go unity furthermore project unlikely project directory python separate demo,issue,positive,positive,neutral,neutral,positive,positive
530489965,"Hi @awjuliani

Ok. I have now tried with an empty custom action and observation and I get the same result. Maybe this is a version issue? ",hi tried empty custom action observation get result maybe version issue,issue,negative,neutral,neutral,neutral,neutral,neutral
530481861,"Hi @jsalli 

I would try thinking through the reward function a little. It may be the case that the agent prefers one or the other because there is not greater reward that would be achieved from equally preferring both. Perhaps you could try switching off between which goals are actually rewarding, so that the agent must learn to try both.",hi would try thinking reward function little may case agent one greater reward would equally perhaps could try switching actually rewarding agent must learn try,issue,positive,positive,positive,positive,positive,positive
530480461,"Hi @tecniuf 

There is no official support for ML-Agents and AirSim integration. There is experimental AirSim Unity support however. From there ML-Agents would need to be implemented manually. ",hi official support integration experimental unity support however would need manually,issue,positive,positive,neutral,neutral,positive,positive
530480169,"Hi @yakmyakmyakm 

Can you try building the protobuf definitions without changing anything related to the custom observation or action? ",hi try building without anything related custom observation action,issue,negative,positive,neutral,neutral,positive,positive
530473170,"Well, perhaps what you say makes sense. You know where it says [_The Python instance of the custom action looks like:_](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Creating-Custom-Protobuf-Messages.md)
`from mlagents.envs.communicator_objects import CustomAction
env = mlagents.envs.UnityEnvironment(...)
...
action = CustomAction(direction=CustomAction.NORTH, walkAmount=2.0)
env.step(custom_action=action)`

Are we supposed to create this **python instance** or do we expect it to be **generated automatically** with the other files? I cannot seem to find it anywhere using **_grep_**

Edit*
Sorry, for your issue this should not be causing the problem (e.g. the python instance). I am able to go a step further than you (up to building the game) without having to deal with this **instance**

Do you get _any_ errors when you **Run the .bat from the terminal**?",well perhaps say sense know python instance custom action like import action supposed create python instance expect automatically seem find anywhere edit sorry issue causing problem python instance able go step building game without deal instance get run terminal,issue,negative,negative,neutral,neutral,negative,negative
530459299,Glad that helped. I'll make an issue to try to make the error messages clearer in this case (since `UnityWorkerInUseException` isn't really appropriate),glad make issue try make error clearer case since really appropriate,issue,negative,positive,positive,positive,positive,positive
530455597,"I've also found this issue which may be of help to you.
https://github.com/Unity-Technologies/ml-agents/pull/932",also found issue may help,issue,negative,neutral,neutral,neutral,neutral,neutral
530455242,"Yeah I have done that after resetting my repo and I have the same results. Maybe we have to add something to the python instance? 

<img width=""890"" alt=""Screen Shot 2019-09-11 at 9 14 30 AM"" src=""https://user-images.githubusercontent.com/2358507/64715250-f8625700-d474-11e9-845c-493370f5736c.png"">",yeah done maybe add something python instance screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
530451231,"Did you _Un-comment line 7 in make.bat (for Windows, use make_for_win.bat), and set to correct Grpc.Tools sub-directory._ **and** _Run the .bat from the terminal by navigating to $MLAGENTS_ROOT\protobuf-definitions and entering make.bat_ ?
I have [a problem](https://github.com/Unity-Technologies/ml-agents/issues/2503) with the Custom Protobuf, yet at least I get it to compile.",line use set correct terminal entering problem custom yet least get compile,issue,negative,negative,negative,negative,negative,negative
530377213,"Hi @chriselion, 
Thank you very much again. 
I've been able to solve the issue, which isn't ML-Agent related, but due entirely to my machine. 

To anyone that might face the same issue here what I did. 

Before I couldn't run 
`ping localhost `
but 
`ping 127.0.0.1`
worked as expected. 

I followed these suggestions [HERE](https://apple.stackexchange.com/questions/253817/cannot-ping-my-local-machine) 
and tried the following:

Changing my local hostname, using System Preferences > Sharing > Edit.
`sudo killall -HUP mDNSResponder`
`dscacheutil -flushcache`
`sudo launchctl unload -w /System/Library/LaunchDaemons/com.apple.mDNSResponder.plist`
`sudo launchctl load -w /System/Library/LaunchDaemons/com.apple.mDNSResponder.plist`

Then I changed my hosts file:

`sudo nano /etc/hosts`
Back up my old file contents. 
Removed everything expect
`127.0.0.1 localhost`

Rebooting. 

`ping localhost ` 
Now works as expected. ",hi thank much able solve issue related due entirely machine anyone might face issue could run ping ping worked tried following local system edit unload load file back old file content removed everything expect ping work,issue,positive,positive,neutral,neutral,positive,positive
530367642,"@xiaomaogy - I think having a consistent place for users to place and manage their .demo files is helpful. It is confusing to have a default directory for reading the .demo file that is different from the default directory of where the file is saved. (Esp if the reason for it is to support an internal training tool.) I would propose that we explore a new default directory that works scenarios. 

cc: @ervteng ",think consistent place place manage helpful default directory reading file different default directory file saved reason support internal training tool would propose explore new default directory work,issue,positive,positive,neutral,neutral,positive,positive
530223368,"Hi I've created a custom action. I can run mlagents-learn in the terminal and I see the unity logo like the documentation says. 

The issue is when I returned back to unity it looks like a bunch of references to the protobuf scripts which were generated do not match correctly.

<img width=""622"" alt=""Screen Shot 2019-09-10 at 10 17 50 PM"" src=""https://user-images.githubusercontent.com/2358507/64670130-d4712800-d418-11e9-9542-3409e44f94ba.png"">

I am on macOS Mojave",hi custom action run terminal see unity like documentation issue returned back unity like bunch match correctly screen shot,issue,positive,positive,neutral,neutral,positive,positive
530190006,"<img width=""501"" alt=""Screen Shot 2019-09-10 at 7 02 24 PM"" src=""https://user-images.githubusercontent.com/2358507/64662920-f198fd00-d3fe-11e9-8021-44507efbd94c.png"">
I believe I have ran into the same issue. I left my computer idle for a few hours and when I came back I got this error. ",screen shot believe ran issue left computer idle came back got error,issue,negative,neutral,neutral,neutral,neutral,neutral
530161071,"Hi @ulyssesp,
Thanks for the suggestion. We'll take it under consideration, but Academy::maxSteps is likely going to be removed soon (in https://github.com/Unity-Technologies/ml-agents/pull/2519/files).",hi thanks suggestion take consideration academy likely going removed soon,issue,negative,positive,neutral,neutral,positive,positive
530156626,"I'm not familiar with the tensorflow graph transform tools, and it's not something that we can provide any support for.

Backing up a bit, what are you trying to accomplish by editing the network? ",familiar graph transform something provide support backing bit trying accomplish network,issue,positive,positive,positive,positive,positive,positive
530155933,"Hi @jkim447,
Sorry for the delay on this. There's nothing special in ML-Agents to get the original dimensions. However, you could use custom reset parameters to scale the object around each axis (say between .9x and 1.1x of its original size), for example
https://github.com/Unity-Technologies/ml-agents/blob/72088537f9ce3a3fde5ab21940d2027a42cae0fc/UnitySDK/Assets/ML-Agents/Examples/BananaCollectors/Scripts/BananaAgent.cs#L289",hi sorry delay nothing special get original however could use custom reset scale object around axis say original size example,issue,negative,positive,positive,positive,positive,positive
530153630,"Hi @tomatenbrei,
Taking over this from @ervteng 
I played around with docopt a bit, and I think this is slightly cleaner by allowing repeated arguments. Here's an [example](
http://try.docopt.org/?doc=++++Usage%3A%0D%0A++++++mlagents-learn+%3Ctrainer-config-path%3E+%5B--env-args%3D%3Cstring%3E...%5D+%5Boptions%5D%0D%0A++++++mlagents-learn+--help%0D%0A%0D%0A++++Options%3A%0D%0A++++++--env%3D%3Cfile%3E++++++++++++++++Name+of+the+Unity+executable+%5Bdefault%3A+None%5D.%0D%0A++++++--env-args%3D%3Cstring%3E+++++++++Arguments+of+the+Unity+executable.%0D%0A++++++--curriculum%3D%3Cdirectory%3E++++Curriculum+json+directory+for+environment+%5Bdefault%3A+None%5D.%0D%0A++++++--sampler%3D%3Cfile%3E++++++++++++Reset+parameter+yaml+file+for+environment+%5Bdefault%3A+None%5D.%0D%0A++++++--keep-checkpoints%3D%3Cn%3E++++++How+many+model+checkpoints+to+keep+%5Bdefault%3A+5%5D.%0D%0A++++++--lesson%3D%3Cn%3E++++++++++++++++Start+learning+from+this+lesson+%5Bdefault%3A+0%5D.%0D%0A++++++--load++++++++++++++++++++++Whether+to+load+the+model+or+randomly+initialize+%5Bdefault%3A+False%5D.%0D%0A++++++--run-id%3D%3Cpath%3E+++++++++++++The+directory+name+for+model+and+summary+statistics+%5Bdefault%3A+ppo%5D.%0D%0A++++++--num-runs%3D%3Cn%3E++++++++++++++Number+of+concurrent+training+sessions+%5Bdefault%3A+1%5D.%0D%0A++++++--save-freq%3D%3Cn%3E+++++++++++++Frequency+at+which+to+save+model+%5Bdefault%3A+50000%5D.%0D%0A++++++--seed%3D%3Cn%3E++++++++++++++++++Random+seed+used+for+training+%5Bdefault%3A+-1%5D.%0D%0A++++++--slow++++++++++++++++++++++Whether+to+run+the+game+at+training+speed+%5Bdefault%3A+False%5D.%0D%0A++++++--train+++++++++++++++++++++Whether+to+train+model%2C+or+only+run+inference+%5Bdefault%3A+False%5D.%0D%0A++++++--base-port%3D%3Cn%3E+++++++++++++Base+port+for+environment+communication+%5Bdefault%3A+5005%5D.%0D%0A++++++--num-envs%3D%3Cn%3E++++++++++++++Number+of+parallel+environments+to+use+for+training+%5Bdefault%3A+1%5D%0D%0A++++++--docker-target-name%3D%3Cdt%3E+++Docker+volume+to+store+training-specific+files+%5Bdefault%3A+None%5D.%0D%0A++++++--no-graphics+++++++++++++++Whether+to+run+the+environment+in+no-graphics+mode+%5Bdefault%3A+False%5D.%0D%0A++++++--debug+++++++++++++++++++++Whether+to+run+ML-Agents+in+debug+mode+with+detailed+logging+%5Bdefault%3A+False%5D.%0D%0A++++++--multi-gpu+++++++++++++++++Setting+this+flag+enables+the+use+of+multiple+GPU%27s+%28if+available%29+during+training%0D%0A++++++++++++++++++++++++++++++++++%5Bdefault%3A+False%5D.&argv=mypath+--env-args+foo+--env-args%3Dbar%3D3) (with all the Unity options; please forgive the long url).

The relevant parts are 
* Add ` [--env-args=<string>...]` in the usage section
* Add `      --env-args=<string>         Arguments of the Unity executable.` to the options section (no default)

I couldn't decipher the syntax enough to get `...` working in just the options, but if you can, great.

This way you don't need to include `shlex` and do the splitting in mlagents code; it's all handled by the parser (and would also be supported by `argparse` if we ever switch to that).

Do you mind switching to this approach?",hi taking around bit think slightly cleaner repeated example unity please forgive long relevant add string usage section add string unity section default could decipher syntax enough get working great way need include splitting code handled parser would also ever switch mind switching approach,issue,positive,positive,positive,positive,positive,positive
530146191,"Hey @ervteng!

Please note that the provided implementation does not include argument parsing (aside from not splitting strings at whitespaces). It basically only forwards the value of `--env-args` to the executable environment(s). So there is actually no ""proper"" format in that sense.

The simplest argument format (which is straightforward and does not require any ""complicated"" parsing routines inside Unity) looks like

````--env-args=""--arg1 2 --arg2 'string value'""````

with spaces between the arguments name and value. This allows you to manually parse arguments just like it is done with the the native `--port` parameter used by the `Academy` class inside `ReadArgs`.

I think the type `string` in `--env-args=<string>` already implies that one should use quotation marks when using multiple environment arguments. Aside from that, there are no real formatting constraints.

However, I can still add an example to the help text and/or documentation, if you find that helpful!

",hey please note provided implementation include argument aside splitting basically forward value executable environment actually proper format sense argument format straightforward require complicated inside unity like value name value manually parse like done native port parameter used academy class inside think type string string already one use quotation multiple environment aside real however still add example help text documentation find helpful,issue,positive,positive,neutral,neutral,positive,positive
530116920,"Hey @tomatenbrei, thank you for submitting this change! The functionality seems good to me. If possible would it be OK to add an example of args to the help string? e.g. `--env-args=""--arg1=2 --arg2=3""` 

Would help people format them properly when using it. Thanks!",hey thank change functionality good possible would add example help string would help people format properly thanks,issue,positive,positive,positive,positive,positive,positive
530116124,"Thanks, that worked. Able to visualize the graph now. Do you think tensorflow graph transform tools can help in editing this neural net architecture.",thanks worked able visualize graph think graph transform help neural net architecture,issue,positive,positive,positive,positive,positive,positive
530079972,"I added the line ""self.summary_writer.add_graph(self.policy.graph, step)"" in trainer.write_summary(). But in the tensorboard i am seeing no graph definitions files were found.Also, receiving the message while training:

> INFO:mlagents.trainers:Cannot write text summary for Tensorboard. Tensorflow version must be r1.2 or above.

My tensorflow version is 1.7.1",added line step seeing graph message training write text summary version must version,issue,negative,neutral,neutral,neutral,neutral,neutral
530072383,"Hi @robertsolero,
I think this is a different underlying error than https://github.com/Unity-Technologies/ml-agents/issues/1505 - in that issue, `socket.bind()` raised `OSError: [Errno 98] Address already in use` but yours raises `socket.gaierror: [Errno 8] nodename nor servname provided, or not known`. I think we're incorrectly treating this error with `UnityWorkerInUseException`.

I'm not sure the best way to debug this, but it seems like there are some related stackoverflow questions, e.g.
* https://stackoverflow.com/questions/39970606/gaierror-errno-8-nodename-nor-servname-provided-or-not-known-with-macos-sie (same error, but thrown from different place)
* https://stackoverflow.com/questions/50376329/s-bind-nodename-nor-servname-provided-or-not-known
* https://apple.stackexchange.com/questions/253817/cannot-ping-my-local-machine

Can you try some of the diagnostic steps there and see if you can get any socket binding (outside of mlagents) working first?",hi think different underlying error issue raised address already use provided known think incorrectly treating error sure best way like related error thrown different place try diagnostic see get socket binding outside working first,issue,negative,positive,positive,positive,positive,positive
530062422,"Hi @chriselion ,
Just confirming, so I should add this line in trainer.py inside def write_summary?
",hi confirming add line inside,issue,negative,neutral,neutral,neutral,neutral,neutral
530056521,"Hi @rajatpaliwal,
I'm looking into the best way to do this; it looks like we don't have an option for it currently, but you can add
```python
self.summary_writer.add_graph(self.policy.graph, step)
```
to Trainer.write_summary(), and a visualization of the graph will be available in Tensorboard (see https://www.tensorflow.org/tensorboard/r1/graphs).

I don't think allowing editing of the graph is something that we plan to support.",hi looking best way like option currently add python step visualization graph available see think graph something plan support,issue,positive,positive,positive,positive,positive,positive
530051202,"@mmattar Oh I should have provided more description for this change :). This PR is to enable us to use ml-agents-cloud toolkit for the offline bc testing. We don't put the UnitySDK folder into the docker image since it is too big, that's why we point the config to demo folder, which we put into the docker image. 

It is not very necessary to change the default path, because that's the default path the demonstration recorder uses, and we refer to this path in our documentation here. https://github.com/Unity-Technologies/ml-agents/blob/3ee09631089e7e25425e1a5c1795bf995eb76ac0/docs/Training-Behavioral-Cloning.md",oh provided description change enable u use testing put folder docker image since big point folder put docker image necessary change default path default path demonstration recorder refer path documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
530044360,"Hey @chriselion @vincentpierre the tests are still stochastic. e.g. for the last test (Marwan's commit 72088537) it was failing, and I re-ran it, and it passed. Seems like the culprit is usually PPO discrete. 

Not sure what would be causing it as we've set the tensorflow and numpy seeds. ",hey still stochastic last test commit failing like culprit usually discrete sure would causing set,issue,negative,positive,neutral,neutral,positive,positive
530034481,Glad you got it working. Closing this issue...,glad got working issue,issue,negative,positive,positive,positive,positive,positive
530034102,"Version 0.9.3 was released yesterday, which contains the fix for this. I'll close this issue now.",version yesterday fix close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
529773999,"@xiaomaogy - I'm curious as to why we're making this change? (The PR description doesn't provide any documentation as to why.) Additionally, should we change the default path? ",curious making change description provide documentation additionally change default path,issue,negative,negative,neutral,neutral,negative,negative
529767865,"Thank you, chriselion! I hope that we could solve the issue soon. please comment it, if you find some clues :) ",thank hope could solve issue soon please comment find,issue,positive,neutral,neutral,neutral,neutral,neutral
529752922,Hopefully https://github.com/Unity-Technologies/ml-agents/pull/2524 and https://github.com/Unity-Technologies/ml-agents/pull/2521 mean that we'll have deterministic runs in the tests now. Can you hold off merging this and see if the others help?,hopefully mean deterministic hold see help,issue,positive,negative,negative,negative,negative,negative
529752347,"My testing is on MacOS using Unity 2017.410f1.

I looked at this today (along with @surfnerd) and we were able to reproduce similar results to what you were seeing on Mac too - the total allocated memory reported by the Unity profiler leveled off, but the amount reported by the system (Activity Monitor on Mac) kept increasing.

We think that what you're seeing is at least partly due to memory fragmentation as explained here: https://docs.unity3d.com/Manual/BestPracticeUnderstandingPerformanceInUnity4-1.html
The total memory allocated hits a peak, but because of fragmentation, the amount of memory from the OS increases.

The material swapping that happens in the hallway scene makes the fragmentation worse that it normally would be. If you comment out this part of the code
https://github.com/Unity-Technologies/ml-agents/blob/develop/UnitySDK/Assets/ML-Agents/Examples/Hallway/Scripts/HallwayAgent.cs#L47-L49
you should see that the memory increase is much slower than before. 

We're going to keep looking at other allocations that are still happening (especially if they're part of the SDK, not the ""demo"" code) to see if any of them can be reduced or eliminated.",testing unity today along able reproduce similar seeing mac total memory unity profiler amount system activity monitor mac kept increasing think seeing least partly due memory fragmentation total memory peak fragmentation amount memory o material swapping hallway scene fragmentation worse normally would comment part code see memory increase much going keep looking still happening especially part code see reduced,issue,negative,positive,neutral,neutral,positive,positive
529715231,"Hi @zako42 ,
Can you tell me how are you opening .pb files outside unity . What tensorflow tools are you using . Is there to configure these pb files according to our need.",hi tell opening outside unity configure according need,issue,negative,neutral,neutral,neutral,neutral,neutral
529714499,Would actually support @chriselion's change since it doesn't require mutating or deepcopying anything.,would actually support change since require anything,issue,negative,neutral,neutral,neutral,neutral,neutral
529711007,"You could iterate over the dictionaries to get a list of the fields the differ, e.g.
```
different_keys = {k, v: policy_brain.items() if v != expert_brain[k]} - {""brain_name"", ""vector_action_descriptions""}
if different_keys:
  raise UnityTrainerException(...)
```
but not a big deal.",could iterate get list differ raise big deal,issue,negative,neutral,neutral,neutral,neutral,neutral
529705151,"For background, we set the tf and np seeds here https://github.com/Unity-Technologies/ml-agents/blob/5ada924bab9b57d1f6fbc078dedaf07a088ace5b/ml-agents/mlagents/trainers/trainer_controller.py#L66-L67
but not python's global random seed.

The only other place this appears to be used is https://github.com/Unity-Technologies/ml-agents/blob/5ada924bab9b57d1f6fbc078dedaf07a088ace5b/ml-agents/mlagents/trainers/buffer.py#L218",background set python global random seed place used,issue,negative,negative,negative,negative,negative,negative
529694598,"@surfnerd / @vincentpierre - realized that my editor tests weren't passing, made the appropriate fix to tests and added relevant documentation.",editor passing made appropriate fix added relevant documentation,issue,negative,positive,positive,positive,positive,positive
529692384,@chriselion The whole reason we use dictionaries to compare the brain parameters is because we do not want to look at their fields individually.,whole reason use compare brain want look individually,issue,negative,positive,neutral,neutral,positive,positive
529680974,"Sorry for the delay on this. I confirmed with our platform team that ""Headless"" needs to be removed from the documentation (e.g. https://docs.unity3d.com/2019.3/Documentation/Manual/BuildSettings.html). ""Server Build"" is what you should be using instead; I'll update our docs to reflect this.",sorry delay confirmed platform team headless need removed documentation server build instead update reflect,issue,negative,negative,neutral,neutral,negative,negative
529675228,"Hi @MarkTension 
It looks like some of the Player settings have changed since that part of the documentation was written (That settings appears in the 2019.1 documentation https://docs.unity3d.com/2019.1/Documentation/Manual/class-PlayerSettingsStandalone.html but was apparently removed in 2019.2). You should be fine ignoring that setting for now; we'll update the documentation to be clearer.",hi like player since part documentation written documentation apparently removed fine setting update documentation clearer,issue,positive,positive,positive,positive,positive,positive
529670977,Can you also update the error message to be more informative about which fields differ? That might have made https://github.com/Unity-Technologies/ml-agents/issues/2434#issuecomment-529146845 easier to debug for the user.,also update error message informative differ might made easier user,issue,negative,neutral,neutral,neutral,neutral,neutral
529669652,"Hi @chenmingxiang110,
I think you're correct that they rays hit another object besides the ones that you're looking for.

I'm not an expert on Unity's Physics system, but I think the best fix would be to change where we perform the raycast (actually a sphere cast) here:
https://github.com/Unity-Technologies/ml-agents/blob/035045e75c04287a8ba6a3cdf65100995db7c283/UnitySDK/Assets/ML-Agents/Examples/SharedAssets/Scripts/RayPerception3D.cs#L61-L63
and pass a layerMask to the `SphereCast` call:
https://docs.unity3d.com/ScriptReference/Physics.SphereCast.html

More information on forming the LayerMask is here https://docs.unity3d.com/Manual/Layers.html

Can you give that a try and see if it helps? If so, I'll log a request to add it to a future version of the SDK.",hi think correct hit another object besides looking expert unity physic system think best fix would change perform actually sphere cast pas call information forming give try see log request add future version,issue,positive,positive,positive,positive,positive,positive
529661294,"Hi @banish09 
The scene in `UnitySDK/Assets/ML-Agents/Examples/BananaCollectors/Scenes/BananaIL.unity` should be usable with Behavioral Cloning. 

There are some sections in the docs https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Training-Behavioral-Cloning.md and a related section on recording demonstrations https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Training-Imitation-Learning.md#recording-demonstrations that might be helpful.

Note that there was a bug in the 0.9.2 release that broke BC. We just released a bugfix release (0.9.3) that addresses this.

You may also want to look into GAIL as an alternative to BC: https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Reward-Signals.md#gail-reward-signal",hi banish scene usable behavioral related section recording might helpful note bug release broke release may also want look alternative,issue,negative,neutral,neutral,neutral,neutral,neutral
529592227,"Yep, the first test breaks if `step` isn't initialized",yep first test step,issue,negative,positive,positive,positive,positive,positive
529579774,"Is this a problem with ML Agents? If so, please create a Bug report issue and fill in the requested information. If not, we are unable to provide any additional help.",problem please create bug report issue fill information unable provide additional help,issue,positive,negative,negative,negative,negative,negative
529269096,"Thank you for your reply. as you said, i track the memory state, and i want to see the point that memory eventually stop. But, in my environment, the memory state is just linearly increasing. 
following are snapshots... for checking the issue.

1. at start
![start](https://user-images.githubusercontent.com/53593292/64497776-d38ea800-d2eb-11e9-8ec5-f9fcde2e2698.PNG)
2. after 2min
![2min](https://user-images.githubusercontent.com/53593292/64497778-d9848900-d2eb-11e9-8d5b-4849b7817819.PNG)
3.after 8min
![8min](https://user-images.githubusercontent.com/53593292/64497782-df7a6a00-d2eb-11e9-9138-072e80197301.PNG)
4.after 20min
![20min](https://user-images.githubusercontent.com/53593292/64497785-e43f1e00-d2eb-11e9-8084-ca97742e11c4.PNG)

The example i used is ""HallWay"" in same configuration, as i explained above.
and here is etc...
ml-agent version: 0.9.2v
unity version: 2018.3.14
os: Window 10(x64)
cpu: i7-8700K
gpu: 2080RTX
ram: 32.0GB


+) could you tell me, your unity version & ml-agent version?",thank reply said track memory state want see point memory eventually stop environment memory state linearly increasing following issue start start min min min min min min example used hallway configuration version unity version o window ram could tell unity version version,issue,negative,neutral,neutral,neutral,neutral,neutral
529232539,"Hi, 

I would like to re open the subject, sorry if it's a noob issue but I'm struggling for few days now on exactly the same issue of cawelsky. The solution of DBuckley101 doesn't work for me as there is no proxy variable in my environment variables. 

I installed and run successfully the training for 3Dball example 1 month ago. I developed my own little game but I faced this issue. I got back at 3Dball to see if it was my own game the problem but I have still the issue with 3Dball now. 
I think the issue is  global as I have the same failed message when I try to run the --help command. And the .conda environment is created and the mlagents--learn command is in some way recognized as when I test another kind of command, the command is not recognized (I have well activated the ml-agents environment). 

Any idea ? 
Thanks a lot for your help",hi would like open subject sorry issue struggling day exactly issue solution work proxy variable environment run successfully training example month ago little game faced issue got back see game problem still issue think issue global message try run help command environment learn command way test another kind command command well environment idea thanks lot help,issue,positive,positive,neutral,neutral,positive,positive
529146845,"@tillmusshoff 
I have just started working with the Unity ML-Toolkit. I see there are plenty of examples in the ML agents for most of the algorithms, however, none for the behavioral cloning.

I am trying to use offline behavioral cloning method to train a NPC in a FPS game. I don't know how to go around the Academy and Agent Scripts for this and have found neither proper documentation nor remotely close example to be able to understand and reciprocate something similar.

Any help would be appreciated.",working unity see plenty however none behavioral trying use behavioral method train game know go around academy agent found neither proper documentation remotely close example able understand reciprocate something similar help would,issue,negative,neutral,neutral,neutral,neutral,neutral
529130857,"Closing, I accidentally made a spelling error in my brain name...",accidentally made spelling error brain name,issue,negative,neutral,neutral,neutral,neutral,neutral
529123753,I solved it. I read the FAQ and there was the solution. Sorry for not looking there first. Apparently the problem was that i had the http_proxy and https_proxy environment variables setted. Thanks for your help,read solution sorry looking first apparently problem environment thanks help,issue,positive,neutral,neutral,neutral,neutral,neutral
529114054,"well i might actually figured out what was causing the issue,
i miss understood the GetReward and SetReward methods.

i was sure that SetReward actually rewrite the reward of the entire run,
but there is the occomulative reward,
so, i was using to much SetReward, at the end of the run by multiplying my GetReward, value.
and that caused the issue (i guess).

still wierd that at first it worked perfectly fine with the desired result, and after a while ,it ""degenerated"" simply because of miss use of SetReward.",well might actually figured causing issue miss understood sure actually rewrite reward entire run reward much end run multiplying value issue guess still first worked perfectly fine desired result simply miss use,issue,positive,positive,positive,positive,positive,positive
529081315,"Thanks for your feedback @Avoca-do and @chriselion.
I did actually get better training results after splitting and normalizing the decimal places as suggested above. Great idea!",thanks feedback actually get better training splitting decimal great idea,issue,positive,positive,positive,positive,positive,positive
529042755,It shouldn't be a CPU issue. It looks like for some reason the environment and the Python aren't communicating. We've tested on both those Unity versions on Windows 10 with a Core i7 and it worked OK. By any chance could you post the output of the Player.log (https://docs.unity3d.com/Manual/LogFiles.html) after builded run and see if there are any errors?,issue like reason environment python communicating tested unity core worked chance could post output run see,issue,positive,neutral,neutral,neutral,neutral,neutral
528993826,"Hi @mbaske,
As mentioned in the issue template we don't have the resources to help debug issues with custom environments.

In general, I would say that if your model is that sensitive to the position, it's going to have trouble learning.

Some general suggestions would be to use the position of the ball relative to the agent (for example `agent.position - ball.position`) instead of the ball's absolute position, or consider using the velocity of the ball as observations. Hopefully these will make the model less sensitive to the small changes.

Hope that helps...",hi issue template help custom general would say model sensitive position going trouble learning general would use position ball relative agent example instead ball absolute position consider velocity ball hopefully make model le sensitive small hope,issue,positive,positive,neutral,neutral,positive,positive
528991845,"Hi @WannyWanny,
As mentioned in the template, we have limited resources and can't help much with custom environments like yours.

That being said - I would definitely increase the ray distance to make sure the agents can ""see"" the other objects.

Another option might be to try using curriculum learning (https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Curriculum-Learning.md) to gradually increase the size of the field once the agents have learned to play on a smaller field.",hi template limited ca help much custom like said would definitely increase ray distance make sure see another option might try curriculum learning gradually increase size field learned play smaller field,issue,positive,positive,positive,positive,positive,positive
528973748,"Nop, i have my firewall disabled. I'm using it on a computer without internet access. It is that a posible reason? Could it be a cpu issue?",disabled computer without access reason could issue,issue,negative,negative,negative,negative,negative,negative
528967638,"Hi @chriselion
Thank you for the fast support, much appreciated.
I can confirm the error is gone updating python to 3.6.1.
If I only knew it was that easy ;).    ",hi thank fast support much confirm error gone python knew easy,issue,positive,positive,positive,positive,positive,positive
528965220,"Hi @Avoca-do,
Are you seeing this behavior in a provided example environment? Unfortunately we don't have the resources to help debug issues with custom environments.",hi seeing behavior provided example environment unfortunately help custom,issue,negative,negative,negative,negative,negative,negative
528964473,There might be an issue with Windows Firewall - I'd check there and see if Unity is blocked. Did you get a message asking for access to your ports at any time?,might issue check see unity blocked get message access time,issue,negative,neutral,neutral,neutral,neutral,neutral
528949579,"Hi @hanseoup,
I observed similar behavior to what you described, using the Unity profiler to track the memory usage. However, the memory usage eventually levels out (about maybe 30 seconds of real time) and stays flat after this.

![image](https://user-images.githubusercontent.com/6877802/64447724-e9546100-d090-11e9-8d6f-040adc3f383d.png)

Can you confirm (preferably in the profiler) whether the memory growth eventually stops? If you're still concerned, we should be able to dig some more and see where the growth is coming from.",hi similar behavior unity profiler track memory usage however memory usage eventually maybe real time stay flat image confirm preferably profiler whether memory growth eventually still concerned able dig see growth coming,issue,positive,positive,positive,positive,positive,positive
528929664,"Hi @robertsolero,
It looks like typing.Deque was added in python 3.6.1 https://docs.python.org/3.6/library/typing.html#typing.Deque
so it's not available in your version (3.6.0).

Are you able to upgrade to 3.6.1? If not, if you're comfortable modifying the source locally, you should be able to remove the imports and usage (it's only used for type annotations).

I'll review our min python version separately to make this clearer in the future.",hi like added python available version able upgrade comfortable source locally able remove usage used type review min python version separately make clearer future,issue,positive,positive,positive,positive,positive,positive
528732285,"Hi cloudjay,

I came across the Deep Deterministic Policy Gradients (DDPG) algorithm
which builds a deterministic policy and was much more suited to my needs.
At the time of my problem there was no implementation for ML Agents and so
I moved over to using openAI gym and their baselines package.

Since then unity may have added more algorithms, I don't know.

I hope that helps!

On Fri, 6 Sep 2019, 01:30 cloudjay, <notifications@github.com> wrote:

> Hi! I also want to use the mean action for my project, so could you
> explain somewhere how you did this? Thanks!
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/1856?email_source=notifications&email_token=AHYPOMVLYOO56RGYCJ5LEXLQIGQDNA5CNFSM4HANUTEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD6BLCVI#issuecomment-528658773>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AHYPOMUPDD3GHOTZ5FHXARLQIGQDNANCNFSM4HANUTEA>
> .
>
",hi came across deep deterministic policy algorithm deterministic policy much need time problem implementation gym package since unity may added know hope wrote hi also want use mean action project could explain somewhere thanks reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
528658773,"Hi! I also want to use the mean action for my project, so could you explain somewhere how you did this? Thanks!",hi also want use mean action project could explain somewhere thanks,issue,negative,negative,neutral,neutral,negative,negative
528650600,"if you really wish,
you can normalize the position.
so -100, -100 would become something like -0.7, -0.7
and then send the distance,
the original magnitude to something like 0-1.


Second way i can think is:
lets assume the position of the ball is (423, 0) so magnitude is 423.
Again we send the position normalized.
and then we just pass each position digit separately.
so first 423 is 400 + 20 + 3
so we send the 400, 20 , 3 separately.
this way the agent, can know any tiny change in out position.

is that really necessary? idk, but that is the best way i can think of.

let say one vector stores the distance of the first number : 3 / 10 (or / 9)
second input : 2 / 10
third one : 4 / 10


so now instead of sending the ml agent 
423, 0 / 1000
or something 
you send it

1 , 0 , 0.3 , 0.2, 0.4

so any tiny change in you'r ball position would directly be sent to the agent.
i think that might be the best way.",really wish normalize position would become something like send distance original magnitude something like second way think assume position ball magnitude send position pas position digit separately first send separately way agent know tiny change position really necessary best way think let say one vector distance first number second input third one instead sending agent something send tiny change ball position would directly sent agent think might best way,issue,positive,positive,positive,positive,positive,positive
528522042,"Fixed in https://github.com/Unity-Technologies/ml-agents/pull/2498 - during a refactor, the step initialization got moved to `RLTrainer` instead of `Trainer` which explains why it didn't affect PPO training.

We'll work on some more automated testing to make sure there's better coverage on the BC Trainer code.",fixed step got instead trainer affect training work testing make sure better coverage trainer code,issue,positive,positive,positive,positive,positive,positive
528483406,"Same problem and i dont find any solution. I have windows 10 1803, cpu i5 6200U. I tested on unity 2018.3 and 2019.1, and with mlagents v0.7 and v0.9. it works for inference (control checkbox unchecked) but not for training (control checkbox checked)",problem dont find solution tested unity work inference control unchecked training control checked,issue,negative,neutral,neutral,neutral,neutral,neutral
528480167,"Hi @mnsmuts - thanks for the bug report. I think I see the problem, I'll work on a fix today.",hi thanks bug report think see problem work fix today,issue,negative,positive,positive,positive,positive,positive
528389425,@weizhuang-93 any reason why are you doing Batch normalisation on axis=1? Typically it should be feature axis (-1) and we support only this case.,reason batch typically feature axis support case,issue,negative,negative,negative,negative,negative,negative
528371662,"![mlagents0 9 2error2](https://user-images.githubusercontent.com/22680369/64347272-878deb80-cfeb-11e9-88b7-4604e0d4449a.PNG)
Same error with offline training using the basic model and a demo file.",error error training basic model file,issue,negative,neutral,neutral,neutral,neutral,neutral
528230822,"@zako42 Sorry for the late reply.
I think we have confirmed the possibilities based on your opinion and I think it's worth a try also.
Thank you for your kind explanation. I hope you get good results in your case too.
",sorry late reply think confirmed based opinion think worth try also thank kind explanation hope get good case,issue,positive,positive,positive,positive,positive,positive
528156693,Looks ok overall. I think a few things that we want to keep got aggressively dead-stripped.,overall think want keep got aggressively,issue,negative,neutral,neutral,neutral,neutral,neutral
528040677,"Oh, okay, I see. I was actually thinking that pre-training was a stage on its own. 
Thanks ! 

Closing the issue.  ",oh see actually thinking stage thanks issue,issue,negative,positive,neutral,neutral,positive,positive
527996810,"Hi @MoMe36, the pretraining is happening at the same time as the RL (it's better for the value network) and anneals over time. If you view the Tensorboard, you can see the pretraining loss and learning rate decreasing, and 0 out when pretraining `steps` is reached. ",hi mome pretraining happening time better value network time view see pretraining loss learning rate decreasing pretraining,issue,negative,positive,positive,positive,positive,positive
527835656,"Ok now I see where the problem is coming from. If I run the training on my local machine using my local X server linked to my physical monitor (my display hardware), it works properly, however, if I run it on a distant machine with no display hardware using my singularity image (see this [link](https://github.com/maystroh/RL_cataract/blob/master/README.md) for more details about it), the textures becomes black as you can see in the following images:
1. Local machine
![2019-09-04 09:51:05](https://user-images.githubusercontent.com/4221393/64245923-25aa8480-cf0c-11e9-8e7e-e46ca1f7d357.png)
2. Distant machine 
![2019-09-04 11:55:48](https://user-images.githubusercontent.com/4221393/64245941-2ba06580-cf0c-11e9-8696-0c815099b46c.png)

In this singularity image, I installed Xvfb which is the X server you proposed to run on machines with no display hardware and no physical input devices. It turns out the problem is in Xvfb but not sure why!! Still checking it and will keep you posted.
",see problem coming run training local machine local server linked physical monitor display hardware work properly however run distant machine display hardware singularity image see link becomes black see following local machine distant machine singularity image server run display hardware physical input turn problem sure still keep posted,issue,negative,positive,neutral,neutral,positive,positive
527835530,"Hi @mantasp 
I attached archive with original .pb file and converted .nn.

[testcnn.zip](https://github.com/Unity-Technologies/ml-agents/files/3573950/testcnn.zip)
",hi attached archive original file converted,issue,negative,positive,positive,positive,positive,positive
527697733,Good idea. I switched the target branch and added a note about the baselines version I used in the readme.,good idea switched target branch added note version used,issue,negative,positive,positive,positive,positive,positive
527689696,"In the Unity Documentation it says that ""Server Build"" was added in 2018.3, so maybe it replaced Headless? Headless is still listed as an option in the documentation for 2018.3 as well as the 2019 versions though so I don't know.",unity documentation server build added maybe headless headless still listed option documentation well though know,issue,negative,neutral,neutral,neutral,neutral,neutral
527687786,"If I cannot reproduce the issue, I will not be able to help. The fact that the materials in the Shared Assets are working indicates there might be something wrong with the texture. I encourage you to experiment with different textures and try to identify what is causing the issue. It might be resolution.",reproduce issue able help fact asset working might something wrong texture encourage experiment different try identify causing issue might resolution,issue,positive,neutral,neutral,neutral,neutral,neutral
527681443,"So you got the permissions straightened out? Here's a basic guide https://www.tutorialspoint.com/unix/unix-file-permission.htm but the short answer is that you'll probably need to run `chmod a+x [your file name]` to make the file executable.

I think for now, you can use ""Server Build"" instead of ""Headless Build"". I'll do some more digging into when ""Headless Build"" was removed from the build settings, and update the documentation accordingly.",got basic guide short answer probably need run file name make file executable think use server build instead headless build digging headless build removed build update documentation accordingly,issue,negative,neutral,neutral,neutral,neutral,neutral
527674625,"Huh. @chriselion This is how the build menu looks for me (Using Unity Editor 2018.3.11f19):

![Unity_2019-09-04_01-06-52](https://user-images.githubusercontent.com/3768115/64214345-712f4500-ceb0-11e9-97a7-dc9dda2ed58b.png)

I'm a newbie when it comes to Linux and VMs. ""the file doesn't have execute permissions"" how do I fix this? EDIT: I already fixed this part. And it appears to work (with a few exceptions) even without headless mode. Still I wonder why headless mode doesn't show up for me?",huh build menu unity editor come file execute fix edit already fixed part work even without headless mode still wonder headless mode show,issue,negative,positive,neutral,neutral,positive,positive
527674555,"@qfettes thank you again for submitting the PR. Can you target `develop` instead of `master`? I can merge it after that. Thanks! 

I'd also add under `Running OpenAI Baselines Algorithms` a third line saying ""These examples were tested with baselines version --your version of baselines--"", since we've found that OpenAI tends to break things pretty often in baselines. ",thank target develop instead master merge thanks also add running third line saying tested version version since found break pretty often,issue,positive,positive,positive,positive,positive,positive
527670706,"Done - I just opened a pull request which fixes the issue.

I just pulled from baselines today, so it's the latest version.",done pull request issue today latest version,issue,negative,positive,positive,positive,positive,positive
527659417,"Hi @linko994,
""Headless mode"" should be an option in the Build Settings for your scene:
![image](https://user-images.githubusercontent.com/6877802/64211098-efacc800-ce59-11e9-9b4b-5270228a3b91.png)

As for the exceptions, I think the first thing to solve is
`PermissionError: [Errno 13] Permission denied: '/data/home/linko994/dqn/PushBlock16/PushBlock16.x86_64'`
You'll need to debug this on your own, but my guess is either the file doesn't have execute permissions, or it was build for the wrong platform. 

For reference, the relevant parts of the code where the extensions are checked are here:
https://github.com/Unity-Technologies/ml-agents/blob/fe97df800b6c39e70beeb321abe4f9ed59ef8bfa/ml-agents-envs/mlagents/envs/environment.py#L187-L208
 ",hi headless mode option build scene image think first thing solve permission need guess either file execute build wrong platform reference relevant code checked,issue,negative,positive,neutral,neutral,positive,positive
527651140,"Hi @qfettes, I think this guidance would help a lot of people - it'd be great if you could make a PR to our `gym_unity` docs so you get credit for the fix :P.  

Also, out of curiosity, which version of Baselines are you using?",hi think guidance would help lot people great could make get credit fix also curiosity version,issue,positive,positive,positive,positive,positive,positive
527640893,"This is still an issue, but has a simple fix. Baselines expects the environments to be wrapped as SubprocessVecEnv or DummyVecEnv. Thus, change the make_unity_env function in the tutorial to:

```
from gym_unity.envs import UnityEnv

from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv
from baselines.common.vec_env.dummy_vec_env import DummyVecEnv
from baselines.bench import Monitor
from baselines import logger
import baselines.ppo2.ppo2 as ppo2

try:
    from mpi4py import MPI
except ImportError:
    MPI = None

def make_unity_env(env_directory, num_env, visual, start_index=0):
    """"""
    Create a wrapped, monitored Unity environment.
    """"""
    def make_env(rank, use_visual=True): # pylint: disable=C0111
        def _thunk():
            env = UnityEnv(env_directory, rank, use_visual=use_visual, uint8_visual=True)
            env = Monitor(env, logger.get_dir() and os.path.join(logger.get_dir(), str(rank)))
            return env
        return _thunk
    if visual:
        return SubprocVecEnv([make_env(i + start_index) for i in range(num_env)])
    else:
        rank = MPI.COMM_WORLD.Get_rank() if MPI else 0
        return DummyVecEnv([make_env(rank, use_visual=False)])
```

Edit: I should point out the only change is the final line:

`        return DummyVecEnv([make_env(rank, use_visual=False)])`",still issue simple fix wrapped thus change function tutorial import import import import monitor import logger import try import except none visual create wrapped unity rank rank monitor rank return return visual return range else rank else return rank edit point change final line return rank,issue,negative,negative,negative,negative,negative,negative
527639205,"Actually, the material I use was different from those used ""UnitySDK/Assets/ML-Agents/Examples/SharedAsset"" but I modified it to have the same settings as the ones used in the SharedAsset but it is still showing black at runtime. Here are the settings I'm using for my material:
![Screenshot from 2019-09-03 22-58-06](https://user-images.githubusercontent.com/4221393/64208389-01648e80-ce9f-11e9-9760-d3f0f89356a1.png)
",actually material use different used used still showing black material,issue,negative,negative,neutral,neutral,negative,negative
527628307,"@Usmaniatech, gridworld should work fine. Multi-agent might not though. ",work fine might though,issue,negative,positive,positive,positive,positive,positive
527585454,Thank you for the discussion. We are closing this issue due to inactivity. ,thank discussion issue due inactivity,issue,negative,negative,negative,negative,negative,negative
527583639,"I tried to reproduce this bug but I did not succeed. I think it is because I do not know how to : 

> Add a texture image to a material

Is this different from what we do with the Checker materials in PushBlock ? How is this different from the Materials present in ""UnitySDK/Assets/ML-Agents/Examples/SharedAsset"" ?
It could be that the Material or texture you are using have different settings.",tried reproduce bug succeed think know add texture image material different checker different present could material texture different,issue,negative,neutral,neutral,neutral,neutral,neutral
527561828,"Hi @sergipv - Thanks for catching this. Can you change the branch from master to develop? We don't allow direct contributions to master in this way.

Once that's done, I'll merge the change.",hi thanks catching change branch master develop allow direct master way done merge change,issue,positive,positive,positive,positive,positive,positive
527464966,"I am still experiencing this issue in ubuntu 18.04, ml-agents 0.9.1. I have to wait sometime before I re-run mlagents-learn.",still issue wait sometime,issue,negative,neutral,neutral,neutral,neutral,neutral
527061825,"You can find the singularity image in this link: https://github.com/maystroh/RL_cataract
",find singularity image link,issue,negative,neutral,neutral,neutral,neutral,neutral
526971010,"@Santorayo3  You are input tensor error， the input need set [n, 80, 100, 3], do you set Stacked Vectors?",input tensor input need set set,issue,negative,neutral,neutral,neutral,neutral,neutral
526964645,"Actually, it is still an issue.  But only if curiosity is turned on.",actually still issue curiosity turned,issue,negative,neutral,neutral,neutral,neutral,neutral
526931825,"Hi all. To have the simulation proceeds from python, you must call `env.step()` from python. Unity will wait otherwise for the action from python. If you'd like to run in a simple loop, you can call `env.step()` without passing any actions. Otherwise, you would compute actions for the agents in your scene to take.",hi simulation proceeds python must call python unity wait otherwise action python like run simple loop call without passing otherwise would compute scene take,issue,negative,positive,neutral,neutral,positive,positive
526872678,"Hi there,
we also came across similar problems. The code hangs after we run the env.reset() block. and in the small window, we could still control the angle we see using our mouses. The whole program just gets stuck on the reset part. Does anyone have any idea what kind of error we are running into? And we checked the brain attached to the agent, or any solutions above. None of them has worked yet.",hi also came across similar code run block small window could still control angle see whole program stuck reset part anyone idea kind error running checked brain attached agent none worked yet,issue,negative,positive,positive,positive,positive,positive
526832152,"Hi, i have the same problem, i like to know if you did find the solution",hi problem like know find solution,issue,negative,neutral,neutral,neutral,neutral,neutral
526832139,"I hope we can direct use ML in unity editor, I mean needn't to install python or other tools outside;
need check this one:  https://github.com/tcmxx/UnityTensorflowKeras",hope direct use unity editor mean need install python outside need check one,issue,negative,negative,neutral,neutral,negative,negative
526766203,"Fantastic, thank you for confirming that.
Looking forward to testing it out. ",fantastic thank confirming looking forward testing,issue,positive,positive,positive,positive,positive,positive
526708673,"As @arixlin is saying, you can indeed have multiple brains in the Academy Hub at once. You can put several learning brains in it and only the ones with the `Control` checkbox will be trained.",saying indeed multiple brain academy hub put several learning brain control trained,issue,negative,neutral,neutral,neutral,neutral,neutral
526708048,"This error arrises because your Python ml-agents and your Unity ml-agents do not have the same version. To upgrade python, you need to run `pip3 install mlagents==X.Y.Z` where X.Y.Z is the version you want. To upgrade the unity code version, you must reclone and update all the files of [this folder](https://github.com/Unity-Technologies/ml-agents/tree/master/UnitySDK/Assets/ML-Agents) in you project.

Closing this issue as it is a duplicate of #1883, #2444 and #2182",error python unity version upgrade python need run pip install version want upgrade unity code version must update folder project issue duplicate,issue,negative,neutral,neutral,neutral,neutral,neutral
526645436,"@Junggy I added your report to our issue tracking system.
@AkhilRaja I don't think your issue is related to BatchNormalization import, could you please make separate report?",added report issue system think issue related import could please make separate report,issue,negative,neutral,neutral,neutral,neutral,neutral
526622543,"With stacked vector = 5 and gamma = 0.99 agents learn well shooting each other.

Thank you,
for the point of multi-agent i'll open a new discussion.",vector gamma learn well shooting thank point open new discussion,issue,positive,positive,neutral,neutral,positive,positive
526586746,"Just kidding, it works with graphics on. Sorry for being silly.",work graphic sorry silly,issue,negative,negative,negative,negative,negative,negative
526507910,"I've made several changes over time and don't know the settings I used there. All I can say that the brains had a visual observation with 100x80px (color) and some vector observations like their position, health, ammo and so on. Those vector observations were stacked to about 15-30 as far as I remember. Both brains had the same settings.",made several time know used say brain visual observation color vector like position health ammo vector far remember brain,issue,negative,positive,neutral,neutral,positive,positive
526502899,"Hi, i encountered similar problems but i do not know if it is the same bug. i have a cnn model, using DataFrmt NHWC, and named input and output. When i try to convert the .pb file with barracuda, i get the following error.
`(ml-agents) C:\>python ml-agents-0.9.1\ml-agents\mlagents\trainers\tensorflow_to_barracuda.py D:\05_Programm_und_Code\TFSharpTest\Assets\CNN_TF_1-7-1_BNAxis-1.pb D:\05_Programm_und_Code\TFSharpTest\Assets\CNN_TF_1-7-1_BNAxis-1.nn
Converting D:\05_Programm_und_Code\TFSharpTest\Assets\CNN_TF_1-7-1_BNAxis-1.pb to D:\05_Programm_und_Code\TFSharpTest\Assets\CNN_TF_1-7-1_BNAxis-1.nn
Traceback (most recent call last):
  File ""path\ml-agents-0.9.1\ml-agents\mlagents\trainers\tensorflow_to_barracuda.py"", line 26, in <module>
    tf2bc.convert(args.source_file, args.target_file, args.trim_unused_by_output, args)
  File ""path\ml-agents-0.9.1\ml-agents\mlagents\trainers\tensorflow_to_barracuda.py"", line 1606, in convert
    data=np.reshape(get_tensor_data(o_model.tensors[x]), shape).astype(
  File ""path\ml-agents-0.9.1\ml-agents\mlagents\trainers\tensorflow_to_barracuda.py"", line 697, in get_tensor_data
    return np.array(data).reshape(dims)
UnboundLocalError: local variable 'data' referenced before assignment`

any ideas why this happen? i am new to ml-agents and have been experimenting with tensorflow since may. i am out of idea. i am using:
tensorflow-gpu = 1.7.1,
[CNN_TF_1-7-1_BNAxis-1.zip](https://github.com/Unity-Technologies/ml-agents/files/3558900/CNN_TF_1-7-1_BNAxis-1.zip)
python 3.6.6
windows 10, visual studio 17",hi similar know bug model input output try convert file barracuda get following error python converting recent call last file line module file line convert shape file line return data local variable assignment happen new since may idea python visual studio,issue,negative,positive,neutral,neutral,positive,positive
526493746,@EnthusedDragon  Hi， you can let other brain running in internal.,let brain running internal,issue,negative,neutral,neutral,neutral,neutral,neutral
526365240,"I did it like 2-3 days prior to running the agents.

UPDATE:
I did as you said and that error's gone, however I get this now:
""Key Error: 'reward_signals'""
Any idea what could that mean?

![image](https://user-images.githubusercontent.com/16209835/63978403-8c681200-cab6-11e9-986f-a3499547d87d.png)
",like day prior running update said error gone however get key error idea could mean image,issue,negative,negative,negative,negative,negative,negative
526361986,"Your assets may be of the newest version, but when last did you install/update your ml agents environment? I think my main issue was the need to update my ml agents environment, in which case I just deleted it and installed it from scratch. If this doesn't fix it then I'm not sure what the problem may be.",asset may version last environment think main issue need update environment case scratch fix sure problem may,issue,negative,positive,positive,positive,positive,positive
526299501,"The error posted is after i followed the steps mentioned in the blog. @Deepak-HpyAda  I got the errors after doingg all the steps mentioned in the blog and further. @xiaomaogy  I cannot repoen the issue, could you please do that and discuss further on how to solve this issue ? I see a lot of people facing similar issues.
",error posted got issue could please discus solve issue see lot people facing similar,issue,negative,neutral,neutral,neutral,neutral,neutral
526274157,"I am experiencing this as well. It happens more frequently when my agent hits its maximum reward (I call Done() when this happens) and the ""Std of reward"" is 0.00. I would like to understand how it determines this, so it doesn't interfere with my training. When it is hitting max reward, I increase difficulty, so I want it to continue learning.",well frequently agent maximum reward call done reward would like understand interfere training reward increase difficulty want continue learning,issue,positive,positive,neutral,neutral,positive,positive
525956056,I got a verbal approval so will merge. ,got verbal approval merge,issue,negative,neutral,neutral,neutral,neutral,neutral
525944206,"> Have you resolved this issue? I'm getting the exact same error when running mlagents-learn, after uninstalling 'regular' TF and installing the TF-gpu as mentioned in the Installation-Windows doc

I have not, no. Could be an issue with TensorFlow 1.7",resolved issue getting exact error running doc could issue,issue,negative,positive,positive,positive,positive,positive
525882604,"From the code [here](https://github.com/Unity-Technologies/ml-agents/blob/3ee09631089e7e25425e1a5c1795bf995eb76ac0/UnitySDK/Assets/ML-Agents/Examples/SharedAssets/Scripts/RayPerception3D.cs#L18) : 
```csharp
        /// <summary>
        /// Creates perception vector to be used as part of an observation of an agent.
        /// Each ray in the rayAngles array adds a sublist of data to the observation.
        /// The sublist contains the observation data for a single ray. The list is composed of the following:
        /// 1. A one-hot encoding for detectable objects. For example, if detectableObjects.Length = n, the
        ///    first n elements of the sublist will be a one-hot encoding of the detectableObject that was hit, or
        ///    all zeroes otherwise.
        /// 2. The 'length' element of the sublist will be 1 if the ray missed everything, or 0 if it hit
        ///    something (detectable or not).
        /// 3. The 'length+1' element of the sublist will contain the normalised distance to the object hit.
        /// NOTE: Only objects with tags in the detectableObjects array will have a distance set.
        /// </summary>
        /// <returns>The partial vector observation corresponding to the set of rays</returns>
        /// <param name=""rayDistance"">Radius of rays</param>
        /// <param name=""rayAngles"">Angles of rays (starting from (1,0) on unit circle).</param>
        /// <param name=""detectableObjects"">List of tags which correspond to object types agent can see</param>
        /// <param name=""startOffset"">Starting height offset of ray from center of agent.</param>
        /// <param name=""endOffset"">Ending height offset of ray from center of agent.</param>
```",code summary perception vector used part observation agent ray array data observation observation data single ray list composed following detectable example first hit otherwise element ray everything hit something detectable element contain distance object hit note array distance set partial vector observation corresponding set param radius param starting unit circle param list correspond object agent see param starting height offset ray center param ending height offset ray center,issue,negative,negative,neutral,neutral,negative,negative
525880379,"Discrete : 
```pyton
self.loss = tf.reduce_sum(-tf.log(self.action_probs + 1e-10) * self.action_oh
```
Continuous : 
```pyton
self.loss = tf.reduce_sum(tf.squared_difference(self.clipped_true_action, self.sample_action))
```
In both continuous and discrete, the value of the loss will depend on the batch size and on the size of the action space. 
The ideal value is as low as possible !
",discrete continuous continuous discrete value loss depend batch size size action space ideal value low possible,issue,positive,positive,positive,positive,positive,positive
525840572,"Hi @DanAmador, please fill the CLA and I'm happy to merge in your PR. ",hi please fill happy merge,issue,positive,positive,positive,positive,positive,positive
525832672,"@jjangshan yes, this is what I'm trying to do as well.  I have agents I train in Unity and want to use them in another program (not Unity).  What I am trying to do is take the pb file, and open it in the other program using Tensorflow library to create the Tensorflow graph and session.  Then I fill in the observations and run the session to get the output action.

I guess another possibility is to create a small Tensorflow program C to run the pb file, and your Game A could communicate with program C (over the network, etc) to get decisions.

I think it will work if your testbed B and Game A are similar enough.  But I haven't gotten everything working yet in my situation, so I can't say for sure.",yes trying well train unity want use another program unity trying take file open program library create graph session fill run session get output action guess another possibility create small program run file game could communicate program network get think work game similar enough gotten everything working yet situation ca say sure,issue,positive,negative,neutral,neutral,negative,negative
525782095,I'm using the newest version cloned from the repo so when I deleted the ML-Agents folder and reimported it - nothing changed. Any other suggestions on how to fix that problem?,version folder nothing fix problem,issue,negative,neutral,neutral,neutral,neutral,neutral
525743479,"Yeah sure. Here's a [link](https://gofile.io/?c=caxd3J)

The .nn file is the actual file used in my project currently, the .pb file is a very similar network I've just been working on, accidentally overwriting the original one. It shouldn't make a difference though, it's just a few extra conv layers.",yeah sure link file actual file used project currently file similar network working accidentally original one make difference though extra,issue,positive,positive,positive,positive,positive,positive
525713346,"> In the meantime I've also enabled ""Require ES3.1"" and ""Require ES3.1+AEP"" and I've fiddled with texture settings and Project Settings (specifically related to graphics, enabling only GLES3.1 and Vulkan) so that might have had an effect.

This is expected, though we should better document it.

> I still don't know exactly why this works or what separates ComputeRef from Compute (I can't find any documentation), so if someone (@mantasp ?) can enlighten me that would be swell. For now though I guess the issue is closed.

This actually sounds like a bug. Could you share your network (random weights are fine) for us to test?",also require e require texture project specifically related graphic might effect though better document still know exactly work separate compute ca find documentation someone enlighten would swell though guess issue closed actually like bug could share network random fine u test,issue,positive,positive,neutral,neutral,positive,positive
525692195,"Update: I found a solution to this problem. Neither Compute nor ComputePrecompiled worked, so I tested ComputeRef. I still have no idea about the difference, but Ref seems to work. 

In the meantime I've also enabled ""Require ES3.1"" and ""Require ES3.1+AEP"" and I've fiddled with texture settings and Project Settings (specifically related to graphics, enabling only GLES3.1 and Vulkan) so that might have had an effect. 

I still don't know exactly why this works or what separates ComputeRef from Compute (I can't find any documentation), so if someone (@mantasp ?) can enlighten me that would be swell. For now though I guess the issue is closed.",update found solution problem neither compute worked tested still idea difference ref work also require e require texture project specifically related graphic might effect still know exactly work separate compute ca find documentation someone enlighten would swell though guess issue closed,issue,negative,positive,neutral,neutral,positive,positive
525626925,"@mantasp I'm already doing that. I create a new Tensor(with RenderTexture and channels) and pass it to my worker. My inference code is below if that helps:

```
var tensor = new Tensor(intermediateRendTex, 3);
            
worker.Execute(tensor);
worker.WaitForCompletion(); //this doesn't make a difference, it's just here to show I've tried it

var results = worker.Fetch(); //peek is the exact same

//code to print results exists here but is not shown

//we are done processing the results so we dispose
results.Dispose();
tensor.Dispose();
```",already create new tensor pas worker inference code tensor new tensor tensor make difference show tried exact print shown done dispose,issue,negative,positive,positive,positive,positive,positive
525577246,"Hi @zako42 , Thank you for your help. I found both nn, pb and ckpt files.

May i ask you one more question?

There is Game A, which is difficult to repeat for learning but, available inference/test using tensorflow  (Not based on Unity)

In Unity, I create a small testbed B similar to Game A, and learn by ml-agent using equalized input (observation) and output (action).

If then, can I use that model file(bytes/nn) in Game A? (Not using Unity)",hi thank help found may ask one question game difficult repeat learning available based unity unity create small similar game learn input observation output action use model file game unity,issue,positive,negative,negative,negative,negative,negative
525541491,"Ok, sorry if this is a dumb question:

Looking at the `Multinomial::Eval()` it is using a random number with the CDF it calculates with the action_probs.  In my use case with the C++ simulation with a trained agent, I'm running inference only -- no training.  So should I not use random number with CDF and just use the highest action probability then?  I'm thinking the randomness is to allow for exploration?  In reinforcement learning when we run inference, do we only exploit?  Or should I still allow for exploring during inference?  (Sorry for my ignorance, I'm still trying to learn these things)
",sorry dumb question looking multinomial random number use case simulation trained agent running inference training use random number use highest action probability thinking randomness allow exploration reinforcement learning run inference exploit still allow exploring inference sorry ignorance still trying learn,issue,negative,negative,negative,negative,negative,negative
525493977,"It does not work, you have to change it in the Unity editor. ",work change unity editor,issue,negative,neutral,neutral,neutral,neutral,neutral
525493503,I think you haven't installed the tensorflow correctly. Following the tensorflow guide and install 1.7.1 would be what you want. ,think correctly following guide install would want,issue,negative,neutral,neutral,neutral,neutral,neutral
525490979,"Thanks for reaching out to us. Hopefully you were able to resolve your issue.  We are closing this due to inactivity, but if you need additional assistance, feel free to reopen the issue.",thanks reaching u hopefully able resolve issue due inactivity need additional assistance feel free reopen issue,issue,positive,positive,positive,positive,positive,positive
525480891,Each time you update content of the RenderTexture you have to re-create input Tensor and pass new instance to the Barracuda.,time update content input tensor pas new instance barracuda,issue,negative,positive,positive,positive,positive,positive
525472086,"It looks like the `AgentAction` might be set by the code in `DiscreteActionOutputApplier` class, using the `Multinomial::Eval()`.  I don't understand it yet, but looks promising.",like might set code class multinomial understand yet promising,issue,positive,positive,positive,positive,positive,positive
525464688,"Posting a little more information for context, if that helps:

I ran the Tensorflow session using the `frozen_graph_def.pb` file.  Visualizing the graph in Tensorboard shows a bunch of nodes, but these are the ones I was looking at in particular:

`vector_observation`
Operation: Placeholder
Attributes(2)
dtype {""type"": ""DT_FLOAT""}
shape {""shape"":{""dim"":[{""size"":-1}, {""size"": 6}]}}
Inputs(0)
Outputs(1)
main_graph_0/hidden_0/MatMul

`action_masks`
Operation: Placeholder
Attributes(2)
dtype {""type"":""DT_FLOAT""}
shape {""shape"":{""dim"":[{""size"":-1},{""size"":9}]}}
Inputs(0)
Outputs(1)
strided_slice_1

`action`
Operation: Identity
Attributes(1)
T {""type"":""DT_FLOAT""}
Inputs(1)
concat_1
Outputs(0)

I plugged in numbers into the `vector_observation` and `action_masks`, ran the session, and then looked at `action` to see what the result was.  I was thinking the result would be the same value I get when calling `AgentAction()` in my Unity agent (a discrete number from 0 to 8 in this case).  However the output from running the tensorflow session manually was:

action Summary: [-21.1059818 -13.4949427 -0.000504382479 -10.6514177 -15.5597267 -7.64783669 -21.7325554 -19.7437592 -13.1423416]

while troubleshooting, I also printed out values for some other nodes:

value_estimate Summary: [0.545044422]
is_continuous_control Summary: 0
action_output_shape Summary: 9
action_probs Summary: [-9.94507313 -1.98709774 11.5074854 0.856563628 -4.05288267 3.86015272 -11.020257 -8.31383801 -1.63445354]

I tried manually adding input masks when running the session, and the values of the `action` array dropped to about -23 on the masked actions.

My guess is that I should be argmax-ing the return value of `action`, and it seems that the `action_probs` line up with the actions (they sort the same).  This is just my guess though.  I'm still trying to look through the ML Agents C# code to see if it does this kind of argmax, but haven't found anything yet.
",posting little information context ran session file graph bunch looking particular operation type shape shape dim size size operation type shape shape dim size size action operation identity type plugged ran session action see result thinking result would value get calling unity agent discrete number case however output running session manually action summary also printed summary summary summary summary tried manually input running session action array masked guess return value action line sort guess though still trying look code see kind found anything yet,issue,positive,positive,positive,positive,positive,positive
525452925,"Hi @jjangshan , my understanding is that you can use the pb file which is included in the subfolder where the .nn file is.

For example if your trained nn model is at `models/my-run-id/MyAgent.nn` you would have your pb file located at `models/my-run-id/MyAgent/frozen_graph_def.pb`.",hi understanding use file included file example trained model would file,issue,negative,neutral,neutral,neutral,neutral,neutral
525411965,"Hi, @vincentpierre 
Is there any way to customize or access the data sent to the neural net for training.",hi way access data sent neural net training,issue,negative,neutral,neutral,neutral,neutral,neutral
525244322,"Have you resolved this issue? I'm getting the exact same error when running mlagents-learn, after uninstalling 'regular' TF and installing the TF-gpu as mentioned in the Installation-Windows doc",resolved issue getting exact error running doc,issue,negative,positive,positive,positive,positive,positive
525079393,I think this might be very difficult or even impossible. We would not be able to load those with the demo_loader if there are custom fields. This is not how the demonstrations were meant to be used.,think might difficult even impossible would able load custom meant used,issue,negative,negative,negative,negative,negative,negative
525078547,"Hi @vincentpierre  Since the protobuf messages are used for storing data in .demo files , is it possible to customize this data storage process through this guide provide in the document : https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Creating-Custom-Protobuf-Messages.md.
",hi since used data possible data storage process guide provide document,issue,negative,neutral,neutral,neutral,neutral,neutral
524991003,"This is embarassing . I could have sworn I copy-pasted the command line from the Basic-Guide.md
Oh well perhaps this should be marked as an improvement request for people like me who randomly add command line arguments, some message that says ""no such command line argument exists"".",could sworn command line oh well perhaps marked improvement request people like randomly add command line message command line argument,issue,positive,negative,negative,negative,negative,negative
524967913,"Hi @joobei , what is the ""-s"" used for here? mlagents-learn doesn't use such an argument. ",hi used use argument,issue,negative,neutral,neutral,neutral,neutral,neutral
524924744,You seem to have modified the Dockerfile in the project. We can only attend bugs on existing features of ML-Agents. Please make sure the feature training in Docker is not working in a clean clone of `master`.,seem project attend please make sure feature training docker working clean clone master,issue,positive,positive,positive,positive,positive,positive
524768248,"The docker that I use now is built by https://github.com/Unity-Technologies/ml-agents/blob/master/Dockerfile, just remove EXPOSE 5005 and ENTRYPOINT [""mlagents-learn""] (last 2 lines) in order to operate in docker container.

I have tried the --no-graphics version, its ok. However, without ""--no-graphics"" the timeout error happens. Then I opened xvfb and try again, it seems better but still cannot run. 

**The Player.log:**
Desktop is 1024 x 768 @ 0 Hz
Unable to find a supported OpenGL core profile
Failed to create valid graphics context: please ensure you meet the minimum requirements
E.g. OpenGL core profile 3.2 or later for OpenGL Core renderer
Vulkan detection: 0
No supported renderers found, exiting

input: (inside container) 
glxinfo | grep ""OpenGL version""
and get error:
Error: couldn't find RGB GLX visual or fbconfig

If not open xfvb, the error becomes ""Error: unable to open display"", so I think the ""key problem"" is virtual screen. google says that the nvidia driver in host machine should be reinstall without opengl, but it seems impossible for me... I have tried different docker images but the errors are same.

Can xfvb be replaced with, for example, Xserver? 
I followed https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md, the error happens when I input nvidia-xconfig:
bash: nvidia-xconfig: command not found

After google and ""apt-get install nvidia-current nvidia-settings""
the problem becomes:
NVIDIA: API mismatch: the NVIDIA kernel module has version 410.79,
but this NVIDIA driver component has version 304.135.  Please make
sure that the kernel module and all NVIDIA driver components
have the same version.

So I do not konw what to do next....


After read some discussions in this github project, I also tried to build this Dockerfile (based on nvidia/cudagl:9.0-devel-ubuntu16.04, without last 2 lines) https://github.com/mneilly/linux-unity-ml-agents-nvidia-docker/blob/master/Dockerfile

But it also does not work. 

So... What can I do?

BTW, The Nvidia Driver Version in my server is 410.79, and nvidia-smi command works fine both in the image ""nvidia/cudagl:9.0-devel-ubuntu16.04"" and host machine. I do not have the administrator previlege of host machine so I cannot reinstall nvidia driver.",docker use built remove expose last order operate docker container tried version however without error try better still run unable find core profile create valid graphic context please ensure meet minimum core profile later core renderer detection found input inside container version get error error could find visual open error becomes error unable open display think key problem virtual screen driver host machine reinstall without impossible tried different docker example error input bash command found install problem becomes mismatch kernel module version driver component version please make sure kernel module driver version next read project also tried build based without last also work driver version server command work fine image host machine administrator host machine reinstall driver,issue,positive,negative,neutral,neutral,negative,negative
524725369,"Hi! I'm not an expert, but maybe your agent is exploring unwanted regions. How about lowering beta for reducing random exploration? By default, UnityML PPO lowers beta from 1e-3 to 1e-5 in models.py",hi expert maybe agent exploring unwanted lowering beta reducing random exploration default beta,issue,negative,negative,negative,negative,negative,negative
524644616,"I advise you to look the rayperception script in the ml-agents folder. It is a ready to use raycast for ML. It is used with the pyramid example. There is a 2d and 3d version of it. Consult this example, look at the agent prefab. You have to add a component for make it works. Then you create a variable RayPerception2/3d in your agent script and use a getcomponent function for  set this variable. 

Once done use a player brain, launch a simulation. You will get an error about the number of observation vectors you provide to your brain. Correct this and start your training.  ",advise look script folder ready use used pyramid example version consult example look agent prefab add component make work create variable agent script use function set variable done use player brain launch simulation get error number observation provide brain correct start training,issue,negative,positive,positive,positive,positive,positive
524643983,"Did you modified and saved some scripts as the training was ongoing ? 

If yes, it's what caused this error. 

You can always restart the training where it stopped by adding --load when you relaunch your training in the python/anaconda window. Just place it before the --training. ",saved training ongoing yes error always restart training stopped load relaunch training window place training,issue,negative,neutral,neutral,neutral,neutral,neutral
524543808,"I've found if I convert a TensorFlow network to Barracuda manually on the inference machine, the resulting nn will work, even if the nn generated automatically on the training machine fails with the above error.

To do this copy the folder inside ./models to the inference machine and run tensorflow_to_barracuda.py.
For example:
`(ml-agents) C:\Users\<username>\Documents\Unity Projects\UnityML>python ./ml-agents/mlagents/trainers/tensorflow_to_barracuda.py ./models/<run-id>/<brain name>/frozen_graph_def.pb ./models/<run-id>/<brain name>.nn`

I thought the problem could be a slight difference in the transcoding. I haven't compared the nns. I am fairly sure I have the same ml-agents revision on both machines. 

Neither have I tested this intensively, but it works on a couple of different training runs, between a W10 trainer and W7 inferrer, that have never successfully shared a network before.",found convert network barracuda manually inference machine resulting work even automatically training machine error copy folder inside inference machine run example python brain name brain name thought problem could slight difference fairly sure revision neither tested intensively work couple different training trainer inferrer never successfully network,issue,negative,negative,neutral,neutral,negative,negative
524469770,"@rajatpaliwal reward signals only work with the ""ppo"" trainer, as you need PPO to learn to maximize the reward signal. To use BC with reward signals, you could use the `pretraining` feature of PPO with a 0 `steps` parameter. ",reward work trainer need learn maximize reward signal use reward could use pretraining feature parameter,issue,positive,neutral,neutral,neutral,neutral,neutral
524466509,"> Changes LGTM -- but do you think we should do this at the BrainInfo level as well?

Possibly - maybe that's the better place to do it? So it's float32 all the way through, and we don't have to explicitly change it here in the buffer. ",think level well possibly maybe better place float way explicitly change buffer,issue,positive,positive,positive,positive,positive,positive
524429153,You could refer to our https://github.com/Unity-Technologies/ml-agents/blob/master/UnitySDK/Assets/ML-Agents/Scripts/DemonstrationRecorder.cs to see how it works. ,could refer see work,issue,negative,neutral,neutral,neutral,neutral,neutral
524314853,Hi @xiaomaogy thanks for the reply. Another temporary avenue to this issue that I have considered is running the environment through python and recording through that. Is there anywhere I could read more about reading and writing demo files?,hi thanks reply another temporary avenue issue considered running environment python recording anywhere could read reading writing,issue,negative,positive,positive,positive,positive,positive
524251564,"Thanks, that helped a little. Unfortunately I've run straight into the next error:
  File ""c:\program files\python\envs\ml-agents\lib\site-packages\tensorflow\python\client\session.py"", line 905, in run
    run_metadata_ptr)
  File ""c:\program files\python\envs\ml-agents\lib\site-packages\tensorflow\python\client\session.py"", line 1116, in _run
    str(subfeed_t.get_shape())))

ValueError: Cannot feed value of shape (2, 32, 80, 100, 3) for Tensor 'visual_observation_0:0', which has shape '(?, 80, 100, 3)'

The player and learning brain both have one visual observation with a 100x80 resolution.",thanks little unfortunately run straight next error file line run file line feed value shape tensor shape player learning brain one visual observation resolution,issue,negative,negative,neutral,neutral,negative,negative
524169148,"So I decided to test a little more. Apparently, if I wait 2 seconds before calling the inference function for the first time (to make sure the camera is running), the results remain the same but the app lags out for about a second. 

It seems like the inference sequence is never run again (if running it is supposed to lag it out) but I am not catching any exceptions which I would assume would happen if any of the method calls did not work.

Any idea how to further debug this?",decided test little apparently wait calling inference function first time make sure camera running remain second like inference sequence never run running supposed lag catching would assume would happen method work idea,issue,negative,positive,positive,positive,positive,positive
524093052,"Yes, the problem was I was editing the .yaml file in visual studio that was adding \t instead of spaces and was giving out error. Editing in Notepad++ solved the issue.

Query regarding gail_config:
We are performing offline behavioral cloning with added reward signals by using ""gail_config"" file. But I don't see any reward points being added while the training is taking place , also the student agent is barely learning anything compared to training with offline_bc_config.yaml file.
Any views on this point. ",yes problem file visual studio instead giving error issue query regarding behavioral added reward file see reward added training taking place also student agent barely learning anything training file point,issue,positive,positive,neutral,neutral,positive,positive
524078050,Your file seems correct. Maybe there is some hidden \t that's not showing up. I would recommend taking one of our existing .yaml and replace the fields directly in that file to fine out what's wrong. ,file correct maybe hidden showing would recommend taking one replace directly file fine wrong,issue,negative,negative,neutral,neutral,negative,negative
524040063,"So basically I have created a track on which the car can train. The car gets inputs from ray perceptions from five different angles and detects any fuel(which when hit gives a reward of +1) and walls (which when hit gives a reward of -1). It also gets the input of its current velocity in both x and z direction and also the position displacement since last frame. The rays are just like in this ->https://imgur.com/a/giF6b82

A -1 reward is given whenever the car hits the wall and a +1 reward is given whenever the car hits the fuel tanks. The fuel tanks are just white spheres placed in the middle of the road. Also a reward of -0.05 is also given whenever the velocity is below 0.25 magnitude and a reward of +0.05 whenever the velocity is more than 0.5. This is how I have setup my road track and fuels(white spheres)-->https://imgur.com/a/giF6b82

When I try to train it, it just keeps on hitting the walls. It perform good in certain random cases but doesn't improve on it and then in the next episode it again keeps on hitting the walls. Any help would be much appreciated because I am stuck with this for a really long time.",basically track car train car ray five different fuel hit reward hit reward also input current velocity direction also position displacement since last frame like reward given whenever car wall reward given whenever car fuel fuel white middle road also reward also given whenever velocity magnitude reward whenever velocity setup road track white try train perform good certain random improve next episode help would much stuck really long time,issue,positive,positive,neutral,neutral,positive,positive
524022629,"Got it. It's the checkpoint file in the models/folder . Changed the model name and is able to load any model of my choice.Thanks @xiaomaogy 
",got file model name able load model,issue,negative,positive,positive,positive,positive,positive
524016914,"I think this is a barracuda specific issue, so @mantasp ",think barracuda specific issue,issue,negative,neutral,neutral,neutral,neutral,neutral
524016490,"Hi @rajatpaliwal, --load will restore the model from the latest saved one. However you can also tweak one of the file within the models/ folder to change another model as the latest model (I don't remember which file, but you should be able to figure it out by looking at the files). ",hi load restore model latest saved one however also tweak one file within folder change another model latest model remember file able figure looking,issue,positive,positive,positive,positive,positive,positive
524015326,"Hi @DIPLO22, we in general don't help with a problem that is relevant to a custom environment due to the resource constraint. You can post more information like how you set up your rewards, how you setup your ray perceptions, so that other people in the community might help. ",hi general help problem relevant custom environment due resource constraint post information like set setup ray people community might help,issue,positive,positive,positive,positive,positive,positive
524014236,"Hi @superjayman, larger the image, longer the training would take. In our example environment, we use 84 * 84, and it is already good enough for most of the cases. ",hi image longer training would take example environment use already good enough,issue,negative,positive,positive,positive,positive,positive
524013824,"Hi @isotropic-panda This is a nice feature and we've internally discussed this before. However to make this workflow really good for the users, it takes some nontrivial effort. ",hi nice feature internally however make really good effort,issue,positive,positive,positive,positive,positive,positive
524012812,I think posting your .yaml file would help us see where the issue is. ,think posting file would help u see issue,issue,negative,neutral,neutral,neutral,neutral,neutral
524012260,"It lives here. And it is used to convert a model trained by ml-agents into something unity can use. 

https://github.com/Unity-Technologies/ml-agents/blob/3ee09631089e7e25425e1a5c1795bf995eb76ac0/ml-agents/mlagents/trainers/tensorflow_to_barracuda.py

@Akaash03 ",used convert model trained something unity use,issue,negative,neutral,neutral,neutral,neutral,neutral
523760508,"where to find tensorflow_to_barracuda.py file in the ml-agents repo and what is that file mean
please anyone can explain it?",find file file mean please anyone explain,issue,negative,negative,negative,negative,negative,negative
523570011,"`Unable to preload the following plugins: ScreenSelector.so` is not an error. The training should start regardless (We see this message systematically when using Linux). If the training does not start, it could be due to something else. 
Did you make sure your environment was working outside of Docker? Using in editor training might help you diagnose if something is wrong.

Also, in your steps to reproduce, are not using Docker the way it is presented in [this document](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Using-Docker.md). It says to start training with :

```
docker run -it --name 3DBallContainer.first.trial \
           --mount type=bind,source=""$(pwd)""/unity-volume,target=/unity-volume \
           -p 5005:5005 \
           -p 6006:6006 \
           balance.ball.v0.1:latest 3DBall \
           --docker-target-name=unity-volume \
           trainer_config.yaml \
           --env=3DBall \
           --train \
           --run-id=3dball_first_trial
```

What do you mean by _Environment: in the official docker_ ?",unable following error training start regardless see message systematically training start could due something else make sure environment working outside docker editor training might help diagnose something wrong also reproduce docker way document start training docker run name mount latest train mean official,issue,negative,negative,neutral,neutral,negative,negative
523452248,"With visual input say from a web-cam to ml-agents, would it be feasable for the system to learn to output a vec2 screen space position of recognized object?",visual input say would system learn output screen space position object,issue,negative,neutral,neutral,neutral,neutral,neutral
523220990,"Cool, thanks for reviewing it. 

@vincentpierre just checking if your concerns were addressed. 

@harperj I'm currently running a test on all the example environments. Below are the reported numbers for the environments that completed training. Reacher, Crawler, and Walker are still running. I still haven't found a solution for training Pyramids, though. Not sure what the right solution is here in terms of merging.

Max Score

- Basic - 0.939
- 3Ball - 100
- 3DBall Hard - 100
- Banana - 28 (!!)
- Bouncer - 19.6
- PushBlock - 4.9
- Tennis - 2.4-2.6
- Reacher - 39
- Hallway - 0.926
- GridWorld - 0.975
- WallJump - Big: ~0.4 Small: 0.9
- Dynamic Crawler - 420
- Static Crawler - 3000
 ",cool thanks currently running test example training reacher crawler walker still running still found solution training though sure right solution score basic ball hard banana bouncer tennis reacher hallway big small dynamic crawler static crawler,issue,positive,positive,positive,positive,positive,positive
522926721,"My real project must include visual observations, so I do not select the headless option.",real project must include visual select headless option,issue,negative,positive,neutral,neutral,positive,positive
522785420,"> We currently have benchmark reward for PPO in this page. But for SAC we currently don't have the corresponding value in this page. Should we included them as well? https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md

They ideally should be the same values as the environment doesn't change, and those values generally correspond to ""solving"" the problem. But there might be some differences. ",currently reward page sac currently corresponding value page included well ideally environment change generally correspond problem might,issue,positive,positive,positive,positive,positive,positive
522766550,We currently have benchmark reward for PPO in this page. But for SAC we currently don't have the corresponding value in this page. Should we included them as well? https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md,currently reward page sac currently corresponding value page included well,issue,positive,neutral,neutral,neutral,neutral,neutral
522702930,"I believe you should be able to run CUDA 9 on your GPU according to [this thread](https://devtalk.nvidia.com/default/topic/1055573/cuda-setup-and-installation/does-cuda-9-0-support-rtx-2060-/).
 i.e. running CUDA 9 on a GPU that has a newer driver should still work.  Note that the GPUs driver version is different than the CUDA version you have installed.

Have you run into errors when trying to make this work?",believe able run according thread running driver still work note driver version different version run trying make work,issue,negative,positive,positive,positive,positive,positive
522687489,"Hi, 
We have been experimenting internally with TensorFlow 1.13 and newer versions of CUDA on cloud machines. We have seen some results but we do not have a guide at this time.",hi internally cloud seen guide time,issue,negative,neutral,neutral,neutral,neutral,neutral
522682060,"There is OS specific limitation on Linux in that you cannot reuse a port right after you close it. The delay before it becomes available again is something like 60 seconds.
If you want to open multiple environments one after the other on Linux, consider changing ports every time and re-use ports only after a certain delay.
There might be bugs related to closing environments independent of this Linux limitation on previous versions of ML-Agents. Since ML-Agents is still in Beta, we only support the most recent version.
If you found a bug, please use the bug report template which must include steps to reproduce the bug.
Thank you!",o specific limitation reuse port right close delay becomes available something like want open multiple one consider every time certain delay might related independent limitation previous since still beta support recent version found bug please use bug report template must include reproduce bug thank,issue,positive,positive,neutral,neutral,positive,positive
522678571,"If you are trying to train two brains with two demos at the same time (either offline BC or GAIL), you need to specify the demo files for each of the brains : 

```
default:
    <default parameters>
TheFirstBrainLearning:
    trainer: offline_bc
    demo_path: <first_demo>.demo
TheSecondBrainLearning:
    trainer: offline_bc
    demo_path: <second_demo>.demo
```

If you want to train a single Brain with two demo files, we do not currently support this use case.",trying train two brain two demo time either need specify brain default default trainer trainer want train single brain two currently support use case,issue,negative,negative,neutral,neutral,negative,negative
522676987,"From the error, your python version of ML-Agents is the most recent one (API-9) but your Unity SDK is out of date. You need to replace the [C# code of ML-Agents](https://github.com/Unity-Technologies/ml-agents/tree/master/UnitySDK/Assets/ML-Agents) in your project. ",error python version recent one unity date need replace code project,issue,negative,neutral,neutral,neutral,neutral,neutral
522675896,"The ml-agents toolkit is focused on reinforcement learning, we do not support image classification, segmentation, labeling as of 0.9.1. We do support training RL agents using images as input, to do so, we feed the input image into several convolution layers. The original papers for the architectures are referenced [here](https://github.com/Unity-Technologies/ml-agents/blob/3ee09631089e7e25425e1a5c1795bf995eb76ac0/docs/Training-PPO.md#optional-visual-encoder-type)",reinforcement learning support image classification segmentation support training input feed input image several convolution original,issue,positive,positive,positive,positive,positive,positive
522479272,"> I think he means eg. image classification, segmentation, labeling ect

I've experimented with using YOLO within Unity. Although the project isn't utilizing ml-agents, I thought I'd mention it here, since it seems to do what you're looking for.
https://github.com/mbaske/yolo-unity
",think image classification segmentation experimented within unity although project thought mention since looking,issue,negative,neutral,neutral,neutral,neutral,neutral
522468149,"We convert to pytorch to onnx
https://pytorch.org/docs/stable/onnx.html then
to nn with onnx_to_barracuda
https://github.com/mantasp/barracuda-release/tree/release/0.2.4/Tools

On Fri, Aug 16, 2019 at 8:36 PM kumarict <notifications@github.com> wrote:

> Hi! Are there any updates on this? I have generated pytorch models as .pt
> or .pth files and would like to load them into Unity. It would be great to
> convert them into .nn files or import them directly or at least know when
> they will be supported. Thank you.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/1789?email_source=notifications&email_token=ABC3MW6A63K3NZHFPELHJSDQE3XUVA5CNFSM4G3ZVCDKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4PMGYI#issuecomment-522109793>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABC3MWYQET4TV4HAXGNFLODQE3XUVANCNFSM4G3ZVCDA>
> .
>
",convert wrote hi would like load unity would great convert import directly least know thank reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
522382658,"I think he means eg. image classification, segmentation, labeling ect. Would it be possible to train ml-agents to recognize peoples faces and put a box around it?

Also, The documentation says you added support for  Nature and ResNet , can you please elaborate on this, how do we actually use them(What is Nature vs ResNet)",think image classification segmentation would possible train recognize put box around also documentation added support nature please elaborate actually use nature,issue,positive,positive,positive,positive,positive,positive
522160555,"@ervteng I was thinking of this as a stepping stone towards automatically ticking the development version -- we should confirm the process works manually and then potentially add it to an automated process.  I think it's OK if we have `0.10.devX` be at the same position as `0.9.2` if something changes with our release schedule.

Will update the release checklist.",thinking stepping stone towards automatically ticking development version confirm process work manually potentially add process think position something release schedule update release,issue,negative,neutral,neutral,neutral,neutral,neutral
522155604,Glad it helped. Closing the issue for now - feel free to open it back up if you're still running into problems. ,glad issue feel free open back still running,issue,positive,positive,positive,positive,positive,positive
522154902,"Looks fine to me - I'd just update the release checklist to be sure to remove the `dev` from the `setup.py`. There's also the possibility of a major release becoming a minor release if a certain feature doesn't make it in by the end of the month. 

What do you think about auto-ticking the dev version, e.g. https://the-hitchhikers-guide-to-packaging.readthedocs.io/en/latest/specification.html#development-releases?",fine update release sure remove dev also possibility major release becoming minor release certain feature make end month think dev version,issue,positive,positive,positive,positive,positive,positive
522119125,"@vincentpierre 
I tried Renaming the json file from robotLearning.json to RobotLearning.json and it works.
It turned out to be case sensitive.
Thanks so much for your help!",tried file work turned case sensitive thanks much help,issue,positive,positive,positive,positive,positive,positive
522109793,Hi! Are there any updates on this? I have generated pytorch models as .pt or .pth files and would like to load them into Unity. It would be great to convert them into .nn files or import them directly or at least know when they will be supported. Thank you.,hi would like load unity would great convert import directly least know thank,issue,positive,positive,positive,positive,positive,positive
522105343,`robotLearning.json` or `RobotLearning.json`? I think the string matching between the curriculum file and the name of the Learning brain is case sensitive. Could you try with `RobotLearning.json` if you haven't tried yet?,think string matching curriculum file name learning brain case sensitive could try tried yet,issue,negative,positive,neutral,neutral,positive,positive
522104606,"robotLearning.json

Vincent-Pierre BERGES <notifications@github.com> schrieb am Fr. 16. Aug.
2019 um 20:13:

> What did you name the file in the config/curricula/robot directory?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/2438?email_source=notifications&email_token=AHID6NB5Q343HHNZFVSYJ43QE3U5JA5CNFSM4IL6PDHKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4PKNVY#issuecomment-522102487>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AHID6NCISL43XQQSNFGKMUTQE3U5JANCNFSM4IL6PDHA>
> .
>
-- 
viele Grüße
Lei Wei
",um name file directory thread reply directly view mute thread lei,issue,negative,positive,neutral,neutral,positive,positive
522104464,"> The steps given in the above link doesn't match to the environment while following the tutorial.

Can you be a little more specific? If there are errors in the documentation we would be happy to review. Additionally, if you want to suggest improvements, we would be happy to review a PR.
Fo a more beginner guide, you can look at the [RollerBall tutorial](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Create-New.md)

The ML-Agents toolkit is only meant for Reinforcement Learning and Imitation Learning applications (as of 0.9). You can use visual observations to train an agent but I am not sure what you mean by  ""train custom models for computer vision applications"".",given link match environment following tutorial little specific documentation would happy review additionally want suggest would happy review beginner guide look tutorial meant reinforcement learning imitation learning use visual train agent sure mean train custom computer vision,issue,positive,positive,positive,positive,positive,positive
522102814,"Yes, uninstalling ml-agents-envs and ml-agents and reinstalling them with pip install -e ./ allowed code modification to work I was able to delay the timeout_wait parameter to more than 30 secs. Thanks @ervteng  for your help. ",yes pip install code modification work able delay parameter thanks help,issue,positive,positive,positive,positive,positive,positive
522088198,"Yes, do `pip uninstall ml-agents` and `pip uninstall ml-agents-envs`. Then browse to your ml-agents directory, enter the `ml-agents-envs` sub-directory, and run `pip install -e ./`. Then enter the `ml-agents` sub-directory and do the same. That will install your source version of ml-agents into pip, so any code modifications will work ",yes pip pip browse directory enter run pip install enter install source version pip code work,issue,negative,neutral,neutral,neutral,neutral,neutral
522084597,"Hi @ervteng . Yes, I did installed ml-agents through pip. Is there a way to remove that pre-compiled version of ML-Agents. Or, I need to  uninstall the ML-Agent and then re-install them through some other method?",hi yes pip way remove version need method,issue,negative,neutral,neutral,neutral,neutral,neutral
522078562,"Hi @rajatpaliwal, did you install ml-agents through pip? if you did there's a pre-compiled version of ML-Agents installed somewhere that doesn't change when you edit your files, this might explain why the behavior isn't changing. ",hi install pip version somewhere change edit might explain behavior,issue,negative,neutral,neutral,neutral,neutral,neutral
521993654,"This happened to me and I figured it out. You probably grabbed a .nn file from a training run that had a greater number of observations than your current LearningBrain. For example, in my case, when I dragged a .nn model from a training run where I had 85 vector observations into a LearningBrain with only 54 vector observations, it gave me this error.

Here's a breakdown of what was happening in my case. The agentInfos dictionary is supposed to be cleared out at the end of LearningBrain.DecideAction(), however that line is never reached because _tensorGenerator.GenerateTensors() fails several lines earlier. The error message ""An item with the same key has already been added."" happens because the dictionary was never cleared.

The reason GenerateTensors() fails with an ArgumentOutOfRangeException is because of this part in GeneratorImpl.Generate(), line 96:
```
for (var j = 0; j < vecObsSizeT; j++)
{
    tensorProxy.Data[agentIndex, j] = vectorObs[j];
}
```
vecObsSizeT is pulled from the .nn file by Barracuda in LearningBrain, line 115:
`_barracudaModel = ModelLoader.Load(model.Value);`

So essentially, if your current agent has X vector observations but the .nn file was actually trained with Y vector observations and X < Y, you will get this error. If X >= Y, you may not get an exception, but the agent won't work right because it's the wrong trained model.

",figured probably file training run greater number current example case dragged model training run vector vector gave error breakdown happening case dictionary supposed end however line never several error message item key already added dictionary never reason part line file barracuda line essentially current agent vector file actually trained vector get error may get exception agent wo work right wrong trained model,issue,negative,positive,neutral,neutral,positive,positive
521987992,"@vincentpierre 
thanks for the reply.
But I also tried my own scene, it shows the error info too.
for my academy, it has only one brain and I did check the 'control'
![console](https://user-images.githubusercontent.com/30424884/63166734-93f7d780-c02f-11e9-92bf-d5c2422c1d80.PNG)
![inspector](https://user-images.githubusercontent.com/30424884/63166735-93f7d780-c02f-11e9-8104-742ac95b7edc.PNG)
![academy](https://user-images.githubusercontent.com/30424884/63166738-93f7d780-c02f-11e9-918a-e9b549bfce1f.PNG)
![curriculum](https://user-images.githubusercontent.com/30424884/63166878-e9cc7f80-c02f-11e9-9f54-a7d24bc7657b.PNG)

",thanks reply also tried scene error academy one brain check console inspector academy curriculum,issue,negative,positive,positive,positive,positive,positive
521851552,"@ervteng 
Yes, some of them will get stuck but no all when i using tensorflow-gpu.
I guess the situation is not caused by the tensorlfow-gpu or Barracuda setting, because both internal and external occur, Thanks your reply.",yes get stuck guess situation barracuda setting internal external occur thanks reply,issue,positive,positive,neutral,neutral,positive,positive
521851253,"They're sent over as numpy arrays - you can see them in the `environment.py` file. 

But usually just viewing the camera is enough. ",sent see file usually camera enough,issue,negative,negative,negative,negative,negative,negative
521851017,I'm assuming there is some sparse extrinsic reward right? You may be able to lower the weight of the curiosity in the later parts of training so they won't be as incentivized to be curious with each other,assuming sparse extrinsic reward right may able lower weight curiosity later training wo curious,issue,positive,positive,positive,positive,positive,positive
521850781,"@arixlin, just wanted to elaborate on my prior answer. The toggle inside the Unity editor changes the inference mode of the in-game Unity Inference Engine (Barracuda). It shouldn't be affected by Tensorflow at all. 

Tensorflow-GPU not working is another issue, though. In my experience tf-gpu works fine with multiple environments. Do the agents still get stuck when you're training with one env?",elaborate prior answer toggle inside unity editor inference mode unity inference engine barracuda affected working another issue though experience work fine multiple still get stuck training one,issue,negative,positive,positive,positive,positive,positive
521846985,"Hi @awjuliani , 
After each iteration of training a separate .nn file is created which is imported into the learning brain after the training is done. Can you elaborate how student agent continues to learn from data collected in the past when a separate .nn file is being created with every iteration. Ideally student agent should learn from scratch from the current iteration .
",hi iteration training separate file learning brain training done elaborate student agent learn data collected past separate file every iteration ideally student agent learn scratch current iteration,issue,negative,positive,positive,positive,positive,positive
521843393,"The `.demo` files are storing data in protobuf messages. These messages are the same used for communication between C# and Python. I am afraid extracting this data won't be easy, but you can look at the [demo_loader](https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/demo_loader.py) if you really need to. If you are interested in Behavioral Cloning, you should check [GAIL](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Imitation-Learning.md) out.",data used communication python afraid data wo easy look really need interested behavioral check,issue,positive,positive,neutral,neutral,positive,positive
521839338,"Hi @rajatpaliwal 

The student agent continues to learn from data collected in the past as well as data being immediately collected in each training iteration.",hi student agent learn data collected past well data immediately collected training iteration,issue,negative,negative,negative,negative,negative,negative
521809484,"I was able to reproduce your error. In the curricula folder for the walljump, there are two files: one corresponding to the BigWallBrain and one corresponding to the SmallWallBrain. Both these brains need to be trained, if one of them is not checked to `Control` in the Academy, the trainer will look into the curriculum folder, realize there is a setting for a brain that is not controlled by Python and raise an error.

In the walljump scene, each platform can have 3 settings : No wall, small wall and big wall. The agent will use a different brain depending on which platform is used.
The reset parameters in the academy are settings corresponding to each type of platform : 

 - big_wall_min_heigh is the minimum height for a wall on the big wall platforms
 - small_wall_height is the fixed height of the wall in small wall platforms
 - no_wall_height is the fixed height of the wall in the no wall platforms (usually 0)
 - big_wall_max_height is the maximum height for a wall on the big wall platforms",able reproduce error curriculum folder two one corresponding one corresponding brain need trained one checked control academy trainer look curriculum folder realize setting brain python raise error scene platform wall small wall big wall agent use different brain depending platform used reset academy corresponding type platform minimum height wall big wall fixed height wall small wall fixed height wall wall usually maximum height wall big wall,issue,negative,negative,neutral,neutral,negative,negative
521803932,"Somehow not working in mine even after just making changes in rpc_communicator or in both environment.py and rpc_communicator. 
Do you think I should delete some summaries or some log data to make this command work.",somehow working mine even making think delete log data make command work,issue,negative,neutral,neutral,neutral,neutral,neutral
521792100,"@arixlin I guess I was asking about CPU inference from inside Unity, not Python - the Barracuda setting shouldn't affect training at all. ",guess inference inside unity python barracuda setting affect training,issue,negative,neutral,neutral,neutral,neutral,neutral
521788375,"> Latest changes LGTM. Can you briefly summarize how you tested this PR?

Tested GAIL with Crawler and Curiosity + GAIL with Pyramids. Tested GAIL with GridWorld for visual obs. ",latest briefly summarize tested tested crawler curiosity tested visual,issue,negative,positive,positive,positive,positive,positive
521784434,Latest changes LGTM.  Can you briefly summarize how you tested this PR?,latest briefly summarize tested,issue,negative,positive,positive,positive,positive,positive
521737500,"Hi @awjuliani , 
Does the student agent learns from scratch in a particular iteration of training or does it also retains some learning from previous iteration of training while learning from current iteration of training.
",hi student agent scratch particular iteration training also learning previous iteration training learning current iteration training,issue,negative,neutral,neutral,neutral,neutral,neutral
521735754,"Hi @ervteng , I tried changing it in both rpc_communicator as well as environment.py. But the timeout was taking place at 30 seconds only. Do you suggest changing the value only in rpc_communicator file and not in environment.py file.",hi tried well taking place suggest value file file,issue,positive,neutral,neutral,neutral,neutral,neutral
521726212,"I am currently getting this same error. I've managed to narrow it down to find that the root problem is inside of the Brain.cs script when it calls this function.
![image](https://user-images.githubusercontent.com/35203126/63051122-683eeb00-be91-11e9-97fd-fdb0bc4eed00.png)

![image](https://user-images.githubusercontent.com/35203126/63051063-4a718600-be91-11e9-943c-c6061bbff7f2.png)
",currently getting error narrow find root problem inside script function image image,issue,negative,negative,neutral,neutral,negative,negative
521638279,"> Hey @mnsmuts, @yamashin0922, I can confirm that there is a bug in v0.9.1 that is preventing PlayerBrains from being used. I've pushed a fix to the branch `hotfix-onlinebc` - try checking out that branch and see if it fixes your issue.
> 
> If you want to edit yourself, add `if brain_name in self.policies:` at line 253 in `subprocess_env_manager.py`. Thank you for bringing this up; it will be fixed in the next release.

Many thanks",hey confirm bug used fix branch try branch see issue want edit add line thank fixed next release many thanks,issue,positive,positive,positive,positive,positive,positive
521619830,"ok I know it...
I must also manually set up in Academy inspector...

thanks for help!",know must also manually set academy inspector thanks help,issue,positive,positive,positive,positive,positive,positive
521570950,Thanks for helping out. There was a typo in the vector_action_description. Never thought that this causes the issue. Thanks again!,thanks helping typo never thought issue thanks,issue,positive,positive,positive,positive,positive,positive
521540674,"I had the same problem moving up to vs 0.9. However, it seems removing the player brain from the academy hub solves the issue and the agent is training when control is checked. And at the same time, you can still use the player brain any way when unchecking control from the learning brain.",problem moving however removing player brain academy hub issue agent training control checked time still use player brain way control learning brain,issue,negative,neutral,neutral,neutral,neutral,neutral
521530688,"@awjuliani I did a quick setup and took a few screenshots to show my setup and highlight the issue.
Here is the agent code and this is the only script used in the project except for an empty Academy script:

using System.Collections;
using System.Collections.Generic;
using MLAgents;
using UnityEngine;

```
public class Circle : Agent {

    private RayPerception2D rayPerception;

    public override void InitializeAgent () {
        rayPerception = GetComponent<RayPerception2D> ();
    }

    public override void AgentReset () {

    }

    public override void CollectObservations () {
        float rayDistance = 5f;
        float[] rayAngles = { 0f };
        string[] detectableObjects = { ""wall"" };

        var rayValues = rayPerception.Perceive (rayDistance, rayAngles, detectableObjects);

        Debug.Log (rayValues[0]);
        Debug.Log (rayValues[1]);
        Debug.Log (rayValues[2]);

        AddVectorObs (rayValues);
    }

    public override void AgentAction (float[] vectorAction, string textAction) {

    }
}
```

The first screenshot shows a non working scenario. You can see in the console output that all the values that are returned from the rayPerception2D are zeroes. Note that the agent is untagged while the Wall is tagged as ""wall"" and both are inside the ""Default"" layer.

![NotWorking](https://user-images.githubusercontent.com/8180556/63076693-624f1580-bf36-11e9-868f-18362432b9d2.PNG)

In this second screenshot you can see that we actually get non zero values back from the rayPerception2D. The only difference now is that the agent is put inside the ""Ignore Raycast"" layer.

![Working](https://user-images.githubusercontent.com/8180556/63076765-96c2d180-bf36-11e9-875b-71f34007ecea.PNG)
",quick setup took show setup highlight issue agent code script used project except empty academy script public class circle agent private public override void public override void public override void float float string wall public override void float string first non working scenario see console output returned note agent untagged wall tagged wall inside default layer second see actually get non zero back difference agent put inside ignore layer working,issue,negative,positive,neutral,neutral,positive,positive
521521944,"@ervteng 
Right, CPU inference works fine,  but it is slow.
When I train multiple envs at the same time using tensorflow-gpu==1.7.1（--num-envs=5 --slow）, there will be some agents to stop acting and others is fine, Thanks!",right inference work fine slow train multiple time stop acting fine thanks,issue,positive,positive,positive,positive,positive,positive
521503172,"Thanks @ervteng to reply,
How can i check visual observation output, where does the images get stored.
",thanks reply check visual observation output get,issue,negative,positive,neutral,neutral,positive,positive
521479807,"> Recommendation would be to scale down the Heuristic brain output to [-1,1], then in game code (DecideAction) multiply that by the opposite of the scale. That way you'll have the same output space in both brains.

Actually I tried but the loss descent very fast lower than 0.1. It is hard to decide whether the training is converge or not since all the outputs scaled down to [-1,1]",recommendation would scale heuristic brain output game code multiply opposite scale way output space brain actually tried loss descent fast lower hard decide whether training converge since scaled,issue,negative,negative,neutral,neutral,negative,negative
521471912,There should be no difference between the two in terms of how they do or don't ignore objects. Are you sure that the ray begins within the object that needs to be ignored? ,difference two ignore sure ray within object need,issue,negative,positive,positive,positive,positive,positive
521448840,"Nice paper! I do feel tempted to play around with it but it would blow my current scope : / 

As I wouldn't want to rely on demonstrations,  I'll consider separating the agents in the intial phase. However, when the agents are joined latter in the training, won't they get curious with each other in the same way?
",nice paper feel play around would blow current scope would want rely consider separating phase however latter training wo get curious way,issue,positive,positive,positive,positive,positive,positive
521393598,"@awjuliani are you familiar with the collider for 2D RayPercieve?

As for 3. I believe that is the case for the 3D case. But not sure why this would be different for 2D. ",familiar believe case case sure would different,issue,negative,positive,positive,positive,positive,positive
521393132,I completely agree - we'll definitely take this feedback into account in future updates of the UI. Thanks!,completely agree definitely take feedback account future thanks,issue,positive,positive,neutral,neutral,positive,positive
521392544,"You can train two brains at the ""same time"", we do this in WallJump. Of course, only the brain that is switched in is actually learning at any given time. ",train two brain time course brain switched actually learning given time,issue,negative,neutral,neutral,neutral,neutral,neutral
521392272,"@narayan907 is there a reason you give a positive reward for switching? The agent will be incentivized to switch continuously. 

I'd try to simplify the problem a bit and see if it learns. For instance, you could reduce the angle number or make the game board smaller. Also, be sure to check your visual observations and make sure they're being read properly. ",reason give positive reward switching agent switch continuously try simplify problem bit see instance could reduce angle number make game board smaller also sure check visual make sure read properly,issue,positive,positive,positive,positive,positive,positive
521391575,"Recommendation would be to scale down the Heuristic brain output to [-1,1], then in game code (DecideAction) multiply that by the opposite of the scale. That way you'll have the same output space in both brains. ",recommendation would scale heuristic brain output game code multiply opposite scale way output space brain,issue,negative,negative,negative,negative,negative,negative
521391055,"Hope you are feeling better!

The next release will actually make this a lot easier by moving a lot of the `learn.py` functionality into a separate method. This change is currently on the `develop` branch if you'd like to test it out. ",hope feeling better next release actually make lot easier moving lot functionality separate method change currently develop branch like test,issue,positive,positive,positive,positive,positive,positive
521390624,"It really depends on the game. In general, your agent should implement AgentReset(), and it should move the Agent back to the starting position. In some games, however, that would cause issues or are really difficult to do. 

I'm actually uncertain why you would use DestroyImmediate. The Unity docs recommend against it - I'd stick with Destroy if it works. https://docs.unity3d.com/ScriptReference/Object.DestroyImmediate.html",really game general agent implement move agent back starting position however would cause really difficult actually uncertain would use unity recommend stick destroy work,issue,negative,negative,negative,negative,negative,negative
521389757,"Hi @rajatpaliwal, I tested changing it in `rpc_communicator` and it did extend the timeout. If you change the `30` in line 34 it should work. ",hi tested extend change line work,issue,negative,neutral,neutral,neutral,neutral,neutral
521388297,"Hey @mnsmuts, @yamashin0922, I can confirm that there is a bug in v0.9.1 that is preventing PlayerBrains from being used. I've pushed a fix to the branch `hotfix-onlinebc` - try checking out that branch and see if it fixes your issue. 

If you want to edit yourself, add `if brain_name in self.policies:` at line 253 in `subprocess_env_manager.py`. Thank you for bringing this up; it will be fixed in the next release.",hey confirm bug used fix branch try branch see issue want edit add line thank fixed next release,issue,negative,positive,neutral,neutral,positive,positive
521350893,"Glad you fixed it, @SirSundays! For anyone else that hits this error but doesn't want to download the zip file, this was his workaround. Hopefully no big side effects. :)
subprocess_env_manager.py -- Comment out line 62:
```
    def close(self):
        try:
            #self.conn.send(EnvironmentCommand(""close""))
        except (BrokenPipeError, EOFError):
            pass
        self.process.join()
```
He also added the note: 'In the official version this line is executed: self.conn.send(EnvironmentCommand(""close"")). In this unofficial version this isnt executed. Seems to work anyway. Have nice time with MLAgent. SirSundays'",glad fixed anyone else error want zip file hopefully big side effect comment line close self try close except pas also added note official version line executed close unofficial version executed work anyway nice time,issue,positive,positive,positive,positive,positive,positive
521339978,"Hey,
Just to be sure, your parameter is not actually called `<parameter name>` right ?
This error arrises when the parameter name in the curriculum file does not match EXACTLY the reset parameter name in the Academy. Please make sure the two match.
If they do and you still have this error, please post a screenshot of your Academy inspector, your curriculum file and your full console output.",hey sure parameter actually parameter name right error parameter name curriculum file match exactly reset parameter name academy please make sure two match still error please post academy inspector curriculum file full console output,issue,negative,positive,positive,positive,positive,positive
521308109,"Hi,
The error `The provided demonstration is not compatible with the brain being used for performance evaluation` arrises when the parameters of the brain used to record the demonstrations are not strictly identical to the parameters of the learning brain. These parameters are : 

 - vector_observation_space_size
 - num_stacked_vector_observations
 - number_visual_observations
 - camera_resolutions
 - vector_action_space_size
 - vector_action_descriptions
 - vector_action_space_type

Please make sure that all of these were identical to the learning brain when recording demonstrations.",hi error provided demonstration compatible brain used performance evaluation brain used record strictly identical learning brain please make sure identical learning brain recording,issue,negative,positive,positive,positive,positive,positive
521268500,"Hi @MentalGear,
In my case, I just use `ls -la` to check all files in the 'curricula/[projectName]/' directory, and except my original [lesson].json file, it still has the other file called ._[lesson].json generated. I just deleted this file.  Note that I am talking about the Linux now. With respect to my mac, there is other file called `.DS_Store` and I deleted it as well.
  
In terms of the encoding, you need to check the source code of python API, then open the `curriculum.py` and in the `__init__` of Curriculum class,  then specify the encoding method like below:
```
            with open(location, encoding = ""ISO-8859-1"") as data_file:
                self.data = json.load(data_file)
``` ",hi case use check directory except original lesson file still file lesson file note talking respect mac file well need check source code python open curriculum class specify method like open location iso,issue,positive,positive,positive,positive,positive,positive
521250656,Sorry for the late reply. The paper in question is DeepSpeech https://arxiv.org/abs/1412.5567,sorry late reply paper question,issue,negative,negative,negative,negative,negative,negative
521194348,"Hi @gzrjzcx , I'm facing the same issue with JSONDdecodeError. I tried to follow your hints, but can't locate a file called ._[my file name], even with hidden files shown on Mac.
I assume the file is generated in the directory of ""curricula/[projectname]"" or is it somewhere else, like in the virtual python env ? (I use conda).
Could you also specify where you use ""open(location, encoding = ""ISO-8859-1"")"" ?
Thanks for your help !",hi facing issue tried follow ca locate file file name even hidden shown mac assume file directory somewhere else like virtual python use could also specify use open location iso thanks help,issue,positive,positive,neutral,neutral,positive,positive
521179441,"Hi Yes all correct, the agent has a learning brain, the academy has a learning brain without control when creating a build for offline training (.demo), or if I am online training,  both a learning brain with control and a player brain.",hi yes correct agent learning brain academy learning brain without control build training training learning brain control player brain,issue,negative,neutral,neutral,neutral,neutral,neutral
521136569,"Thank you for sharing it. 
As you said, there seems to be bug around there since the behavior differs between the two versions

To train RollerBallBrain (not RollerBallPlayer), we have to drag the RollerBallBrain asset to the RollerAgent GameObject Brain field to the learning brain.
Can you check your Brain field of RollerAgent?",thank said bug around since behavior two train drag asset brain field learning brain check brain field,issue,negative,negative,neutral,neutral,negative,negative
521119321,"Fixed the issue with reinstalling Anaconda, but this time installing Anaconda version 5.1 and not the newest version.",fixed issue anaconda time anaconda version version,issue,negative,positive,neutral,neutral,positive,positive
521110094,Great. Currently playing around with allowing them to evolve their morphology while they are learning. A preliminary experiment saw them develop a galloping gait rather than the trot they currently employ. Will let you know how I go.,great currently around evolve morphology learning preliminary experiment saw develop galloping gait rather trot currently employ let know go,issue,positive,positive,positive,positive,positive,positive
521069498,"@ervteng, Thank you very much. CPU inference works fine. The issue just happens in GPU inference.",thank much inference work fine issue inference,issue,positive,positive,positive,positive,positive,positive
521061918,"> @ sunrui19941128你可以分享学院督察窗口的截屏吗？

Ok,this 0.9.0version screenshort  with Acadamy !  Picture one is No delete PlayerBrain .  Picture Two is deleted PlayerBrain! I tried your idea!but There are key errors!I suspect a version problem!sam time 0.8.1 no Problem!



![nodelete](https://user-images.githubusercontent.com/34094384/62987037-9e1fa780-be70-11e9-85d8-16dbc24b0768.png)
![delete](https://user-images.githubusercontent.com/34094384/62987060-b42d6800-be70-11e9-8a95-9b8914cee955.png)

",version picture one delete picture two tried idea key suspect version problem sam time problem delete,issue,negative,neutral,neutral,neutral,neutral,neutral
521034244,Stuck with the exact same JSON.decode error. Has anyone already figured this out ?,stuck exact error anyone already figured,issue,negative,positive,positive,positive,positive,positive
521008072,"Hey @zheyangshi, @arixlin, just to let you know we're looking into this. Just to confirm, CPU inference works fine, right?",hey let know looking confirm inference work fine right,issue,negative,positive,positive,positive,positive,positive
520992863,Thanks for the comments - closing this issue due to inactivity. Feel free to open a new issue if you're still running into problems. ,thanks issue due inactivity feel free open new issue still running,issue,positive,positive,positive,positive,positive,positive
520991220,"@MoMe36 if you've gotten transfer learning working, absolutely!",mome gotten transfer learning working absolutely,issue,negative,positive,positive,positive,positive,positive
520990574,Thanks for the discussion - closing this issue due to inactivity. Feel free to open a new issue if you're still running into problems. ,thanks discussion issue due inactivity feel free open new issue still running,issue,positive,positive,positive,positive,positive,positive
520988642,confirmed new Matrix4x4 code works. if you don't have any objections @awjuliani lets merge this to develop. thanks again for the PR @RunSwimFlyRich. please keep us updated on your experiments with ML-Agents! :D,confirmed new code work merge develop thanks please keep u,issue,positive,positive,positive,positive,positive,positive
520940507,"Very interesting! It might be that the agents' interaction is sufficiently stochastic that it forever remains ""interesting"" to the curiosity module. There are some new techniques in research (https://arxiv.org/abs/1906.04161) that may be able to mitigate this but with the current ML-Agents, you may have to resort to demonstrations, or separating the agents during initial training, and combine them later in the run. ",interesting might interaction sufficiently stochastic forever remains interesting curiosity module new research may able mitigate current may resort separating initial training combine later run,issue,positive,positive,positive,positive,positive,positive
520938886,"> Found the GetReward() function in Agent.cs. Would suggest to add these to the Monitor guide page for other users to find. What I'm still unsure about is how to display it over an agent ?

Thanks for the feedback! We'll take another look at the Monitor docs. ",found function would suggest add monitor guide page find still unsure display agent thanks feedback take another look monitor,issue,negative,positive,positive,positive,positive,positive
520934704,"Hi @ervteng 
Thank you!! That makes sense why I see improvements when I play around with the buffer/batch then.
I'm almost certain I didn't experience the dropout time when I only used vector observations, but of cause images is a completely different size to process.

Thank you once again!",hi thank sense see play around almost certain experience dropout time used vector cause completely different size process thank,issue,positive,positive,positive,positive,positive,positive
520928556,"Hi @KristianLN, this ""dropout"" time is the time when the model is being trained. When the simulation is running, the trainer is gathering experiences (images) and collecting them in a buffer, then the TensorFlow code performs a model update. So it isn't lost time! To decrease the dropout time I'd suggest using a computer with CUDA + GPU, and installing tensorflow-gpu (especially for visual observations). ",hi dropout time time model trained simulation running trainer gathering buffer code model update lost time decrease dropout time suggest computer especially visual,issue,negative,neutral,neutral,neutral,neutral,neutral
520928121,"Hello,
I am trying to modify the timeout_wait parameter to greater than 30 seconds. I did the necessary changes in the environment.py file and rpc_communicator.py file but still the timeout is happening at 30 seconds. 
Can anyone suggest any other changes in order to delay the timeout .",hello trying modify parameter greater necessary file file still happening anyone suggest order delay,issue,negative,positive,positive,positive,positive,positive
520834017,@sunrui19941128 Can you share the screen capture of your inspector window of the Academy?,share screen capture inspector window academy,issue,negative,neutral,neutral,neutral,neutral,neutral
520814130,"> Can you check if RollerBallPlayer exist in Broadcast Hub in the inspector window of your Academy?
> If it is TRUE, please try to remove RollerBallPlayer from the inspector window of the Academy.

This removes the keyError but the model does not train",check exist broadcast hub inspector window academy true please try remove inspector window academy model train,issue,positive,positive,positive,positive,positive,positive
520769477,"Just in case it helps - here is the anaconda prompt when trying exactly the same training (imitation 3DBall) in v0.8.2
![image](https://user-images.githubusercontent.com/22680369/62932617-62290b80-bdb8-11e9-8682-38397baa789d.png)

Seems to work well, which is why it feels like v0.9.1 has a bug
",case anaconda prompt trying exactly training imitation image work well like bug,issue,positive,positive,neutral,neutral,positive,positive
520763774,this is version v0.9.1 and I am removing the player brain from the academy.,version removing player brain academy,issue,negative,neutral,neutral,neutral,neutral,neutral
520763268,"In my case I get : 
![image](https://user-images.githubusercontent.com/22680369/62930978-66075e80-bdb5-11e9-8c33-7020c38752ac.png)

then if I remove the player brain:
![image](https://user-images.githubusercontent.com/22680369/62931165-c3031480-bdb5-11e9-96df-aa8925e760ee.png)


",case get image remove player brain image,issue,negative,neutral,neutral,neutral,neutral,neutral
520761378,"Hi, I can train in v0.8.2 but get the key error in v0.9.X. I am currently trying to roll back my environments to get back to a working system. removing the player brain made no difference, yaml file 
![image](https://user-images.githubusercontent.com/22680369/62930751-ebd6da00-bdb4-11e9-80e9-ea75359750f4.png)

pretty much nothing in it except the default - unless that is not allowed? ",hi train get key error currently trying roll back get back working system removing player brain made difference file image pretty much nothing except default unless,issue,negative,positive,neutral,neutral,positive,positive
520735730,"Just curious about the reasoning behind using Destroy or DestroyImmediate, as well as when to use AgentReset() and when not, as team suggestions seems to vary.",curious reasoning behind destroy well use team vary,issue,negative,negative,negative,negative,negative,negative
520729319,"OK, it's the 3rd Monitor parameter, target, which one needs to feed this.transform to.",monitor parameter target one need feed,issue,negative,neutral,neutral,neutral,neutral,neutral
520728139,Found the GetReward() function in Agent.cs. Would suggest to add these to the Monitor guide page for other users to find. What I'm still unsure about is how to display it over an agent ?,found function would suggest add monitor guide page find still unsure display agent,issue,negative,neutral,neutral,neutral,neutral,neutral
520718824,"I understand your frustration.
Do you mean that ""KeyError: 'RollerBall Player'"" is still shown in the error message even if you remove it from the inspector window in 0.9.0?",understand frustration mean player still shown error message even remove inspector window,issue,negative,negative,negative,negative,negative,negative
520714905,"
I tried it and it didn't work with 0.9.0!!  Then it worked normally in 0.8.1!!It's really frustrating",tried work worked normally really,issue,negative,positive,positive,positive,positive,positive
520711785,"I'm not sure if issue is around there.  However, I faced the same issue. And I found it as workaround.",sure issue around however faced issue found,issue,negative,positive,positive,positive,positive,positive
520708625,"Can you check if RollerBallPlayer exist in Broadcast Hub in the inspector window of your  Academy?
If it is TRUE, please try to remove RollerBallPlayer from the inspector window of the Academy.

",check exist broadcast hub inspector window academy true please try remove inspector window academy,issue,positive,positive,positive,positive,positive,positive
520654004,"@ervteng I has same errors, when I using vsiual observation and GPU in the Brain settings run on internal, but Console dont print any errors. Thanks!
VisualPyramidsLearning.nn file:  https://drive.google.com/file/d/1FmT8Ysw8CkW4dA1cY-KTkSXB84L1t-YC/view?usp=sharing",observation brain run internal console dont print thanks file,issue,negative,positive,neutral,neutral,positive,positive
520652658,"I didn't have this problem in version 0.8.1,But this problem was encountered in 0.9.0,Now I am using 0.8.1, so I can train the Demo normally.I tried a lot in 0.9.0 but nothing worked!Later it was reloaded with a 0.8.1 environment to train normally!

> Hi @sunrui19941128, @mnsmuts, can you post your yaml config file?

I didn't have this problem in version 0.8.1,But this problem was encountered in 0.9.0,Now I am using 0.8.1, so I can train the Demo normally.I tried a lot in 0.9.0 but nothing worked!Later it was reloaded with a 0.8.1 environment to train normally!",problem version problem train tried lot nothing worked later environment train normally hi post file problem version problem train tried lot nothing worked later environment train normally,issue,negative,positive,neutral,neutral,positive,positive
520651250,"Hi @ervteng, Here is the picture of our brain setting.
![image](https://user-images.githubusercontent.com/27290494/62908426-939ada80-bdaa-11e9-9196-900adfc5c02d.png)
We tried on the following environments, but we still meet the same problem.
**Unity version:** 2018.2.17/2017.4.17f1
**GPU:** GTX 1080/ GTX 1070  

What's more, we found that the value of **act[0]** would become 0 after several steps and that's why the agents stayed in the same place. (the accident usually happens when tasks are being finished.)",hi picture brain setting image tried following still meet problem unity version found value act would become several stayed place accident usually finished,issue,negative,negative,neutral,neutral,negative,negative
520581549,"Got it, this looks good to me. Thanks for the contribution @niskander!",got good thanks contribution,issue,positive,positive,positive,positive,positive,positive
520539160,"Hi @ervteng 
Thanks for your answer.

In terms of using two brain, that would be pre-trained brains, right? As there aren't no support for training two brains at the same time currently, right? Or have I missed something?",hi thanks answer two brain would brain right support training two brain time currently right something,issue,positive,positive,positive,positive,positive,positive
520524521,Hi @niskander -- we won't be able to accept this PR unless you accept the CLA.  The link is provided in the comment above.,hi wo able accept unless accept link provided comment,issue,positive,positive,positive,positive,positive,positive
520520484,"hello @ervteng  , yes i have seen that blog of snoopy pop game, my reward structure looks like
0.001 for switching ball,
0.01*no of balls for match hit,
-0.001 for no match hit
I am training at 10 scale time, with one environment but i see no improvement in reward even after 150000 steps.
I have tried the multi environment concept, explained in blog but i see crashes happening in that.



",hello yes seen snoopy pop game reward structure like switching ball match hit match hit training scale time one environment see improvement reward even tried environment concept see happening,issue,positive,negative,negative,negative,negative,negative
520519818,"@tjad thanks for following up.  In the latest v0.9 release the ""step"" metric (global step) should reflect the number of total steps across all environments.  So if you have 4 parallel environments which have each taken 1000 steps the steps you'd see in stdout or Tensorboard will be 4000.  ",thanks following latest release step metric global step reflect number total across parallel taken see,issue,negative,positive,positive,positive,positive,positive
520519357,"Hi @FlimFlamm, it's definitely doable without touching the C# or Unity side. You'd have to be pretty familiar with TensorFlow, though, and modify the `models.py` to create an auto encoder instead of the usual visual encoder. Then you'll have to add a loss for the encoding that's some correlation between the input and the output of the decoder, and add it to the PPO loss. 

If there's an existing implementation, you might also be able to use our `gym_unity` wrapper to interact with your Unity project instead of the PPO trainer. ",hi definitely doable without touching unity side pretty familiar though modify create auto instead usual visual add loss correlation input output add loss implementation might also able use wrapper interact unity project instead trainer,issue,positive,positive,positive,positive,positive,positive
520517736,"Hi @narayan907, what's your reward structure for the game? We do have some experience doing bubble shooters with ML-Agents (see https://blogs.unity3d.com/en/2019/04/15/unity-ml-agents-toolkit-v0-8-faster-training-on-real-games/ for an example) so in theory it's a learnable problem. ",hi reward structure game experience bubble see example theory learnable problem,issue,negative,negative,negative,negative,negative,negative
520517265,"Hi @MentalGear, every game is different :P If AgentReset() and Destroy() works for you, then it's definitely OK to use. AgentReset() is just called when Reset() is called, so it's definitely recommend to implement something there. ",hi every game different destroy work definitely use reset definitely recommend implement something,issue,positive,negative,negative,negative,negative,negative
520510401,"Hi @KristianLN, unfortunately this isn't currently supported at the time. You can change the Brain parameters, but the model won't train as the network architecture is fixed. 

A potential workaround might be to have two brains, and swap the brain during runtime. An example of this can be found in our WallJump example. ",hi unfortunately currently time change brain model wo train network architecture fixed potential might two brain swap brain example found example,issue,negative,positive,neutral,neutral,positive,positive
520509569,"Hi @zheyangshi, do you mean GPU in the Brain settings, and inference with CPU works? 

This could be a number of issues - I haven't been able to replicate on my end. What Unity version and GPU are you running? Thanks!",hi mean brain inference work could number able replicate end unity version running thanks,issue,negative,positive,positive,positive,positive,positive
520478755,"did you find a solution? I am hitting the same problem (suspect it is something simple) same setup trains offline but will not train online
",find solution problem suspect something simple setup train,issue,negative,neutral,neutral,neutral,neutral,neutral
520373077,"It was my fault. I reinstalled anaconda-envs and updated mlagents up to date, now everything runs OK! Thank you for your help!",fault date everything thank help,issue,negative,neutral,neutral,neutral,neutral,neutral
520315774,"Thank you for the feedback. I’ve committed the requested change in CrawlerAgent. There are conflicts with the model data files that have arisen in the time since I made my pull request which I'm not authorised to resolve. Please let me know if there’s anything else I need to do.
",thank feedback change model data arisen time since made pull request resolve please let know anything else need,issue,positive,neutral,neutral,neutral,neutral,neutral
520244333,"I fixxed it!
I just did the good old: ""Hm dont know what this line does. Lets comment it out""-thing and after that it worked. I can now train my model and afterwards I can drag my model into unity and use it.

If someone else get this error:
Just change this file with the one I uploaded below
C:\Users\%YOUR_USERNAME%\Anaconda3\Lib\site-packages\mlagents\envs\subprocess_env_manager
[Typeerror Fix.zip](https://github.com/Unity-Technologies/ml-agents/files/3490222/Typeerror.Fix.zip)

",good old dont know line comment worked train model afterwards drag model unity use someone else get error change file one,issue,negative,positive,positive,positive,positive,positive
520238631,Updating Conda and/or Unity to a newer version do not help to fix this problem.,unity version help fix problem,issue,negative,neutral,neutral,neutral,neutral,neutral
520216211,it was an error in the online_bc file probably caused by me messing around with the configurations. Fresh file from the new dist v0.9 and all seems to be fine. M,error file probably messing around fresh file new fine,issue,negative,positive,positive,positive,positive,positive
520181986,"Thank you for your response. Sorry for the late reply, I caught the flu.

It is sad to hear that this is not there but very helpful to know I was not just missing something.

I will have a look into it when I have some time. It might be more sensible to create helper classes/functions to make the internals a bit more accessible with standard calls.",thank response sorry late reply caught flu sad hear helpful know missing something look time might sensible create helper make internals bit accessible standard,issue,negative,negative,negative,negative,negative,negative
520137603,"The docs' code reads ""assign a Brain instance to the new Agent and initialize it by calling its AgentReset() method""
However, several members of the ml agents dev team said, one shouldn't manually call AgentReset(). Could we get a clarification on this plz @xiaomaogy ?
Link: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Design-Agents.md#instantiating-an-agent-at-runtime",code assign brain instance new agent initialize calling method however several dev team said one manually call could get clarification link,issue,negative,positive,neutral,neutral,positive,positive
520132341,"@olympus12 Kirin 620 comes with old generation Mali450 GPU, which doesn't support OpenGL ES 3.1 / Compute Shaders, so Barracuda won't work on this GPU.",come old generation mali support e compute barracuda wo work,issue,negative,positive,neutral,neutral,positive,positive
520127534,"@ervteng Yes, I understand. It appeared at the time that this was solely responsible for eating the majority of the memory (and when applying this patch, I could not reproduce.

I do see that you've changed the composition of PPO considerably too.

Closing for now. - bear in mind (https://github.com/Unity-Technologies/ml-agents/issues/1402) is still open",yes understand time solely responsible eating majority memory patch could reproduce see composition considerably bear mind still open,issue,positive,positive,positive,positive,positive,positive
520127382,"Hi @harper-u3d  @harperj   @unityjeffrey 

Apologies for the delay - proposed during a time I was quite busy.

Thanks for your feedback.  I'll re-access the code base and see what needs to be done.

@harperj Did you make progress on your changes? ",hi delay time quite busy thanks feedback code base see need done make progress,issue,positive,negative,negative,negative,negative,negative
520127127,"Hi @vincentpierre 

Apologies for the delay - this proposal was made during a time where I was quite busy

Thanks for your feedback. I'll re-access the code base, and reconfirm my thoughts. I do see quite a bit has changed in ml-agents since this proposal.
Perhaps you guys had some considerations in the interim as well.

",hi delay proposal made time quite busy thanks feedback code base reconfirm see quite bit since proposal perhaps interim well,issue,negative,negative,negative,negative,negative,negative
520123223,"> Also - no reason why you can't multiply the vectorAction by some value in AgentAction. However, the neural network will only output values between [-1,1].

The problem is I have decisions which spacesize is 120 in Heuristic brain. and most of their values larger than 1. And I want to imitate the Heuristic brain but cannot get output larger than 1. It is like a supervised learning and let the 120 output work like as labels",also reason ca multiply value however neural network output problem heuristic brain want imitate heuristic brain get output like learning let output work like,issue,positive,neutral,neutral,neutral,neutral,neutral
520123061,"> @Hallam-Liu
> ` Mathf.Clamp(vectorAction[3], -2f, 2f);`

Then all the out put will be -2 or 2 if the output of model larger than 1 or smaller than -1
",put output model smaller,issue,negative,neutral,neutral,neutral,neutral,neutral
520123033,"> Also - no reason why you can't multiply the vectorAction by some value in AgentAction. However, the neural network will only output values between [-1,1].

Yeah，But I want the output of model larger than [-1,1]. Can I do it in ml agent? ",also reason ca multiply value however neural network output want output model agent,issue,negative,neutral,neutral,neutral,neutral,neutral
520061627,"Also - no reason why you can't multiply the vectorAction by some value in AgentAction. However, the neural network will only output values between [-1,1]. ",also reason ca multiply value however neural network output,issue,negative,neutral,neutral,neutral,neutral,neutral
520060567,"Hi @Sohojoe, big thanks for spotting this issue! It's an error on our part in the release notes - the PR enabled the parameters to be passable when creating a `UnityEnvironment` class, but it was never intended to pass down through `learn.py`. We're looking into doing this properly - but basically I would add a parameter to the docopt, turn it into a list of strings, and pass it into the `create_environment_factory` method in `learn.py`. ",hi big thanks spotting issue error part release passable class never intended pas looking properly basically would add parameter turn list pas method,issue,negative,positive,neutral,neutral,positive,positive
520032514,"Hi Ervin,
Barracuda works with Editor (Windows) and both Android and iOS builds. It
tested with different devices, but this specific phone model didn't work
with ""Compute"" worker, so I had to change it to ""CSharp"".

On Fri, Aug 9, 2019, 21:54 Ervin T. <notifications@github.com> wrote:

> Adding @mantasp <https://github.com/mantasp>
>
> @olympus12 <https://github.com/olympus12>, does Barracuda work with
> desktop/Windows/Mac builds in your setup?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/2417?email_source=notifications&email_token=AMT7MO7KHSJWYWO3BD7SBBLQDW4QBA5CNFSM4IKUMCFKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD37PZMI#issuecomment-520027313>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AMT7MO25CN6GIOMDVWWP3EDQDW4QBANCNFSM4IKUMCFA>
> .
>
",hi barracuda work editor android tested different specific phone model work compute worker change wrote barracuda work setup reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
520026446,"Hey @tjad, this will work for visual obs, but not in general. We'll look into a better solution for this issue. In the meantime, you can use Barracuda to do inference without memory leaks. ",hey work visual general look better solution issue use barracuda inference without memory,issue,positive,positive,positive,positive,positive,positive
520010852,"Hi @ervteng ,
I tried modifying the timeout_wait parameter to greater than 30 seconds. I set it to 60 secs but it is still stopping at 30 secs. 
What else can I change to delay the timeout parameter.",hi tried parameter greater set still stopping else change delay parameter,issue,negative,positive,positive,positive,positive,positive
519976863,@trulyspinach Could you maybe show an example on how to call this function ? Thanks !,could maybe show example call function thanks,issue,negative,positive,positive,positive,positive,positive
519974473,"2 years later, can we use Coroutines now without issues when training (even at high scales), or do we still need to use a workaround as described by @trulyspinach  ?",later use without training even high scale still need use,issue,negative,positive,neutral,neutral,positive,positive
519893031,"Wouldn't it makse sense to provide a GUI, like with the action space, to put your observation variables in, for that if you refactor your code, you don't mess up the order.
Or even better: Let us assign them fixed names when defining them, like:

AddVectorObs(C, ""velocity"")

I mean, there must be a better way to anchor them than just hope not to mess up the observation order, which can quickly happen when coding.",would sense provide like action space put observation code mess order even better let u assign fixed like velocity mean must better way anchor hope mess observation order quickly happen,issue,positive,positive,positive,positive,positive,positive
519857009,"Thanks for your answer! But when i used ""export_model()"" right after ""save_model()"" to save model(.nn file) with my ""--save-freq"", It seems to be extremely slow saving model(.nn file) from 7th~8th model. At the same time, memory usage also increase linearly. I think that ""freeze_graph.freeze_graph()"" is main problem causing above issue. 

please give me your advice to solve it... :(",thanks answer used right save model file extremely slow saving model file model time memory usage also increase linearly think main problem causing issue please give advice solve,issue,positive,positive,neutral,neutral,positive,positive
519826860,"Hey @mantasp, have you had a chance to investigate this issue yet? I would greatly appreciate if you could estimate how serious this error is and whether it could be fixed in the upcoming releases.

If you are aware of any Barracuda compatible alternatives or workarounds for using tensors with three dimensions (of which the first two have variable size) as inputs for a dense layer, please let me know.

",hey chance investigate issue yet would greatly appreciate could estimate serious error whether could fixed upcoming aware barracuda compatible three first two variable size dense layer please let know,issue,positive,positive,positive,positive,positive,positive
519720607,"Hi @awjuliani,
So I can't continue training in this case right? Because it has reached the max_step, which means the training has finished. If I change the max_step, it cannot restore to the previous situation right?
  
BTW, I have calculated velocity directly now. But I want to know is this possible to observe the velocity via stacking vectors? Because I have checked your Tennis environment, you have observed the velocity directly, but you still use the 3 stacked vectors.",hi ca continue training case right training finished change restore previous situation right calculated velocity directly want know possible observe velocity via checked tennis environment velocity directly still use,issue,negative,positive,positive,positive,positive,positive
519717879,"Fixed a couple bugs (mostly to do with SAC) I've tested with 3DBall, Hallway, and GridWorld. ",fixed couple mostly sac tested hallway,issue,negative,positive,positive,positive,positive,positive
519709388,I suspected when I looked into this issue that there may have been lack of direction where the actual memory leak was. I trained fine for hours on end. Evaluation wasn't happening as easily.,suspected issue may lack direction actual memory leak trained fine end evaluation happening easily,issue,negative,positive,positive,positive,positive,positive
519707607,"The training wont have a leak - as far as I remember, I'll confirm that. The way in which the training works clears the buffer ,but during eval it wasn't training, and hence not clearing the buffer.",training wont leak far remember confirm way training work buffer training hence clearing buffer,issue,negative,positive,neutral,neutral,positive,positive
519652392,"Sure thing. I've made numerous changes to my local project but the only change I'd want to commit is to parent my workingTransform under its CrawlerAgent to stop it spamming the hierarchy view and leaving gameObjects behind when an agent is destroyed. I'll leave it for now though seeing as you have some changes.
This stuff is a lot of fun to play around with. I was initially looking to just add some quadrupedal robots to my game but I think I'll be adding a number of different contraptions and configurations now.",sure thing made numerous local project change want commit parent stop hierarchy view leaving behind agent leave though seeing stuff lot fun play around initially looking add quadrupedal game think number different,issue,positive,negative,neutral,neutral,negative,negative
519617329,"You're right, this seems like an inconsistent behavior. I've logged the issue and we'll take a look. Thanks!",right like inconsistent behavior logged issue take look thanks,issue,positive,positive,positive,positive,positive,positive
519617005,"It's possible - the easiest way would be to open the `checkpoint` file in that directory, change the name to the cptk you want, re-run mlagents-learn with `--load`, and then quit it again. This will load that checkpoint and save it as a `.nn`. 

The code that does this can be found in `tf_policy.py`. There are two methods, `save_model` and  `export_model`. `save_model` saves the TF graph to both a kept and a `.pb` file. you can modify it to save a `.pb` at every checkpoint, rather than overwriting the old one each time. `export_model` freezes it to a `.pb` file, then converts the `.pb` to `.nn`. You could easily import these into your own script to load and convert any checkpoint in the directory. ",possible easiest way would open file directory change name want load quit load save code found two graph kept file modify save every rather old one time file could easily import script load convert directory,issue,positive,positive,positive,positive,positive,positive
519615095,"- A step corresponds to one action of the agent. 
- Yes, the order matters - if you train a model with one order, those inputs are mapped in that order into the neural network. It will have to re-learn what each input means if you change the order.
- We currently don't support changing observations - it's still an active area of research as to how to do that effectively in deep RL. But definitely something we're looking into for the future. 

",step one action agent yes order train model one order order neural network input change order currently support still active area research effectively deep definitely something looking future,issue,positive,negative,neutral,neutral,negative,negative
519613831,"You might be able to pass through the path to the .nn file as a reset parameter, and trigger a model reload on reset, then call the reset with the right path from Python. 

Related: a fellow contributor implemented self-play in ML-Agents, you can check out his branch here: https://github.com/Unity-Technologies/ml-agents/pull/1975",might able pas path file reset parameter trigger model reload reset call reset right path python related fellow contributor check branch,issue,negative,positive,positive,positive,positive,positive
519612528,"@DooblyNoobly it really does seem like a bug in the demo loader from your error message though 🤔. The only thing I can think of is if any of the brain settings changed between recording and training. Anyways, feel free to re-open the issue if you're unable to get it to work. ",really seem like bug loader error message though thing think brain recording training anyways feel free issue unable get work,issue,positive,positive,neutral,neutral,positive,positive
519597669,Thanks for the info. I'm finishing up my code review. I'll have some suggested changes but overall i think the new observation vectors are working much better.,thanks finishing code review overall think new observation working much better,issue,positive,positive,positive,positive,positive,positive
519549285,"OK, I solved it. Somehow I had a problem with the collider disabled on my agents which produced strange behavior. They all run normally and at a certain point all of them signal Done() to my API out of the blue (After like 16 steps in the environment). Re-enabling the collider fixed the isse ",somehow problem disabled produced strange behavior run normally certain point signal done blue like environment fixed,issue,negative,positive,neutral,neutral,positive,positive
519441610,"Oh..! my team leader has just found that we can load model(.nn file) from external folder of unity(""Asset"" folder) !
Here is one of solution he told me! :)
![image](https://user-images.githubusercontent.com/53593292/62691010-bc1c8080-ba08-11e9-9c61-35da364514e9.png)
1. we use ""WWW"" to load external file(.nn file) 
2. and extract byte[] by using ""www.bytes""
3. and just set equal to (Barracuda.NNModel)preloadedModel.Value !
4. It works!
![image](https://user-images.githubusercontent.com/53593292/62691454-afe4f300-ba09-11e9-99d4-264fc7dc54d9.png)
It would be possible ""NNModel"" class only have its byte array(byte[] value)!

But i still need your advice about ""auto self training system""(build program version!)",oh team leader found load model file external folder unity asset folder one solution told image use load external file file extract set equal work image would possible class array value still need advice auto self training system build program version,issue,positive,neutral,neutral,neutral,neutral,neutral
519405228,"Hi @ervteng 
Thank you for the response.

1. There is no argument for the offset in the 2D RayPercieve. Only in the 3D.
2. The 3D version seems to work without using an offset, why should the 2D behave differently?
3. Since we use detectable objects, wouldn't it be more intuitive if ONLY objects with these tags were considered? Meaning, the objects itself (unless assigned one of the detectable tags) should not be considered.",hi thank response argument offset version work without offset behave differently since use detectable would intuitive considered meaning unless assigned one detectable considered,issue,negative,neutral,neutral,neutral,neutral,neutral
519331819,I just confirmed that hallway example works fine. I have even tested recording a new demonstration and there wasn't any problem. Must be something I'm doing specifically with my project.,confirmed hallway example work fine even tested recording new demonstration problem must something specifically project,issue,negative,positive,positive,positive,positive,positive
519294215,"Hi @mbaske, there is some subtlety to using PreTraining and GAIL. PreTraining attempts to force the PPO policy to directly imitate your demonstrations, while GAIL rewards the policy for encountering the states in your demonstrations (unless `use_actions` is True). 

You might have better results turning off PreTraining for using visual obs, unless you have tons of demonstrations. I'd try training with the raycasts first and see if the agent can learn the problem - then give GAIL another try! It might take a while to learn as visual observations do. ",hi subtlety pretraining pretraining force policy directly imitate policy unless true might better turning pretraining visual unless try training first see agent learn problem give another try might take learn visual,issue,negative,positive,positive,positive,positive,positive
519293311,"Hi @mrshanepaul, making the Trainer a usable library is a good idea, though it isn't designed that way today. You can look at the `learn.py` Python script (that is what is being called in mlagents-learn) and copy that into your Jupyter notebook. If you have a working notebook we'd be happy to merge it into the examples! ",hi making trainer usable library good idea though designed way today look python script copy notebook working notebook happy merge,issue,positive,positive,positive,positive,positive,positive
519292734,"Hi @AidinD, is your RayPercieve inside the object? You might be able to increase the startOffset when calling Perceive to get out of the object. ",hi inside object might able increase calling perceive get object,issue,negative,positive,positive,positive,positive,positive
519292509,Do you get any errors on the python terminal side?,get python terminal side,issue,negative,neutral,neutral,neutral,neutral,neutral
519292337,"Hi @gzrjzcx 

The learning rate is set to a ratio of the total steps based on the completed steps. When you increase the total steps, you increase the learning rate again, hence the nature of the curve you see. Going along with this is a decrease in stability of learning, which you see in your performance results as well.",hi learning rate set ratio total based increase total increase learning rate hence nature curve see going along decrease stability learning see performance well,issue,positive,neutral,neutral,neutral,neutral,neutral
519292115,"@DooblyNoobly - I wasn't able to replicate your issue on the sample environments, unfortunately. Could you try on the Hallway example and see if you're getting the same behavior? Thanks!

@arixlin - I was able to replicate this issue, it's a naming conflict between Curiosity and GAIL. It will be fixed in a patch in the next couple days. For now, I'd use GAIL only - usually, curiosity + GAIL doesn't add too much value. ",able replicate issue sample unfortunately could try hallway example see getting behavior thanks able replicate issue naming conflict curiosity fixed patch next couple day use usually curiosity add much value,issue,negative,positive,neutral,neutral,positive,positive
519097222,"I am also encountering the error like ```AttributeError: 'function' object has no attribute 'num_envs'``` in  [gym-unity](https://github.com/Unity-Technologies/ml-agents/tree/master/gym-unity) 
This problem was solved as follows.
```
env=env => env=env()
```
",also error like object attribute problem,issue,negative,neutral,neutral,neutral,neutral,neutral
519085771,"Hi @awjuliani,
In this case, the first training ended at 5M step. Then I continue the training by `--load` without any vary in the config file except the increasing of `max_steps` from 5M to 10M. 
Then you can see the mean reward decreased dramatically from the start of the following training(i.e. 5M+1 step).  Although then it increases finally, it is not same as a direct training of 10M. 
  
<img width=""1066"" alt=""屏幕快照 2019-08-07 下午1 51 01"" src=""https://user-images.githubusercontent.com/33338567/62624715-949bba00-b91b-11e9-82e7-0154aa04452f.png"">

And I have checked the statistics, the learning rate looks weird and I can't explain why it increased. Is it relevant to the decrease? 
  
<img width=""1053"" alt=""屏幕快照 2019-08-07 下午1 51 16"" src=""https://user-images.githubusercontent.com/33338567/62624718-99f90480-b91b-11e9-8ff8-48a4d7e26414.png"">

",hi case first training ended step continue training load without vary file except increasing see mean reward dramatically start following training step although finally direct training checked statistic learning rate weird ca explain relevant decrease,issue,negative,negative,neutral,neutral,negative,negative
518975150,"@ervteng my mistake, turns out I was already on latest master. I'll edit my bug report:

![image](https://user-images.githubusercontent.com/20813925/62602948-3cbf7c00-b938-11e9-8c47-0f592d4a6afa.png)
",mistake turn already latest master edit bug report image,issue,negative,positive,positive,positive,positive,positive
518970516,"Hi @DooblyNoobly, thanks for raising this issue. Could you try with the latest `master` branch? `develop` is the bleeding edge and may have issues. 

@arixlin, are you on `develop` or `master`? If `master` this seems like a bug and we'll investigate. Thanks!",hi thanks raising issue could try latest master branch develop bleeding edge may develop master master like bug investigate thanks,issue,positive,positive,positive,positive,positive,positive
518969300,"The easiest way would be to navigate to the environment file (https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents-envs/mlagents/envs/environment.py) and modify the `timeout_wait` parameter to greater than 30 seconds.

Note that there is also a known issue with the online BC trainer that will be resolved shortly. If you're having further trouble, try checking out the `hotfix-0.9.0a` branch. ",easiest way would navigate environment file modify parameter greater note also known issue trainer resolved shortly trouble try branch,issue,positive,positive,positive,positive,positive,positive
518966876,"Describe the bug
When using a visual demo brain with gail, mlagents throws an error after pressing play in the editor.
```
Traceback (most recent call last):
  File ""e:\program files\anaconda\envs\mlagents\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""e:\program files\anaconda\envs\mlagents\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""E:\Program Files\anaconda\envs\mlagents\Scripts\mlagents-learn.exe\__main__.py"", line 9, in <module>
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\mlagents\trainers\learn.py"", line 319, in main
    run_training(0, run_seed, options, Queue())
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\mlagents\trainers\learn.py"", line 118, in run_training
    tc.start_learning(env, trainer_config)
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\mlagents\trainers\trainer_controller.py"", line 283, in start_learning
    self.initialize_trainers(trainer_config, env_manager.external_brains)
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\mlagents\trainers\trainer_controller.py"", line 206, in initialize_trainers
    run_id=self.run_id,
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\mlagents\trainers\ppo\trainer.py"", line 68, in __init__
    self.policy = PPOPolicy(seed, brain, trainer_parameters, self.is_training, load)
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\mlagents\trainers\ppo\policy.py"", line 57, in __init__
    self, reward_signal, config
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\mlagents\trainers\components\reward_signals\reward_signal_factory.py"", line 40, in create_reward_signal
    class_inst = rcls(policy, **config_entry)
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\mlagents\trainers\components\reward_signals\gail\signal.py"", line 51, in __init__
    policy.model, 128, learning_rate, encoding_size, use_actions, use_vail
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\mlagents\trainers\components\reward_signals\gail\model.py"", line 41, in __init__
    self.make_inputs()
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\mlagents\trainers\components\reward_signals\gail\model.py"", line 125, in make_inputs
    False,
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\mlagents\trainers\models.py"", line 249, in create_visual_observation_encoder
    name=""conv_1"",
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\tensorflow\python\layers\convolutional.py"", line 619, in conv2d
    return layer.apply(inputs)
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\tensorflow\python\layers\base.py"", line 825, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\tensorflow\python\layers\base.py"", line 696, in __call__
    self.build(input_shapes)
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\tensorflow\python\layers\convolutional.py"", line 144, in build
    dtype=self.dtype)
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\tensorflow\python\layers\base.py"", line 546, in add_variable
    partitioner=partitioner)
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\tensorflow\python\training\checkpointable.py"", line 415, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 1297, in get_variable
    constraint=constraint)
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 1093, in get_variable
    constraint=constraint)
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 439, in get_variable
    constraint=constraint)
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 408, in _true_getter
    use_resource=use_resource, constraint=constraint)
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 747, in _get_single_variable
    name, """".join(traceback.format_list(tb))))
ValueError: Variable stream_0_visual_obs_encoder/conv_1/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:

  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\tensorflow\python\framework\ops.py"", line 1654, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\tensorflow\python\framework\ops.py"", line 3290, in create_op
    op_def=op_def)
  File ""e:\program files\anaconda\envs\mlagents\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
```
config
```
    trainer: ppo
    batch_size: 1024
    beta: 5.0e-3
    buffer_size: 10240
    epsilon: 0.2
    hidden_units: 128
    lambd: 0.95
    learning_rate: 3.0e-4
    max_steps: 5.0e4
    memory_size: 256
    normalize: false
    num_epoch: 3
    num_layers: 2
    time_horizon: 64
    sequence_length: 64
    summary_freq: 1000
    use_recurrent: true
    vis_encode_type: simple
    pretraining:
        demo_path: ./UnitySDK/Assets/Demonstrations/VisualPushBlock.demo
        strength: 0.5
        steps: 10000
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99
        curiosity:
            strength: 0.02
            gamma: 0.99
            encoding_size: 256
        gail:
            strength: 0.01
            gamma: 0.99
            encoding_size: 128
            demo_path: ./UnitySDK/Assets/Demonstrations/VisualPushBlock.demo
```
When I remove curiousity config， error had disappeared
",describe bug visual brain error pressing play editor recent call last file line file line code file line module file line main queue file line file line file line file line seed brain load file line self file line policy file line file line file line false file line file line return file line apply return file line file line build file line file line file line file line file line file line file line name variable already mean set originally defined file line file line file line trainer beta epsilon normalize false true simple pretraining strength extrinsic strength gamma curiosity strength gamma strength gamma remove error,issue,positive,negative,neutral,neutral,negative,negative
518917030,"@harperj 
Hello Again Harper, were you able to reproduce the bug? The version is no longer up to date since the 9.0 release (I think). Should I update to make sure the error persists?

Just to reiterate the problem, using multiple agents and brains with recurrence simply does not work (if you add one of the basic agents to the environment of another, and give one of the two  trainer_config profiles in use ""use_recurrent: true"", then it will throw a ""value error"" while unity itself hangs. Turning off recurrence allows both types of agents to train in the same scene at the same time, so I am positive that something about recurrence itself is the problem (i believe it is related to the way the engine packages memory data. if the memory arrays are differently shaped or different in number, and the handler doesn't know how to deal with unique instances, that would explain the ""need at least one array to concatenate"" issue, as the agent with the smaller memory buffer would lack enough inputs.

The only exception to reproducing this error as i have described is when only one agent file is used, and all the different brains have the same settings and input/output shape.",hello harper able reproduce bug version longer date since release think update make sure error reiterate problem multiple brain recurrence simply work add one basic environment another give one two use true throw value error unity turning recurrence train scene time positive something recurrence problem believe related way engine memory data memory differently shaped different number handler know deal unique would explain need least one array concatenate issue agent smaller memory buffer would lack enough exception error one agent file used different brain shape,issue,negative,positive,positive,positive,positive,positive
518888898,I saw other people met the same issue as I described.,saw people met issue,issue,negative,neutral,neutral,neutral,neutral,neutral
518811298,"Hi @awjuliani,
I have calculated it directly now. But I want to know is this possible to observe the velocity via stacking vectors?  Because I have checked your `Tennis` environment, you have observed the velocity directly, but you still use the 3 stacked vectors.
  
In addition,  is the `--load` command used for continuing training? I find that using the `--load` for curriculum learning, although I have specify the lesson to the corresponding lesson at the end point, the mean reward and return(specify `--debug`) still decreased dramatically.
In my case, at the end of the first 5M steps, the return is 0.26 and the mean reward for the last 1000 steps is about 0.6. However, when I started the following 5M steps, the mean reward at the 5M + 1000 steps decreased to -0.1 and the returns decreased to the 0.06. 
I mean the problem is that it cannot restore to the parameters at the end  of the last training. Or why the mean reward and return decreased?",hi calculated directly want know possible observe velocity via checked tennis environment velocity directly still use addition load command used training find load curriculum learning although specify lesson corresponding lesson end point mean reward return specify still dramatically case end first return mean reward last however following mean reward mean problem restore end last training mean reward return,issue,positive,negative,neutral,neutral,negative,negative
518794868,"> The visual observations are indeed being recorded

Thanks @awjuliani, good to know.",visual indeed thanks good know,issue,positive,positive,positive,positive,positive,positive
518781533,Could you link the paper please?,could link paper please,issue,negative,neutral,neutral,neutral,neutral,neutral
518778220,"Hi @awjuliani ,
Thank you for the response.
Yes, I understand that they are calculated every step. But are they also evaluated by the machine every step?
Is the machine thinking, alright, this step I got positive points, I did something right, next step I should try to do something similar?
The thing that bothers me seems to be the fact that we never call done() if the machine does everything right, and I have come to believe that the done() is crucial for the evaluation and therefor I feel like if the machine happens to be good then done() never runs and then the machine will never understand that he did good?",hi thank response yes understand calculated every step also machine every step machine thinking alright step got positive something right next step try something similar thing fact never call done machine everything right come believe done crucial evaluation therefor feel like machine good done never machine never understand good,issue,positive,positive,positive,positive,positive,positive
518776270,"Hi @AidinD 

Rewards are calculated every agent step. In the case of this game, I would add a small positive reward for staying alive, and a negative reward for death, as you have here.",hi calculated every agent step case game would add small positive reward alive negative reward death,issue,positive,negative,negative,negative,negative,negative
518775874,"Hi @mbaske 

Thanks for bringing this to our attention. The visual observations are indeed being recorded, there is just an issue with correctly reporting the resolution in the inspector. ",hi thanks attention visual indeed issue correctly resolution inspector,issue,negative,positive,neutral,neutral,positive,positive
518747967,"We are doing some internal experimentation, but nothing to share publicly yet. ",internal experimentation nothing share publicly yet,issue,negative,neutral,neutral,neutral,neutral,neutral
518742551,"Hi @AcelisWeaven 

Thanks for bringing this issue to our attention. ",hi thanks issue attention,issue,negative,positive,positive,positive,positive,positive
518735194,"> I want to learn how use raycast for OBJECT avoidance

One way of doing that is to calculate a weighted average angle from raycast hits. Let's say your car aims to drive forward, but has to dodge obstacles placed on the road. It sends out a spread of raycasts from left to right at various angles, e.g. from -30 degrees to +30 degrees (0 being the car's forward axis) at 1 degree increments. The input values for the weighted average function are the individual raycast angles. The weights are each raycast's hit distance divided by the maximum raycast length. Set weights to 1 whenever no obstacle was detected. The resulting average angle tends to point in the direction with the least or farthest away obstacles.
https://en.wikipedia.org/wiki/Weighted_arithmetic_mean",want learn use object avoidance one way calculate weighted average angle let say car drive forward dodge road spread left right various car forward axis degree input weighted average function individual hit distance divided maximum length set whenever obstacle resulting average angle point direction least farthest away,issue,negative,negative,neutral,neutral,negative,negative
518609989,"Maybe the problem is in your input / output names are not correct

",maybe problem input output correct,issue,negative,neutral,neutral,neutral,neutral,neutral
518598257,"Yes, I want to use a speech recognition model that uses dilated convolutions and conv1ds",yes want use speech recognition model dilated,issue,negative,neutral,neutral,neutral,neutral,neutral
518571934,Thats a shame! I have 8 computers was hoping to put them to work:) Any plans for distributed training in the future? this would be a huge benefit. Whats the advantage of running multiple instances on the same machine?,thats shame put work distributed training future would huge benefit whats advantage running multiple machine,issue,negative,positive,positive,positive,positive,positive
518468850,"I figured out this:
1. whatever v0.9 example files I opened, I cannot train with the same exception message above.
2. I turned back to mlagents v0.8 and everything runs well
it seems like a environment conflict",figured whatever example train exception message turned back everything well like environment conflict,issue,negative,neutral,neutral,neutral,neutral,neutral
518460826,Turns out this broke multi-GPU. We're working on a fix - will wait until it's done before pushing. ,turn broke working fix wait done pushing,issue,negative,neutral,neutral,neutral,neutral,neutral
518458797,my python environment is correctly installed and has run ml agents training successfully several times before,python environment correctly run training successfully several time,issue,negative,neutral,neutral,neutral,neutral,neutral
518452465,"(mlagents) D:\0Unity\ml-agents-0.9.0>mlagents-learn config\trainer_config.yaml --run-id=Test --train
that's what I typed before run unity project.
I forgot to get a screenshot.",train run unity project forgot get,issue,negative,neutral,neutral,neutral,neutral,neutral
518452031,"My Asset, a pretty simple scene as above
[Asset.zip](https://github.com/Unity-Technologies/ml-agents/files/3469846/Asset.zip)

",asset pretty simple scene,issue,positive,positive,positive,positive,positive,positive
518451681,"Hi @Peng2017 

You have control checked on the brain in the academy, this means that you cannot run from the editor without first launching the external python process to communicate with the brain and academy. Here is a document describing how to perform behavioral cloning, which I think might be relevant for you: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Behavioral-Cloning.md",hi control checked brain academy run editor without first external python process communicate brain academy document perform behavioral think might relevant,issue,negative,positive,positive,positive,positive,positive
518449792,"That is correct. You will either need to instantiate the agent already with a brain, or you will need to give the brain to the agent once it is instantiated. 

Can I ask a little more about your use-case? Are you trying to collect data using Unity, and hence the broadcast feature?",correct either need agent already brain need give brain agent ask little trying collect data unity hence broadcast feature,issue,negative,negative,negative,negative,negative,negative
518448167,"Ah! I just found that there is ""LearningBrain"" class... And It has model property :)  (but not in Brain class) Thank you",ah found class model property brain class thank,issue,negative,neutral,neutral,neutral,neutral,neutral
518441139,"Each time I Instantiate a GameObject Prefab with Agent.cs and Brain pre-combined with it, need I GiveBrain to the script again?",time prefab brain need script,issue,negative,neutral,neutral,neutral,neutral,neutral
518439984,"Sorry , bug is still there when I try to broadcast my brain",sorry bug still try broadcast brain,issue,negative,negative,negative,negative,negative,negative
518398934,I'm not sure how to find the version but I updated 3 weeks ago and most files in my downloaded repo have date modified at 01/07/2019.,sure find version ago date,issue,negative,positive,positive,positive,positive,positive
518387125,"I see, well thank you again for pointing out my mistake. ",see well thank pointing mistake,issue,negative,neutral,neutral,neutral,neutral,neutral
518386591,This is because it is defaulting to the base class method which simply returns 0 https://github.com/Unity-Technologies/ml-agents/blob/master/UnitySDK/Assets/ML-Agents/Examples/SharedAssets/Scripts/RayPerception.cs,base class method simply,issue,negative,negative,negative,negative,negative,negative
518385812,"Hello @itisyeetimetoday 

This project may be of interest to you: https://dtransposed.github.io/blog/GEAR.html",hello project may interest,issue,negative,neutral,neutral,neutral,neutral,neutral
518385378,"Oh dear...my bad as I was actually sending in those two additional values... Thanks for pointing that out.
But why isn't it giving me an error saying it can't find an overloaded method for those parameters?",oh dear bad actually sending two additional thanks pointing giving error saying ca find method,issue,negative,negative,negative,negative,negative,negative
518385282,"Hi @Peng2017 

Can you give a little more context about under what situation you are using ML-Agents? Are you attempting to perform training, or inference? Have you walked through the getting started tutorial before?",hi give little context situation perform training inference getting tutorial,issue,negative,negative,negative,negative,negative,negative
518384571,"Hello @superjayman 

What you are asking about is typically referred to as distributed training, and is currently not possible with ML-Agents. We do allow for training from multiple instances of Unity on a single machine however, which provides similar benefits, albeit at a smaller scale. ",hello typically distributed training currently possible allow training multiple unity single machine however similar albeit smaller scale,issue,negative,negative,neutral,neutral,negative,negative
518384066,"Hi @AidinD 

Just to confirm, when you use the 2D ray perception, you don't pass those additional two zero float values, correct? 

Here is the function for reference: https://github.com/Unity-Technologies/ml-agents/blob/master/UnitySDK/Assets/ML-Agents/Examples/SharedAssets/Scripts/RayPerception2D.cs#L32",hi confirm use ray perception pas additional two zero float correct function reference,issue,negative,neutral,neutral,neutral,neutral,neutral
518382981,"Hi @gzrjzcx,

As @cgiles mentions, I would recommend calculating it directly using differences between positions, and then passing those variables as observations to the agent.",hi would recommend calculating directly passing agent,issue,negative,positive,neutral,neutral,positive,positive
518382603,"Hello, @brandonhotdog 

This is a known issue with the v0.9 release. Can you confirm which version of ML-Agents you are using?",hello known issue release confirm version,issue,negative,neutral,neutral,neutral,neutral,neutral
518382143,"Hi @hanseoup 

It is possible to change the model. You will need to access the brain object, and change the model property. This is not an officially documented part of ML-Agents, but there is nothing which prevents this. I would however suggest that the simpler method is to use multiple brains with different models, and switch them that way, as each brain is meant to correspond to a specific behavior.",hi possible change model need access brain object change model property officially part nothing would however suggest simpler method use multiple brain different switch way brain meant correspond specific behavior,issue,negative,neutral,neutral,neutral,neutral,neutral
518350877,"> Code looks good to me -- my only thought is about our discussion of this tying us to certain numbers of epochs, ordering of updates, etc. on each of the reward signals and that removing flexibility. On the other hand, maybe that's just an implementation detail best left up to the trainer. In any case, that's a bridge to cross when we come to it I think-- 🚢 🇮🇹

One thought I had about this - the user really shouldn't be touching them IMO. For instance, setting num_epochs to > 1 for GAIL in SAC breaks training. This change will make it easier for us to enforce ""good defaults"" across trainers, which at the end of the day might be better. ",code good thought discussion tying u certain reward removing flexibility hand maybe implementation detail best left trainer case bridge cross come think ship one thought user really touching instance setting sac training change make easier u enforce good across end day might better,issue,positive,positive,positive,positive,positive,positive
518334147,Hi @manmeetsingh0013 -- if you are using the same run-id and want to continue training with the same model you'd currently need to use the `--load` flag to `mlagents-learn`,hi want continue training model currently need use load flag,issue,negative,neutral,neutral,neutral,neutral,neutral
518279007,thanks @MikeWise2718 for catching this. would you be open to updating the document on the develop branch and opening a PR?  https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Microsoft-Azure.md document?,thanks catching would open document develop branch opening document,issue,negative,positive,positive,positive,positive,positive
518260391,What about my issue? I have no BatchMultiClassNonMaxSuppression and passed converting  after fix in converter code. ,issue converting fix converter code,issue,negative,neutral,neutral,neutral,neutral,neutral
518192685,The provided script will create a frozen.pb frozen graph file that should then be converted to Barracuda using ```tensorflow_to_barracuda.py```,provided script create frozen graph file converted barracuda,issue,negative,neutral,neutral,neutral,neutral,neutral
518157917,"Err, yeah, that is actually what I did. What I wrote is for running the command line version of unity directly, as opposed to letting the python wrapper **ml-agents** do it.",err yeah actually wrote running command line version unity directly opposed python wrapper,issue,negative,positive,neutral,neutral,positive,positive
518107846,"Velocity is equal to distance/time, so if you do position(t)-position(t-1) t being the instant, you will get your velocity at this instant. You should store your previous position, then subtract it to your actual position, and you will get your velocity.",velocity equal position instant get velocity instant store previous position subtract actual position get velocity,issue,negative,negative,neutral,neutral,negative,negative
518081429,"Hello, @ervteng  i am working on a similar game of bubble shooter can u help me, i want to know how you are taking visual observation and whether you are using two cameras for block of bubbles and the shooting bubble.
Thanks",hello working similar game bubble shooter help want know taking visual observation whether two block shooting bubble thanks,issue,negative,negative,neutral,neutral,negative,negative
518002026,"Same problem.
What I do now is creating more brains to load different models.",problem brain load different,issue,negative,neutral,neutral,neutral,neutral,neutral
517992259,"Alternatively you can specify ""--no-graphics"" as an option to the ml-agents command. But yeah that should be put into the Azure docs. ",alternatively specify option command yeah put azure,issue,negative,neutral,neutral,neutral,neutral,neutral
517953343,"INFO:mlagents.envs:Saved Model
INFO:mlagents.trainers:List of nodes to export for brain :Fighter
INFO:mlagents.trainers: is_continuous_control
INFO:mlagents.trainers: version_number
INFO:mlagents.trainers: memory_size
INFO:mlagents.trainers: action_output_shape
INFO:mlagents.trainers: action
INFO:mlagents.trainers: action_probs
INFO:mlagents.trainers: value_estimate
INFO:tensorflow:Restoring parameters from ./models/JetStrike-0/Fighter/model-1128190.cptk
INFO:tensorflow:Froze 24 variables.
Converted 24 variables to const ops.
Converting ./models/JetStrike-0/Fighter/frozen_graph_def.pb to ./models/JetStrike-0/Fighter.nn
IGNORED: Cast unknown layer
IGNORED: StopGradient unknown layer
GLOBALS: 'is_continuous_control', 'version_number', 'memory_size', 'action_output_shape'
IN: 'vector_observation': [-1, 1, 1, 29] => 'sub_3'
IN: 'epsilon': [-1, 1, 1, 4] => 'mul_1'
OUT: 'action', 'action_probs', 'value_estimate'
DONE: wrote ./models/JetStrike-0/Fighter.nn file.
INFO:mlagents.trainers:Exported ./models/JetStrike-0/Fighter.nn file

I've also found it usually happens after/during a save. Here's some of the lines from one of the saves that happened at the same time as the CPU dropped. ",saved model list export brain fighter action froze converted converting cast unknown layer unknown layer done wrote file file also found usually save one time,issue,positive,negative,neutral,neutral,negative,negative
517868315,"Hi @awjuliani ,
While trying to perform online imitation learning in my custom made environment, as I give training command ("" mlagents-learn config/online_bc_config.yaml --train --slow"") in the command prompt it prompts me to press the play button. Since my environment is heavy it takes some time to start after pressing the play button , in the meantime command prompt gives me the error"" The Unity environment took too long to respond"". 
Any suggestion on increasing the wait time of the training command so that I can start the training of the environment.
",hi trying perform imitation learning custom made environment give training command train slow command prompt press play button since environment heavy time start pressing play button command prompt error unity environment took long respond suggestion increasing wait time training command start training environment,issue,negative,negative,negative,negative,negative,negative
517860894,Sorry for the bother @awjuliani . It was some problem with my Unity . Restarting the project solved it.,sorry bother problem unity project,issue,negative,negative,negative,negative,negative,negative
517857893,"Hi @awjuliani 
I am trying to perform imitation learning in my custom made environment. But, I am not able to edit the discrete inputs for my actions in player brain.
Any thoughts.
",hi trying perform imitation learning custom made environment able edit discrete player brain,issue,negative,positive,positive,positive,positive,positive
517728666,oh sorry it specifically says 1.7.1 I will close this issue now,oh sorry specifically close issue,issue,negative,negative,negative,negative,negative,negative
517727095,"Hey, just wanted to know how serious you are considering this? I am working on a Project that needs to implement Mobilenet+SSD till the end of year for a Demo. Should I wait for the next release or should I make it work with the older TensorFlowSharp Plugin? I just need Object Detection in a WebCam and Baracuda would make it quite simple.",hey know serious considering working project need implement till end year wait next release make work older need object detection would make quite simple,issue,negative,negative,neutral,neutral,negative,negative
517480304,Any update on the tests here?  I'd also update this from a draft PR to a normal PR.,update also update draft normal,issue,negative,positive,positive,positive,positive,positive
517475189,"Hey,
I must have accidentally clicked submit.

I found my answer. :)",hey must accidentally submit found answer,issue,negative,neutral,neutral,neutral,neutral,neutral
517469380,"Hi @nikola-j -- thanks for the comment.  It was a mistake on my part to close this issue, we will keep it open until the bug is fixed and I'll also make sure it's captured on our internal bug tracker.",hi thanks comment mistake part close issue keep open bug fixed also make sure internal bug tracker,issue,negative,positive,positive,positive,positive,positive
517461231,"This should probably be squashed and we should add a good commit message.  Glad to take care of that, just give a 👍 if you're ready for it to be merged.",probably add good commit message glad take care give ready,issue,positive,positive,positive,positive,positive,positive
517451511,"Hi @EnthusedDragon 

Can you please provide detail in your issue? It seems that you simply copied the template without changing anything.",hi please provide detail issue simply copied template without anything,issue,negative,neutral,neutral,neutral,neutral,neutral
517392621,"Hello @unnamed7 

Thank you for bringing this to our attention. We will take a look at it and try to better understand the issue. ",hello unnamed thank attention take look try better understand issue,issue,positive,positive,positive,positive,positive,positive
517163316,"I have one more question it would be great if you can give some input on it. suppose first time learning process it created .nn model and for second time learning if are using same run id will it be taking some learning inputs from 1st .nn model?  hope you get my question.
Thanks,",one question would great give input suppose first time learning process model second time learning run id taking learning st model hope get question thanks,issue,positive,positive,positive,positive,positive,positive
517136888,"Hi @mantasp I have cloned the github file from the link provided by Unity i.e.https://github.com/Unity-Technologies/ml-agents 
Also I got my code by referring to the **LearningBrain** script available in the examples bundle. I am trying to access webcam input and using the nn model along with a labels TextFile using the  **model** & **worker** API from https://github.com/Unity-Technologies/ml-agents/blob/e3b86a27a2547cdd177bf9e848b4337aa71b887a/UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda.md 
Now I have my code loading my trained model and creating model but when it comes to 
**worker.Execute(inputs);** 
It throws the error as posted above.
My target platform is iOS.",hi file link provided unity also got code script available bundle trying access input model along model worker code loading trained model model come error posted target platform,issue,negative,positive,positive,positive,positive,positive
517128829,Thank for the reply @harperj . I really appreciate your feedback.,thank reply really appreciate feedback,issue,positive,positive,positive,positive,positive,positive
517090440,That's OK. I think it might be a complex problem with muti-libs which is hard to fix.,think might complex problem hard fix,issue,negative,negative,negative,negative,negative,negative
517083182,"Try commenting out ForcedFullReset() in Academy.cs, around line number 590.
This eliminates the trouble, although I am not totally sure this is the proper way to fix. 

https://github.com/Unity-Technologies/ml-agents/issues/2378
",try around line number trouble although totally sure proper way fix,issue,negative,positive,neutral,neutral,positive,positive
517067289,"I've attached the log which was fortuitously still available in my Terminal window. It was trained for a reported total of 261,000 steps though it had plateaued by step 200,000.  However I was running 5 environments. Once again I'm new to this but based upon the rewards I've seen the actual number of steps needs to be multiplied by the number of environments. So based on that assumption it was trained for 1.3 million steps but had plateaued by step 1 million.

On a previous 5 environment run it hit a maximum mean reward of 672 at reported step 261,000 but then fell into irretrievable decline around step 400,000.  

Note that I've lifted the mass of the body object from 1 to 20 (part of the commit) for all of these. 

In a seperate uncommitted project I've increased default friction. I've also trialed training with increased joint strength or decreased decision interval time but top speed was unmoved at a simulated 80km/h. Rescaling by 1.5 times (about the size of a small car)and retraining lifted top speed to 130km/h. Not too bad and sufficient for my needs. It would be interesting to co-evolve morphology with behaviour one day.

Also in this seperate project I've found that I can retain the same performance by setting the Physics Project Settings Default Solver Iterations and Velocity Iterations back to their defaults of 6 and 1 respectively (ultimately I'm running on mobile and possibly Oculus Quest) and then lifting these values back to 12 and 12 for the forelegs in code.

[DynamicTraining.pdf](https://github.com/Unity-Technologies/ml-agents/files/3454269/DynamicTraining.pdf)
",attached log fortuitously still available terminal window trained total though step however running new based upon seen actual number need number based assumption trained million step million previous environment run hit maximum mean reward step fell irretrievable decline around step note mass body object part commit uncommitted project default friction also training joint strength decision interval time top speed unmoved time size small car top speed bad sufficient need would interesting morphology behaviour one day also project found retain performance setting physic project default solver velocity back respectively ultimately running mobile possibly oculus quest lifting back code,issue,positive,positive,neutral,neutral,positive,positive
517057093,"Hi @rajatpaliwal 

In this case you would use two separate agents with separate brains. You don't need to set up rewards, if you only plan to use imitation learning. 

As a note, you can check out the ""BananaIL"" scene which demonstrates online imitation learning: https://github.com/Unity-Technologies/ml-agents/blob/master/UnitySDK/Assets/ML-Agents/Examples/BananaCollectors/Scenes/BananaIL.unity",hi case would use two separate separate brain need set plan use imitation learning note check scene imitation learning,issue,negative,negative,negative,negative,negative,negative
517043055,"@stefanpantic we will use it as input for our roadmap planning, but I yet don't have specific timeframes when it could be actually implement. ",use input yet specific could actually implement,issue,negative,neutral,neutral,neutral,neutral,neutral
517040901,How did you integrate Barracuda in your project? And what platform are you targeting?,integrate barracuda project platform,issue,negative,neutral,neutral,neutral,neutral,neutral
517032171,"Thanks @awjuliani  for such prompt reply. I have two question - 
1) To implement this method do  I need to create two separate agents map them separately to the teacher and student brain?
2). While creating the agent do I need to setup reward for the agent or the simple mapping of the agent action to the keyboard buttons will do the job?",thanks prompt reply two question implement method need create two separate map separately teacher student brain agent need setup reward agent simple agent action keyboard button job,issue,positive,positive,positive,positive,positive,positive
517030034,"Hi @rajatpaliwal 

It sounds like you'd like to do online imitation learning. We have documentation on this here: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Imitation-Learning.md#online-training

Please let us know i this works for you.",hi like like imitation learning documentation please let u know work,issue,positive,negative,negative,negative,negative,negative
516993033,"Hi @harperj, I know it has been inactive, but shouldn't it be closed once the bug is fixed? Or do you guys use something internal to track issues?",hi know inactive closed bug fixed use something internal track,issue,negative,neutral,neutral,neutral,neutral,neutral
516992357,"I am also experiencing the same problem.
Even while only calling Done() in the AgentAction.

The Resest is clearly triggered, but it keeps on logging No episode was completed since last summary",also problem even calling done clearly triggered logging episode since last summary,issue,negative,positive,neutral,neutral,positive,positive
516988003,Thanks for the PR! i'm having a look at the project. The walk/body rotation def looks better to me. Can you tell me a bit more about training times? Do you know how many steps this CrawlerDynamicTraining model was trained for? Less than 1M steps?,thanks look project rotation better tell bit training time know many model trained le,issue,positive,positive,positive,positive,positive,positive
516978708,"Hey @ScriptBono I think you can do a little trick that is to increase the number of steps in the trainer_config.yaml of that brain and keep training from there.
",hey think little trick increase number brain keep training,issue,negative,negative,negative,negative,negative,negative
516956871,How many rays you need to effectively detect walls depends a bit on your environment setup.  One thing I can suggest is to add a small positive reward for getting closer to the goal if it is something you're able to do.  The Pyramids scene is a particularly hard scene to solve particularly because it only gives rewards _after_ solving a complex task rather than giving small incremental rewards for progress.,many need effectively detect bit environment setup one thing suggest add small positive reward getting closer goal something able scene particularly hard scene solve particularly complex task rather giving small incremental progress,issue,positive,positive,neutral,neutral,positive,positive
516955932,Sorry @HanwGeek -- I'm not sure.  This might depend on your setup and what component of the system failed.  Unfortunately I'm unable to help much with how ML-Agents is used with external tools.,sorry sure might depend setup component system unfortunately unable help much used external,issue,negative,negative,neutral,neutral,negative,negative
516940406,"Yes, the trained models for static and dynamic target seeking are part of the commit. 
The static one runs at the same speed as the old one but will now actually run to the designated target regardless of direction (the old version broke if the target direction was different to the one it was exposed to during training.)
The dynamic version gets to targets a little faster and looks more natural than the old version as it reliably turns to face the target rather than strafing and crab walking.",yes trained static dynamic target seeking part commit static one speed old one actually run target regardless direction old version broke target direction different one exposed training dynamic version little faster natural old version reliably turn face target rather crab walking,issue,positive,positive,positive,positive,positive,positive
516937371,"Hi @RunSwimFlyRich 

Thanks for making this PR. Have you tested training these models again, to compare performance?",hi thanks making tested training compare performance,issue,negative,positive,positive,positive,positive,positive
516770349,"Thanks for replying.
Which model did you deploy for training your data?
**As the Mobilenet+SSD is currently not supported by inference engine.**
I tried with a different model as well but the same error persists though!



",thanks model deploy training data currently inference engine tried different model well error though,issue,negative,positive,neutral,neutral,positive,positive
516706697,@HanwGeek There is no change in the communication when you use `no_graphics` -- I'd recommend trying it again and looking into the logs to see what went wrong.,change communication use recommend trying looking see went wrong,issue,negative,negative,negative,negative,negative,negative
516706247,"> Hmm, `no_graphics` should have worked. If you aren't using visual observations I'd highly recommend giving that another try (and reporting back if there are any other errors). That said, if you want to try to set up XServer we don't have a general guide but this might help: https://github.com/Unity-Technologies/obstacle-tower-env/blob/master/examples/gcp_training.md#set-up-xserver
> 
> I am not sure what kind of cluster setup you're using, but this was only tested on Google Cloud -- hope it helps.

When I train the agent alone, it worked very well. But when I train it in the cluster in `ray` lib by UCB, it comes out the `took too long to respond` problem. I think it maybe a process communication problem.",worked visual highly recommend giving another try back said want try set general guide might help sure kind cluster setup tested cloud hope train agent alone worked well train cluster ray come took long respond problem think maybe process communication problem,issue,positive,positive,positive,positive,positive,positive
516705314,"Hmm, `no_graphics` should have worked.  If you aren't using visual observations I'd highly recommend giving that another try (and reporting back if there are any other errors).  That said, if you want to try to set up XServer we don't have a general guide but this might help: https://github.com/Unity-Technologies/obstacle-tower-env/blob/master/examples/gcp_training.md#set-up-xserver

I am not sure what kind of cluster setup you're using, but this was only tested on Google Cloud -- hope it helps.",worked visual highly recommend giving another try back said want try set general guide might help sure kind cluster setup tested cloud hope,issue,positive,positive,positive,positive,positive,positive
516704118,"> Yep, looks like you're running into the issue that Unity requires xserver to render on Linux. You have a couple of options.
> 
> First off, if you're using only vector observations (no visual), you can use the ""no graphics"" mode to avoid the need for xserver. Alternatively, you can use xvfb to perform CPU rendering of your visual observations. If you're using the ML-Agents gym wrapper this is an option you can pass to the constructor. Finally, you could use a GPU and install drivers / xserver. This is a trickier option, but it will have the best performance.

I already set the `no_graphics=True` at the first time. But it seems didn't work. Please tell me how to use a GPU to do the trick? Thanks a lot!",yep like running issue unity render couple first vector visual use graphic mode avoid need alternatively use perform rendering visual gym wrapper option pas constructor finally could use install option best performance already set first time work please tell use trick thanks lot,issue,positive,positive,positive,positive,positive,positive
516702369,"> @TashaSkyUp We used to use TensorFlowSharp, which doesn't have 1.8+ support for those platform. Now we switch to barracuda, which should have 1.8+ support for those platforms, but we haven't tested them yet.

Is the barracuda conversion possible now ? I am facing difficulty in conversion of tensorflow 1.13.1 to .nn.",used use support platform switch barracuda support tested yet barracuda conversion possible facing difficulty conversion,issue,negative,neutral,neutral,neutral,neutral,neutral
516697642,"Yep, looks like you're running into the issue that Unity requires xserver to render on Linux.  You have a couple of options.

First off, if you're using only vector observations (no visual), you can use the ""no graphics"" mode to avoid the need for xserver.  Alternatively, you can use xvfb to perform CPU rendering of your visual observations.  If you're using the ML-Agents gym wrapper this is an option you can pass to the constructor.  Finally, you could use a GPU and install drivers / xserver.  This is a trickier option, but it will have the best performance.",yep like running issue unity render couple first vector visual use graphic mode avoid need alternatively use perform rendering visual gym wrapper option pas constructor finally could use install option best performance,issue,positive,positive,positive,positive,positive,positive
516666130,"I check the log but there is only one line showed below:
``` bash
Desktop is 0 x 0 @ 0 Hz
```
By the way, I train my model on Ubuntu server without GUI.",check log one line bash way train model server without,issue,negative,neutral,neutral,neutral,neutral,neutral
516643067,"Hi @gzrjzcx -- unfortunately the code is the only full reference for the model at this time.  To your specific question, the `use_recurrent` method adds an LSTM after the input / observation encoding (CNN in the case of camera observations).  I think what you've described with DQRN should be similar to this approach.  With PPO we have both the policy and value network, which will have their own LSTM.

Stacked vector observations mean we're stacking a history of the vector observations before inputting them into the network, but won't have an actual recurrent component in the network.",hi unfortunately code full reference model time specific question method input observation case camera think similar approach policy value network vector mean history vector network wo actual recurrent component network,issue,negative,negative,neutral,neutral,negative,negative
516639025,"Thanks @harperj , clear now!
  
So sorry to disturb you again.
The PPO algorithm employs neural network to parameterize the policy and does the policy gradient right? Is is possible to know the specific structure of the network? For example, as for the DQN, it is consisted of several convolution networks plus a fully connected layer. 
I am not familiar with the TensorFlow, and I have checked the source code, but not sure where is intended to stack the layers.
  
And in the LSTM case, if I using the LSTM(i.e. set the use_recurrent = true), is it means that I have used the RNN? And what is the specific structure of current networks? Is it like DQRN, just replacing the fully connected layer as the recurrent layer? 
  
Also, what is the difference between stacked vector and LSTM? ",thanks clear sorry disturb algorithm neural network policy policy gradient right possible know specific structure network example several convolution plus fully connected layer familiar checked source code sure intended stack case set true used specific structure current like fully connected layer recurrent layer also difference vector,issue,positive,positive,positive,positive,positive,positive
516636674,"@gzrjzcx -- you're correct, it should be 4 agents with 0.5 episode each.  I would agree that in that case it's probably best to increase your buffer size so that you are seeing experiences from all parts of an episode, though you can really only determine the best parameters via exploration :-)",correct episode would agree case probably best increase buffer size seeing episode though really determine best via exploration,issue,positive,positive,positive,positive,positive,positive
516636465,"> Looks like this broke some tests. We'll need to fix them but otherwise this looks like a really good change to me. Even with the really small buffer used for Hallway I'm seeing a ~15% improvement and of course you have seen an even bigger one with Obstacle Tower.

I only made changes to ppo so tests for other models failed. I'll fix them soon.",like broke need fix otherwise like really good change even really small buffer used hallway seeing improvement course seen even bigger one obstacle tower made fix soon,issue,positive,positive,positive,positive,positive,positive
516635027,"Hi, @harperj 
In terms of your example, the time horizon is 64 steps means every 64 steps the observations of these 64 steps are collected to the buffer right? 
Then in the 4 agents case, 4 agents generate observations in the same time, then once the buffer is full each agent only contributes 0.5 episode right? 
If it is, then is it should be:
4 agents -- ~ 4 half episodes per buffer (**4** agents, 0.5 episode each) ?
  
And in this case, we need to increase the buffer size to 2048, so that each agent is able to have 1 episode when the buffer is full?",hi example time horizon every collected buffer right case generate time buffer full agent episode right half per buffer episode case need increase buffer size agent able episode buffer full,issue,negative,positive,positive,positive,positive,positive
516627615,Looks like this broke some tests.  We'll need to fix them but otherwise this looks like a really good change to me.  Even with the really small buffer used for Hallway I'm seeing a ~15% improvement and of course you have seen an even bigger one with Obstacle Tower.,like broke need fix otherwise like really good change even really small buffer used hallway seeing improvement course seen even bigger one obstacle tower,issue,positive,positive,positive,positive,positive,positive
516620914,"Hi @gzrjzcx -- I'm not totally sure what you mean by `reset the buffer size` exactly -- but here's an example of what I mean:

Buffer size is 1024
Average episode length is 512 steps.
Time horizon is 64 steps.
1 agent -- ~2 full episodes per buffer (1 agent, 2 episodes)
2 agents -- ~2 full episodes per buffer (2 agents, 1 episode each)
4 agents -- ~ 4 half episodes per buffer (2 agents, 0.5 episode each)

You can see how this progresses until you may only have small parts of an episode in each buffer.  So there is some relationship between the number of agents you are using and your buffer size.",hi totally sure mean reset buffer size exactly example mean buffer size average episode length time horizon agent full per buffer agent full per buffer episode half per buffer episode see may small episode buffer relationship number buffer size,issue,negative,positive,neutral,neutral,positive,positive
516619749,@Guidosalimbeni No -- if you don't set `--slow` it will always use the _training_ configuration.,set slow always use configuration,issue,negative,negative,negative,negative,negative,negative
516617583,Hi all -- this issue has been inactive for some time so I'm going to close it.  Feel free to reopen or create a new issue if you have more to discuss.,hi issue inactive time going close feel free reopen create new issue discus,issue,positive,positive,positive,positive,positive,positive
516617167,~Hi all -- this issue has been inactive for some time so I'm going to close it.  Feel free to reopen or create a new issue if you have more to discuss.~,issue inactive time going close feel free reopen create new issue,issue,positive,positive,positive,positive,positive,positive
516615664,Hi @gameanimation3d -- unfortunately we don't have any function for adding custom stats within ML-Agents at this time.  I think this is a good suggestion for the future and we'd welcome the contribution if you end up adding this functionality.,hi unfortunately function custom within time think good suggestion future welcome contribution end functionality,issue,positive,positive,positive,positive,positive,positive
516615418,"Hi @Guidosalimbeni --

1) You shouldn't need anything extra in your scene other than the .nn model file and the same scene setup you used for training.

2) Yes, the agent will reset on done if this is set during inference mode.

3) Sure, you can do this.",hi need anything extra scene model file scene setup used yes agent reset done set inference sure,issue,positive,positive,positive,positive,positive,positive
516614907,Hi @HanwGeek -- you should be able to use ML-Agents within a Ray cluster / with RLLib.  Unfortunately I can't give much advice on the best way to do so at this time.  You can look at the Ray logs or the [Unity Player logs](https://docs.unity3d.com/Manual//LogFiles.html) on the workers to try to find out more specifically why your connection is failing.,hi able use within ray cluster unfortunately ca give much advice best way time look ray unity player try find specifically connection failing,issue,negative,positive,positive,positive,positive,positive
516614350,Hi @Guidosalimbeni -- it would be helpful for us if you could fill out the bug report from the issue template.  You can see the form including the bug report template here: https://github.com/Unity-Technologies/ml-agents/issues/new?assignees=&labels=bug&template=bug_report.md&title=,hi would helpful u could fill bug report issue template see form bug report template,issue,negative,neutral,neutral,neutral,neutral,neutral
516613974,"Hi @Guidosalimbeni -- we have limited ability to speculate on custom environments.  That said, the agents should be seeded using the same seed so initially it's very possible they will take similar actions.  The models we use inherently include random behavior, so you could say that is built in.

I'm not sure I can categorize the issue you're having, so I'm not sure how OnDecision would help.",hi limited ability speculate custom said seeded seed initially possible take similar use inherently include random behavior could say built sure categorize issue sure would help,issue,positive,positive,neutral,neutral,positive,positive
516562097,"> @harperj I‘m having the same problem and it starts ""No episode was completed since last summary."" After 10000 steps; Actually I just change a small part of the code for the rollerAgent in the tutorial guide on this page https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Create-New.md#final-editor-setup , when I ran the tutorial script there's no problem, but when I'm running the script after some modification it just doesnt work... And problem still occurs after I tried the flag method mentioned above...
<


I solve this by setting the maximum step finally...",problem episode since last summary actually change small part code tutorial guide page ran tutorial script problem running script modification doesnt work problem still tried flag method solve setting maximum step finally,issue,negative,negative,neutral,neutral,negative,negative
516518699,"There's no guarantee the reward metric will continue to increase over time, but that's certainly the goal of the update steps.

RE: CMD+C that's correct.  We save the model at the current step when you press CMD+C.",guarantee reward metric continue increase time certainly goal update correct save model current step press,issue,positive,positive,positive,positive,positive,positive
516429573,"Hi @harperj & thanks for the answer !
I think I understand now:
The Reward metric fluctuates as the training environment also fluctuates, but in general, the reward metric should increase when viewed over several step statments, right ?

Also, when stopping the training (using CMD & C) the last steps are also saved, not just the last checkpoint, correct ?",hi thanks answer think understand reward metric training environment also general reward metric increase several step right also stopping training last also saved last correct,issue,positive,positive,neutral,neutral,positive,positive
516429012,looks great.  im putting this into the release branch for documentation.  which will go out tomorrow.,great release branch documentation go tomorrow,issue,positive,positive,positive,positive,positive,positive
516335665,"It doesn't works....
But the same code using mlagents 0.5.0 it works, I don't know if there is a bug in the newer version ",work code work know bug version,issue,negative,neutral,neutral,neutral,neutral,neutral
516300823,I confirmed your request and update commit,confirmed request update commit,issue,negative,positive,positive,positive,positive,positive
516292695,Will do. Just experimenting with whether it's best to supply body part velocities relative to the direction to the target or the body forward vector and then I'll make a pull request. In addition to the default training scene I'm also trialing the crawlers with my own spline following code.,whether best supply body part relative direction target body forward vector make pull request addition default training scene also spline following code,issue,positive,positive,positive,positive,positive,positive
516281600,"Hi @cidelab,
I am also trying do the same and having the same issue did get any lead on it?
Thanks,
Manmeet Singh",hi also trying issue get lead thanks singh,issue,negative,positive,positive,positive,positive,positive
516267101,Thanks for reply. I just want to tell you that i am taking the reference of the pyramid scene. i m using the same Raycast and decreasing the reward after the steps like this : AddReward(-1f / agentParameters.maxStep) and also moving the car as per pyramid reference. could you please give the explanation that how how many rays require to detect the left and right walls,thanks reply want tell taking reference pyramid scene decreasing reward like also moving car per pyramid reference could please give explanation many require detect left right,issue,positive,positive,positive,positive,positive,positive
516223549,"It might be something really simple like accidentally feeding in the x and y parameters rather than **x and z** into AddVectorObs(....)

If you did such a thing you would get movement on one axis but not the other.
",might something really simple like accidentally feeding rather thing would get movement one axis,issue,negative,neutral,neutral,neutral,neutral,neutral
516193988,"Thank you.
So if I not set --slow it will run at the speed set in the Inference configuration, even when it is training. Am I correct?",thank set slow run speed set inference configuration even training correct,issue,negative,negative,negative,negative,negative,negative
516190850,Was still happening in 0.8.1. I will test in 0.8.2 next week.,still happening test next week,issue,negative,neutral,neutral,neutral,neutral,neutral
516165374,"Eeverything should be properly set up, so all you need to do is open the SDK folder from the unity hub, and then initiate training with ""mlagents-learn config\trainer_config.yaml --train"" from the ml-agents root",properly set need open folder unity hub initiate training train root,issue,negative,neutral,neutral,neutral,neutral,neutral
516164850,"@harperj Thanks for the response. I just downloaded and tested it, and it definitely throws the error. 

I did not make any new prefabs or scenes, I merely added a crawler agent to the PushBlock scene, and then gave the crawler ""recurrent: true"" in the trainer_config.yaml.

The relevant scene is at ML-Agents\Examples\PushBlock\Scenes\PushBlock

What was the exact issue that you were having? Let me know and I will do whatever is needed to fix it. 

But essentially, the error happens whenever there is more than one type of learning agent in the scene, and where at least one of the agents has recurrence. I just put a crawler into the push-block scene, gave it recurrence, and the error occurs. (I did set it up in the academy and configure the crawler agent itself, and it works when recurrence is turned back off)",thanks response tested definitely error make new merely added crawler agent scene gave crawler recurrent true relevant scene exact issue let know whatever fix essentially error whenever one type learning agent scene least one recurrence put crawler scene gave recurrence error set academy configure crawler agent work recurrence turned back,issue,negative,positive,neutral,neutral,positive,positive
516158579,"Thanks @harperj,
So, both running multiple agents within one scene and parallel environments situations, all need to reset the buffer size right? ",thanks running multiple within one scene parallel need reset buffer size right,issue,negative,positive,positive,positive,positive,positive
516155410,"Hi @gzrjzcx -- with parallel environments I was specifically referring to `--num-envs`.  This means additional Unity instances running in parallel.  That said, it similarly applies to multiple independent agents running within the same Unity instance/scene as you mentioned.

You're correct about `--num-runs`.  This feature was simply intended to allow a way to run independent trials of an experiment in parallel for the purpose of measuring how consistent training performance is.

RE: continuing training, yes, the `--load` flag will continue training from the most recent checkpoint.",hi parallel specifically additional unity running parallel said similarly multiple independent running within unity correct feature simply intended allow way run independent experiment parallel purpose measuring consistent training performance training yes load flag continue training recent,issue,positive,positive,neutral,neutral,positive,positive
516151197,"Hi @harperj,
Thanks for your reply. I am a little confused about the meaning of parallel environments. In fact I have duplicated 24 agents in one scene, just like your example environmet, is it parallel environment?  Should I change the buffer size for this?
  
I have also set the `--num-envs=4` to run concurrent Unity Instances. It shows that the Academy started four times. Is it parallel envs?  Should I change the buffer size for this? And batch size?
  
By the way, what is the different between `--num-envs` and `--num-runs`? According to my understand, `--num-runs` cannot increase performance right?  because each run is an independent session. 
  
And is it possible to continue training  from a checkpoint? For example, my training is interrupted due to time limit. Can I use the `--load` flag to continue this training from the last checkpoint? ",hi thanks reply little confused meaning parallel fact one scene like example parallel environment change buffer size also set run concurrent unity academy four time parallel change buffer size batch size way different according understand increase performance right run independent session possible continue training example training interrupted due time limit use load flag continue training last,issue,positive,negative,neutral,neutral,negative,negative
516148241,"Hi @ScriptBono -- unfortunately we don't currently have a way to do either of these.  It sounds useful, so I will bring it up to the team.",hi unfortunately currently way either useful bring team,issue,negative,positive,positive,positive,positive,positive
516147803,"Hi @manmeetsingh0013 -- as mentioned in our issue template unfortunately our team doesn't have the resources to help support training of user-created environments.  Unfortunately there are a lot of factors (observation setup, action space setup, rewards, hyperparameters) which could contribute to your environment not training.  One bit of feedback I can give is that it seems your reward is always `-1.000`; you'll want to ensure your agent has the ability to see how it can affect the environment to increase or decrease reward in order for it to learn.",hi issue template unfortunately team help support training unfortunately lot observation setup action space setup could contribute environment training one bit feedback give reward always want ensure agent ability see affect environment increase decrease reward order learn,issue,positive,negative,negative,negative,negative,negative
516147008,"Hi @Guidosalimbeni -- this is related to running the environment ""slowly"".  This is so you can see how the agent's behavior looks in real-time (vs. sped up for training).  This speedup difference is specified in the Academy's ""inference configuration""",hi related running environment slowly see agent behavior sped training difference academy inference configuration,issue,negative,negative,negative,negative,negative,negative
516146361,"Hi @MentalGear -- this problem is actually more difficult than it might seem at first glance.  The summary which output the highest mean reward may or may not be the best model, since there is randomness in both the model and the environment.  We only save the model checkpoints at the interval you specify for checkpointing and save the model when you end training.",hi problem actually difficult might seem first glance summary output highest mean reward may may best model since randomness model environment save model interval specify save model end training,issue,positive,positive,positive,positive,positive,positive
516145570,"Hi @lkuich -- you could use `imageTensor.Data` to get access to the values as a float array.  Then you can use any standard operations you would like.
",hi could use get access float array use standard would like,issue,negative,neutral,neutral,neutral,neutral,neutral
516142524,"Hi @FlimFlamm -- looks like the repo you shared is missing a Prefab with the updated agents and training areas, so I can't run it.",hi like missing prefab training ca run,issue,negative,negative,negative,negative,negative,negative
516141995,@RunSwimFlyRich this change makes a lot of sense to me.  We'd welcome a pull request with this change if you'd like to make one!,change lot sense welcome pull request change like make one,issue,positive,positive,positive,positive,positive,positive
516141299,"@roboserg case 1) is because you're running with the same `run-id`.  If you pass a new run-id with the `--run-id` flag you should be able to avoid adding the datapoints to the same graph.

For 2) the intention was always that these `.csv` files should be created whether or not you use `--debug`.  The goal of these files is to provide a plaintext summary (vs. the Tensorflow binary format) which you can use to monitor the performance of your training session.",case running pas new flag able avoid graph intention always whether use goal provide summary binary format use monitor performance training session,issue,negative,positive,positive,positive,positive,positive
516139858,"Hi @gzrjzcx -- this sort of instability is relatively common with reinforcement learning algorithms (including PPO).  You might be able to avoid it by exploring different hyperparameters, though it's very dependent on your environment so it's hard to give specific advice.  One thing you might consider is that if you increase the number of parallel environments we don't automatically increase the buffer size.",hi sort instability relatively common reinforcement learning might able avoid exploring different though dependent environment hard give specific advice one thing might consider increase number parallel automatically increase buffer size,issue,negative,negative,neutral,neutral,negative,negative
516138502,@kpradp we've stopped using Tensorflow Sharp for ML-Agents -- though the project is still being built independently as far as I know so in theory you could use it.,stopped sharp though project still built independently far know theory could use,issue,negative,negative,neutral,neutral,negative,negative
516106427," @harperj  I‘m having the same problem and it starts ""No episode was completed since last summary."" After 10000 steps; Actually I just change a small part of the code for the rollerAgent in the tutorial guide on this page  https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Create-New.md#final-editor-setup , when I ran the tutorial script there's no problem, but when I'm running the script after some modification it just doesnt work... And problem still occurs after I tried the flag method mentioned above...

My script for the agent (It's actually on a cube object but I didnt change script's name)

[public class RollerAgent : Agent
{
    // Start is called before the first frame update
    //Rigidbody rBody;
    public GameObject Obstacle;
    public GameObject Plane;
    public LayerMask unwalkableMask;
    bool TocallDone=true;
    //transform.rotation.x=target.rotation.x;
    //transform.rotation.x = target.rotation.y;
    //transform.rotation.x=target.rotation.z;
    //transform.localEulerAngles = new Vector3(target.rotation.x, target.rotation.y, target.rotation.z);
    void Start()
    {
        //rBody = GetComponent<Rigidbody>();
        Obstacle = GameObject.Find(""Obstacle"");
    }
    public Transform Target;
    public override void AgentReset()
    {
        if (this.transform.position.y < 0)
        {
            // If the Agent fell, zero its momentum
            //this.rBody.angularVelocity = Vector3.zero;
            //this.rBody.velocity = Vector3.zero;
            this.transform.position = new Vector3(0, 0.05f, 0);
            transform.localEulerAngles = new Vector3(0,0,0);
        }

        // Move the target to a new spot
        Target.position = new Vector3(Random.value * 8 - 4,0.05f,Random.value * 8 - 4);
        //Move the obstacle to a new spot
        Obstacle.transform.position = new Vector3(Random.value * 8 - 4, 0.5f, Random.value * 8 - 4);
    }

    public override void CollectObservations()
    {
        // Target and Agent Obstacle positions
        AddVectorObs(Target.position);

        AddVectorObs(this.transform.position);
        AddVectorObs(this.transform.localScale);

        AddVectorObs(Obstacle.transform.position);
        AddVectorObs(Obstacle.transform.localScale);
        // Agent velocity
        //AddVectorObs(rBody.velocity.x);
        //AddVectorObs(rBody.velocity.z);
    }

    public float speed = 2;
    public override void AgentAction(float[] vectorAction, string textAction)
    {
        // Actions, size = 2
        Vector3 controlSignal = Vector3.zero;

        controlSignal.x = vectorAction[0];
        controlSignal.z = vectorAction[1];

        Vector3 targetPos = Vector3.zero;
        targetPos.x = transform.position.x + controlSignal.x;
        targetPos.z = transform.position.z + controlSignal.z;
        //rBody.AddForce(controlSignal * speed);
        transform.position = Vector3.MoveTowards(transform.position, targetPos, speed * Time.deltaTime);
        //HalfExtend
        Vector3 halfExtend;
        halfExtend.x = transform.localScale.x / 2;
        halfExtend.y = transform.localScale.y / 2;
        halfExtend.z = transform.localScale.z / 2;

        // Rewards
        float distanceToTarget = Vector3.Distance(this.transform.position,Target.position);

        // Reached target
        if (distanceToTarget ==0)
        {
            if (TocallDone)
            {
                TocallDone = false;
            }
            else
            {
                SetReward(1.0f);
                Done();
                TocallDone = true;
            }  
        }

        //Hit the obstacle
        if(Physics.CheckBox(transform.position, halfExtend,transform.rotation, unwalkableMask))
        {
            if(TocallDone)
            {
                TocallDone = false;
            }
            else
            {
                //SetReward(-1.0f);
                Done();
                TocallDone = true;
            }        
        }


        // Fell off platform
        if (this.transform.position.y < 0)
        {
            if (TocallDone)
            {
                TocallDone = false;
            }
            else
            {
                //SetReward(-1.0f);
                Done();
                TocallDone = true;
            }
        }

        if (this.transform.position.x >(Plane.transform.lossyScale.x*5+(this.transform.lossyScale.x/2))|| this.transform.position.x <- (Plane.transform.lossyScale.x * 5 + (this.transform.lossyScale.x / 2)))
        {
            if (TocallDone)
            {
                TocallDone = false;
            }
            else
            {
                //SetReward(-1.0f);
                Done();
                TocallDone = true;
            }
        }

        if (this.transform.position.z > (Plane.transform.lossyScale.z*5 + (this.transform.lossyScale.z / 2))|| this.transform.position.z <- (Plane.transform.lossyScale.z * 5 + (this.transform.lossyScale.z / 2)))
        {
            if (TocallDone)
            {
                TocallDone = false;
            }
            else
            {
                //SetReward(-1.0f);
                Done();
                TocallDone = true;
            }
        }
    }
}](url)",problem episode since last summary actually change small part code tutorial guide page ran tutorial script problem running script modification doesnt work problem still tried flag method script agent actually cube object didnt change script name public class agent start first frame update public obstacle public plane public bool new vector void start obstacle obstacle public transform target public override void agent fell zero momentum new vector new vector move target new spot new vector obstacle new spot new vector public override void target agent obstacle agent velocity public float speed public override void float string size vector vector speed speed vector float target false else done true obstacle false else done true fell platform false else done true false else done true false else done true,issue,negative,positive,neutral,neutral,positive,positive
515961614,"> Just a heads up that this has been referenced before: #1117

Oh, yeah! It has't been solved yet even in the version of v0.8?",oh yeah yet even version,issue,negative,neutral,neutral,neutral,neutral,neutral
515779746,"I've fixed this on my local version now by modifying the following two functions in CrawlerAgent.cs. Training times, rewards and standard deviations were not noticeably affected but the crawler now operates optimally regardless of direction to target. Note that the crawler brain Vector Observation Space Size was reduced to 126 in the inspector since we are no longer passing in the normalized dirToTarget (this information is now implicit in the transformed body forward vector).
`
public override void CollectObservations()
    {
        jdController.GetCurrentJointForces();
      //  AddVectorObs(dirToTarget.normalized);

        workingTrans.position = body.position;
        workingTrans.rotation = Quaternion.LookRotation(dirToTarget);
        // Forward & up to help with orientation
        AddVectorObs(body.transform.position.y); //This should ultimately be generalised to a raycast 
        AddVectorObs(workingTrans.InverseTransformVector(body.forward));
        AddVectorObs(workingTrans.InverseTransformVector(body.up));
        foreach (var bodyPart in jdController.bodyPartsDict.Values)
        {
            CollectObservationBodyPart(bodyPart);
        }
    }

    public void CollectObservationBodyPart(BodyPart bp)
    {
        var rb = bp.rb;
        AddVectorObs(bp.groundContact.touchingGround ? 1 : 0); // Whether the bp touching the ground
        AddVectorObs(workingTrans.InverseTransformVector(rb.velocity));
        AddVectorObs(workingTrans.InverseTransformDirection(rb.angularVelocity));

        if (bp.rb.transform != body)
        {
            Vector3 localPosRelToBody = body.InverseTransformPoint(rb.position);
            AddVectorObs(localPosRelToBody);
            AddVectorObs(bp.currentXNormalizedRot); // Current x rot
            AddVectorObs(bp.currentYNormalizedRot); // Current y rot
            AddVectorObs(bp.currentZNormalizedRot); // Current z rot
            AddVectorObs(bp.currentStrength / jdController.maxJointForceLimit);
        }
    }
`",fixed local version following two training time standard noticeably affected crawler regardless direction target note crawler brain vector observation space size reduced inspector since longer passing information implicit body forward vector public override void forward help orientation ultimately public void whether touching ground body vector current rot current rot current rot,issue,negative,positive,neutral,neutral,positive,positive
515686603,"> I have something similar happening, but in my case it always recovers https://puu.sh/DXQ1M/20af243bdc.png

I have tried to trained more time, it increased again indeed, but very slow. You can see that I have trained 10M times. It needs about a whole day... And I am not sure if I trained more time(20M), it will excess than before?
<img width=""350"" alt=""屏幕快照 2019-07-27 下午3 00 52"" src=""https://user-images.githubusercontent.com/33338567/61995386-c5f4ca00-b07f-11e9-86ec-7cf22f22ddd8.png"">
",something similar happening case always tried trained time indeed slow see trained time need whole day sure trained time excess,issue,negative,positive,neutral,neutral,positive,positive
515686250,"> Which version of ml-agents are you using? I had a similar issue but it got fixed during the most recent update to v0.8.2.

I am using 0.8.1 now, what is your previous version?",version similar issue got fixed recent update previous version,issue,negative,negative,neutral,neutral,negative,negative
515670631,Which version of ml-agents are you using? I had a similar issue but it got fixed during the most recent update to v0.8.2. ,version similar issue got fixed recent update,issue,negative,positive,neutral,neutral,positive,positive
515640078,"I also noticed that adding one agent in an inference mode (while training the other) adds around 10 seconds to the 1000 steps of training. In other words it takes 10 seconds for only one single agent to train. If you add a second agent that is NOT training (only uses inference) the time jumps to 20s per 1000 training steps. I find it really weird, as the impact of the inference should theoretically not be so huge. ",also one agent inference mode training around training one single agent train add second agent training inference time per training find really weird impact inference theoretically huge,issue,negative,negative,neutral,neutral,negative,negative
515639567,"I have something similar happening, but in my case it always recovers https://puu.sh/DXQ1M/20af243bdc.png",something similar happening case always,issue,negative,neutral,neutral,neutral,neutral,neutral
515627775,"I guess it is testing the rl aspect of the environment, so either rename it something like test_simple_rl or move it to test_ppo? The PR looks good to me, it is ok to merge. ",guess testing aspect environment either rename something like move good merge,issue,positive,positive,positive,positive,positive,positive
515623483,"> Don't merge this! We've merged the release fix back into develop to fix daily testing.

Sure then I'll close this one.",merge release fix back develop fix daily testing sure close one,issue,negative,positive,positive,positive,positive,positive
515623241,Don't merge this! We've merged the release fix back into develop to fix daily testing.,merge release fix back develop fix daily testing,issue,negative,neutral,neutral,neutral,neutral,neutral
515591737,"Right now it's only testing PPO, but I think it's general enough to test other combinations of settings (memory, visual obs) and other trainers (SAC).",right testing think general enough test memory visual sac,issue,negative,positive,neutral,neutral,positive,positive
515585241,I think we should move the actual running of the test to the test_ppo.py file - what do you think?,think move actual running test file think,issue,negative,neutral,neutral,neutral,neutral,neutral
515564321,"> Whoops, we fixed this on the release branch but didn't merge it to develop:
> https://github.com/Unity-Technologies/ml-agents/pull/2330/files
> 
> You might want to make that change instead, otherwise there are going to be merge conflicts later.

Fixed",whoop fixed release branch merge develop might want make change instead otherwise going merge later fixed,issue,negative,positive,neutral,neutral,positive,positive
515563192,"Whoops, we fixed this on the release branch but didn't merge it to develop:
https://github.com/Unity-Technologies/ml-agents/pull/2330/files

You might want to make that change instead, otherwise there are going to be merge conflicts later.",whoop fixed release branch merge develop might want make change instead otherwise going merge later,issue,negative,positive,neutral,neutral,positive,positive
515439175,"Thanks for your research @AcelisWeaven. I just found out, that out of my 12-cores (24-logic-cores) only two are used. That seems odd, doesn't it?

![96a126620a3b35ee1a12f727d7282ecd](https://user-images.githubusercontent.com/53266983/61952813-28cd5f00-afb5-11e9-8189-b62d0ec7e4a1.png)
",thanks research found two used odd,issue,negative,positive,neutral,neutral,positive,positive
515422735,"Hi @tillmusshoff ,

I saw your post and ran some tests to see what would happen, and indeed it seemed faster.
But I don't think it really is, the reason for this is that 1 step = 1 tick in Unity. So if you have one agent, in one step your agent will train on one observation, whereas with ten agents it'll train on ten observations (thus, it should learn faster)

Anyway I wanted to benchmark it, so here's a graph from a training session on a virtual server with 1 vCPU, where I changed my number of agents. I'm using PR #1975, where an agent learn to play against himself. ELO rating is a good metric in my environment, but I don't know what metric you could use to measure the ""learning speed"".

![image](https://user-images.githubusercontent.com/6735195/61949367-f6b6ff80-afaa-11e9-8dc9-20a03366f6b6.png)

From this graph, my ELO increased like this:

| Session | From | To | Time | ELO increase | ELO points/minute |
| :--- | :---: | :---: | :---: | :---: | ---: |
| 20 agents (first run) | 1529 | 1599 | 95 min. | +70 | **+0,73** |
| 2 agents (second run)  | 1599 | 1604 | 15 min. | +5 | **+0.33** |
| 20 agents (third run)  | 1604 | 1666 | 90 min. | +62 | **+0.69** |

In my case, running 10x more agents make the training twice faster. I'll need more tests to see if I can find a good number of agents to maximise this ratio.

Also (I don't have metrics for this, sorry!), switching my virtual server to a 8 vCPU instance actually reduced the training speed a lot! And I don't know why.

Unity team, do you have some insights?",hi saw post ran see would happen indeed faster think really reason step tick unity one agent one step agent train one observation whereas ten train ten thus learn faster anyway graph training session virtual server number agent learn play rating good metric environment know metric could use measure learning speed image graph like session time increase first run min second run min third run min case running make training twice faster need see find good number ratio also metric sorry switching virtual server instance actually reduced training speed lot know unity team,issue,positive,positive,positive,positive,positive,positive
515362217,"Considering this codes, for now they are very simple.

CarAgent:

    public override void AgentAction(float[] vectorAction, string textAction) {
        steer = Mathf.Clamp(vectorAction[0], -1f, 1f);
        throttle = 1;
        
        if(Collided) {  //car hit the walls
            AddReward(-1.0f);
            Done();
        } else {
            AddReward(0.05f);
        }
    }

I use an interface to pass the steer and throttle value at the CarController provided by Unity.
Anr these are the brains and the academy setup

![CarAgentBrain](https://user-images.githubusercontent.com/18141767/61938162-be0a2c80-af90-11e9-9a35-0c10b452f8e3.png)
![CarLearningBrain](https://user-images.githubusercontent.com/18141767/61938168-c19db380-af90-11e9-9318-fe4104ca71ef.png)
![Academy](https://user-images.githubusercontent.com/18141767/61938170-c4000d80-af90-11e9-95a8-aaf0538129ca.png)
",considering simple public override void float string steer throttle hit done else use interface pas steer throttle value provided unity brain academy setup academy,issue,negative,neutral,neutral,neutral,neutral,neutral
515353826,"@harperj Fair point on the needs for gameplay!
Say I want to set the drag realistically, is the density in an environment given somewhere? I can't seem to find any note on that.",fair point need say want set drag realistically density environment given somewhere ca seem find note,issue,negative,positive,positive,positive,positive,positive
515342050,"Thanks for you reply. I attached the Academy config as a screenshot. I also tested the training again with only one enivornment. The time for 1000 steps got reduced from 120 seconds to about 30 seconds. However only 10% of the processing power is being used. Is there a way to increase this drastically? I run a 1000$ processor thats not being used to it's full potential. 

Also would you recommend training with multiple environments and longer timesteps or one environment with shorter timesteps? 

Thanks a lot!

![c4cce7a5602eed2daec256a0780093e2](https://user-images.githubusercontent.com/53266983/61934046-af6b4780-af87-11e9-9011-214e663f4dd6.png)
",thanks reply attached academy also tested training one time got reduced however power used way increase drastically run processor thats used full potential also would recommend training multiple longer one environment shorter thanks lot,issue,positive,positive,positive,positive,positive,positive
515340274,What worked for me is to Install a different Unity version,worked install different unity version,issue,negative,neutral,neutral,neutral,neutral,neutral
515257515,"The crawler agent has a bit of configuring required (you need to add its body parts to the agent script manually) so my apologies if you had any difficulties getting it running. The forked repository has it set correctly. (if you have any trouble, please let me know)

IF either the pushblock or the crawler brain has recurrence, it breaks the training process. At first I assumed it was my own spaghetti code, but I've narrowed the issue down to recurrence.

In cases where the observations and actions spaces match (and the agent scripts are all the same), I do not believe that this is actually a problem, as I've had no issue with 4 unique but identical brains training across multiple agents simultaneously. It's also possible that the problem pertains to using more than one sort of AgentScript. The 4 unique but identical brains I've been using also use the same agent script, so that might be why I've not had the problem until now.
",crawler agent bit need add body agent script manually getting running forked repository set correctly trouble please let know either crawler brain recurrence training process first assumed spaghetti code issue recurrence match agent believe actually problem issue unique identical brain training across multiple simultaneously also possible problem one sort unique identical brain also use agent script might problem,issue,negative,positive,positive,positive,positive,positive
515250617,@harperj I forgot to mention that it is the first push block scene that has the crawler added.,forgot mention first push block scene crawler added,issue,negative,positive,positive,positive,positive,positive
515248083,"> Hi @FlimFlamm -- could you share the example where you changed the Crawler / PushBlock environments to reproduce? Maybe on a fork of this repo?

I exported the crawler + push block + dependencies into a package (and zipped it). Please let me know if that is not a suitable format:  [crawler-pushblock bug.zip](https://github.com/Unity-Technologies/ml-agents/files/3433541/crawler-pushblock.bug.zip)


To trigger the bug, the following trainer config file can be used (it has recurrent set to true)
[trainer_config.zip](https://github.com/Unity-Technologies/ml-agents/files/3433548/trainer_config.zip)

",hi could share example crawler reproduce maybe fork crawler push block package please let know suitable format trigger bug following trainer file used recurrent set true,issue,positive,positive,positive,positive,positive,positive
515241525,"Hi @kpradp -- you might be interested in checking out the documentation we have in place for Barracuda, our inference library: https://github.com/Unity-Technologies/ml-agents/blob/master/UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda.md

This project is still in early stages of development, so our current level of support and documentation is limited.",hi might interested documentation place barracuda inference library project still early development current level support documentation limited,issue,positive,positive,neutral,neutral,positive,positive
515240799,Hi @LOYOJO990618 -- unfortunately this video is based on an older version of ML-Agents so it won't be accurate to what you see today.  The information in our docs around creating environments might be useful to you: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Create-New.md,hi unfortunately video based older version wo accurate see today information around might useful,issue,negative,positive,neutral,neutral,positive,positive
515240190,"In general we'd expect game developers to pick mass and drag based on needs for gameplay rather than how it affects training.  Unfortunately I'm not sure I can provide any guidance here, since the effect of the rigidbody settings on training can depend on a lot of factors specific to your environment and observations / actions.",general expect game pick mass drag based need rather training unfortunately sure provide guidance since effect training depend lot specific environment,issue,negative,positive,neutral,neutral,positive,positive
515238273,Hi @FlimFlamm -- could you share the example where you changed the Crawler / PushBlock environments to reproduce?  Maybe on a fork of this repo?,hi could share example crawler reproduce maybe fork,issue,negative,neutral,neutral,neutral,neutral,neutral
515237629,Hi @tillmusshoff -- it's a bit hard to know why your environment steps are taking so long without knowing the specifics of how your environment is set up.  You might consider the time scale settings you've used within the Academy config.  One thing that can currently slow down training with parallel environments is also when some steps take longer than others (since all of the environments step in parallel but wait for the others).  This type of slowdown should be improved in the v0.9 release which is coming soon.,hi bit hard know environment taking long without knowing environment set might consider time scale used within academy one thing currently slow training parallel also take longer since step parallel wait type slowdown release coming soon,issue,negative,negative,negative,negative,negative,negative
515236801,"Hi @the-zer07 -- as mentioned in the issue template we don't have the resources to help debug issues with custom environments.  It would be easier to help if you can show how to reproduce on an example environment, possibly with a minimal set of changes to show the issue you're having.",hi issue template help custom would easier help show reproduce example environment possibly minimal set show issue,issue,positive,negative,neutral,neutral,negative,negative
515223702,"I have duplicated this error using default examples.

By putting a crawler into a push-block environment, and giving the push-block and crawler brains ""recurrence"", the same error occurs",error default crawler environment giving crawler brain recurrence error,issue,negative,neutral,neutral,neutral,neutral,neutral
515218600,So it turns out that RECURRENCE is the problem. Something about any of these 5 brains being an RNN throws the crash-causing exception.,turn recurrence problem something brain exception,issue,negative,neutral,neutral,neutral,neutral,neutral
514955354,"Hi bro,
Please follow this first
https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation-Windows.md

Install all the requirements.
Then Clone the project to your location..

then follow this step 
**Converting TensorFlow and ONNX models to Barracuda format**
here
https://github.com/Unity-Technologies/ml-agents/blob/master/UnitySDK/Assets/ML-Agents/Plugins/Barracuda.Core/Barracuda.md

All the best




",hi please follow first install clone project location follow step converting barracuda format best,issue,positive,positive,positive,positive,positive,positive
514846171,"Actually the docs on `--debug` are wrong - it always outputs the csv, and setting the flag only changes the log level. I'll update that too.",actually wrong always setting flag log level update,issue,negative,negative,negative,negative,negative,negative
514812567,"Summary:
* Fixed broken links in https://github.com/Unity-Technologies/ml-agents/pull/2327
* Will update release instructions with full commandline
* Blocked on either https://github.com/pre-commit/pre-commit/issues/394 or https://github.com/tcort/markdown-link-check/issues/78 to implement this in pre-commit right now.",summary fixed broken link update release full blocked either implement right,issue,negative,positive,neutral,neutral,positive,positive
514788550,"To run full check locally:
`git ls-files | grep \.md$ | grep -v localized | xargs -n1 -t markdown-link-check -c markdown-link-check.config.json`
This currently only checks a subset of files - pre-commit splits them into batches, but `markdown-link-check` only runs on the first file in the batch (because it doesn't take multiple files as arguments).",run full check locally git currently subset first file batch take multiple,issue,negative,positive,positive,positive,positive,positive
514609850,Best practice in such case would be cut the network in several parts and only execute core part of it in Barracuda and move postprocessing nodes like `SecondStagePostprocessor/BatchMultiClassNonMaxSuppression` to either C# or Unity Compute shader.,best practice case would cut network several execute core part barracuda move like either unity compute shader,issue,positive,positive,positive,positive,positive,positive
514609308,"Currently there are no specific plans for custom ops, but there is plan to open more of the source code. So it might be possible to insert custom code in the future.",currently specific custom plan open source code might possible insert custom code future,issue,negative,neutral,neutral,neutral,neutral,neutral
514603043,"Also you could group some tags to the same category if it makes sense to react to them the same way, you would need to adjust the RayPerception yourself in that case",also could group category sense react way would need adjust case,issue,negative,neutral,neutral,neutral,neutral,neutral
514602194,"Generally, one hot encoding means that to encode N possible values, you don't use one number from 0 to N-1, but and encode it as N numbers with one of them being 1 and the rest 0. From what I understand, this is quite necessary for the neural network to be able to understand this kind of discrete information.
That's what the RayPerception does for each ray, plus a value that's set to 1 if no object is hit, plus another value for the distance, if an object is hit. If you will have multiple types of objects that it should react differently to, then you need it and all you can do is balance the number of rays and size of the network",generally one hot encode possible use one number encode one rest understand quite necessary neural network able understand kind discrete information ray plus value set object hit plus another value distance object hit multiple react differently need balance number size network,issue,positive,positive,positive,positive,positive,positive
514579531,"> Actually, if you look into the RayPerception, the resulting vector's length is rayAngles * (detectableObjects + 2) so you went from 1 input to 37. Having only one detectable object, or if it is not important which one you hit, you could reduce the size since you don't need the one-hot value(s) describing what object was hit. Last thing to note, angle 0 goes towards positive x (right)

Could you elaborate a bit more on this please. I'm not sure I know what you mean with the ""one-hot values"". My goal is to have multiple separate tags, walls, hazards (pits for the actor to fall in, and spikes to dodge by rolling), collectibles and enemies. So a rather healthy list of things to recognize, with different actions to take.  

So I would need to optimize the amount, to make the training more efficient. One thing of course it to have as few rays as possible. Thanks for the help.",actually look resulting vector length went input one detectable object important one hit could reduce size since need value object hit last thing note angle go towards positive right could elaborate bit please sure know mean goal multiple separate actor fall dodge rolling rather healthy list recognize different take would need optimize amount make training efficient one thing course possible thanks help,issue,positive,positive,positive,positive,positive,positive
514459456,"Originally, since replacing folders can solve the problem, that's fine, but when I use the .nn model trained by MAC on win, I encountered the same mistake. I hope the trained model can be used across platforms.",originally since solve problem fine use model trained mac win mistake hope trained model used across,issue,positive,positive,positive,positive,positive,positive
514392371,"We think the root cause for this was fixed in https://github.com/Unity-Technologies/ml-agents/pull/2276. Going to close this, but if you have more problems, please reopen.",think root cause fixed going close please reopen,issue,negative,positive,neutral,neutral,positive,positive
514373784,Closing this issue due to inactivity. Feel free to open another issue if you're still have issues.,issue due inactivity feel free open another issue still,issue,positive,positive,neutral,neutral,positive,positive
514351150,"I'll close this issue, as I got my question answered.
Thank you @ervteng !",close issue got question thank,issue,negative,neutral,neutral,neutral,neutral,neutral
514294130,"Thanks for your help, but I noob level doesn't enable me to find the way to do it —'

If somebody tried it on his side and would be ready to share it, would be incredible :D",thanks help level enable find way somebody tried side would ready share would incredible,issue,positive,positive,positive,positive,positive,positive
514257040,"@NonoLG 
That means any of ouput you want to see
like output from certain cnn later.
like
here you can pick one of output (i.e. after first cnnl layer)
output = tf.conv2d(input, ...)
**self.out = output** 
output = tf.conv2d(output, ...)
output = tf.conv2d(output, ...)

",want see like output certain later like pick one output first layer output input output output output output output,issue,positive,positive,positive,positive,positive,positive
514252864,"Thanks a lot for your answer.
Just, what do you mean by ""your_output"" please ?",thanks lot answer mean please,issue,positive,negative,neutral,neutral,negative,negative
514236317,"@NonoLG
It's absolutely possible. (I've been modifying python API twice for my projects)

So, this is way how I got my information from python API side.
But be careful. Its very complicated. I'd recommend you to backup your anaconda ml-agent module before making any modification.

Here, I'd assume you are training via PPO / without curiosity 

1. Go to model.py at  **./mlagents/trainers/model.py**
2. choose your prefered output and declare your output as class attribute (i.e. self.out_1 = your_output).
   Since you are using visual observation, you might want to go to class method definition for 
   **create_visual_observation_encoder**.
3. Go to policy.py at  **./mlagents/trainers/ppo/policy.py**
    There, around line 46, you will find inference_dict. Add your class attribute ( from 2 -i.e.  self.out_1) to 
    inference dict, as self.model.attribute_name ( i.e. self.model.out_1)
```
           self.inference_dict = {'action': self.model.output, 'log_probs': self.model.all_log_probs,
                               'value': self.model.value, 'entropy': self.model.entropy,
                               'learning_rate': self.model.learning_rate,
                               'my_out_1' : self.model.out_1,
                               'my_out_2' : self.model.out_2}
```
4. Go to policy.py at  **./mlagents/trainers/policy.py**
   find class method definition of **_excute_model()**
   You will see **run_out** dict variable.
   read what you want to read. (i.e. out_1_np = run_out['my_out_1'])
   it would be in shape of (Batch,Height,Width,Channel) if it is image. You can either plot using matplotlib
  or save  as image or any type you want. But I'd recommend you to declare some counter to plot / save image every few steps and then after few steps make it stop. Because, if it plots, unity will stop until you close  the plot. Also if you save image, steps are so fast. you will have to spend huge amount of disk storage if you don't stop saving at certain point. ",absolutely possible python twice way got information python side careful complicated recommend backup anaconda module making modification assume training via without curiosity go choose output declare output class attribute since visual observation might want go class method definition go around line find add class attribute inference go find class method definition see variable read want read would shape batch height width channel image either plot save image type want recommend declare counter plot save image every make stop unity stop close plot also save image fast spend huge amount disk storage stop saving certain point,issue,positive,positive,neutral,neutral,positive,positive
514230697,"I will stick with 12 areas for now, since each experiment takes around 1h to do and I spend the whole day yesterday to try out different set ups. I think the sudden drop for 12 is some kind of an anomaly. 

Bit I still dont understand why 15 areas would not rise faster. For the same amount of steps it gather more experience with 15 areas, so it should learn faster.",stick since experiment around spend whole day yesterday try different set think sudden drop kind anomaly bit still dont understand would rise faster amount gather experience learn faster,issue,negative,positive,positive,positive,positive,positive
514226963,"Okay, that makes the drop (at 12) even more strange.
Unfortunately, I do not have a good guess on why the difference is so significant between 6 and 9 runs.
How you tried running each number of parallel environments over multiple runs, to definitely rule out randomness? Perhaps just 9 and 6 as a start, to limit the number of curves on the graph.",drop even strange unfortunately good guess difference significant tried running number parallel multiple definitely rule randomness perhaps start limit number graph,issue,negative,positive,positive,positive,positive,positive
514224512,"I am also waiting out on the answer to the last question. Not sure what @mantasp  and unity think about it. I need to get a prototype working using an externally trained tensorflow graph soon. 
``
from __future__ import print_function, division
import scipy

import tensorflow as tf
import keras

from keras.layers import Input, Dense, Flatten
from keras.models import Model
from keras.optimizers import Adam
from keras.engine import InputSpec
from keras.layers import Conv2D, UpSampling2D

import datetime
import sys
import os

def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):
    graph = session.graph
    with graph.as_default():
        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))
        output_names = output_names or []
        output_names += [v.op.name for v in tf.global_variables()]
        input_graph_def = graph.as_graph_def()
        if clear_devices:
            for node in input_graph_def.node:
                node.device = ''
        frozen_graph = tf.graph_util.convert_variables_to_constants(
            session, input_graph_def, output_names, freeze_var_names)
        return frozen_graph

keras.backend.set_learning_phase(0)

_input = Input(shape=(32,32,1))

d1 = Conv2D(16, kernel_size=4, strides=2, padding='same')(_input)
d2 = Conv2D(16, kernel_size=4, strides=2, padding='same')(d1)
d3 = Conv2D(16, kernel_size=4, strides=2, padding='same')(d2)
d3 = UpSampling2D(size=(2,2))(d3)

flat = Flatten()(d3)
_output = Dense(32)(flat)

model = Model(_input, _output)
model.compile(loss=['mae'], loss_weights=[100], optimizer=Adam(0.0002, 0.5))

print(model.summary())

with keras.backend.get_session() as sess:
    graph = sess.graph
    with graph.as_default():
        graphdef_inf = tf.graph_util.remove_training_nodes(graph.as_graph_def())
        graphdef_frozen = tf.graph_util.convert_variables_to_constants(sess, graphdef_inf, [out.op.name for out in model.outputs])
        tf.train.write_graph(graphdef_frozen, './dummy_model', 'conv_graph.pb', as_text=False)
`


Graphs trained using that code get converted without any error. I do not understand why though.``",also waiting answer last question sure unity think need get prototype working externally trained graph soon import division import import import import input dense flatten import model import import import import import import o session graph list set node session return input flat flatten dense flat model model print sess graph sess trained code get converted without error understand,issue,positive,positive,neutral,neutral,positive,positive
514222531,"Hi @mantasp  @rejunity  I am also getting a similar error with the faster_rcnn_inception_resnet_v2_atrous_oid_v4_2018_12_12 data set from the tensorflow zoo training datasets. 

Here is the attached error log 


>     

IGNORED: Cast unknown layer
IGNORED: Shape unknown layer
IGNORED: Shape unknown layer
IGNORED: Enter unknown layer
WARNING: rank unknown for tensor SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/Switch:1 while processing node SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/Identity
Traceback (most recent call last):
  File ""tensorflow_to_barracuda.py"", line 26, in <module>
    tf2bc.convert(args.source_file, args.target_file, args.trim_unused_by_output, args)
  File ""/Users/apappu/Downloads/ml-agents-master/ml-agents/mlagents/trainers/tensorflow_to_barracuda.py"", line 1537, in convert
    i_model, args
  File ""/Users/apappu/Downloads/ml-agents-master/ml-agents/mlagents/trainers/tensorflow_to_barracuda.py"", line 1383, in process_model
    process_layer(node, o_context, args)
  File ""/Users/apappu/Downloads/ml-agents-master/ml-agents/mlagents/trainers/tensorflow_to_barracuda.py"", line 1198, in process_layer
    -1 not in input_ranks
AssertionError

> ",hi also getting similar error data set zoo training attached error log cast unknown layer shape unknown layer shape unknown layer enter unknown layer warning rank unknown tensor node recent call last file line module file line convert file line node file line,issue,negative,negative,negative,negative,negative,negative
514221469,"Yea, forgot to say the max steps is 3000, so its time limited and the agent has to score as much as possible. Also each time step the agent receives small negative reward (-1/3000)",yea forgot say time limited agent score much possible also time step agent small negative reward,issue,negative,negative,negative,negative,negative,negative
514220243,"Hmm.. Does anything result in the agent receiving a penalty? Furthermore, have you specified a number of maximum steps allowed for the agent to take?

If your environment contains a certain degree of complexity and/or randomness, and there are no/too high a threshold to prevent the agent from taking unfavorable paths, you can end up with some agents searching forever, while others continuously improves. Those agents that search forever, will not benefit from the parallel environments until reset, which is a lack of feedback, and can result in the overall training becoming noisy. The lack of feedback from some agents effectively reduces the quality of the shared experience. ",anything result agent penalty furthermore number maximum agent take environment certain degree complexity randomness high threshold prevent agent taking unfavorable end searching forever continuously search forever benefit parallel reset lack feedback result overall training becoming noisy lack feedback effectively quality experience,issue,negative,positive,positive,positive,positive,positive
514208527,"@KristianLN 3 and 6 areas perform very poorly, as expected. 12 and 9 still rise faster then 15. The environment is soccer w/o the defender - https://puu.sh/DWmZf/d11e8e0b33.mp4
Rewards for touching the ball (0.01) and scoring the net (1) + every time step if the ball rolls towards the net + if the agent moves towards the ball.

What do you mean by the feedback-loop-effect in the context of reinforcement learning?

![Photoshop_2019-07-23_15-19-13](https://user-images.githubusercontent.com/4758917/61715366-443e2d00-ad5d-11e9-8e25-46a3604cdd1b.png)

ps. I have no clue why the blue graph with 12 areas suddenly dropped to 0. I didnt change anything during the training. I know DQN suffers from catastrophic  forgetting, didnt think PPO does as well? 
",perform poorly still rise faster environment soccer defender touching ball scoring net every time step ball towards net agent towards ball mean context reinforcement learning clue blue graph suddenly didnt change anything training know catastrophic forgetting didnt think well,issue,negative,negative,neutral,neutral,negative,negative
514171546,"Hi guys. I am also trying to convert a posenet.pb to a .nn file using the tensorflow_to_barracuda.py. I get the following error


> Converting openpose.pb to Models/openpose.bytes
IGNORED: Shape unknown layer
IGNORED: Pack unknown layer
Traceback (most recent call last):
  File ""tensorflow_to_barracuda.py"", line 26, in <module>
    tf2bc.convert(args.source_file, args.target_file, args.trim_unused_by_output, args)
  File ""/Users/apappu/Downloads/ml-agents-master/ml-agents/mlagents/trainers/tensorflow_to_barracuda.py"", line 1537, in convert
    i_model, args
  File ""/Users/apappu/Downloads/ml-agents-master/ml-agents/mlagents/trainers/tensorflow_to_barracuda.py"", line 1366, in process_model
    nodes, var_tensors, const_tensors, o_context
  File ""/Users/apappu/Downloads/ml-agents-master/ml-agents/mlagents/trainers/tensorflow_to_barracuda.py"", line 472, in <lambda>
    inputs[0],

> 

> ",hi also trying convert file get following error converting shape unknown layer pack unknown layer recent call last file line module file line convert file line file line lambda,issue,negative,negative,neutral,neutral,negative,negative
514103703,"I am using the latest version of mlagents and installed from the directory. 
Got the same problem. I print the `self._n_agents.keys()`, which is empty.
@ervteng 

In another issue #1256, I find that the reason is that I open two environments at the same time. But I have set different `worker_id` for them.",latest version directory got problem print empty another issue find reason open two time set different,issue,negative,positive,neutral,neutral,positive,positive
514080256,"@ervteng  Thank you so much for confirming my suspicion! As I remember from the test I did, my computer did not max out with multiple areas within one application compared to running --num-env 1= 1.

@roboserg First of all, remember that the benefit of additional training areas are not linear in the number of areas, as of this [article](https://blogs.unity3d.com/2019/04/15/unity-ml-agents-toolkit-v0-8-faster-training-on-real-games/)
Furthermore, and I could be wrong, but I think what you see is the feedback-loop-effect, which is increasingly likely to affect your training, especially early on, as you increase the number of training areas. The reason one could suspect that to be the cause, is because I think it looks like, with that sparse information in the graph, that you have smoother training with 15 training areas. 
How does it look with less training environments? Let's say 3?",thank much confirming suspicion remember test computer multiple within one application running first remember benefit additional training linear number article furthermore could wrong think see increasingly likely affect training especially early increase number training reason one could suspect cause think like sparse information graph smoother training training look le training let say,issue,positive,positive,neutral,neutral,positive,positive
514071592,"Thanks for your response @ervteng. You can disable this check by disabling Gatekeeper with the following terminal command:

`sudo spctl --master-disable`

Everything works again once this is done.

However I would advice to update libgrpc_csharp_ext.x64.bundle before Catalina becomes public, as every Mac user will face this issue then.

I'd love to help but this is beyond my knowledge unfortunately - I don't even understand what this bundle is....",thanks response disable check gatekeeper following terminal command everything work done however would advice update catalina becomes public every mac user face issue love help beyond knowledge unfortunately even understand bundle,issue,positive,positive,neutral,neutral,positive,positive
514040226,"Re-installed the entire process and got a step further, but now I get an error running the command:

""Traceback (most recent call last):
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\MyName\.conda\envs\ml-agents\Scripts\mlagents-learn-script.py"", line 11, in <module>
    load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')()
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\site-packages\pkg_resources\__init__.py"", line 489, in load_entry_point
    return get_distribution(dist).load_entry_point(group, name)
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\site-packages\pkg_resources\__init__.py"", line 2843, in load_entry_point
    return ep.load()
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\site-packages\pkg_resources\__init__.py"", line 2434, in load
    return self.resolve()
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\site-packages\pkg_resources\__init__.py"", line 2440, in resolve
    module = __import__(self.module_name, fromlist=['__name__'], level=0)
  File ""c:\users\MyName\downloads\ml-agents-master\ml-agents-master\ml-agents\mlagents\trainers\__init__.py"", line 5, in <module>
    from .models import *
  File ""c:\users\MyName\downloads\ml-agents-master\ml-agents-master\ml-agents\mlagents\trainers\models.py"", line 4, in <module>
    import tensorflow as tf
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *  # pylint: disable=redefined-builtin
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\MyName\.conda\envs\ml-agents\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.""",entire process got step get error running command recent call last file line return file line return name level package level file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line load dynamic link library routine handling exception another exception recent call last file line module import file line module file line return file line return name level package level module handling exception another exception recent call last file line module file line return group name file line return file line load return file line resolve module file line module import file line module import file line module import file line module import file line module raise recent call last file line return file line return name level package level file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line load dynamic link library routine handling exception another exception recent call last file line module import file line module file line return file line return name level package level module load native see common include entire stack trace error message help,issue,positive,negative,neutral,neutral,negative,negative
514020761,It would be interesting to have Barracuda extensible this way.,would interesting barracuda extensible way,issue,negative,positive,positive,positive,positive,positive
514014858,"@sebjf @ervteng I also encountered the same problem. When I copied the project created on Win to mac, there was no problem in training, and this error occurred infering. Is it caused by different computers or different systems? When I replaced the ml-agents and Gizmo with the same folders downloaded from GitHub, everything was OK.",also problem copied project win mac problem training error different different everything,issue,negative,positive,positive,positive,positive,positive
513976966,"Thanks for the info. For guys as me who do not have that much knowledge of TensorFlow and Tensorboard, do you any example or some code I could use please ?

There many code files, I do not find where modify things.",thanks much knowledge example code could use please many code find modify,issue,positive,positive,positive,positive,positive,positive
513976490,"I am trying a solution Hunter suggested, if it works by EOD then great else I will remove crawler from this branch itself
",trying solution work great else remove crawler branch,issue,positive,positive,positive,positive,positive,positive
513974263,"Thus: 

By analyzing the flow of the CollectObservations () function inside the ML-Agents, I also realized that I can distribute rewards within it, relative to the previous step. Because the rewards are only sent to Brain at the end of the SendInfoToBrain () function.

See my project that used the ML-Agents 0.5v : https://github.com/MeLL-UFF/pip/tree/master/Assets/ML-Agents

I edit some files. See the Agent.sc file for example.",thus flow function inside also distribute within relative previous step sent brain end function see project used edit see file example,issue,negative,negative,neutral,neutral,negative,negative
513940887,"For the first point:
yes, but I'm little confused on an aspect. My future goal is to train a multi-agent environment, with two team fighting together. So in my mind, when a bullet hit a target (or another agent), the reward goes to the agent that shooted the bullet. It's right to think on the reward as reward of a single agent? Or it has to be a global reward?

For second point:
So, steps between shoot and hit can change, depends on the size of the map. In my first little training i think the bullet hit the target in 20-30 steps. Interesting, I missed the gamma parameter, I think adjusting this can be a good try. I'm trying the ""slow"" bullet choise to test the other agent's ability to avoid incoming bullets. ( It's an open possibility, not sure it could be practical )

Thanks for your interest.",first point yes little confused aspect future goal train environment two team fighting together mind bullet hit target another agent reward go agent bullet right think reward reward single agent global reward second point shoot hit change size map first little training think bullet hit target interesting gamma parameter think good try trying slow bullet test agent ability avoid incoming open possibility sure could practical thanks interest,issue,positive,positive,neutral,neutral,positive,positive
513929601,@dongruoping can you make sure to address the comments here from @chriselion in a following PR?,make sure address following,issue,negative,positive,positive,positive,positive,positive
513926342,"Actually, if you look into the RayPerception, the resulting vector's length is rayAngles * (detectableObjects + 2) so you went from 1 input to 37. Having only one detectable object, or if it is not important which one you hit, you could reduce the size since you don't need the one-hot value(s) describing what object was hit. Last thing to note, angle 0 goes towards positive x (right)",actually look resulting vector length went input one detectable object important one hit could reduce size since need value object hit last thing note angle go towards positive right,issue,positive,positive,positive,positive,positive,positive
513908830,"Dear @ervteng ,
thank you for your answer. The reason is mainly practical but I can easily switch between the two if I am in training mode or inference mode. Not a bit issue. During training, I am also running a process from Tensorflow which is not related to ml-agents, that I don't need it during inference but it would be useful to debug the prediction of that TF model on the console. I guess if I want something fast I will keep only barracuda. Thank you .. all clear",dear thank answer reason mainly practical easily switch two training mode inference mode bit issue training also running process related need inference would useful prediction model console guess want something fast keep barracuda thank clear,issue,positive,positive,positive,positive,positive,positive
513905291,"This looks pretty good. One consideration is that each trainer type (e.g. ppo, sac, online_bc, etc.) has different parameters and different defaults. So maybe we need a different TrainerParameters object for each type rather than one for all. ",pretty good one consideration trainer type sac different different maybe need different object type rather one,issue,positive,positive,positive,positive,positive,positive
513902781,"Custom ops will work during training and inference using Python. Currently they're unsupported in the Unity Inference Engine (Barracuda), though. ",custom work training inference python currently unsupported unity inference engine barracuda though,issue,negative,neutral,neutral,neutral,neutral,neutral
513901936,"Yes, absolutely. You can definitely add a CNN output layer to the `inference_dict` in `policy.py` and save it out to numpy or use opencv to view it live. Tensorflow also offers a general solution to visualizing Tensors using Tensorboard: https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/debugger/README.md",yes absolutely definitely add output layer save use view live also general solution,issue,positive,positive,neutral,neutral,positive,positive
513900519,"Hi @spaggi, we can't comment on Catalina since it's unreleased. But you may have to disable some of the security settings in System Settings -> Security and Privacy, at least until libgrpc is updated for Catalina. 

Leaving this issue open for our reference as Catalina becomes available. ",hi ca comment catalina since unreleased may disable security system security privacy least catalina leaving issue open reference catalina becomes available,issue,positive,positive,neutral,neutral,positive,positive
513899779,"Yes, it's expected - you have an observation space that's 12 (edit: 37) times larger than before. And a neural network that's larger as well. ",yes observation space edit time neural network well,issue,positive,neutral,neutral,neutral,neutral,neutral
513899048,"Hi @thekiwininja99, did you follow the install instructions? You may have to activate your virtual environment before the command works. 

https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md",hi follow install may activate virtual environment command work,issue,negative,neutral,neutral,neutral,neutral,neutral
513898377,"I'm not sure I understand your question for 1. You'd call AddReward() when it hits the target, no?

For 2, it is possible for RL algorithms to learn long-term credit assignment, though it is harder to learn than immediate rewards. How many steps are between firing and receiving the reward? You may have to increase your `gamma` parameter; Stacked vectors should help as well. ",sure understand question call target possible learn credit assignment though harder learn immediate many firing reward may increase gamma parameter help well,issue,positive,positive,positive,positive,positive,positive
513897864,"@ervteng I am still not sure how to determine the perfect amount of training areas within one unity scene. My i7 6700K with 4 cores / 8 HT is not maxing out even at 15 areas, but I don't see it training faster then with 9 areas. It seems something different is the bottleneck.

**1.** 
Should I just test a different amount of training areas and see which one converges faster? 
For example in the picture below orange line are 15 areas vs a grey line with 9 areas. How come orange is lagging even-though it has 60% more training areas? 

![chrome_2019-07-22_20-19-50](https://user-images.githubusercontent.com/4758917/61655191-561cc300-acbe-11e9-8cdf-4460d495c565.png)

**2.** Will switching from 4 cores / 8HT to 12 cores / 24 HT (AMD Ryzen) speed up the training? In other words If I would use several instances of the unity scene will all them run in parallel on the 12 cores?
In other words will I be able to run three times more training areas in parallel with 12 cores instead of with 4 cores as of right now? ",still sure determine perfect amount training within one unity scene even see training faster something different bottleneck test different amount training see one faster example picture orange line grey line come orange lagging training switching speed training would use several unity scene run parallel able run three time training parallel instead right,issue,positive,positive,positive,positive,positive,positive
513896910,"CNNs do encode positional information. However, in applications such as image classification, it is _undesirable_ to have this information, and we force the network to be translation invariant through data augmentation and/or pooling. We don't do that in ML-Agents, so the CNN will know the position of the pixels. ",encode positional information however image classification information force network translation invariant data augmentation know position,issue,negative,neutral,neutral,neutral,neutral,neutral
513895835,"Hi @Guidosalimbeni, we don't support running both Tensorflow Sharp and Barracuda at the same time. Is there a reason you'd want to use Tensorflow over Barracuda for inference? In most cases Barracuda is faster and more efficient. 

If you want to use Tensorflow, you will need to rename your `.pb` file to `.bytes`, and import it into Unity while the ENABLE_TENSORFLOW define is enabled in your Build Settings.",hi support running sharp barracuda time reason want use barracuda inference barracuda faster efficient want use need rename file import unity define build,issue,positive,negative,negative,negative,negative,negative
513895001,"@KristianLN there isn't one right answer for this. In many games, it's challenging (or impossible) to create multiple training areas, hence the `--num-envs` feature. But generally if you're on a CPU-limited single machine, creating multiple training areas will be more efficient. Furthermore, adding more envs or training areas when your computer is running at full throttle already will just slow all of them down, and will have little benefit. ",one right answer many impossible create multiple training hence feature generally single machine multiple training efficient furthermore training computer running full throttle already slow little benefit,issue,positive,negative,neutral,neutral,negative,negative
513888983,"worker.Execute(input_tensor) + worker.Peek() is the right choice for most applications, 
ExecuteAndWaitForCompletion is mostly used for profiling GPU implementation.",right choice mostly used implementation,issue,negative,positive,positive,positive,positive,positive
513888487,"Hey @sankalp04, did you remove the length of limbs for Crawler in this PR, or are you going to open a new one?",hey remove length crawler going open new one,issue,negative,positive,neutral,neutral,positive,positive
513887183,"I tested all of the usual suspects (Visual, Discrete Actions (+ Branched), Continuous Actions, LSTM) and they all seem to work fine. ",tested usual visual discrete branched continuous seem work fine,issue,negative,positive,neutral,neutral,positive,positive
513881814,"@DeusExMachina1993 it's part of full Barracuda release, which can be found here: https://github.com/Unity-Technologies/barracuda-release",part full barracuda release found,issue,negative,positive,positive,positive,positive,positive
513879809,"BarracudaWorkerFactory.Type.CSharpFast <- fast CPU path
BarracudaWorkerFactory.Type.CSharp <- slow reference CPU implementation, useful for debugging
BarracudaWorkerFactory.Type.ComputeFast <- fast GPU path
BarracudaWorkerFactory.Type.Compute <- slow reference GPU implementation, useful for debugging

if your model uses visual inputs, then ComputeFast is good choice, otherwise for small ML Agents models CSharpFast should be best choice.

P.S. starting with ML Agents v0.8.2, it comes with Barracuda 0.2.x, which changed definitions of the worker types.",fast path slow reference implementation useful fast path slow reference implementation useful model visual good choice otherwise small best choice starting come barracuda worker,issue,positive,positive,positive,positive,positive,positive
513875570,"Thank you. Though unfortunately it didn't work for me. The 3d ball example works OK. I am training on a different computer, I wonder if that could have something to do with it. I'll try the others and debug some more.",thank though unfortunately work ball example work training different computer wonder could something try,issue,negative,negative,negative,negative,negative,negative
513872538,"yea, just manually deleted the ML-Agents folder and drag and dropped again. If you are having same problems be sure to check if examples from ML-Agents work. ",yea manually folder drag sure check work,issue,negative,positive,positive,positive,positive,positive
513871755,"Hi @roboserg, could you say what you mean by re-imported the SDK? Delete from Unity assets & re-import, or rebuild the Aanconda env.? I am having the exact same problem!",hi could say mean delete unity asset rebuild exact problem,issue,negative,negative,neutral,neutral,negative,negative
513862626,@mantasp That merge conflict was due to fixing line endings in that file (https://github.com/Unity-Technologies/ml-agents/pull/2208). IIRC it touched the whole file.,merge conflict due fixing line file touched whole file,issue,negative,positive,neutral,neutral,positive,positive
513832580,"Hi,

Do you have the full updated graph please ?

Cheers",hi full graph please,issue,negative,positive,positive,positive,positive,positive
513770868,I am wondering which reinforcement algorithm uses rewards as observations at all. I can't think of one.,wondering reinforcement algorithm ca think one,issue,negative,neutral,neutral,neutral,neutral,neutral
513767253,"Hi guys,

At the moment we are on a break from this as our client paused the project.
We thank you very much for your support and we'll let it know when we can
pick up the work.

Sincerely / Met vriendelijke groeten,
Aaron van der Brugge


On Mon, Jul 22, 2019 at 1:41 PM rash1994 <notifications@github.com> wrote:

> Did you get the conversion of .pb to .nn ?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/1991?email_source=notifications&email_token=AKZ2ZW6B3ICAADOHN3RFQ2TQAWMHTA5CNFSM4HI7NPSKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2PU3JI#issuecomment-513756581>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AKZ2ZWYEXOFH5643CUJQQNTQAWMHTANCNFSM4HI7NPSA>
> .
>
",hi moment break client project thank much support let know pick work sincerely met van mon rash wrote get conversion reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
513719050,"For me, the question is a bit unclear. What do you want to achieve by this?",question bit unclear want achieve,issue,negative,neutral,neutral,neutral,neutral,neutral
513716332,"To specify, it's this error:

https://www.fireebok.com/resource/how-to-fix-that-app-can-not-open-on-macos-catalina.html

Seems that the program libgrpc_csharp_ext.x64.bundle needs to be notarized

https://developer.apple.com/documentation/security/notarizing_your_app_before_distribution",specify error program need,issue,negative,neutral,neutral,neutral,neutral,neutral
513667727,"So currently in the latest MLAgents repo 0.8.2 installs the 0.4.2 version of GYM Unity. Does it not support that version. I am following this link to train gridworld problem.
https://github.com/Unity-Technologies/ml-agents/tree/master/gym-unity
@ervteng ",currently latest version gym unity support version following link train problem,issue,negative,positive,positive,positive,positive,positive
513633352,I had the same issue and it was fixed after running pip install pypiwin32,issue fixed running pip install,issue,negative,positive,neutral,neutral,positive,positive
513557231,"My missing parts are as below:
1) using `learning brain` to replace the `player brain` for the agent
2) choose the learning brain in my test.py",missing learning brain replace player brain agent choose learning brain,issue,negative,negative,negative,negative,negative,negative
513486127,"I see.. I think you are absolutely right.
However, it is interesting if there is a difference between using --num-envs and multiple training areas within one application, or if the ability to run on multiple environments is speed up the process even more.",see think absolutely right however interesting difference multiple training within one application ability run multiple speed process even,issue,positive,positive,positive,positive,positive,positive
513474553,"Solved.  
The problem is that there is a new curriculum file called `._[your file]` generated automatically, and then python API will read this file and raised the other error:  
` raise JSONDecodeError(""Expecting value"", s, err.value) from None`  
Just delete the `._[your file]` then it will work.

Plus: The system generates a file called `.DS_Store` automatically, it is still possible to result in the above problem, just delete it. 

But I still don't know why this file is generated. And why sometimes raised the UnicodeDecodeError, but I have tried using `open(location, encoding = ""ISO-8859-1"")` then this error disappeared. Then delete the specific encoding it will work. BTW,  even I specify the encoding = ""utf-8"", it still raised the unicodeDecodeError... I have no idea about these encoding and decoding stuff...",problem new curriculum file file automatically python read file raised error raise value none delete file work plus system file automatically still possible result problem delete still know file sometimes raised tried open location iso error delete specific work even specify still raised idea stuff,issue,negative,positive,neutral,neutral,positive,positive
513473531,"Did you manage to solve this?
I have a similar challenge but I am considering to use On Demand Decision. did you already try this route?",manage solve similar challenge considering use demand decision already try route,issue,negative,neutral,neutral,neutral,neutral,neutral
513471295,"From what I understand you are correct that each instance of the application in which your NN is training adds some overheads. If you had N computors then running N instances of the app would make sense to me.

If like me you are training on your personal machine then running a single app with as many environments as you can fit would seem the most efficient.

However I can imagine there are some environmental designs that are not well suited to running concurrently within the same application, for instance if you were teaching your NN to do stuff with your app's local file system.
",understand correct instance application training running would make sense like training personal machine running single many fit would seem efficient however imagine environmental well running concurrently within application instance teaching stuff local file system,issue,positive,positive,positive,positive,positive,positive
513467908,"Ok, it works now. I just re-imported  SDK into the project. It's weird, since I did not make any changes to the imported SDK in the project. The only thing I did after I started having problems was to pip uninstall mlagets and pip install them again. But that procedure should not affect the imported unity ml agents SDK. It's also weird that if you assume there were some problems with the imported SDK, why would both soccer scenes work and both pong scenes not work. Magic. ",work project weird since make project thing pip pip install procedure affect unity also weird assume would soccer work pong work magic,issue,negative,negative,negative,negative,negative,negative
513466082,"If you mean `ENABLE_TENSORFLOW ` in `Scripting Define Symbols` then no, its empty -https://puu.sh/DV9x5/2082863439.png

I would understand if I had errors while training, then I would have something wrong with my project. But training works, inference - doesn't. I trained countless agents with countless different small projects and never had issues with inference.

ps. starting a new project would probably work w/o errors since your example projects works. But I'd like to figure out what the problem of my current project is. It's weird. Using mlagents 0.8.2 both from pip install and SDK from github. Unity 2019.1.0f2. It's probably not a bug but I honestly don't know what else to try to fix it.
pss. the current project I am having problems with has soccer-1 and soccer-2 scenes that work (the brain inference works). But pong-1 and pong-2 seems to be messed up - https://puu.sh/DV9DZ/3d84077086.png",mean define empty would understand training would something wrong project training work inference trained countless countless different small never inference starting new project would probably work since example work like figure problem current project weird pip install unity probably bug honestly know else try fix current project work brain inference work,issue,negative,negative,neutral,neutral,negative,negative
513451769,"@Nanocentury Thank you for commenting!
Do you mind elaborating, and preferably related to the above mentioned use case?",thank mind preferably related use case,issue,negative,neutral,neutral,neutral,neutral,neutral
513451539,"I have the same problem, the value of vectorAction doesn't change is always 0.
I have this problem with the ml-agents version 0.8.2, 0.8.1, 0.8.0, 0.7.0, but using the version 0.5.0 it works, I'd rather use the recent version with gpu support",problem value change always problem version version work rather use recent version support,issue,negative,neutral,neutral,neutral,neutral,neutral
513450137,I can see multiple apps being useful in a distrubuted computing environment,see multiple useful environment,issue,negative,positive,positive,positive,positive,positive
513436908,"> Hi @Usmaniatech, could you please give me some examples of how to train it with MADDPG? Can you work with version 0.8?
> 
> Hi @YuhangSong, is that possible to use my own environment in your platform with other baseline algorithms?

Yes, the baseline part: https://github.com/YuhangSong/Arena-Baselines, does not restrict that the game should be built from https://github.com/YuhangSong/Arena-BuildingToolkit, you can plug your own game in.

The only restriction is that all agents should share the same action space, in order to support things like self-play.",hi could please give train work version hi possible use environment platform yes part restrict game built plug game restriction share action space order support like,issue,positive,negative,negative,negative,negative,negative
513424508,"The only thing suspect is that error message under your Brain, which says ""There is no model for this brain"". This seems to point to the fact that something isn't right with the Barracuda plugin. 

In your project's build settings, is ENABLE_TENSORFLOW on by any chance?",thing suspect error message brain model brain point fact something right barracuda project build chance,issue,negative,positive,positive,positive,positive,positive
513374807,"> Noticed this doesn't actually hook the different encoder types up to any config. Is that intended to be a separate PR / planned at all?
> 
> BTW, also please make sure to make a descriptive commit message and PR description (they can be the same) for this.

Added config option for encoder type in trainer config",actually hook different intended separate also please make sure make descriptive commit message description added option type trainer,issue,positive,positive,positive,positive,positive,positive
513369058,"Thanks, @ervteng 
But I have tried your WallJump example with your curriculum json file, same problem. But I am sure I can run it at my local laptop through unity editor.",thanks tried example curriculum file problem sure run local unity editor,issue,negative,positive,positive,positive,positive,positive
513360171,"Did check for None, but same issue arises. Would setting a default value and altering type annotations be the better way out?",check none issue would setting default value type better way,issue,positive,positive,positive,positive,positive,positive
513338183,"@Usmaniatech Thanks so much for your kindly help.
Currently, my input is the visual input, as for the example codes, they are for the vector input. So the shape or type for the inputs is different. (This might be one of the questions)
Another is my agent number is more than two, when the actions passed to the network, the baselines algorithms only accept and send One action (but unity-gym requires a list of actions). 
Those are the questions I found for version 0.8.

However, I can not work in version 0.4 too. This might be visual input and the number of the agent. Any suggestions are welcome, and I will keep solving this.",thanks much kindly help currently input visual input example vector input shape type different might one another agent number two network accept send one action list found version however work version might visual input number agent welcome keep,issue,positive,positive,positive,positive,positive,positive
513326495,Closing this issue due to inactivity. Feel free to re-open if you'd like to ask additional questions or still have trouble. ,issue due inactivity feel free like ask additional still trouble,issue,negative,positive,neutral,neutral,positive,positive
513326118,"This issue was fixed with 0.8.2. Closing this issue for now, feel free to open it again if it's still an issue. ",issue fixed issue feel free open still issue,issue,positive,positive,positive,positive,positive,positive
513319685,"Noticed this doesn't actually hook the different encoder types up to any config.  Is that intended to be a separate PR / planned at all?

BTW, also please make sure to make a descriptive commit message and PR description (they can be the same) for this.",actually hook different intended separate also please make sure make descriptive commit message description,issue,positive,positive,positive,positive,positive,positive
513318598,"@ervteng 

It seems the problem is not linked to stacked vectors, but yes I used the same brain. Right now I use `stacked vectors = 1`, build and train the model. I dont change anything in the editor, load the model and have the same error. 

ps. just created a new brain, trained for 10 seconds, load back - error:
https://puu.sh/DUPoD/ea28265665.mp4",problem linked yes used brain right use build train model dont change anything editor load model error new brain trained load back error,issue,negative,positive,positive,positive,positive,positive
513316034,"By copying the *.nn file, you are using Barracuda, it is the plugin that does inference on your saved model. 

Are you using stacked vectors in training as well? The *.nn won't work unless the brain settings are exactly the same as during training. ",file barracuda inference saved model training well wo work unless brain exactly training,issue,positive,positive,positive,positive,positive,positive
513314561,"Hi @Usmaniatech, to the best of my knowledge, the OpenAI gym API for multi-agent isn't fixed, and is implemented differently for different trainers (and changes with gym versions). You might have to adapt our interface to work with Baselines DQN, or adapt DQN to work with our interface. ",hi best knowledge gym fixed differently different gym might adapt interface work adapt work interface,issue,positive,positive,positive,positive,positive,positive
513313689,This seems like a text editor issue - make sure your text editor is saving your JSON file in UTF-8 if that's what the server is using. ,like text editor issue make sure text editor saving file server,issue,positive,positive,positive,positive,positive,positive
513250557,"I want to train the baseline model DQN on tennis example. Multi agent example using same brain. 
My code:

> import gym
> 
> from baselines import deepq
> from baselines import logger
> 
> from gym_unity.envs.unity_env import UnityEnv
> 
> def main():
>     env = UnityEnv(""./envs/Tennis/Skyforge 8.2"",0, use_visual= False, multiagent=True)
>     logger.configure('./logs') # Çhange to log in a different directory
>     act = deepq.learn(
>         env,
>         ""cnn"", # conv_only is also a good choice for GridWorld
>         lr=2.5e-4,
>         total_timesteps=1000,
>         buffer_size=50000,
>         exploration_fraction=0.05,
>         exploration_final_eps=0.1,
>         print_freq=20,
>         train_freq=5,
>         learning_starts=20000,
>         target_network_update_freq=50,
>         gamma=0.99,
>         prioritized_replay=False,
>         checkpoint_freq=1000,
>         checkpoint_path='./logs', # Change to save model in a different directory
>         dueling=True
>     )
>     print(""Saving model to unity_model.pkl"")
>     act.save(""unity_model.pkl"")
> 
> if __name__ == '__main__':
>     main()

But it's not working. Getting the error for no of actions i guess.
![error](https://user-images.githubusercontent.com/14370981/61542617-5cf7cb80-aa5b-11e9-8147-03fc50fd3b8c.PNG)
@CicieChen ",want train model tennis example agent example brain code import gym import import logger import main false log different directory act also good choice change save model different directory print saving model main working getting error guess error,issue,negative,positive,positive,positive,positive,positive
513243580,"In order to train multiple agents having the same brain, Is there any restriction related discrete and continuous observations. Do gym supports both type of observations.",order train multiple brain restriction related discrete continuous gym type,issue,negative,neutral,neutral,neutral,neutral,neutral
513228363,I think you can do it. Check OnDemandDecisions and then call RequestDecision at each slot.,think check call slot,issue,negative,neutral,neutral,neutral,neutral,neutral
513124776,"I got the final issue solved as well! I was trying to export data to a folder there didn't existed, from the builds point of view, which prevented the Done() being reached. 

Thank you for your time @ervteng - I'll close this issue now.",got final issue well trying export data folder point view done thank time close issue,issue,positive,neutral,neutral,neutral,neutral,neutral
513107187,"Okay, I just tried setting timeScale to 1 and 50 respectively, without luck.
I'll try to build one of the suggested environments now, and if that one is good, I'll go over differences.
I'll update within a couple of hours.

Update:

I found the error, in terms of the agent getting stuck, which appeared to be because I modified the velocity of the Rigidbody directly, instead of using AddForce (Note to future me).
However, solving that resulted in another problem appearing, namely that collisions doesn't appear to be detected.
There is no resetting of the agent when colliding with the wall/goal. 

I'll look into that next.",tried setting respectively without luck try build one one good go update within couple update found error agent getting stuck velocity directly instead note future however another problem namely appear agent look next,issue,negative,positive,positive,positive,positive,positive
513091819,I think the type error is due to `self.lesson_duration` which you don't check for None,think type error due check none,issue,negative,negative,negative,negative,negative,negative
513083485,"The mistake that many people do, is they install unityagents along with mlagents. They are the same, with different names. If you are working on 0.4, only install unityagents and If you are working on version > 0.4, only install mlagents according to version requirements. ",mistake many people install along different working install working version install according version,issue,negative,positive,positive,positive,positive,positive
513066260,"CircleCI failing from stylistic check of modulo unsupported between int and Optional[int], not sure how to resolve it as code does check for its existence prior to conducting operation.",failing stylistic check modulo unsupported optional sure resolve code check existence prior operation,issue,negative,positive,positive,positive,positive,positive
512998167,"This looks pretty standard, it should be similar to our cube environments (e.g. Hallway, PushBlock, Pyramids) - I'd try to build one of those and see if you have the issue. The only thing I can think of is if the timescale is affecting the physics, but AFAIK the rotate doesn't use physics. ",pretty standard similar cube hallway try build one see issue thing think affecting physic rotate use physic,issue,negative,positive,neutral,neutral,positive,positive
512993255,"Using continous actions, clamped between -1 and 1, which makes up a rotation and then moving the agent forward with an constant speed.
The AgentAction function looks the following:

     `public override void AgentAction(float[] vectorAction,string textAction)
     {
      Vector3 rotateDir = Vector3.zero;
      if(brain.brainParameters.vectorActionSpaceType == SpaceType.continuous)
      {
        rotateDir = transform.up * Mathf.Clamp(vectorAction[0],-1f,1f);
      }
      // Rotate
      transform.Rotate(rotateDir,Time.deltaTime * 150f);
      // Move
      rbd.velocity = transform.forward * speed;

      // Time penalty
      AddReward(-0.0005f);
    }`",rotation moving agent forward constant speed function following public override void float string vector rotate move speed time penalty,issue,negative,neutral,neutral,neutral,neutral,neutral
512982452,"Hmm, it's possible there's some naming conflict - i.e. somewhere else in the project there's something else named Agent or Academy. 

Also, in your build settings do you have ENABLE_TENSORFLOW? If so, you'd need to remove that for 0.8.2. ",possible naming conflict somewhere else project something else agent academy also build need remove,issue,negative,neutral,neutral,neutral,neutral,neutral
512981590,"Those settings should work - it doesn't seem like it's an RL problem since the player brain also exhibits the same behavior, and it doesn't seem to be a settings problem since you can build 3DBall OK. How are you moving the agent?",work seem like problem since player brain also behavior seem problem since build moving agent,issue,negative,neutral,neutral,neutral,neutral,neutral
512975639,"Yes, it's broken because of this: https://github.com/Unity-Technologies/ml-agents/blob/c085438d36323da9239ceee1f1534528c051b3ab/ml-agents/mlagents/trainers/components/bc/model.py#L68-L84 Need to get @ervteng 's advice on what to do with it",yes broken need get advice,issue,negative,negative,negative,negative,negative,negative
512936321,"The PR looks good to me. The only thing is the ""Some unused variables in barracuda code"" you mentioned. Maybe it would be better to ask Barracuda team or maybe remove that change for the moment? @chriselion ",good thing unused barracuda code maybe would better ask barracuda team maybe remove change moment,issue,positive,positive,positive,positive,positive,positive
512933236,"@ervteng There appears to be no problems when I build the 3DBall example.

I got the window to become bigger by changing the width/height values in the training configuration but the agent still gets stuck to the wall immediately after training starts.

Update:
I have also tried without curriculum learning, but without luck.",build example got window become bigger training configuration agent still stuck wall immediately training update also tried without curriculum learning without luck,issue,negative,neutral,neutral,neutral,neutral,neutral
512928696,"> I can confirm that this does fix VisualBanana. However, it breaks the regular Banana scene - all of the bananas will fall through the ground.
> 
> I think if you remove the `transform.position.y` from that sum it will work for both scenes.

That's right. I just fixed it and committed. Is that OK?",confirm fix however regular banana scene fall ground think remove sum work right fixed,issue,negative,positive,positive,positive,positive,positive
512923398,"Hi @ervteng,

Yes, most certainly!

Training Configuration:
Width: 84
Height: 84
Quality Level: 1
TImeScale: 20
TFR: -1

Inference Configuration:
Width: 1280
Height: 720
Quality Level: 5
TImeScale: 1
TFR: 60

I haven't done any inference yet, by used a trained model, so I haven't played with those settings.

**Player settings:**

**Resolution and Presentation:** 
Fullscreen mode: Fullscreen Window
Default is Native Resolution: Checked
Max Retina Support: Checked
Run in background: Checked

Capture single screen: Unchecked
Display Resolution Dialog: Disabled
Use Player Log: Checked
Resizable Window: Unchecked
Visible in Background: Checked
Allow Fullscreen Switch: Checked
Force Single instance: Unchecked
Supported Aspect Ratios: All Checked

Do you need information of the other categories within Player settings?

I haven't tried to build an example environment, I'll try that shortly.",hi yes certainly training configuration width height quality level inference configuration width height quality level done inference yet used trained model player resolution presentation mode window default native resolution checked retina support checked run background checked capture single screen unchecked display resolution disabled use player log checked window unchecked visible background checked allow switch checked force single instance unchecked aspect checked need information within player tried build example environment try shortly,issue,positive,negative,neutral,neutral,negative,negative
512915714,"It does not give any warnings or errors and I have copied the scripts from a tutorial, I have also run the scripts on my brother's computer where he has unity 2018.2.21f1 and it works on that.",give copied tutorial also run brother computer unity work,issue,negative,neutral,neutral,neutral,neutral,neutral
512915510,"I can confirm that this does fix VisualBanana. However, it breaks the regular Banana scene - all of the bananas will fall through the ground. 

I think if you remove the `transform.position.y` from that sum it will work for both scenes. ",confirm fix however regular banana scene fall ground think remove sum work,issue,negative,neutral,neutral,neutral,neutral,neutral
512911659,"HI @KristianLN, can you post your Training and Inference Configuration (in Academy) and your Player Settings (Under Build Settings -> Player Settings, shows up in the inspector).

The resolution can be fixed in the Player Settings. The movement issues shouldn't happen - are you seeing similar things when building an example environment?",hi post training inference configuration academy player build player inspector resolution fixed player movement happen seeing similar building example environment,issue,negative,positive,neutral,neutral,positive,positive
512910639,Thanks @ervteng  for the answer. Will definitely try it.,thanks answer definitely try,issue,positive,positive,neutral,neutral,positive,positive
512909889,"if the fields aren't showing up, it sounds like you may have a compiler error - did you check the Unity console to see if your scripts are compiling properly?",showing like may compiler error check unity console see properly,issue,negative,neutral,neutral,neutral,neutral,neutral
512888668,"I also ran into this issue on Windows 10 - brain was set to external - only restarting Jupyter Lab solved the issue. 

The same Jupyter Lab session (although different Python kernel) was used to connect ML Agents to a different Unity environment (which did work) (maybe it caused the bug?).",also ran issue brain set external lab issue lab session although different python kernel used connect different unity environment work maybe bug,issue,negative,neutral,neutral,neutral,neutral,neutral
512860018,"thanks @xiaomaogy 

In my case I have two agents that play pong against each other (like the tennis environment). In that case what is the advantage of setting the max steps in each individual agents and not in the academy?",thanks case two play pong like tennis environment case advantage setting individual academy,issue,positive,positive,neutral,neutral,positive,positive
512780035,I was using the wrong config file. Use trainer_config.yaml and it will work,wrong file use work,issue,negative,negative,negative,negative,negative,negative
512743524,"@mattinjersey 

I think experience is everything you received from unity after action, like dictionary.
like, exp = {observation : some_vector, visual_observation : some_images, reward : some_scalar}
something like this
and seems like it will be changed depending on the setup.",think experience everything received unity action like dictionary like observation reward something like like depending setup,issue,positive,positive,neutral,neutral,positive,positive
512742429,"@awjuliani thanks, its getting more clear!

This is what I understood at last. can you clarify whether its wrong or not?

let's say, time_horizon = 4, buffer_size= 4, batch_size=2, n_epoch=2

1. time_horizon amount of experience is gathered, and discounted advantage is calculated - trajectory
   (i.e. time_horizon_buffer = [exp_1, exp_2, exp_3, exp_4] -> adv_1)

2. first experience and advantage goes into buffer.
   (i.e. buffer : [(exp_1,adv_1)]

3. discard first experience in time_horizon buffer
   (i.e. time_horizon_buffer = [exp_2, exp_3, exp_4]

------------------- repeat until buffer is filled ------------------------
i.e. 

1. new experience received, next discounted advantage is calculated - new trajectory
   (i.e. time_horizon_buffer = [exp_2, exp_3, exp_4, exp_5] -> adv_2)

2. first experience and advantage goes into buffer.
   (i.e. buffer : [(exp_1,adv_1),(exp_2,adv_2)]

3. discard_first experience in the time_horizon_buffer
   (i.e time_horizon_buffer = [exp_3, exp_4, exp_5]

......
......
...... repeated 2 more times

--------------------------  buffer is filled   ---------------------------
 (i.e. buffer = : [(exp_1,adv_1),(exp_2,adv_2),(exp_3,adv_3),(exp_4,adv_4)])

calculate gradient with batch_size number of samples in buffer (i.e. sample size = 2)
repeat until it goes through all samples in buffer (i.e. 4/2 = 2 times)
repeat this n_epoch times (i.e. 2 times)

empty everything and start over again
(time_horizon_buffer = [], buffer = [])


Is this correct? 
Thanks in advance!


",thanks getting clear understood last clarify whether wrong let say amount experience advantage calculated trajectory first experience advantage go buffer buffer discard first experience buffer repeat buffer filled new experience received next advantage calculated new trajectory first experience advantage go buffer buffer experience repeated time buffer filled buffer calculate gradient number buffer sample size repeat go buffer time repeat time time empty everything start buffer correct thanks advance,issue,positive,positive,positive,positive,positive,positive
512677942,"I can't share my code, It's for a client. But if you face any issues, I''ll be happy to help. 
for refrence: https://github.com/katnoria/unityml-tennis",ca share code client face happy help,issue,positive,positive,positive,positive,positive,positive
512667967,"Hi @Usmaniatech, thanks for your valuable comments. I have tested in version 0.6 and 0.8, it just can not work. I will test version 0.4 today. However, they did not offer the support of gym wrapper this version. Would you mind sharing your code or project? ",hi thanks valuable tested version work test version today however offer support gym wrapper version would mind code project,issue,positive,positive,positive,positive,positive,positive
512620336,"This should be possible. But it will take some Unity knowledge - you'll have to make two Agents, one for training, and one for the guidance. In the guidance agent you can implement the AgentAction method to just show the action it would take, rather than actually taking the action, while adding the observations as usual. ",possible take unity knowledge make two one training one guidance guidance agent implement method show action would take rather actually taking action usual,issue,negative,negative,neutral,neutral,negative,negative
512596756,"Hi Ervin,

Thanks for replying.
By real time guidance I meant that If I have trained a brain to perform a
particular task and after it is trained properly, if I am performing that
same task, how can the trained brain can guide me to take the right steps
to successfully complete the task?

Thanks,
Rajat

On Wed, Jul 17, 2019, 3:39 PM Ervin T. <notifications@github.com> wrote:

> Hi @rajatpaliwal <https://github.com/rajatpaliwal>, what do you mean by
> real time guidance? You'll have to frame your problem as a Markov decision
> process (i.e., at each timestep, the agent sees a state, takes an action,
> and receives a reward) to use ML-Agents.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/Unity-Technologies/ml-agents/issues/2278?email_source=notifications&email_token=AHHWP3YNQLYS3QIYRJY7GWLP76NTBA5CNFSM4IETLT62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2GZFMQ#issuecomment-512594610>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AHHWP32DZIJ7MP5ADWY6UDLP76NTBANCNFSM4IETLT6Q>
> .
>
",hi thanks real time guidance meant trained brain perform particular task trained properly task trained brain guide take right successfully complete task thanks wed wrote hi mean real time guidance frame problem decision process agent state action reward use reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
512594610,"Hi @rajatpaliwal, what do you mean by real time guidance? You'll have to frame your problem as a Markov decision process (i.e., at each timestep, the agent sees a state, takes an action, and receives a reward) to use ML-Agents. ",hi mean real time guidance frame problem decision process agent state action reward use,issue,negative,negative,neutral,neutral,negative,negative
512587387,"Hey @sankalp04, can you merge the latest `develop` to clear the circleci errors? It shouldn't change any of your code. Once that's clear go ahead and squash+merge",hey merge latest develop clear change code clear go ahead,issue,positive,positive,positive,positive,positive,positive
512548002,"> > Just tried this, still uses the CPU. Should I also uninstall regular TensorFlow?
> 
> I made a mistake in `pip -e .` it must be `pip install -e .` Then i tried, both versions installed and there was no problem. Make sure you saved everything after changing setup.py

I just ended up assuming you meant `pip install -e .`, and yes I did save the file. I just checked the setup.py and saw it was still changed to tensorflow-gpu.",tried still also regular made mistake pip must pip install tried problem make sure saved everything ended assuming meant pip install yes save file checked saw still,issue,positive,positive,positive,positive,positive,positive
512475463,"Hi @EfveZombie, we found the root cause of this issue and fixed it on the latest `develop` branch. Let us know if it fixes your issue. Thanks!",hi found root cause issue fixed latest develop branch let u know issue thanks,issue,negative,positive,positive,positive,positive,positive
512219705,I'd make a build of the environment and then use the gym wrapper to gain most flexibility in designing your own algorithm and agent structure.,make build environment use gym wrapper gain flexibility designing algorithm agent structure,issue,positive,neutral,neutral,neutral,neutral,neutral
512211666,Unfortunately I am not able to work with 0.8 version of MLAgents to train using MADDPG. I am using 0.4 version to work with MADDPG. It don't work with versions > 0.4,unfortunately able work version train version work work,issue,negative,positive,positive,positive,positive,positive
512156749,"Hi @Usmaniatech, could you please give me some examples of how to train it with MADDPG? Can you work with version 0.8?

Hi @YuhangSong, is that possible to use my own environment in your platform with other baseline algorithms? ",hi could please give train work version hi possible use environment platform,issue,negative,neutral,neutral,neutral,neutral,neutral
512153191,"Hi @awjuliani, I have tested my environment with PPO, it can work. But I can not use baselines algorithm to train it. ",hi tested environment work use algorithm train,issue,negative,neutral,neutral,neutral,neutral,neutral
512151283,"> hi @LuoYouRen - thanks for the issue. Would you be open to proposing a fix on develop and submitting a PR? Thanks!

I have already submitted (PR #2277).",hi thanks issue would open fix develop thanks already,issue,positive,positive,positive,positive,positive,positive
512096061,"correct, you can start here in the trainers - https://github.com/Unity-Technologies/ml-agents/blob/develop/ml-agents/mlagents/trainers

If you make any PRs, please use the develop branch. ",correct start make please use develop branch,issue,negative,neutral,neutral,neutral,neutral,neutral
512066196,"Hi @ervteng,
Thanks for your reply.
Yeah, in fact the agent has learned to listen less times, because I set the `max_step=1000`, and even longer episodes have only about 15 listen actions. But it is still too long for my target, I want to let it learn to make decision within 5 steps. So, any clue to tune it? I will try to add the negative reward for each listen action.
  
And today I have also trained a model with same settings with previous successful one, but used the LSTM and stacked vectors(set to 5). But the result is not good, the model is not stable, the result can be shown below:
<img width=""696"" alt=""屏幕快照 2019-07-17 上午2 13 45"" src=""https://user-images.githubusercontent.com/33338567/61340315-ae287500-a839-11e9-9893-b094f80ff794.png"">
It looks very unstable, like there are two lines but it is just very acutely fluctuant. It means that the first 1000 steps the mean reward is 0.7, but the next 1000 steps the mean reward will reduce to -0.4 or lower. I have no idea that why only adding the LSTM and stacked vectors techniques, will lead to this problem. But the model looks have learned something certainly. 
  
I really use the +/-1 scale reward, a small problem is that for the +/-1 reward, is the  mean reward possible to more than 1? 
  
I am working on the POMDP problem that I need to use previous 3 or 5 observations to make a decision. That is to say, the next decision is influenced by the previous 3 observations, is the stacked vector enough for this case? ",hi thanks reply yeah fact agent learned listen le time set even longer listen still long target want let learn make decision within clue tune try add negative reward listen action today also trained model previous successful one used set result good model stable result shown unstable like two acutely fluctuant first mean reward next mean reward reduce lower idea lead problem model learned something certainly really use scale reward small problem reward mean reward possible working problem need use previous make decision say next decision previous vector enough case,issue,positive,positive,neutral,neutral,positive,positive
512059654,"Hi @gzrjzcx, shouldn't the agent be incentivized to listen less times, since it gets a negative reward each time? If it is listening a lot, it might have learned that it is far less risky to listen a lot than to risk running into the tiger. You can play with the size of the negative reward to see if there's a difference.

I would also cap out the reward to +/-1 to make the learning more stable. You can just divide all of the rewards by 100 to do this. You are correct that setting max_step to 5 will force the episode to be at most 5 steps long, but I'd set it longer and let the agent learn to shorten the episode itself.

stacked vectors equal to the max_step might be problematic. You probably also don't need both stacked vectors and LSTM, one or the other should be OK. 

Current_step/max_step gives the agent a notion of time. I don't think you'd need this in your problem; in fact, I believe Hallway is solvable without it. ",hi agent listen le time since negative reward time listening lot might learned far le risky listen lot risk running tiger play size negative reward see difference would also cap reward make learning stable divide correct setting force episode long set longer let agent learn shorten episode equal might problematic probably also need one agent notion time think need problem fact believe hallway solvable without,issue,negative,negative,negative,negative,negative,negative
512026549,"nice, thanks! Is the source for the mlagents-learn.exe in this repo? That's the executable that does the training and saving the models, right? (sorry for naive questions)",nice thanks source executable training saving right sorry naive,issue,positive,positive,neutral,neutral,positive,positive
512019374,"hi @roboserg - I think it makes sense to be able to create a frozen graph pb file from the meta, index, and data files that are created with each save frequency.  And then create a .nn file to be used with the inference engine.  we have it in our backlog.  would you be interested to work on this and create a PR?  If so, I can ensure we provide support to review it.  Otherwise, might take some time for us to work on this given some other priorities.  ",hi think sense able create frozen graph file meta index data save frequency create file used inference engine backlog would interested work create ensure provide support review otherwise might take time u work given,issue,positive,positive,positive,positive,positive,positive
512000023,"could you define the word ""experience"". Does an experience include all the vector observations from a single timestep.  So 1 experience might consist of 40 vector observations for 1 game, but it might consist of 100 vector observations for another game.

Also could you define the word ""trajectory"".",could define word experience experience include vector single experience might consist vector game might consist vector another game also could define word trajectory,issue,negative,negative,negative,negative,negative,negative
511911343,"@Junggy 

The buffer consists of single experiences, and the buffer size corresponds to number of experiences. When a trajectory is added, it is added as single experiences, not as a whole unit. That being said, when dealing with LSTMs the experiences from trajectories are kept in temporal order, so they can be re-used during training.",buffer single buffer size number trajectory added added single whole unit said dealing kept temporal order training,issue,negative,positive,neutral,neutral,positive,positive
511902860,"Thanks @LeSphax for your quick answer!

- For the `SetTextObs` part, you were totally right. I had a logic issue where both agents got the state `win`.
- No rush for the memory issue :) 
- I couldn't reproduce the issue with Tensorboard. Now the graphs load just fine, but I've got another small issue using the `--load` flag; the time starts over at zero, causing overlapping graphs. I'm not 100% sure this is happening because of this PR, but I didn't have this issue using the latest ml-agents release.
![image](https://user-images.githubusercontent.com/6735195/61314020-ef9a2f80-a7fb-11e9-99b2-a98095684461.png) *Edit*: Actually the issue happened again, I can't find a way reproduce the issue. On this screenshot, the training has been running for about 6 hours, and there's no scalars.
![image](https://user-images.githubusercontent.com/6735195/61386675-fd5fbb80-a8b4-11e9-8519-7f4b85e6020c.png)
- I though using the Elo ranking for team was not possible if you were using different brains between the teammates. Is that true? Some people suggested a ranking system like TrueSkill (not open source) but I'm not sure that's possible. In the meantime, a friend suggested using only a single brain to control multiple players of the same team, so I'll try something like this and see how it plays out.

Edit: I also forgot to mention that I'm using Anaconda on Windows 10 to manage my env, and that I'm running tensorflow-cpu. I'm using your fork with the latest master updates (using a rebase)
Edit 2: Beside in-game stats, what does switching agents does?
Edit 3: I figured out why my Tensorboard showed the wrong data. It's because it was actually the ghost's data! Reversing the brains in the Academy object (first the ghost brain, then the learning brain) fixed the issue for me, so this seems like a bug. This also fixed the ""graph continuity"" issue.",thanks quick answer part totally right logic issue got state win rush memory issue could reproduce issue load fine got another small issue load flag time zero causing sure happening issue latest release image edit actually issue ca find way reproduce issue training running image though team possible different brain true people system like open source sure possible friend single brain control multiple team try something like see edit also forgot mention anaconda manage running fork latest master rebase edit beside switching edit figured wrong data actually ghost data reversing brain academy object first ghost brain learning brain fixed issue like bug also fixed graph continuity issue,issue,positive,positive,positive,positive,positive,positive
511898151,"yep, it just points them to all localized kr blog site.  the local team may end up translating later.",yep site local team may end later,issue,negative,neutral,neutral,neutral,neutral,neutral
511887623,"Yeah, I forgot to close this when 0.8.2 was released - that contains the fix.",yeah forgot close fix,issue,negative,neutral,neutral,neutral,neutral,neutral
511882285,"Hi @AcelisWeaven,

Unfortunately I am on holidays without my computer and it has been a while since I looked at this but let's see if I can help anyway :)

- For the elo rating going down my guess would be that match results are not attributed correctly. What matters is this line for the elo computation `SetTextObs(opponent.Id + ""|"" + matchState);`

If that doesn't help the way I debugged that kind of problem is to have only two agents training, don't switch the brain between them and log all the match results to check that they make sense.

- For the memory leak, I don't remember having problems when running overnight with 16GB but I also never looked at it so if it happens on the Tennis environment there is surely a problem.

- I think curiosity should work since the checkpoints should include the curiosity model but I didn't try it.

- I don't think I had problems with Tensorboard.

- For teams I think the ghost could work the same way but we would need to change the elo rating calculation and send a list of opponents and teammates with the result of the match.

I will try to get hold of a computer this weekend and see if I can investigate that memory leak.
",hi unfortunately without computer since let see help anyway rating going guess would match correctly line computation help way kind problem two training switch brain log match check make sense memory leak remember running overnight also never tennis environment surely problem think curiosity work since include curiosity model try think think ghost could work way would need change rating calculation send list result match try get hold computer weekend see investigate memory leak,issue,negative,positive,positive,positive,positive,positive
511864394,"We are closing this due to inactivity, but if you need additional assistance, feel free to reopen the issue.
",due inactivity need additional assistance feel free reopen issue,issue,positive,positive,positive,positive,positive,positive
511745763,"Hi @LeSphax,

Thanks for your contribution, I've tried to use it on my project but I'm having some issues and some questions.

- My ELO rating keeps going down on my game. This doesn't happen on your modified Tennis environnement however. This may be due to the fact that my game is a bit more complex. Do you have an idea of what can cause this? Here's a sample output from my training:
```
INFO:mlagents.trainers: league3-0: PlayerLearning: Step: 354000. Time Elapsed: 27112.426 s Mean Reward: 2.748. Std of Reward: 0.579. Elo Rating: 357.2 Training.
INFO:mlagents.trainers: league3-0: PlayerLearningGhost: Step: 354000. Time Elapsed: 27112.503 s Mean Reward: -2.748. Std of Reward: 0.579. Elo Rating: 1200.0 Not Training.
```
- Do you have a solution to limit RAM usage? Even on the demo env, ml-agents allocates about 2Mo/second. After an seven hours of training, it crashes because of this (got 16Go of RAM). Here's my env configuration, for reference:
```
PlayerLearning:
    batch_size: 1024
    buffer_size: 102400
    max_steps: 10000000
    hidden_units: 64
    num_layers: 3
    summary_freq: 1000
    use_elo_rating: true

PlayerLearningGhost:
    trainer: ghost
    ghost_master_brain: PlayerLearning
    ghost_num_policies: 3
    ghost_recent_ckpts_threshold: 10
    ghost_prob_sample_only_recent: 0.5
```
- Is Curiosity supported?
- Do you had any issue with Tensorboard? With Tennis I can see the graphs fine, but with my env nothing even show up. Maybe that's an issue on my side.
- Also... any plan for team training? :)

Here's a screenshot of my learning scene, red player have to score in the blue zone and vice-versa.
![image](https://user-images.githubusercontent.com/6735195/61285822-e7250300-a7c1-11e9-87f7-aa9701c7de0f.png)
Actions are discrete (move in 8 directions, jump)

Thanks,",hi thanks contribution tried use project rating going game happen tennis however may due fact game bit complex idea cause sample output training step time mean reward reward rating training step time mean reward reward rating training solution limit ram usage even seven training got go ram configuration reference true trainer ghost curiosity issue tennis see fine nothing even show maybe issue side also plan team training learning scene red player score blue zone image discrete move jump thanks,issue,positive,negative,neutral,neutral,negative,negative
511740485,"I've got it to work, is seems like there was a problem with my graph.
(But the logs '!(...)' still appear)",got work like problem graph still appear,issue,negative,neutral,neutral,neutral,neutral,neutral
511732254,"@awjuliani Thanks for the answer.

Pretty much all makes sense.
But just one last thing. This is what I was always confused about.

you said 
""Time horizon is **how many steps of experience** to collect in a single trajectory** before calculating the discounted returns and advantages for that trajectory, **and then adding it to the buffer**.""

So one unit of buffer is not single experience, but time-horizon amount of experiences (means single trajectory)? (i.e. buffer : buffer_size * single_trajectory (means time_horizon * experiences), not buffer_size * single_experience)

Something like, 
Buffer is filled with buffer_size amount of trajectories, trajectories is filled with time_horizon amount of experiences. right?",thanks answer pretty much sense one last thing always confused said time horizon many experience collect single trajectory calculating trajectory buffer one unit buffer single experience amount single trajectory buffer something like buffer filled amount filled amount right,issue,positive,positive,positive,positive,positive,positive
511708979,"Hi,
not really ... I am working on that. What I am trying to do is to use onDemandDecision and overwrite the SendInfoToBrain method in the Agent. The idea is that when the meta-brain delegates the action to a sub-brain some flag is turned into false, so that the fixed update does not triggers a new onDemandDecision. 
I am not yet totally sure that overwriting SendInfoToBrain is necessary; the idea is that I want to guarantee that I am sending the obs taken at the moment in which I delegated, with the right action, and the reward collected at the end of the 'delegated' action. 

The problem I am encountering is exactly guaranteeing that the info sent to the brain is what I want, as described in this other issue
https://github.com/Unity-Technologies/ml-agents/issues/2254

Let me know if you find any good/better solution

 ",hi really working trying use overwrite method agent idea action flag turned false fixed update new yet totally sure necessary idea want guarantee sending taken moment right action reward collected end action problem exactly sent brain want issue let know find solution,issue,positive,positive,positive,positive,positive,positive
511708715,"> Just tried this, still uses the CPU. Should I also uninstall regular TensorFlow?

I made a mistake in `pip -e .` it must be `pip install -e .` Then i tried, both versions installed and there was no problem. Make sure you saved everything after changing setup.py",tried still also regular made mistake pip must pip install tried problem make sure saved everything,issue,negative,positive,positive,positive,positive,positive
511707117,"I have shared my code in my first post, as an attachment. 

Anyhow to summarize..in the scene there is a red and a blue target, alternatively active (only one at the time is active).
The agent has two actions R and B. The idea is that it should select the action corresponding to the active target. The agent resets to done.
When it selects the 'R' (or B) action, it triggers the following method
The agent resets to done and the onCollision method is:
```
private void EvaluateAction(float action){
        if(action == 1){    
            if(blueTarget.activeSelf){
                SetReward(1f);
            }
            else{
                SetReward(-1f);        
            }
        }
        if(action == 2){    
            if(redTarget.activeSelf){
                SetReward(1f);
            }
            else{
                SetReward(-1f);
            }
        }
        Done();
    }
```
And the Done() method triggers the reset of the environment that switches which target is active

```
public override void AgentReset()
	{	
        Debug.Log(GetStepCount() + "" - AgentReset"");
		myEnvironment.ResetArea();
	}

```
```
public override void ResetArea()
	{
        if(counter%2 == 0){
            blueTarget.SetActive(false);
            redTarget.SetActive(true);
        }
        else{
            redTarget.SetActive(false);
            blueTarget.SetActive(true);
        }
        counter+=1;
	}
```

The result is:
- red target active
- I press 'R'
- The collectObservation is called from the SendInfoToTheBrain method of the agent, probably after that the environment has already been reset, and therefore it contains a wrong information ('blue target active', 'action R', positive reward).


On top of this problem the select action function is called several times, but as I was mentioning, currently my first priority is to guarantee that the info sent to the brain is correct. 

Am I doing anything wrong? 



 ",code first post attachment anyhow summarize scene red blue target alternatively active one time active agent two idea select action corresponding active target agent done action following method agent done method private void float action action else action else done done method reset environment target active public override void public override void counter false true else false true result red target active press method agent probably environment already reset therefore wrong information target active positive reward top problem select action function several time currently first priority guarantee sent brain correct anything wrong,issue,positive,negative,neutral,neutral,negative,negative
511670858,"Sorry for the late reply, @awjuliani @Usmaniatech @YuhangSong thanks so much for you guys' suggestion. Currently, I am in travelling. I will update the results here when I come back. <3",sorry late reply thanks much suggestion currently update come back,issue,negative,negative,neutral,neutral,negative,negative
511656286,Did you find a good way to do this? I'm currently trying to do the exact same thing - have a meta-brain control two sub-brains.,find good way currently trying exact thing control two,issue,negative,positive,positive,positive,positive,positive
511633179,"> @ervteng Follow up from Friday about creating a simple ""game"" to test training without mocking anything. Feedback welcome on how to simplify/improve this.
> 
> I think from here, we can add other tests to hit the other components like visual observations, memory, and curiosity. It should hopefully be easy enough to extend to GAIL too.

Yeah, this should be able to be extended to GAIL/Curiosity fairly easily. We can just record a .demo file, or perhaps more easily generate a bunch of steps from an ""optimal"" policy (just stepping in the right direction) and creating a buffer from those. Then setting the GAILRewardSignal's internal buffer to this new buffer, and running training. ",follow simple game test training without anything feedback welcome think add hit like visual memory curiosity hopefully easy enough extend yeah able extended fairly easily record file perhaps easily generate bunch optimal policy stepping right direction buffer setting internal buffer new buffer running training,issue,positive,positive,positive,positive,positive,positive
511620249,"Then the issue occurred, I wasn't using --slow.

Turning --slow on is what solved my issue. (Just to be sure; you have to both write --slow and --train)",issue slow turning slow issue sure write slow train,issue,negative,negative,negative,negative,negative,negative
511618281,Hi @Erethan I am having the same problem. Are you using `--slow` when training instead of `--train`?,hi problem slow training instead train,issue,negative,negative,negative,negative,negative,negative
511614394,"HI @ervteng,
According to the Hallway example, I have trained a model for Tiger problem successfully. And it works as expected. 
But now there still are two problems:  
1. Now, each episode is too long. It means that the agent needs to listen so many times then it can make a decision. I want  to limit the number of listen action in 5 times. Therefore,  I try to set the agent `max_step` to 5, is it correct? Because I think that is the `max_step` is set to 5, then each episode is no more than 5 steps. 
2. How to set LSTM and Stacked vectors parameters.
I have checked the Hallway example trainer config, the LSTM parameter is using the default set, i.e. `sequence_length = 64` and `memory_size = 256`. Then as for the 'stacked vectors', I set it to 5(because the `max_step` is 5, I want to make a decision within 5 times). However, the problem now is the mean reward is not increasing. (Or increasing very slow...). The first success model only need 1m steps then the mean reward started to increase fast. However, for this model even 3m steps the mean reward still slightly decreased. 
  
I know the `max_step=5` is not correct now and makes it is so difficult to learn something. But how could I limit the number of listen actions?  
By the way, the Hallway example also add the `current_step / max_step` as observation, I am confused why need this? And what is the impact of this? In addition, the `stacked vectors = 3` for Hallway example, is that means that the once the  three observation(t),  observation(t-1), observation(t-2) contains the detected target color(e.g. orange target), then the agent know go to orange gate to get reward?
  
Cheers ",hi according hallway example trained model tiger problem successfully work still two episode long agent need listen many time make decision want limit number listen action time therefore try set agent correct think set episode set checked hallway example trainer parameter default set set want make decision within time however problem mean reward increasing increasing slow first success model need mean reward increase fast however model even mean reward still slightly know correct difficult learn something could limit number listen way hallway example also add observation confused need impact addition hallway example three observation observation observation target color orange target agent know go orange gate get reward,issue,positive,negative,neutral,neutral,negative,negative
511595900,All the new file changes happened in fact after i called 'git rebase develop'. Not sure how to revert back to the commit without these changes.,new file fact rebase develop sure revert back commit without,issue,positive,positive,positive,positive,positive,positive
511584952,"@ervteng Follow up from Friday about creating a simple ""game"" to test training without mocking anything. Feedback welcome on how to simplify/improve this.

I think from here, we can add other tests to hit the other components like visual observations, memory, and curiosity. It should hopefully be easy enough to extend to GAIL too.",follow simple game test training without anything feedback welcome think add hit like visual memory curiosity hopefully easy enough extend,issue,positive,positive,positive,positive,positive,positive
511515307,"Hi all. Let me try to clarify:

Time horizon is how many steps of experience to collect in a single trajectory before calculating the discounted returns and advantages for that trajectory, and then adding it to the buffer.

The buffer size is how big this buffer of trajectories can get before we use it for training. Once the buffer reaches this size, we then go through it (num_epoch) number of times, taking a random (batch_size) size batch at a time. After the epochs of training, we then clear the buffer and start filling it again from scratch.",hi let try clarify time horizon many experience collect single trajectory calculating trajectory buffer buffer size big buffer get use training buffer size go number time taking random size batch time training clear buffer start filling scratch,issue,negative,positive,neutral,neutral,positive,positive
511501197,"Hi @JobSmith, please upgrade to v0.8.2. Also the PlayerBrain vs LearningBrain (or ExternalBrain in earlier version) should not influence the way AgentReset gets called. ",hi jobsmith please upgrade also version influence way,issue,negative,neutral,neutral,neutral,neutral,neutral
511494941,I think you need to merge in the latest develop - a bunch of files are changed that shouldn't be,think need merge latest develop bunch,issue,negative,positive,positive,positive,positive,positive
511494838,"Sorry I don't understand your question, maybe it would be helpful to post your code, so that other people in the community could jump in and help. ",sorry understand question maybe would helpful post code people community could jump help,issue,positive,negative,negative,negative,negative,negative
511493380,"Hi @roboserg, I would recommend setting max step in each of the agent, and set Academy's max step to be multiple times of that max step in each of the agent. See the 3dball's setup for example. 

",hi would recommend setting step agent set academy step multiple time step agent see setup example,issue,negative,neutral,neutral,neutral,neutral,neutral
511457327,hi @rafvasq - unfortunately this seems to be a bit outside of the scope of just ml-agents and our support.  hopefully others in the community can comment on a approach.,hi unfortunately bit outside scope support hopefully community comment approach,issue,negative,negative,negative,negative,negative,negative
511371013,"Dear @PanMig ,
yes, sure you are right! Just because in my project I aim to use both (a custom offline CNN for the reward and the PPO for the reinforcement learning) and I forgot about the title of the issue. Sorry. All good!!! ",dear yes sure right project aim use custom reward reinforcement learning forgot title issue sorry good,issue,positive,positive,positive,positive,positive,positive
511369253,"@Guidosalimbeni  you DON'T need to insert ML-agents to unity, just use drag and drop the TensorflowSharp plug-in and you will be able to load you own pre-trained models. But beware this is only for loading and using pre-trained tensorflow models and nothing more (based on the title of the issue) .",need insert unity use drag drop able load beware loading nothing based title issue,issue,negative,positive,positive,positive,positive,positive
511366625,"Dear @PanMig ,
thank you for the link!!
Do you mean that you install the tensorflowsharp plugin first and then you insert the UnitySDK from ML-agents next? 
I probably need to check an older version of the SDK since the latest is using Barracuda... am I right?",dear thank link mean install first insert next probably need check older version since latest barracuda right,issue,positive,positive,positive,positive,positive,positive
511360056,"You can find the plug-in here. [https://github.com/salepaun/ML-agents/blob/master/docs/Using-TensorFlow-Sharp-in-Unity.md]

I personally use tensorflow 1.7.1 and the tensorflow-plugin without loading the ML-agents repository on the assets.",find personally use without loading repository asset,issue,negative,neutral,neutral,neutral,neutral,neutral
511296771,"Hi, thanks for your answer, but for what I can see it is not a problem of custom training setups. As I was mentioning in my first post I have exactly the same issue if I turn off OnDemandDecision.
The most urgent problem for me is the 2nd point: for what I can see in the Academy.EnvironmentStep() method, first the agent is reset if Done, and only later the info is sent to the brain. In my case this implies that the info sent to the brain reflect already the new environment setting.

Am I understanding it correctly? Is there a way to avoid this behaviour?

Thanks again

",hi thanks answer see problem custom training first post exactly issue turn urgent problem point see method first agent reset done later sent brain case sent brain reflect already new environment setting understanding correctly way avoid behaviour thanks,issue,negative,positive,positive,positive,positive,positive
511192906,"@xiaomaogy thanks! That clears it up.

c) If I have several agents would you still recommend setting max steps in each one of them? I mean if there is no difference (Academy doesn't reset the environment and is an empty class) I would rather set the max steps in the academy.
",thanks several would still recommend setting one mean difference academy reset environment empty class would rather set academy,issue,positive,negative,neutral,neutral,negative,negative
511129234,"> Hey @EfveZombie, thanks for adding this! Does using `dtype=np.float32` also fix your issue? IMO that would be a bit cleaner than `float`

I think that would work too. 

In the old version, some part in buffer.py would return a ndarray with dtype=Object. The format was like `array([0.0, array([1.]), array([2.]), 3.0], dtype=object)`. So I tried to set the dtype manually and it work fine so far, but I still dont know what caused this problem.",hey thanks also fix issue would bit cleaner float think would work old version part would return format like array array array tried set manually work fine far still dont know problem,issue,positive,positive,positive,positive,positive,positive
511112076,"Hi, can you point to the download link? It seems broken... I might be wrong,
A side question. Can I have tensorflowsharp 4.0 (with TF 1.7.1) and the latest version of ML-agents live together in the same UNity project? I understand that UnitySDK of ML-agents comes with a folder TensorFlow and I am not sure if adding the latest UNITY-sdk of ML-agents will cause issue if I have already the tensorflowsharp 4.0 package installed.
thank you for your help",hi point link broken might wrong side question latest version live together unity project understand come folder sure latest cause issue already package thank help,issue,negative,positive,positive,positive,positive,positive
511086980,"hello, @unityjeffrey . My max steps is set big enough so it is not reached when the situation occurs I mentioned about. And the **Reset On Done** is checked. But what do you mean ""call agent reset to assign a new brain at runtime"" ? 

My test situation is here:

![image](https://user-images.githubusercontent.com/15700681/61169079-b3ac6380-a58a-11e9-8371-f5ec78fe7d9a.png)

When I start the game, the car will reach the yellow ball and I set the isFinished to 1.

When the car across over the ball, it will crush into the wall. 

![image](https://user-images.githubusercontent.com/15700681/61169117-a348b880-a58b-11e9-9529-8b52d24a54db.png)

( When the car reach the first ball, the target will change to the next one. )

My agent academy just inherits Academy, it as follows:

```
public class CarAcademy : Academy
{
}
```

Now the agent script is updated:

```
public class CarAgent : Agent
{
    private bool isFinished = false;
    public override void InitializeAgent()
    {
    }

    public override void AgentReset()
    {
        Debug.Log(""AgentReset   isFinished = "" + isFinished);
    }

    public override void AgentAction(float[] vectorAction, string textAction)
    {
        if(car reach the ball)
        {
            isFinished = true;
            Debug.Log(AgentAction   isFinished = "" + isFinished);
            Done();
        }
    }

    private void OnCollisionEnter(Collision collision)
    {
	Debug.Log(""collision happened."");
	Done();
    }
}

```
And I add some debug log at Agent.cs:

```
void _AgentReset()
{
	Debug.Log(""_AgentReset is called."");
	ResetData();
	stepCount = 0;
	AgentReset();
}

void ResetIfDone()
{
	// If an agent is done, then it will also 
	// request for a decision and an action
	if (IsDone())
	{
		if (agentParameters.resetOnDone)
		{
			if (agentParameters.onDemandDecision)
			{
				if (!hasAlreadyReset)
				{
					// If event based, the agent can reset as soon
					// as it is done
					Debug.Log(""!hasAlreadyReset,   _AgentReset is called."");
					_AgentReset();
					hasAlreadyReset = true;
				}
			}
			else if (requestDecision)
			{
				// If not event based, the agent must wait to request a
				// decsion before reseting to keep multiple agents in sync.
				Debug.Log(""requestDecision,   _AgentReset is called."");
				_AgentReset();
			}
		}
		else
		{
			terminate = true;
			RequestDecision();
		}
	}
}
```



And then the debug log is as follows:

![image](https://user-images.githubusercontent.com/15700681/61166851-839f9900-a567-11e9-98b7-c0d1ede30460.png)

As you can see, after the isFinished is set to 1 and Done() was called in the AgentAction(), the _AgentReset() was called twice continuously, which called the AgentReset() each time. And after the collision happened, the _AgentReset() was called twice again! This troubles me a lot and don't acts as I expect to ( I thought AgentReset() will be called only once after Done() executed ).

Otherwise, my mlagents version is 0.5.0.",hello set big enough situation reset done checked mean call agent reset assign new brain test situation image start game car reach yellow ball set car across ball crush wall image car reach first ball target change next one agent academy academy public class academy agent script public class agent private bool false public override void public override void public override void float string car reach ball true done private void collision collision collision done add log void void agent done also request decision action event based agent reset soon done true else event based agent must wait request keep multiple sync else terminate true log image see set done twice continuously time collision twice lot expect thought done executed otherwise version,issue,negative,positive,neutral,neutral,positive,positive
511083635,thanks I also find this all confusing so it would be good to clarify.,thanks also find would good clarify,issue,positive,positive,positive,positive,positive,positive
510994256,"Sorry, I'm a bit confused asking it. 

Outside of ML-Agents, I trained a regression model (supervised learning) to make predictions. I want the agent to learn to act within an environment (RL/ML-Agents) using these predictions. The idea is that the pre-trained model would make predictions in real-time during the agent's exploration, and the agent could use these predictions as observations for its learning algorithm. 

I think my question is similar to this one, but I'm thinking about using a trained model within a reinforcement learning algorithm: https://stackoverflow.com/questions/53526233/tensorflow-what-is-the-easiest-way-to-incorporate-predictions-from-one-model-in?rq=1",sorry bit confused outside trained regression model learning make want agent learn act within environment idea model would make agent exploration agent could use learning algorithm think question similar one thinking trained model within reinforcement learning algorithm,issue,negative,negative,negative,negative,negative,negative
510989550,hi @rafvasq - i'm a little bit confused on your question.  are you saying you trained a model outside of ML-Agents?,hi little bit confused question saying trained model outside,issue,negative,negative,negative,negative,negative,negative
510988942,Hi @tealdil -- unfortunately our team doesn't have the bandwidth to help solve problems with custom training setups. I think you might be able to make it work by looking more closely at our environment that uses the ondemanddecision (the bouncer environment).  ,hi unfortunately team help solve custom training think might able make work looking closely environment bouncer environment,issue,negative,neutral,neutral,neutral,neutral,neutral
510961582,"Hey @EfveZombie, thanks for adding this! Does using `dtype=np.float32` also fix your issue? IMO that would be a bit cleaner than `float`",hey thanks also fix issue would bit cleaner float,issue,positive,positive,positive,positive,positive,positive
510960380,"The fields in the buffer should be all numerical, so I don't see an issue with adding the `dtype`. We'll definitely track this as we upgrade to newer versions of Numpy. ",buffer numerical see issue definitely track upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
510957675,"Thank you for quick answer. @unityjeffrey 

Hmmmm okay you got my 5th question wrong.
I know what is batch and epoch completely. I just don't understand what does that sentence mean and especially how the epoch is designed in this case. 

So did you define **buffer_size** amount of experience as one **full dataset** (**one epoch**) ???? 

So how it works internally is : every time buffer_size is filled, it calculates **gradient** with **batch_size** amount of experience  **""buffer_size / batch_size""** times then post process (i.e. averaging gradient) and update to model. continue this with n_epoch times before collecting new buffer_size amount of experiences.

is this correct ?  


regarding Time horizon, 
it is written that 

""time_horizon corresponds to how many steps of experience to collect per-agent before adding it to the **1) experience buffer**. **2) When this limit is reached before the end of an episode, a value estimate is used to predict the overall expected reward from the agent's current state""**

1). **experience buffer** is mentioned in here. is this **experience buffer** means same buffer as buffer with **Buffer Size** (I really don't get this part. what is relation between ""Time Horizon"" and ""Batch Size"" & ""Buffer Size"" & ""Num Epoch""? )
So experience buffer takes time-horizon number of experiences as one unit and it experience buffer takes unit * buffer size amount  as one epoch ?????
or experience buffer mentioned here is different kind of buffer?

2. here now it says buffer number in reached its limit it calculates value estimate. So when the Buffer Size  is filled, that experiences are used to calculate value estimate ? also the layer to calculate value estimate is updated n_epoch time?
",thank quick answer got th question wrong know batch epoch completely understand sentence mean especially epoch designed case define amount experience one full one epoch work internally every time filled gradient amount experience time post process gradient update model continue time new amount correct regarding time horizon written many experience collect experience buffer limit end episode value estimate used predict overall reward agent current state experience buffer experience buffer buffer buffer buffer size really get part relation time horizon batch size buffer size epoch experience buffer number one unit experience buffer unit buffer size amount one epoch experience buffer different kind buffer buffer number limit value estimate buffer size filled used calculate value estimate also layer calculate value estimate time,issue,positive,positive,positive,positive,positive,positive
510946163,"hi @JobSmith - the Agent reset happens when either the number of max steps is reached (determined by the Agent properties) is hit and the ""reset on done"" is checked (also in the agent properties).  Additionally, if you are instantiating agents at runtime, you must also call agent reset to assign a new brain.  

There is also an internal method (_AgentReset()) that is used throughout to update internal data structures.  You can see where these are being used in Agent.cs under Assets\Scripts.

I am not sure why its happening in External but not player mode.  Can you provide more details on your setup?",hi jobsmith agent reset either number determined agent hit reset done checked also agent additionally must also call agent reset assign new brain also internal method used throughout update internal data see used sure happening external player mode provide setup,issue,positive,positive,positive,positive,positive,positive
510942975,"Hi @Junggy 

1 - Processing means to go through one iteration of observation, action, and updating the reward.
2 - 4 Not sure on what you mean by your questions, can you elaborate?
5. This article does a good job explaining Epochs and batches for gradient descent https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/",hi go one iteration observation action reward sure mean elaborate article good job explaining gradient descent,issue,positive,positive,positive,positive,positive,positive
510939895,"hi @Usmaniatech - can you provide your jupyter notebook?  and also what you mean by ""trained exe file""?",hi provide notebook also mean trained file,issue,negative,negative,negative,negative,negative,negative
510939186,hi @EfveZombie - can you change target branch to Develop instead of master?  We do not allow PRs to Master this way.  Thanks!,hi change target branch develop instead master allow master way thanks,issue,positive,positive,positive,positive,positive,positive
510875356,"Hi @xiaomaogy 

Thanks for your reply. I've added the options and the Tensorboard example.",hi thanks reply added example,issue,negative,positive,positive,positive,positive,positive
510831025,"@mantasp  I found a new update on this issues. 
The problem is not getting Tensor buffer to CPU, rather I used worker.ExecuteAndWaitForCompletion(input_tensor);
instead of 
worker.Execute(input_tensor);

This takes long time to process a frame, let me know what's the difference between these two.

Thanks,
Bala",found new update problem getting tensor buffer rather used instead long time process frame let know difference two thanks,issue,negative,positive,neutral,neutral,positive,positive
510775622,"It should be a compatibility problem with higher version of Tensorflow or Numpy.

In my situation (py 3.7 && tf 1.13 && np 1.16), simply adding parameter `dtype = float` to all the calls of `numpy.array()` in [ml-agents/mlagents/trainers/buffer.py](https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/buffer.py) could fix this problem. 

But I dont know if that would cause any potential problems.",compatibility problem higher version situation simply parameter float could fix problem dont know would cause potential,issue,negative,positive,neutral,neutral,positive,positive
510763498,I am working with MADDPG and it don't support mlagents latest version. I am working on mlagents 0.4. @xiaomaogy ,working support latest version working,issue,negative,positive,positive,positive,positive,positive
510744590,"Hi @roboserg. Thanks for the questions. I looked into the code again and realized that one of the stuff I mentioned was wrong. When you call Academy.Done(), even when these Agents haven't reached a done state, their experiences up to this point will still be used to update the model. (Although since the episode length is shorter, the reward will be estimated more by the value function)

Regarding your questions:

a) Your setup still works. Academy.Done() will reset every agent in the scene. 

b) Since the experience is used to learn the model, that's why. 

c) Usually we set the max steps in the agent. Academy reset is usually used as a reset mechanism for the whole scene. 

Hopefully this helps. 
",hi thanks code one stuff wrong call even done state point still used update model although since episode length shorter reward value function regarding setup still work reset every agent scene since experience used learn model usually set agent academy reset usually used reset mechanism whole scene hopefully,issue,positive,negative,negative,negative,negative,negative
510724601,"> Can you try this:
> 
> Change parameter in the `ml-agents/ml-agents/setup.py:`
> 
> `""tensorflow>=1.7,<1.8"" -> ""tensorflow-gpu>=1.7""` then install that setup.py with `pip -e .` And make sure you installed CUDA and CUDNN.

Just tried this, still uses the CPU. Should I also uninstall regular TensorFlow?",try change parameter install pip make sure tried still also regular,issue,negative,positive,positive,positive,positive,positive
510719548,"Dear all,

I have a different question related to Load Model to continue training.

I trained my model for 1.2M steps. After looking at the data, I continued training by loading the old model, with no changes to the environment, rewarding and training parameters (except for only max step increased to 3M).

Here is the statistics on my TensorBoard
[TensorBoard statistics](https://lh3.googleusercontent.com/q6hWUqEHrtK3pFjYiCCs0mdaF-Ybv3zBfT6sA21hlr3tyax7LXXBKGdydZoEfYptQIr85XFm9Hi6ZRDGwzuqBq4l2skgBlVhDww-y2RSOfRjZcm_b_EGI5k7CnN0yrI3qF1mMjByjZbDXnrhUhW76eHAGhUJrf4PfHjw7RfxNpeeZroUT8ivEp59k4Iq4knm8ivMoNHtDI-cdV-C-XjmCGM4DG6dDjYb067_Mf2s6rs7oKKEoxPQFpcfwFkXhlrnadZc3n2FsMuy3M9ZU3eR5kQaQvrBZ5W5aFuCNfW2Gea-eqJOE96egeMd2-D1wJCFH1CNA_L4-a7utkrIQe9b8W8u7HB0Xt0vtbOsJIi2p5t-wKyCswHgs7fLS0lhIoINFZDYRh1xa9RtOCHdbWw3v7P5b5NGqEsMlEpnXOGxmPvHgm3uIH_TGN2zi0mdbAB1siKmVTXt4CU2Pvd_QZVssiqESlFYI0KGEsWacCo947W_NjEr0pLSCiiVqRVY5zrbUC5h6m6cmMIEW80uxUmcq9nsxDqAL1BqFrC4yGccsHxUqrD8wNhYbU2tBX4CiQfqsbCEdTjU3aaRuRBuCuVdpIHNhZ-xxcQv4_WyHAL_uuo7LrOa1IRpy-SzgHUGJ15WjbDMrqDgzvNXsSbaWawN84QofL7b5UNa=w1555-h959-no)

What I am wondering is Losses, Policy Reward etc. at 1.2Mth step, as if it is a completely new training session with using only updated weights. I just want to ask if what I think is true, and will the result (as in agent behaviour) be any different to if I train the agent from the start with 3M as max step?",dear different question related load model continue training trained model looking data continued training loading old model environment rewarding training except step statistic statistic wondering policy reward step completely new training session want ask think true result agent behaviour different train agent start step,issue,positive,positive,positive,positive,positive,positive
510705304,"Hello @awjuliani 
Thank you for pointing me to the code.
I have used the nomenclature as below:

- L - Left
- R - Right
- B - Back
- F - Front
- Up - Upper
- Lo - Lower
- x, y, z - axis labels
- trot - target rotation angle
- T - torque

For the crawler action space vector, the following is the order of actions I understood:

```
[LB_Up_x_trot, LB_Up_y_trot,
RB_Up_x_trot, RB_Up_y_trot,
RF_Up_x_trot, RF_Up_y_trot,
LF_Up_x_trot, LF_Up_y_trot,
LB_Lo_x_trot,
RB_Lo_x_trot,
RF_Lo_x_trot,
LF_Lo_x_trot, 
LB_Up_T,
RB_Up_T,
RF_Up_T,
LF_Up_T,
LB_Lo_T,
RB_Lo_T,
RF_Lo_T,
LF_Lo_T]
```

Please correct me if I am wrong.

",hello thank pointing code used nomenclature left right back front upper lo lower axis trot target rotation angle torque crawler action space vector following order understood please correct wrong,issue,negative,negative,neutral,neutral,negative,negative
510669850,"Hi @adipandas 

Please see the code here for a description of the observation space of Crawler: https://github.com/Unity-Technologies/ml-agents/blob/master/UnitySDK/Assets/ML-Agents/Examples/Crawler/Scripts/CrawlerAgent.cs#L106. 

See here for the action space: https://github.com/Unity-Technologies/ml-agents/blob/master/UnitySDK/Assets/ML-Agents/Examples/Crawler/Scripts/CrawlerAgent.cs#L186. 

The reason the sizes are so large is because in observations we are using position, rotation, velocity, and angular velocity information for each of the parts of the crawler. 

The action space corresponds to setting the target joint velocity and strength to each of the joints in the crawler.

The Walker agent uses a very similar system, and can be found here: https://github.com/Unity-Technologies/ml-agents/blob/master/UnitySDK/Assets/ML-Agents/Examples/Walker/Scripts/WalkerAgent.cs ",hi please see code description observation space crawler see action space reason size large position rotation velocity angular velocity information crawler action space setting target joint velocity strength crawler walker agent similar system found,issue,positive,positive,positive,positive,positive,positive
510668766,hi @LuoYouRen - thanks for the issue.  Would you be open to proposing a fix on develop and submitting a PR?  Thanks!,hi thanks issue would open fix develop thanks,issue,positive,positive,positive,positive,positive,positive
510652473,not sure on that... CC: @ervteng - any thoughts on the thread and inference error using IL with the 3dball?,sure thread inference error,issue,negative,positive,positive,positive,positive,positive
510651874,"Just tried switching it back to the Horizontal and Vertical axes. Trying to use the model generated thus yields the same result.

Anything else? (And thanks for your reply, @unityjeffrey!)",tried switching back horizontal vertical ax trying use model thus result anything else thanks reply,issue,negative,positive,neutral,neutral,positive,positive
510649226,hi @ImBackAgain - the only thing I can think of is there might have been some issues with the mapping between using a mouse to actions that the agent will use.  Have you tried to do the imitation learning without the mouse and see if that works?,hi thing think might mouse agent use tried imitation learning without mouse see work,issue,negative,negative,negative,negative,negative,negative
510596508,Yes you can (not it is .nn file instead of the .bytes file). You just need to move the corresponding code from our python code into the jupyter notebook. @Usmaniatech ,yes file instead file need move corresponding code python code notebook,issue,negative,neutral,neutral,neutral,neutral,neutral
510595151,"This looks good to me (other than the conflict, of course). I'll add this to the GAIL reward signal class as well. ",good conflict course add reward signal class well,issue,positive,positive,positive,positive,positive,positive
510588534,Overall looks good to me. I've learned a lot during the review. ,overall good learned lot review,issue,negative,positive,positive,positive,positive,positive
510583676,Hi @AcelisWeaven Thanks a lot for the PR. Your suggested changes regarding adding '-it' and '-p 6006:6006' make sense. Could you also add an example on how to use the tensorboard once you've made this change?,hi thanks lot regarding make sense could also add example use made change,issue,negative,positive,positive,positive,positive,positive
510550565,"Hi NonoLG,

as far as I understood it works the following: 
in Unity you have the ""_Global Step Count_"". You can get this global step count by calling `academy.GetTotalStepCount`. You can also set a maximum count when the scene should reset.

Now let's assume you have only a single agent. If your agent has a `decision interval` of let's say 5, that means that only every 5 _Global Steps_ the agent takes an action. This means after 100 _Global Steps_ the Agent will have taken 20 local steps. If you set the maximum steps for a single episode (`agent.maxSteps`) to 1000 and you have a `decision interval` of 10 this means that your episode has a maximum length of 100 local steps. When you call `agent.GetStepCount` you will get the local step count of the agent within the current local episode. The number reported to the graph is then the steps the agent needed to complete the episode and not the _Global Step Count_.

> 2 - Is the Cumulative reward averaged over all the agents in the same scene ? <

Yes the Cumulative Reward averages the reward over all completed episodes within the specified summary frequency(usually every 1000 local steps). 

Please let me know if I'm mistaken or if you need some more clarification since it took me also a while to figure out what's going on.
",hi far understood work following unity step get global step count calling also set maximum count scene reset let assume single agent agent decision interval let say every agent action agent taken local set maximum single episode decision interval episode maximum length local call get local step count agent within current local episode number graph agent complete episode step cumulative reward scene yes cumulative reward reward within summary frequency usually every local please let know mistaken need clarification since took also figure going,issue,positive,negative,neutral,neutral,negative,negative
