id,original_comment,comment,source
1972598167,"Hi @Sehun0819 ,

I can see missing validation check for strides in the API Conv2DBackpropFilter.

For other APIs DepthwiseConv2DNativeBackpropFilter & DepthwiseConv2DNativeBackpropInput there is check for `strides.size() `and it should raise error if `strides.size() !=4` .
https://github.com/tensorflow/tensorflow/blob/6fd130f8ad839257a274212fbd935b35f2625584/tensorflow/core/kernels/depthwise_conv_grad_op.cc#L559

https://github.com/tensorflow/tensorflow/blob/6fd130f8ad839257a274212fbd935b35f2625584/tensorflow/core/kernels/depthwise_conv_grad_op.cc#L1070

Could you please confirm whether the behaviour happens with same code above where `strides.size!=4`",hi see missing validation check check raise error could please confirm whether behaviour code,issue
1972586127,"Use the below code snippet to generate gemma2 quantized model and it would be around 2.33gb
```
# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model('gemma_2/')
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

# Save the model
with open('gemma2-quantized.tflite', 'wb') as f:
    f.write(tflite_model)
```",use code snippet generate gemma model would around convert model converter enable lite enable save model open,issue
1972564655,"I have the same issue of having non converted operations, and my model is quite simple as well. It is just a multi perceptron model with fully connected layers, relu activation and softmax activation functions. I wonder how I can suppress the log message of 'having non-converted operations ......' in the converter.convert() method.

```
Summary on the non-converted ops:
---------------------------------
 * Accepted dialects: tfl, builtin, func
 * Non-Converted Ops: 10, Total Ops 19, % non-converted = 52.63 %
 * 10 ARITH ops

- arith.constant:   10 occurrences  (f32: 10)



  (f32: 5)
  (f32: 1)
```",issue non converted model quite simple well model fully connected activation activation wonder suppress log message method summary accepted total,issue
1972483237,"Hi @zzj0402 ,

The code executes fine on Colab as per [gist](https://colab.research.google.com/gist/SuryanarayanaY/8a70d4b8f7772d1da6878a0e9c85507e/63085.ipynb). This moght be related to Kaggle environment which might have incompatible TF and tensorflow_hub version. You may check the Tensorflow and tensorflow_hub in kaggle environment and try to install those works in locally. Neverthless it's not problem with Tensorflow or Keras.

Thanks!",hi code fine per gist related environment might incompatible version may check environment try install work locally problem thanks,issue
1972476068,"Hi @KelseyJing ,

I have tested the import with Keras 2.15v and Keras3 as well and its success.Attached [gist](https://colab.research.google.com/gist/SuryanarayanaY/ca979897e88bbf260b8df5b8613e07e3/63095.ipynb) for reference.Could you please try with fresh installation and let us know the outcome.",hi tested import well gist please try fresh installation let u know outcome,issue
1972454546,"I solve it using this workaround  in colab


```py
file_path = '/content/models/research/object_detection/model_main_tf2.py'

# Read the content of the file
with open(file_path, 'r') as file:
    content = file.readlines()



# Find the index of the import tensorflow line
import_line_index = next(i for i, line in enumerate(content) if 'import tensorflow' in line)

# Define the fix command to insert
fix_command = ""tf.config.optimizer.set_experimental_options({'layout_optimizer': False})\n""

# Insert the fix command after the TensorFlow import
content.insert(import_line_index + 1, fix_command)


# Write the modified content back to the file
with open(file_path, 'w') as file:
    file.writelines(content)


```",solve read content file open file content find index import line next line enumerate content line define fix command insert false insert fix command import write content back file open file content,issue
1972410352,"@freedomtan 

Thanks. It built successfully 

tflite generated. Let me check with Android 

The combined code block is

```
import os
import keras
import keras_nlp

import tensorflow as tf

os.environ[""KAGGLE_USERNAME""] = ""....""
os.environ[""KAGGLE_KEY""] = '....'
os.environ[""KERAS_BACKEND""] = ""tensorflow""  # Or ""tensorflow"" or ""torch"".

print(""<======= LOADING =====>"")
gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(""gemma_2b_en"")

model = gemma_lm.backbone
print(""<======= EXPORTING =====>"")

export_archive = keras.export.ExportArchive()
print(""<======= TRACKING =====>"")

export_archive.track(model)
export_archive.add_endpoint(
    name=""serve"",
    fn=model.call,
    input_signature=[{
        ""token_ids"": tf.TensorSpec(shape=(None, 3), dtype=tf.float32),
        ""padding_mask"": tf.TensorSpec(shape=(None, 3), dtype=tf.float32),
    }],
)
print(""<======= ARCHIVEING =====>"")

export_archive.write_out('gemma_2')
print(""<======= CONVERTING =====>"")

tflite_model = tf.lite.TFLiteConverter.from_saved_model('gemma_2/').convert()

print(""<======= WRITING TDLite =====>"")
with open('gemma_2.tflite', 'wb') as f:
  f.write(tflite_model)

print(""<======= TFLITE GENERATED =====>"")

```",thanks built successfully let check android combined code block import o import import import torch print loading model print print model serve none none print print converting print writing open print,issue
1972372209,"> > @AthiemoneZero Because it still does output a GPU device at the bottom of the log, I am training on GPU, just without cuDNN. It will be slower, but it is better than nothing or training on CPU.
> 
> Yeah. But I just found that when I downgrade to 2.13.0 version, errors in register won't appear again. It looks like this:
> 
> `(TF) ephys3@ZhouLab-Ephy3:~$ python3 -c ""import tensorrt as trt;import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""`
> 
> ```
> 2023-10-11 20:39:12.097457: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
> 2023-10-11 20:39:12.130250: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
> To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
> 2023-10-11 20:39:13.856721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:65:00.0/numa_node
> Your kernel may have been built without NUMA support.
> 2023-10-11 20:39:13.870767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:65:00.0/numa_node
> Your kernel may have been built without NUMA support.
> 2023-10-11 20:39:13.870941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:65:00.0/numa_node
> Your kernel may have been built without NUMA support.
> [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
> ```
> 
> Although I haven't figured out how to solve NUMA node error, I found some clues from another [issue](https://github.com/microsoft/WSL/issues/5025) (as I operated all above in WSL Ubuntu). This bug seems not to be significant as explaination from [NVIDIA forums](https://forums.developer.nvidia.com/t/problem-running-tensorflow/249486) . So I guess errors in register might have something with the latest version and errors in NUMA might be caused by OS enviroment. Hope this information would help some guys.

-------

NUMA non zero problem can be solved this way 

1. Check Nodes
lspci | grep -i nvidia
  
01:00.0 VGA compatible controller: NVIDIA Corporation TU106 [GeForce RTX 2060 12GB] (rev a1)
01:00.1 Audio device: NVIDIA Corporation TU106 High Definition Audio Controller (rev a1)
The first line shows the address of the VGA-compatible device, NVIDIA Geforce, as 01:00 . Each one will be different, so let’s change this part carefully.
2. Check and change NUMA setting values
If you run ls with this path /sys/bus/pci/devicecs/, you can see the following list:
ls /sys/bus/pci/devices/
  
0000:00:00.0  0000:00:06.0  0000:00:15.0  0000:00:1c.0  0000:00:1f.3  0000:00:1f.6  0000:02:00.0
0000:00:01.0  0000:00:14.0  0000:00:16.0  0000:00:1d.0  0000:00:1f.4  0000:01:00.0
0000:00:02.0  0000:00:14.2  0000:00:17.0  0000:00:1f.0  0000:00:1f.5  0000:01:00.1
01:00.0 checked above is visible. However, 0000: is attached in front.
3. Check if it is connected.
cat /sys/bus/pci/devices/0000\:01\:00.0/numa_node
  
-1

1 means no connection, and 0 means connected.
4. Fix it with the command below.
sudo echo 0 | sudo tee -a /sys/bus/pci/devices/0000\:01\:00.0/numa_node
  
0

",still output device bottom log training without better nothing training yeah found downgrade version register wo appear like python import import print custom may see slightly different numerical due different computation turn set environment variable binary use available enable following rebuild appropriate compiler could open file read node kernel may built without support could open file read node kernel may built without support could open file read node kernel may built without support although figured solve node error found another issue bug significant guess register might something latest version might o hope information would help non zero problem way check compatible controller corporation tu rev audio device corporation tu high definition audio controller rev first line address device one different let change part carefully check change setting run path see following list checked visible however attached front check connected cat connection connected fix command echo tee,issue
1972366332,"With the dimensions of `token_ids` and `padding_mask` fixed
```python
model = gemma_lm.backbone
export_archive = keras.export.ExportArchive()
export_archive.track(model)
export_archive.add_endpoint(
    name=""serve"",
    fn=model.call,
    input_signature=[{
        ""token_ids"": tf.TensorSpec(shape=(None, 3), dtype=tf.float32),
        ""padding_mask"": tf.TensorSpec(shape=(None, 3), dtype=tf.float32),
    }],
)
export_archive.write_out('gemma_2')
```
I can convert the saved_model to tflite successfully with Colab Pro + A100 runtime.

the colab notebook I used, https://colab.research.google.com/drive/1zFqfYWK7lcW5_wqGpqoPflr87uiMmykB?usp=sharing",fixed python model model serve none none convert successfully pro notebook used,issue
1972304202,This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further.,issue closed inactive day since marked stale please reopen like work,issue
1972303653,This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you.,issue stale open day activity closed activity thank,issue
1972106601,"@Skillnoob, awesome, if you are confident this issue is resolved and you have no more open items, please feel free to close this issue as completed. Thanks.",awesome confident issue resolved open please feel free close issue thanks,issue
1972050985,@pkgoogle I have already tested with @feranick 's builds a while ago and they work without any issues on my rpi 5 with python 3.11,already tested ago work without python,issue
1971998782,"I have a problem too, I think it's the same problem
ERROR:C:\Users\hp\.gradle\caches\transforms-3\0e1dbadb4b8b682380e0f29f2a20be93\transformed\jetified-guice-5.1.0.jar: D8: com.android.tools.r8.internal.a2: MethodHandle.invoke and MethodHandle.invokeExact are only supported starting with Android O (--min-api 26)

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':app:mergeExtDexDebug'.
> Could not resolve all files for configuration ':app:debugRuntimeClasspath'.
   > Failed to transform guice-5.1.0.jar (com.google.inject:guice:5.1.0) to match attributes {artifactType=android-dex, asm-transformed-variant=NONE, dexing-enable-desugaring=true, dexing-is-debuggable=true, dexing-min-sdk=21, org.gradle.category=library, org.gradle.libraryelements=jar, org.gradle.status=release, org.gradle.usage=java-runtime}.
      > Execution failed for DexingWithClasspathTransform: C:\Users\hp\.gradle\caches\transforms-3\0e1dbadb4b8b682380e0f29f2a20be93\transformed\jetified-guice-5.1.0.jar.
         > Error while dexing.",problem think problem error jar starting android failure build exception went wrong execution task could resolve configuration transform jar match execution jar error,issue
1971987112,"Hi @Skillnoob, as @feranick mentioned, the libedgetpu library is now updated. Can you test your case against master and see if it resolves your issue?",hi library test case master see issue,issue
1971957153,"Thanks very much to @pkgoogle and @namburger at Google for merging PR. The libedgetpu library is now fully updated, and I hope binaries will be made available soon through the official channel.",thanks much library fully hope made available soon official channel,issue
1971867383,"Hi @ldbrouwer, would you happen to know the branch/commit your code is referring to? I just want to ensure I have the right context on what's happening in your situation.",hi would happen know code want ensure right context happening situation,issue
1971701292,"Hi there, I am facing the same issue when trying to convert whisper into int8 for running on TPU, is there any update please? Thank you.",hi facing issue trying convert whisper running update please thank,issue
1971663753,"> @cantonios : I see ""feedback/copybara"" has internal failure. Please let me know if anything I can fix. Thanks.

I think we're okay... it looks like there are some preexisting failures at Google.",see internal failure please let know anything fix thanks think like,issue
1971651487,"@cantonios : I see ""feedback/copybara"" has internal failure. Please let me know if anything I can fix. Thanks.",see internal failure please let know anything fix thanks,issue
1971420529,"This seems to fail API compat test. Can you run the following, please?

```
bazel run tensorflow/tools/api/tests:api_compatibility_test -- --update_goldens True
```

Thank you",fail test run following please run true thank,issue
1971418530,"No, and I don't intend to. I am just saying that a similar error is given when using tf.saved_model.save in keras v3 environments.",intend saying similar error given,issue
1971390272,"@feranick 

Do you able to generate Gemma 2B tflite?

if yes, Could you please provide the full code you used?",able generate gemma yes could please provide full code used,issue
1971338466,"I gather that the correct way to save a `v3` model is with `model.export`, not with `tf.saved_model.save` as indicated in the [transition guide](https://keras.io/guides/migrating_to_keras_3/).",gather correct way save model transition guide,issue
1971326271,"I get the same error in other [code](https://github.com/tensorflow/tensorflow/issues/62989) when saving a model using keras v3 and following the information provided in the [keras v2-to-v3 transition guide](https://keras.io/guides/migrating_to_keras_3/) :
```
 tf.saved_model.save(model, model_name)
```
and loading it back and performing the tflite conversion:
```
import keras
model = keras.layers.TFSMLayer(model_name, call_endpoint='serving_default')
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
```

No error when the model is saved using:
```
model.export(model_name)
```
(This was tested using TF 2.16.0-rc0)",get error code saving model following information provided transition guide model loading back conversion import model converter model error model saved tested,issue
1971020282,"@sushreebarsa 
Even it does not print out ASAN log(as it wasn't built with ASAN flags), the error message seems to be an evidence.
`Expected dimension in the range [-4, 4), but got 1945239553`
`1945239553` is `0x73F20001` in hexadecimal. I guess the right part, `0x0001` came from 2byte input argument `tf.constant(1,shape=[],dtype=tf.int16)` and the left part `0x73F2` is the 2bytes that was not meant to be read.",even print log built error message evidence dimension range got guess right part came input argument left part meant read,issue
1970978100,@bhack I am quite new to this. Can you please explain to me what needs to be done to fix the none gradient. My gradient is all none. Thank you,quite new please explain need done fix none gradient gradient none thank,issue
1970818939,"@thangld201,
For tensorflow v2.13 you can try installing cuda toolkit and cudnn, & update the LD_LIBRARY_PATH and it works.
``` python
> conda activate tf
> conda install cudatoolkit=11.8
> export LD_LIBRARY_PATH=""$HOME/miniconda3/envs/tf-test/lib""
> conda install cudnn=8.6
```
And also please follow the tested build configurations which are compatible for tensorflow v2.13
https://www.tensorflow.org/install/source#gpu
Thank you!


",try update work python activate install export install also please follow tested build compatible thank,issue
1970671493,Hi @ezhulenev Can you please review this PR ? Thank you!,hi please review thank,issue
1970661998,"Hi @tilakrayal , I'm having the same problem. But in my case, CUDA libraries are loaded properly, it's just tensorflow does not detect any gpus (but it works normally with torch and nvidia-smi). Do you have any idea ?
Terminal outputs look like this:
```
(my_env) $ python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU')); print(tf.__version__)""       
2024-02-29 15:36:16.385941: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.                                                                                                                                   2024-02-29 15:36:16.435959: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.                                                                                                                                   
2024-02-29 15:36:16.436407: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.                                                                                
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.         
2024-02-29 15:36:19.414065: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT          
2024-02-29 15:36:24.877011: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.                                          
Skipping registering GPU devices...                                                                                                  []                                                                                                                                   
2.13.1 
```",hi problem case loaded properly detect work normally torch idea terminal look like python import print print could find machine used could find machine used binary use available enable following rebuild appropriate compiler warning could find please make sure missing properly would like use follow guide setup platform skipping,issue
1970621536,"@fgava90,
From the information provided, I can see that you are trying to install the tensorflow v2.14 with python 3.8.7 and gcc 11.3.0 which are incompatible. Tensorflow v2.14 is compatible with python v3.9-3.11 and compiler Clang 16.0.0.

Could you please try to follow the tested build configurations from the official document for the smooth installation of tensorflow.
https://www.tensorflow.org/install/source#tested_build_configurations

Thank you!",information provided see trying install python incompatible compatible python compiler clang could please try follow tested build official document smooth installation thank,issue
1970596016,"seems like using the latest Miniconda3 version base env caused the issue since it was using python3.12 using miniconda3 for 3.11 and v24.1.2 resolved the issue, Thank you!",like latest version base issue since python resolved issue thank,issue
1970568589,"@farmaker47 

/// With the above the conversion finishes and the .tflite model is running into android ////

I tried running your code, but it also gave me the same error ""Could not translate MLIR to FlatBuffer.""

How did you get tflite ?",conversion model running android tried running code also gave error could translate get,issue
1970513614,"why not change the method form ._name to .name?
how to make the name stick to the tensor so it will show up when printed?",change method form make name stick tensor show printed,issue
1970493433,"@nilesh-sengupta,
I tried to install the tensorflow 2.14.0 using pip install and I was able to install without any issues. I suspect you missed following the installation with compatible tested build configurations which are mentioned in the official document.
Also please check the python version you are using.
https://www.tensorflow.org/install/source#linux

![Screenshot 2024-02-29 11 54 16 AM](https://github.com/tensorflow/tensorflow/assets/81610181/470fba2f-780f-4a45-a132-c96ea161591a)


Thank you!",tried install pip install able install without suspect following installation compatible tested build official document also please check python version thank,issue
1970486062,"@freedomtan 

Oh. That is,  you modified the existing codebase of keras-nlp  and installed the pip deps manually from the cloned and modified keras-nlp repo ? Right?

Where I need to modify ?

I am new to this field. See need to understand things",oh pip manually right need modify new field see need understand,issue
1970481464,"> @freedomtan
> 
> I got an error in A100 80 GB, 85 GB RAM pod. I ran that code provided by you
> 
> ```
> Details:
>         tf.StridedSlice(tensor<?x?x1xi1>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> (tensor<?x?x1x1xi1>) : {begin_mask = 7 : i64, ellipsis_mask = 0 : i64, end_mask = 7 : i64, new_axis_mask = 8 : i64, shrink_axis_mask = 0 : i64}
>         tf.StridedSlice(tensor<?x?x?xi32>, tensor<5xi32>, tensor<5xi32>, tensor<5xi32>) -> (tensor<?x1x1x?x?xi32>) : {begin_mask = 25 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 25 : i64, new_axis_mask = 6 : i64, shrink_axis_mask = 0 : i64}
> 
> Traceback (most recent call last):
>   File ""/workspace/gem.py"", line 13, in <module>
>     tflite_model = tf.lite.TFLiteConverter.from_saved_model('/workspace/gemma_saved_model/').convert()
>   File ""/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py"", line 1139, in wrapper
>     return self._convert_and_export_metrics(convert_func, *args, **kwargs)
>   File ""/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py"", line 1093, in _convert_and_export_metrics
>     result = convert_func(self, *args, **kwargs)
>   File ""/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py"", line 1465, in convert
>     return self._convert_from_saved_model(graph_def)
>   File ""/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py"", line 1331, in _convert_from_saved_model
>     result = _convert_saved_model(**converter_kwargs)
>   File ""/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py"", line 212, in wrapper
>     raise converter_error from None  # Re-throws the exception.
>   File ""/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py"", line 205, in wrapper
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py"", line 1001, in convert_saved_model
>     data = convert(
>   File ""/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py"", line 366, in convert
>     raise converter_error
> tensorflow.lite.python.convert_phase.ConverterError: Could not translate MLIR to FlatBuffer.
> ```

Yes, it seems the converter cannot handle dynamic tensor dimensions (`tensor<?x?x1xi1>`). 
That's why I mentioned modifying keras_nlp


```
        tf.StridedSlice(tensor<?x?x1xi1>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> (tensor<?x?x1x1xi1>) : {begin_mask = 7 : i64, ellipsis_mask = 0 : i64, end_mask = 7 : i64, new_axis_mask = 8 : i64, shrink_axis_mask = 0 : i64}
        tf.StridedSlice(tensor<?x?x?xi32>, tensor<5xi32>, tensor<5xi32>, tensor<5xi32>) -> (tensor<?x1x1x?x?xi32>) : {begin_mask = 25 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 25 : i64, new_axis_mask = 6 : i64, shrink_axis_mask = 0 : i64}
```

see 
https://github.com/keras-team/keras-nlp/blob/r0.8/keras_nlp/models/gemma/gemma_backbone.py#L142-L148",got error ram pod ran code provided tensor tensor xi tensor xi tensor xi tensor tensor xi tensor xi tensor xi tensor xi tensor xi device recent call last file line module file line wrapper return file line result self file line convert return file line result file line wrapper raise none exception file line wrapper return file line data convert file line convert raise could translate yes converter handle dynamic tensor tensor tensor tensor xi tensor xi tensor xi tensor tensor xi tensor xi tensor xi tensor xi tensor xi device see,issue
1970469971,"Given that the error message suggests version 2.16.0rc0 is available, it seems you're trying to install TensorFlow in a state where version 2.14 does not exist, but newer versions do",given error message version available trying install state version exist,issue
1970441494,"@sushreebarsa 
Could you please suggest, what may be the reason for the wrong results?",could please suggest may reason wrong,issue
1970423063,"@freedomtan 

I got an error in A100 80 GB, 85 GB RAM pod. I ran that code provided by you


```
Details:
        tf.StridedSlice(tensor<?x?x1xi1>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> (tensor<?x?x1x1xi1>) : {begin_mask = 7 : i64, ellipsis_mask = 0 : i64, end_mask = 7 : i64, new_axis_mask = 8 : i64, shrink_axis_mask = 0 : i64}
        tf.StridedSlice(tensor<?x?x?xi32>, tensor<5xi32>, tensor<5xi32>, tensor<5xi32>) -> (tensor<?x1x1x?x?xi32>) : {begin_mask = 25 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 25 : i64, new_axis_mask = 6 : i64, shrink_axis_mask = 0 : i64}

Traceback (most recent call last):
  File ""/workspace/gem.py"", line 13, in <module>
    tflite_model = tf.lite.TFLiteConverter.from_saved_model('/workspace/gemma_saved_model/').convert()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py"", line 1139, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py"", line 1093, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py"", line 1465, in convert
    return self._convert_from_saved_model(graph_def)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py"", line 1331, in _convert_from_saved_model
    result = _convert_saved_model(**converter_kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py"", line 212, in wrapper
    raise converter_error from None  # Re-throws the exception.
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py"", line 205, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py"", line 1001, in convert_saved_model
    data = convert(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py"", line 366, in convert
    raise converter_error
tensorflow.lite.python.convert_phase.ConverterError: Could not translate MLIR to FlatBuffer.
```",got error ram pod ran code provided tensor tensor xi tensor xi tensor xi tensor tensor xi tensor xi tensor xi tensor xi tensor xi device recent call last file line module file line wrapper return file line result self file line convert return file line result file line wrapper raise none exception file line wrapper return file line data convert file line convert raise could translate,issue
1970414101,"Hi **@ruslankotl** ,

Sorry for the delay, The problem might be arising due to a combination of factors including the TensorFlow version, the CUDA and cuDNN setup, and perhaps how the environment variables are set in your conda environment.
Could you to check the compatibility of these versions with TensorFlow installation. Here i am providing [document](https://www.tensorflow.org/install/source#ubuntu) for your reference.

Thank you!",hi sorry delay problem might due combination version setup perhaps environment set environment could check compatibility installation providing document reference thank,issue
1970413004,"@Icacoding1 The error originates from tensorflow.python.pywrap_tensorflow.py, indicating an issue loading the native TensorFlow runtime, which is a Dynamic Link Library (DLL) on Windows. Please download and install the appropriate Microsoft Visual C++ Redistributable from https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170

Thank you!",error issue loading native dynamic link library please install appropriate visual thank,issue
1970354908,"@freedomtan 
Thanks
But why GPU is irrelevant ?. I have seen 96 % GPU VRAM usage even in 48 GB A6000 ",thanks irrelevant seen usage even,issue
1970348108,"> @freedomtan @farmaker47 @nyadla-sys @zichuan-wei
> 
> Can I know the System configuration needed such as GPU , VRAM and Sys RAM ?

my desktop is an old Linux box with 64 GiB DRAM and 128 GiB swap space.
GPU is irrelevant.  ",know system configuration ram old box gib dram gib swap space irrelevant,issue
1970343085,"@freedomtan @farmaker47 @nyadla-sys @zichuan-wei 

Can I know the System configuration needed such as GPU , VRAM and Sys RAM ?

",know system configuration ram,issue
1970340158,"> @freedomtan can you share your working colab here

nope, because I tested it with a simple script on my local machine; didn't try to deal with memory issues in Colab  :-)
```python
import keras
import keras_nlp
import tensorflow as tf

os.environ[""KAGGLE_USERNAME""] = '....'
os.environ[""KAGGLE_KEY""] = '...'
os.environ[""KERAS_BACKEND""] = ""tensorflow"" 

gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(""gemma_2b_en"")
gemma_lm.backbone.export('/tmp/gemma_saved_model')

tflite_model = tf.lite.TFLiteConverter.from_saved_model('/tmp/gemma_saved_model/').convert()
with open(""model.tflite"", ""wb"") as f:
    f.write(tflite_model)
```

For test, I modified keras_nlp to have fixed tensor dimensions. That's it.

",share working nope tested simple script local machine try deal memory python import import import open test fixed tensor,issue
1970302452,"> I think there is an answer here that it is working: [keras-team/keras#19108](https://github.com/keras-team/keras/issues/19108) It is based on this comment: [keras-team/keras#19108 (comment)](https://github.com/keras-team/keras/issues/19108#issuecomment-1913421572)
> 
> So my code now is:
> 
> ```
> model.export(""test"", ""tf_saved_model"")
> converter = tf.lite.TFLiteConverter.from_saved_model(""test"")
> tflite_model = converter.convert()
> with open(""model.tflite"", ""wb"") as f:
>     f.write(tflite_model)
> ```
> 
> With the above the conversion finishes and the .tflite model is running into android. I have not used quantization since it is failing into android.

I can cofirm that I could get tflite by using:
```python
gemma_lm.backbone.export('/tmp/gemma_saved_model')
```
instead of 
```python
tf.saved_model.save(gemma_lm.backbone, '/tmp/gemma_saved_model/')
```

Note that the converter seems not memory efficient; I observed more than 90 GiB virtual memory was needed on my desktop machine.",think answer working based comment comment code test converter test open conversion model running android used quantization since failing android could get python instead python note converter memory efficient gib virtual memory machine,issue
1970221006,"Thanks for raising that issue with the guice dependency.  I have alerted the TensorFlow Lite team of this new issue.

The tensorflow 2.16.0 preview release is out.  Is the issue still reproducible in 2.16.0?

In general it is possible to use (most) Java 8 language features while depending only on API level 21, rather than 26.
See <https://developer.android.com/studio/write/java8-support> for details.
So if guice is using Java 8 language features, that doesn't necessarily mean that it needs to be incompatible with API level 21.
",thanks raising issue dependency lite team new issue preview release issue still reproducible general possible use language depending level rather see language necessarily mean need incompatible level,issue
1969980769,"In fact, there is one more difference for me between native and Docker. With native, I am also getting:

```
W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
```

And that seems to be fixed by neither a regular nor a local `pip install` of `tensorrt` and friends. So I guess the Docker image still has an added value for me. Unless there is an obvious fix for the local TensorRT problem (I am installing TensorRT according to NVIDIA's instructions as far as I know)?",fact one difference native docker native also getting warning could find fixed neither regular local pip install guess docker image still added value unless obvious fix local problem according far know,issue
1969951102,"The models created with `keras v2` and `v3` appear to be completely incompatible with each other. The way to have a successful conversion is to basically do the training, model loading and conversion within the same keras version.
So for `keras v2`, saving after training:
```
import tf_keras as keras
model.save(dP.model_name)
```
for the TFlite conversion:
```
def convertModelToTFLite(model_file):
    import tensorflow as tf
    import tf_keras as keras
    model = keras.models.load_model(model_file)
    converter = tf.lite.TFLiteConverter.from_keras_model(model)    # TensorFlow 2.15 and earlier
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    tflite_model = converter.convert()
    tf.lite.experimental.Analyzer.analyze(model_content=tflite_model)
    open(convFile, ""wb"").write(tflite_model)
```
For loading a `v2` model:
```
model = keras.models.load_model(model_name)
```
For `keras v3`, saving after training:
```
import keras
model.export(dP.model_name)
```
For loading a `v3` model:
```
model = keras.models.Sequential()
model.add(keras.layers.TFSMLayer(model_name, call_endpoint='serve'))
```
for the TFlite conversion:
```
 def convertModelToTFLite(model_file):
    import tensorflow as tf
    import keras
    model = keras.layers.TFSMLayer(model_file, call_endpoint='serve') # TensorFlow >= 2.16.0
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
   
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    tflite_model = converter.convert()
    tf.lite.experimental.Analyzer.analyze(model_content=tflite_model)
    open(convFile, ""wb"").write(tflite_model)
```

This of course is not ideal in the long term, when ""legacy"" `v2` models will require maintenance of legacy code to deal with it. **A solution could be an ad-hoc conversion API for models from `v2` to `v3`.**",appear completely incompatible way successful conversion basically training model loading conversion within version saving training import conversion import import model converter model open loading model model saving training import loading model model conversion import import model converter model open course ideal long term legacy require maintenance legacy code deal solution could conversion,issue
1969902340,"Hello, after ALOT of Google searches and going through GitHub repos and through Tensorflow and Sparkfun forums, I have determined that all of the info on the Sparkfun Edge board is outdated. On your current Github site, you have instructions on how to compile and run Micro_Speech_Test on a development machine, and how to create new models on Colab and other platforms, but there is no instruction/guidance or means to compile the Micro_Speech code for the Sparkfun Edge - or any microcontroller. I can find no ""make"" file for a TARGET=SparkfunEdge. I guess neither tensorflow not sparkfun support that board any longer and there is no way to compile and use the existing code in any expansion. My understanding has changed recently, such that I now believe that Tensorflow writes the TFLM C++ code that a microcontroller producer must then use to build a library/ compiler/loader for their specific microcontroller. Therefore, if Sparkfun no longer supports/updates a library for the Edge, any buyer is out of luck.
Oh, here is a link to the repo I was using - it is old: tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/micro_model_settings.h",hello going determined edge board outdated current site compile run development machine create new compile code edge find make file guess neither support board longer way compile use code expansion understanding recently believe code producer must use build specific therefore longer library edge buyer luck oh link old,issue
1969831963,"@SuryanarayanaY , I'm having the same problem as the OP on latest-gpu, just run your recommended cross check and am getting:

```
>>> print(""cudnn_version"",build_info.build_info['cudnn_version'])
cudnn_version 8
>>> print(""cuda_version"",build_info.build_info['cuda_version'])
cuda_version 12.2
```

So you're saying I just should ignore the ""unable to register..."" then? They are the only reason I at all went through the trouble of attempting to use Docker images... I'll revert back to my native install then. Thanks.

Note: lines starting with ""E"" tend to be interpreted as ""errors"", I guess, i.e. possibly blocking problems. I guess this makes this issue quite a serious one.",problem run cross check getting print print saying ignore unable register reason went trouble use docker revert back native install thanks note starting tend guess possibly blocking guess issue quite serious one,issue
1969711801,@RageshAntonyHM looks like it is still memory and compute issue,like still memory compute issue,issue
1969700799,"@jondea No problem! Yes, definitely: that's exactly one of the reasons that it's opt-in.",problem yes definitely exactly one,issue
1969628721,"Hi @Idbrouwer, I think that part of the repo is outdated/deprecated, do you have a link to that part of the repo? Reason being here are the current examples: 

https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental
https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/experimental

Thanks for any additional information you can provide.",hi think part link part reason current thanks additional information provide,issue
1969597376,"Loading VIT model in keras error coming 

[ValueError: Unknown layer: 'ClassToken'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See ](https://www.tensorflow.org/guide/keras/save_and_serialize%3C/span%3E%3Cspan)https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",loading model error coming unknown layer please ensure object included scope see,issue
1969578265,"@RageshAntonyHM this looks like the OS terminated the process, maybe due to memory consumption/cpu time limitation?",like o process maybe due memory time limitation,issue
1969539016,"@farmaker47 

I rented 48 GB GPU, now got another error  : 

```
converting
2024-02-28 17:51:39.439053: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.
2024-02-28 17:51:39.439136: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.
2024-02-28 17:51:39.440216: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: test
2024-02-28 17:51:39.459869: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }
2024-02-28 17:51:39.459902: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: test
2024-02-28 17:51:39.760067: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
2024-02-28 17:51:39.808678: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.
2024-02-28 17:51:44.034223: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: test
2024-02-28 17:51:44.392812: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 4952596 microseconds.
2024-02-28 17:51:44.754857: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Summary on the non-converted ops:
---------------------------------
 * Accepted dialects: tfl, builtin, func
 * Non-Converted Ops: 220, Total Ops 2416, % non-converted = 9.11 %
 * 184 ARITH ops, 36 TF ops

- arith.constant:  184 occurrences  (f32: 153, i32: 31)



- tf.StridedSlice:   36 occurrences  (i1: 18, i32: 18)
  (f32: 127)
  (f32: 36)
  (i1: 18)
  (f32: 36, i32: 20)
  (i32: 72)
  (f32: 18)
  (f32: 37)
  (i1: 18)
  (f32: 19, i1: 18)
  (f32: 127)
  (f32: 1, i32: 90)
  (f32: 18)
  (i1: 18)
  (i32: 1)
  (f32: 37)
  (i32: 19)
  (f32: 272)

  (f32: 36, i32: 90)
  (f32: 18)
  (i1: 18)
  (f32: 252)
  (f32: 18)
  (i32: 180)
  (f32: 18)
  (f32: 18)
  (f32: 36)
  (f32: 37)
  (f32: 37)
  (f32: 36, i32: 180)
  (f32: 54)
  (f32: 90)
  (i32: 54)
  (f32: 18)
Killed
```

The process terminates with ""killed"" message. Didn't enter ""writing"" ! ",rented got another error converting reading test reading meta graph serve reading present test optimization pas bundle running bundle path test load serve status success took crash reproducer set enable summary accepted total process message enter writing,issue
1969532180,"You can skip the Kaggle_key...😀

I think it's a memory error",skip think memory error,issue
1969516689,"@farmaker47 

I ran like this:

```
import os
import keras
import os
import numpy as np
import keras_nlp
import tensorflow as tf
import tensorflow_text as tf_text
from tensorflow import keras
from tensorflow.lite.python import interpreter
import time

os.environ[""KAGGLE_USERNAME""] = ""rag""
os.environ[""KAGGLE_KEY""] = 'e7c'
os.environ[""KERAS_BACKEND""] = ""tensorflow""  # Or ""tensorflow"" or ""torch"".

preprocessor = keras_nlp.models.GemmaCausalLMPreprocessor.from_preset('gemma_2b_en', sequence_length=4096, add_end_token=True
)
model = keras_nlp.models.GemmaCausalLM.from_preset(""gemma_2b_en"")
print(""exporting"")
model.export(""test"", ""tf_saved_model"")
print(""converting"")
converter = tf.lite.TFLiteConverter.from_saved_model(""test"")
tflite_model = converter.convert()
print(""writiing"")

with open(""model.tflite"", ""wb"") as f:
    f.write(tflite_model)
```

Iit fails at ""converting"" with this error ""GPU:0 in order to run Identity: Dst tensor is not initialized. [Op:Identity] name:     "" :


```
2024-02-28 17:40:21.522813: I external/local_tsl/tsl/framework/bfc_allocator.cc:1114] Stats: 
Limit:                     23553966080
InUse:                     23553959680
MaxInUse:                  23553959680
NumAllocs:                        1629
MaxAllocSize:               2097152000
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2024-02-28 17:40:21.522846: W external/local_tsl/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************
Traceback (most recent call last):
  File ""/workspace/gem.py"", line 22, in <module>
    converter = tf.lite.TFLiteConverter.from_saved_model(""test"")
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py"", line 2087, in from_saved_model
    saved_model = _load(saved_model_dir, tags)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py"", line 912, in load
    result = load_partial(export_dir, None, tags, options)[""root""]
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py"", line 1043, in load_partial
    loader = Loader(object_graph_proto, saved_model_proto, export_dir,
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py"", line 226, in __init__
    self._restore_checkpoint()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py"", line 561, in _restore_checkpoint
    load_status = saver.restore(variables_path, self._checkpoint_options)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/checkpoint.py"", line 1479, in restore
    checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root,
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/restore.py"", line 62, in restore
    restore_ops = self._restore_descendants(reader)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/restore.py"", line 463, in _restore_descendants
    current_position.checkpoint.restore_saveables(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/checkpoint.py"", line 379, in restore_saveables
    registered_savers).restore(self.save_path_tensor, self.options)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/functional_saver.py"", line 499, in restore
    restore_ops = restore_fn()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/functional_saver.py"", line 467, in restore_fn
    ret = restore_fn(restored_tensors)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saving/saveable_object_util.py"", line 747, in _restore_from_tensors
    return saveable_object_to_restore_fn(self.saveables)(restored_tensors)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saving/saveable_object_util.py"", line 784, in _restore_from_tensors
    restore_ops[saveable.name] = saveable.restore(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saving/saveable_object_util.py"", line 602, in restore
    ret = restore_fn(restored_tensor_dict)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py"", line 779, in _restore_from_tensors
    restored_tensor = array_ops.identity(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py"", line 88, in wrapper
    return op(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py"", line 5883, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from 

/job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Identity: Dst tensor is not initialized. [Op:Identity] name:     
```

Am I doing something wrong ?",ran like import o import import o import import import import import import interpreter import time rag torch model print test print converting converter test print open converting error order run identity tensor identity name limit reserved recent call last file line module converter test file line file line load result none root file line loader loader file line file line file line restore file line restore reader file line file line file line restore file line ret file line return file line file line restore ret file line file line wrapper return file line raise none file line raise none input tensor order run identity tensor identity name something wrong,issue
1969255864,"Hi @Sehun0819 ,

Thanks for reporting. The `dense_shape` can be 1D which means it can have -ve shapes like [-2,3] & [2,-3]. I would like to know the behaviour with the mentioned 2 shapes. It would be appreciated if you test by modifying the Inputs accordingly and report the behaviour.
Thanks!",hi thanks like would like know behaviour would test accordingly report behaviour thanks,issue
1969219548,"Hi there! Any updates on this issue?
I've been trying to run the sample model from https://www.tensorflow.org/lite/android/delegates/nnapi#use_supported_models_and_ops on the Samsung Galaxy S23 Ultra.
However, I'm encountering a similar result. I've tried benchmark tools and the sample app with the NnApiDelegate, but in all cases, I see a full fallback on CPU. There are no signs of using NNAPI.
",hi issue trying run sample model galaxy ultra however similar result tried sample see full fallback,issue
1969160491,"I think there is an answer here that it is working:
https://github.com/keras-team/keras/issues/19108
It is based on this comment:
https://github.com/keras-team/keras/issues/19108#issuecomment-1913421572

So my code now is:
```
model.export(""test"", ""tf_saved_model"")
converter = tf.lite.TFLiteConverter.from_saved_model(""test"")
tflite_model = converter.convert()
with open(""model.tflite"", ""wb"") as f:
    f.write(tflite_model)
```

With the above the conversion finishes and the .tflite model is running into android. I have not used quantization since it is failing into android.",think answer working based comment code test converter test open conversion model running android used quantization since failing android,issue
1969062696,"@NOORLEICESTER without further code it's difficult to say what's going on. I'd guess from the error that you actually have a circular import, so try to check your imports. 

@SomePersonSomeWhereInTheWorld 
Uninstall all tf versions but nightly to avoid version mismatches and the like.
 uninstall keras as tf-nightly also installs keras-nightly.
You may want to remove the previous env and start with a clean slate. 
`Could not find cuda drivers on your machine, GPU will not be used.`
Did you install the Nvidia driver (via apt/rpm/pacman) and cuda on your system?",without code difficult say going guess error actually circular import try check nightly avoid version like also may want remove previous start clean slate could find machine install driver via system,issue
1968956851,"@Sehun0819 Could you please have a look at the [gist](https://colab.research.google.com/gist/sushreebarsa/ccbad423d1f928fdf3f24519ba5f9153/63070.ipynb) and confirm the issue?
Thank you!",could please look gist confirm issue thank,issue
1968898313,"@ACE07-Sev Please ensure that you're using the Python environment where TensorFlow is installed. 
Kindly run `python --version ` to check. If you haven't installed TensorFlow yet, use the following command in your terminal:
```
pip install tensorflow

```
Could you try to use the latest TF version, please use the following;
```
import tensorflow as tf
```
Thank you!
 ",please ensure python environment kindly run python version check yet use following command terminal pip install could try use latest version please use following import thank,issue
1968851785,"@MessDeveloper,
From the information provided above, I can see you are trying with the **tf-nightly**. I faced the same error with tf-nightly in the jupyter notebook. So I will check with the developer team on the same and provide more information.

![skipping gpu 2 17](https://github.com/tensorflow/tensorflow/assets/81610181/69f1a661-ce8e-4c93-b4b3-76ae1f66da96)


As a temporary workaround, you can try using the latest stable version **2.15** where it was detecting and executing the code on the GPU.

![2 15](https://github.com/tensorflow/tensorflow/assets/81610181/c5d88b86-cad2-43e1-ac01-599bd1cc823d)

Thank you!

",information provided see trying faced error notebook check developer team provide information skipping temporary try latest stable version code thank,issue
1968699965,"Thanks for the heads up and for fixing the failure! I should say that if this flag goes from opt-in to opt-out before there’s an optimized implementation of SPMM in oneDNN for AArch64, then it will need a platform specific guard to stick with Eigen.",thanks fixing failure say flag go implementation need platform specific guard stick,issue
1968601020,Building with debug enabled makes the tests pass. This is an indication that there is a programming error causing undefined behaviour.,building pas indication error causing undefined behaviour,issue
1968465878,"@yyingci,
As mentioned it is an error provided by **tensorflow.image.decode_image** where there is a limitation to not support for BMP formats.
Similar issue on the same feature. https://github.com/tensorflow/tensorflow/issues/61893

Thank you!",error provided limitation support similar issue feature thank,issue
1968444356,"Hi @Sehun0819 ,

Thanks for reporting along with debug.Added a fix in the attached PR.",hi thanks along fix attached,issue
1968212765,Hi @lbertho-gpsw Can you please sign CLA? Thank you!,hi please sign thank,issue
1968182895,"@cbreak-black,
Yes as you mentioned, this issue is not related to tensorflow and more likely related to CUDA version which is not handled by the tensorflow team. Also I request to follow the tensorflow installation with the compatible test build configurations to avoid such type of errors/warnings.
 Thank you!",yes issue related likely related version handled team also request follow installation compatible test build avoid type thank,issue
1968164327,"@aditya02shah,
Apologies for the delay. As per the [documentation](https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror), **NonMatchingChecksumError** is raised due to the reasons below.
The website may be down, original datasets files may have been updated or Drive sometimes rejects downloads when too many people access the same URL.

As the workaround you can try to download from the [link](https://drive.usercontent.google.com/corp/download?id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45&export=download&authuser=0) and also request to check in [datasets](https://github.com/tensorflow/datasets/issues) repository for the quick resolution.

https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/image_classification/caltech_birds.py#L227

https://github.com/tensorflow/datasets/issues/1482

Thank you!

",delay per documentation raised due may original may drive sometimes many people access try link also request check repository quick resolution thank,issue
1968122317,"Great -- hopefully this time, we've fixed the nightly tests!",great hopefully time fixed nightly,issue
1967886421,"I believe colab is running out of memory for @LakshmiKalaKadali 's case,

In attempting to replicate the below, I am running into tensorflow-text installation issues (apparently the Gemma tokenizer uses it for the tokenizer), this may be because of the new 2.16 release.
```py
import keras
import keras_nlp
import tensorflow as tf

os.environ[""KAGGLE_USERNAME""] = '....'
os.environ[""KAGGLE_KEY""] = '...'
os.environ[""KERAS_BACKEND""] = ""tensorflow"" 

gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(""gemma_2b_en"")
tf.saved_model.save(gemma_lm.backbone, '/tmp/gemma_saved_model/')

f = tf.lite.TFLiteConverter.from_saved_model('/tmp/gemma_saved_model/').convert()
```

my error:
```py
TypeError: <class 'keras_nlp.src.models.gemma.gemma_tokenizer.GemmaTokenizer'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.

config={'module': 'keras_nlp.src.models.gemma.gemma_tokenizer', 'class_name': 'GemmaTokenizer', 'config': {'name': 'gemma_tokenizer', 'trainable': True, 'dtype': 'int32', 'proto': None, 'sequence_length': None}, 'registered_name': 'keras_nlp>GemmaTokenizer', 'assets': ['assets/tokenizer/vocabulary.spm'], 'weights': None}.

Exception encountered: Error when deserializing class 'GemmaTokenizer' using config={'name': 'gemma_tokenizer', 'trainable': True, 'dtype': 'int32', 'proto': None, 'sequence_length': None}.
```",believe running memory case replicate running installation apparently gemma may new release import import import error class could properly please ensure python object returned explicitly model method true none none none exception error class true none none,issue
1967609913,"So in summary: any model built with `TF2.15` or earlier or with `TF2.16.0-rc0` using `keras 2` (and `tf_keras=2.16rc0`) can be successfully converted in tflite. Any model built with `TF2.16.0-rc0` using `keras 3` **cannot** be converted to tflite. Any model built with `TF2.15` or earlier or with `TF2.16.0-rc0` using `keras 2` **cannot** be converted using `TF2.16.0-rc0` in keras 3 mode.

**In essence, conversion to tflite is completely broken with keras 3.**",summary model built successfully converted model built converted model built converted mode essence conversion completely broken,issue
1967544120,"> Looks like there was a failure on Windows:

Thanks @cantonios for pointing me the issue and the error log. I have fixed it. That test shouldn't be running on CPU, like on master.",like failure thanks pointing issue error log fixed test running like master,issue
1967522358,"> You describe a 0.6% improvement on the performance of a test. I am not approving this.

I didn't run too much iterations, just tried more and the improvement can be 3% to 4% on my end now.

Hi @qukhan in case of it is a correct change, why don't you want to take it? Improvemence accumulates step by step, no?",describe improvement performance test run much tried improvement end hi case correct change want take step step,issue
1967514637,"Tested the above models with the pre-release, TF 2.16.0-rc0, using keras 3. Now they both crash. Log attached.
[TF2.15_convertLog.txt](https://github.com/tensorflow/tensorflow/files/14425168/TF2.15_convertLog.txt)
[TF2.16_convertLog.txt](https://github.com/tensorflow/tensorflow/files/14425169/TF2.16_convertLog.txt)

When used in ""legacy"" mode (using tf-keras~=2.16rc0), the crash is identical as my previous report above (i.e. old TF2.15 models are converted, new TF2.16 are not).",tested crash log attached used legacy mode crash identical previous report old converted new,issue
1967435109,A temporary fix for case 2 is to add `--copt=-Wno-error=unused-command-line-argument` to the bazel command. It will suppress the command not used error. ,temporary fix case add command suppress command used error,issue
1967429167,"I agree with @SuryanarayanaY's analysis, the result should be `inf` and `nan` for the two examples.",agree analysis result nan two,issue
1967390602,"Looks like there was a failure on Windows:

```
FAIL: //tensorflow/core/grappler/optimizers:auto_mixed_precision_test_cpu

[==========] Running 38 tests from 3 test suites.
[----------] Global test environment set-up.
[----------] 2 tests from AutoMixedPrecisionCpuTest
[ RUN      ] AutoMixedPrecisionCpuTest.Simple
2024-02-21 08:25:59.082520: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session
2024-02-21 08:25:59.084137: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-21 08:25:59.159407: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2260] Converted 0/12 nodes to float16 precision using 4 cast(s) to float16 (excluding Const and Variable casts)
external/com_google_googletest/googletest/src/gtest-port.cc(828): error: Failed
Syntax error at index 53 in simple regular expression ""^clr2-0-CastToFp16-0-AutoMixedPrecision-0-CastToFp32-[0-9]-AutoMixedPrecision$"": '[' is unsupported.
external/com_google_googletest/googletest/src/gtest-port.cc(828): error: Failed
Syntax error at index 57 in simple regular expression ""^clr2-0-CastToFp16-0-AutoMixedPrecision-0-CastToFp32-[0-9]-AutoMixedPrecision$"": ']' is unsupported.
tensorflow/core/grappler/optimizers/auto_mixed_precision_test.cc(1339): error: Value of: edge.src.node->name()
Expected: contains regular expression ""^clr2-0-CastToFp16-0-AutoMixedPrecision-0-CastToFp32-[0-9]-AutoMixedPrecision$""
  Actual: ""clr2-0-CastToFp16-0-AutoMixedPrecision-0-CastToFp32-1-AutoMixedPrecision""
external/com_google_googletest/googletest/src/gtest-port.cc(828): error: Failed
Syntax error at index 53 in simple regular expression ""^clr2-0-CastToFp16-0-AutoMixedPrecision-0-CastToFp32-[0-9]-AutoMixedPrecision$"": '[' is unsupported.
external/com_google_googletest/googletest/src/gtest-port.cc(828): error: Failed
Syntax error at index 57 in simple regular expression ""^clr2-0-CastToFp16-0-AutoMixedPrecision-0-CastToFp32-[0-9]-AutoMixedPrecision$"": ']' is unsupported.
tensorflow/core/grappler/optimizers/auto_mixed_precision_test.cc(1339): error: Value of: edge.src.node->name()
Expected: contains regular expression ""^clr2-0-CastToFp16-0-AutoMixedPrecision-0-CastToFp32-[0-9]-AutoMixedPrecision$""
  Actual: ""clr2-0-CastToFp16-0-AutoMixedPrecision-0-CastToFp32-0-AutoMixedPrecision""
external/com_google_googletest/googletest/src/gtest-port.cc(828): error: Failed
Syntax error at index 21 in simple regular expression ""^allow1-0-CastToFp16-[0-9]-AutoMixedPrecision$"": '[' is unsupported.
external/com_google_googletest/googletest/src/gtest-port.cc(828): error: Failed
Syntax error at index 25 in simple regular expression ""^allow1-0-CastToFp16-[0-9]-AutoMixedPrecision$"": ']' is unsupported.
tensorflow/core/grappler/optimizers/auto_mixed_precision_test.cc(1347): error: Value of: edge.dst.node->name()
Expected: contains regular expression ""^allow1-0-CastToFp16-[0-9]-AutoMixedPrecision$""
  Actual: ""allow1-0-CastToFp16-0-AutoMixedPrecision""
[  FAILED  ] AutoMixedPrecisionCpuTest.Simple (80 ms)
```",like failure fail running test global test environment run starting new session binary use available enable following rebuild appropriate compiler converted float precision cast float excluding variable error syntax error index simple regular expression unsupported error syntax error index simple regular expression unsupported error value name regular expression actual error syntax error index simple regular expression unsupported error syntax error index simple regular expression unsupported error value name regular expression actual error syntax error index simple regular expression unsupported error syntax error index simple regular expression unsupported error value name regular expression actual allow,issue
1967338485,"@SuryanarayanaY
Hi!
I built it following [manual](https://www.tensorflow.org/install/source) with `--config=dbg`.

I just found that it runs without any error message when both `ksizes[1]` and `ksizes[2]` are negative([gist](https://colab.research.google.com/drive/11NlRWUTovteiN6JNPpGVK2E1zAkHLz4a#scrollTo=4Y1QLVSnv2tC&line=1&uniqifier=1)), and it seems even worse.
```Python
import tensorflow as tf

x = tf.raw_ops.ExtractImagePatches(
    images=tf.random.normal([1,1,1,1]),
    ksizes=[1,-1,-2,1],
    strides=[1,1,1,1],
    rates=[1,1,1,1],
    padding=""VALID"",
    name=None
)

# Tensor(""ExtractImagePatches_8:0"", shape=(1, 3, 4, 2), dtype=float32)
print(x)
```",hi built following manual found without error message negative gist even worse python import valid tensor print,issue
1967006336,"> @adamjstewart Please ensure NCCL is correctly installed and accessible on your system. This error suggests the @local_config_nccl// package isn't loaded, which often happens if NCCL libraries are missing or not configured properly. Thank you!

I ran into same error yesterday. I'm using NCCL 2.20.3-1 installed using local deb repo for Ubuntu 22.04 according to the very simple instructions posted on NVIDIA website. 

```
dpkg -i nccl-local-repo-ubuntu2204-2.20.3-cuda12.3_1.0-1_amd64.deb
apt install libnccl2=2.20.3-1+cuda12.3 libnccl-dev=2.20.3-1+cuda12.3
```

",please ensure correctly accessible system error package loaded often missing properly thank ran error yesterday local deb according simple posted apt install,issue
1966902270,"> Hi @feranick I don't work with that repo but can you perhaps link your PR that fixes the issue? Maybe I can help push it from here.
> 
> Edit: nevermind.. found it. I think this is it? [google-coral/libedgetpu#59](https://github.com/google-coral/libedgetpu/pull/59)

[pkgoogle](https://github.com/pkgoogle) Just wondering whether you had a chance to move this forward. The correct and current PR is this:

https://github.com/google-coral/libedgetpu/pull/60",hi work perhaps link issue maybe help push edit found think wondering whether chance move forward correct current,issue
1966893896,"@feranick Thanks a lot! Finally after a long journey in the rabbit hole, **this** is the only solution to get working with 3.11 currently",thanks lot finally long journey rabbit hole solution get working currently,issue
1966701915,"Hi @Sehun0819 ,

I think this behaviour exists if negative values passed to strides and rates also. Validation might be costly in that case.Since this behaviour is specific to a debug build and official builds won't have this behaviour, could you please mention the build command used for this build. May be there exists a nice way for fixing this.

Thanks!",hi think behaviour negative also validation might costly behaviour specific build official wo behaviour could please mention build command used build may nice way fixing thanks,issue
1966682261,"Hi @Sehun0819 ,
With official release builds there is exception is raised though message is not much clear. But user can traceback it though.

I have proposed a fix in linked PR above.",hi official release exception raised though message much clear user though fix linked,issue
1966654007,You describe a 0.6% improvement on the performance of a test. I am not approving this.,describe improvement performance test,issue
1966619119,"Was the a one-size-fits-all fix to this ?
I have data generator tile shapes of 1,48,48,1 separated into train and val for day and night time temp data

    import tensorflow as tf
    dg = tf.data.Dataset.from_generator(data_generator, args=(0.2,0), output_types=(tf.float64, tf.float64))
    val_dg = tf.data.Dataset.from_generator(data_generator, args=(0.2,1), output_types=(tf.float64, tf.float64))
    
    dg = dg.unbatch().batch(2)
    
    # use train gen, no of epochs, use val_gen to validate (specify separate data)
    history = model.fit_generator(dg,epochs=3, validation_data=val_dg, callbacks=[checkpoint])

This is giving me:
           ValueError: as_list() is not defined on an unknown TensorShape.
",fix data generator tile train day night time temp data import use train gen use validate specify separate data history giving defined unknown,issue
1966389542,"I suspect [here](https://github.com/tensorflow/tensorflow/blob/41b93a8b310086f69aab6b6369d2af9d5178881d/tensorflow/core/ops/math_ops.cc#L1895-L1900), a shape function of `DenseBincount`:
```C++
      if (c->Rank(c->input(0)) == 1) {
        c->set_output(0, c->MakeShape({size_val}));
      } else if (c->Rank(c->input(0)) == 2) {
        c->set_output(0, c->MakeShape({c->Dim(c->input(0), 0), size_val}));
      }
      return absl::OkStatus();
```
Because it assumes 1 or 2 ranked input tensors only, 0-ranked input tensor makes output unset.
How about to add a guard so that it can make sure only 1 or 2 ranked input tensors reach there?",suspect shape function rank input else rank input dim input return ranked input input tensor output unset add guard make sure ranked input reach,issue
1966374207,"Hi @pkgoogle,

I have reproduced the issue in Colab with TF 2.15, the session crashed at the step `generator = keras_nlp.models.GemmaCausalLM.from_preset(""gemma_2b_en"")` . Please take a look.

Thank You",hi issue session step generator please take look thank,issue
1966337758,"Hi @sushreebarsa, thank you for the response! 
Actually, my main doubt here is precisely defining what is the sequence of operations that would lead to the SVDF fusion when converting to TFLite. I already tried using [Google Research's implementation](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/svdf.py) and the implementation of a ""low latency svdf model"" I found in [this tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/models.py#L458) from Tensorflow's repository (for this one I had to adapt the code to use TF2 operations equivalent to the ones used in the TF1 implementation), but none of them got fused as an SVDF operator when converting to TFLite. In fact, when listing the set of operations from the TFLite models generated from Keras models with these SVDF layers, I get all separate ops, such as `'DEPTHWISE_CONV_2D', 'FULLY_CONNECTED', 'PAD', ...`.
So I was wondering what would be the correct sequence of operations that would be recognized as an SVDF op after converting to TFLite.",hi thank response actually main doubt precisely sequence would lead fusion converting already tried research implementation implementation low latency model found tutorial repository one adapt code use equivalent used implementation none got fused operator converting fact listing set get separate wondering would correct sequence would converting,issue
1966326209,"In release build, we can observe unstable intermediate tensor with invalid dim([gist](https://colab.research.google.com/drive/11NlRWUTovteiN6JNPpGVK2E1zAkHLz4a?usp=sharing)).
```Python
import tensorflow as tf

tf.compat.v1.disable_eager_execution()

x = tf.raw_ops.ExtractImagePatches(
    images=tf.random.normal([1,1,1,1]),
    ksizes=[1,-1,2,1],
    strides=[1,1,1,1],
    rates=[1,1,1,1],
    padding=""VALID"",
    name=None
)

# Prints ""True""
print(tf.is_tensor(x))

# Ends up with ""ValueError: Dimension -2 must be >= 0""
print(x)
```",release build observe unstable intermediate tensor invalid dim gist python import valid true print dimension must print,issue
1966241839,"Note that many other APIs have same problem because `SetOutputToSizedImage`(a function where error location resides) is a common subroutine of APIs in [`image_ops.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/image_ops.cc).
I was able to find below APIs have same behavior.
`tf.raw_ops.QuantizedResizeBilinear`
`tf.raw_ops.ResizeArea`
`tf.raw_ops.ResizeBicubic`
`tf.raw_ops.ResizeBilinear`
`tf.raw_ops.ResizeNearestNeighbor`
`tf.raw_ops.ScaleAndTranslate`
",note many problem function error location common able find behavior,issue
1966224788,"@menayemeskele Could you please fill the above template with relevant information?
Please refer to this [doc](https://www.tensorflow.org/guide/keras)   for building and training the neural networks.
Thank you!",could please fill template relevant information please refer doc building training neural thank,issue
1966221698,"In release build, we can observe unstable intermediate tensor with invalid dim([gist](https://colab.research.google.com/drive/1R454RoZHPVs_ax3WFd-tVEHHp5GHCvpz?usp=sharing)).
```Python
import tensorflow as tf

tf.compat.v1.disable_eager_execution()

x = tf.raw_ops.ExtractGlimpse(
    input=tf.random.normal([1,1,1,1]),
    size=[-2,-2],
    offsets=tf.random.normal([1,2]),
    centered=True,
    normalized=True,
    uniform_noise=True,
    noise='uniform',
    name=None
)

# Prints ""True""
print(tf.is_tensor(x))

# Ends up with ""ValueError: Dimension -2 must be >= 0""
print(x)
```",release build observe unstable intermediate tensor invalid dim gist python import true print dimension must print,issue
1966217306,"@QuantumCoder4 You're right. The tf.math.sigmoid function outputs NaN when it receives an infinity complex tensor as input. This behavior is expected and aligns with the mathematical properties of the sigmoid function.
Thank you!",right function nan infinity complex tensor input behavior mathematical sigmoid function thank,issue
1966209698,"@lgemc Please ensure that you're using TensorFlow version 2.16.0 or higher, as it includes better M1 compatibility and might resolve the XLA compilation error.
Thank you!",please ensure version higher better compatibility might resolve compilation error thank,issue
1966204950,"@VictorDominguite TensorFlow doesn't have a dedicated SVDF layer in Keras, you can achieve a similar functionality by manually defining the sequence of operations and convert them to TFLite, where the SVDF fusion might occur.
Thank you!",layer achieve similar functionality manually sequence convert fusion might occur thank,issue
1966187691,"In release build, we can observe an unstable intermediate tensor with invalid dims([gist](https://colab.research.google.com/drive/1xW-D_MJjE-WBbL5pn_SuqP-7mZjgkGLV?usp=sharing)).
```Python
import tensorflow as tf

tf.compat.v1.disable_eager_execution()

x = tf.raw_ops.DecodeAndCropJpeg(
    contents=""abc"",
    crop_window=[0,0,0,-2],
    channels=0,
    ratio=1,
    fancy_upscaling=True,
    try_recover_truncated=False,
    acceptable_fraction=1,
    dct_method='',
    name=None
)

# Prints ""True""
print(tf.is_tensor(x))

# Ends up with ""ValueError: Dimension -2 must be >= 0""
print(x)
```",release build observe unstable intermediate tensor invalid gist python import true print dimension must print,issue
1966112248,"@Milehigh-wrld,
Could you please confirm if the issue is resolved. if yes, please feel free to move this issue to closed status. Thank you!",could please confirm issue resolved yes please feel free move issue closed status thank,issue
1966107339,"Hi @SuryanarayanaY ,

In fact I'm searching for **trilinear interpolation**, that's why I mentioned [this interpolation function](https://www.tensorflow.org/graphics/api_docs/python/tfg/math/interpolation/trilinear/interpolate). [UpSampling3D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling3D) simply does nearest-neighbor interpolation.",hi fact searching trilinear interpolation interpolation function simply interpolation,issue
1966066419,"Hi @RocaVincent ,

You can use Upsampling layers for it for eg please refer [Upsampling3D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling3D) API.

```
input_image=x
tf.keras.layers.UpSampling3D(size=2)(x)
```

If x is of shape (W,H,D), then the above code will upsample it to (2W,2H,2D).",hi use please refer shape code,issue
1965981179,"Glad to see all you guys solve the problem, but I have another question.

How can I use the profile tool of tensorboard, instead of chrome://tracing, to analyze my model? Now, I am using profiler hook in an estimator and when I try to profile the model in tensorboard, ""No profile data could be found"" will be raised.",glad see solve problem another question use profile tool instead chrome analyze model profiler hook estimator try profile model profile data could found raised,issue
1965961403,"Thank you for the clarification 💯👍

On Mon, Feb 26, 2024 at 11:12 PM tilakrayal ***@***.***>
wrote:

> @Milehigh-wrld <https://github.com/Milehigh-wrld>,
> Without the reproducible code, it would be difficult for us to debug the
> issue. In order to expedite the trouble-shooting process, could you please
> provide a minimal code snippet and the TensorFlow version you are using.
>
> Also if you are trying to get the information on the tensorflow lite, I
> request to have a look at this official document for reference.
> https://www.tensorflow.org/lite/guide
>
> TensorFlow Lite is a set of tools that enables on-device machine learning
> by helping developers run their models on mobile, embedded, and edge
> devices.
>
> Thank you!
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorflow/tensorflow/issues/63053#issuecomment-1965918681>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/BFGS7JIG5OSULB3LMDGZSL3YVWBM7AVCNFSM6AAAAABDZ27ZCSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSNRVHEYTQNRYGE>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
",thank clarification mon wrote without reproducible code would difficult u issue order expedite process could please provide minimal code snippet version also trying get information lite request look official document reference lite set machine learning helping run mobile edge thank reply directly view id,issue
1965918681,"@Milehigh-wrld,
Without the reproducible code, it would be difficult for us to debug the issue. In order to expedite the trouble-shooting process, could you please provide a minimal code snippet and the TensorFlow version you are using.

Also if you are trying to get the information on the tensorflow lite, I request to have a look at this official document for reference.
https://www.tensorflow.org/lite/guide

TensorFlow Lite is a set of tools that enables on-device machine learning by helping developers run their models on mobile, embedded, and edge devices.

Thank you!",without reproducible code would difficult u issue order expedite process could please provide minimal code snippet version also trying get information lite request look official document reference lite set machine learning helping run mobile edge thank,issue
1965865666,"Hi @QuantumCoder4 ,

I have replicated the reported behaviour and attached [gist](https://colab.research.google.com/gist/SuryanarayanaY/8a29b771b8b907b5a9b2f53fba16bfa5/63052.ipynb) for reference.

IMO, `inf-inf `should be `Nan` as both inf can be anything and their difference could be anything from -inf to +inf. I have checked this behaviour with numpy and observed `np.inf-np.inf = Nan`. In TF, `tf.linalg.det([[np.inf, np.inf], [1, 1]])=0 `also doesn't seem right IMO.

`np.inf-1=np.inf` make sense to me.In TF, `tf.linalg.det([np.inf, 1], [1, 1]])=nan` should be `inf` IMO.

However this behaviour with inf may be different in different frameworks. Let's hear from Devteam. Thanks!",hi replicated behaviour attached gist reference nan anything difference could anything checked behaviour nan also seem right make sense however behaviour may different different let hear thanks,issue
1965834881,"Hi **@niemiaszek** ,

Sorry for the delay, I will share another approach with you now is to use tf.signal.frame, which is designed for creating frames of a signal (which can be your audio data or melspectrogram data) with specified frame length and step. This function will be suitable for creating a strided sliding window.

Here you are encountering error is due to trying to use python's native iteration and slicing mechanisms directly on tensorflow tensors within a graph-executed context such as within @tf.function or when using tf.data.Dataset.map.

TensorFlow's execution model doesn't allow for Python-level iteration over tensors directly because the shapes and operations need to be graph-compile-time determinable for performance and distributability.

Here's how you can adjust your function to use tf.signal.frame for creating a sliding window over a 1D tensor (like your audio data). If your tensor has more dimensions (like your MelSpectrogram [time, freq]), you may need to adjust the approach slightly, but the principle remains the same:

Here I am providing [gist](https://colab.sandbox.google.com/gist/Venkat6871/6e9d8b31f110c495c7c5ffa1abd3955a/62526_2-15-nightly-v.ipynb) for your reference.

This code uses tf.signal.frame to create the sliding window over the tensor. Note that tf.signal.frame expects at least a 2D tensor, so if you're working with a 1D tensor (like an audio waveform), you might need to expand its dimensions first. After processing, you can optionally squeeze the tensor to remove any unwanted dimensions.

Thank you!",hi sorry delay share another approach use designed signal audio data data frame length step function suitable sliding window error due trying use python native iteration slicing directly within context within execution model allow iteration directly need determinable performance adjust function use sliding window tensor like audio data tensor like time may need adjust approach slightly principle remains providing gist reference code create sliding window tensor note least tensor working tensor like audio might need expand first optionally squeeze tensor remove unwanted thank,issue
1965658003,"> @fergushenderson I have just tried tensorflow-lite version 2.15.0. Now I am getting the following compilation error:
> 
> ```
> > Failed to transform guice-5.1.0.jar (com.google.inject:guice:5.1.0) to match attributes {artifactType=android-dex, asm-transformed-variant=NONE, dexing-enable-desugaring=true, dexing-enable-jacoco-instrumentation=false, dexing-is-debuggable=true, dexing-min-sdk=21, org.gradle.category=library, org.gradle.libraryelements=jar, org.gradle.status=release, org.gradle.usage=java-runtime}.
>       > Execution failed for DexingWithClasspathTransform: /Users/nijatahmadli/.gradle/caches/modules-2/files-2.1/com.google.inject/guice/5.1.0/da25056c694c54ba16e78e4fc35f17fc60f0d1b4/guice-5.1.0.jar.
>          > Error while dexing.
>            Increase the minSdkVersion to 26 or above.
> ```
> 
> Looks like `guice` was introduced in 2.15.0 and requires minSdkVersion 26 which I guess stems from the use of Java 8 language features. Unfortunately, upgrading to 26 is not an option for our project.

Same problem here. ",tried version getting following compilation error transform jar match execution jar error increase like guess use language unfortunately option project problem,issue
1965471736,"ldd fused_bias_act_c7a32bd65776e03679106c6556928c8c.so 
	linux-vdso.so.1 (0x00007ffc6f256000)
	_pywrap_tensorflow_internal.so => not found
	libstdc++.so.6 => /home/chengjun/anaconda3/lib/libstdc++.so.6 (0x00007f190d600000)
	libgcc_s.so.1 => /home/chengjun/anaconda3/lib/libgcc_s.so.1 (0x00007f190d9df000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f190d200000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f190dad6000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f190d8f8000)

I use ldd and find there is nothing linked with _pywrap_tensorflow_internal.so, I don't know whether the problems is about this or not.

By the way, I find I have this file 
which is in /home/chengjun/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so",found use find nothing linked know whether way find file,issue
1965353241,"@pkgoogle Yes, now things make more sense to me. Still, it leaves me wondering why I'm getting the ""non-converted operations"" message even in this situation where I'm not using any quantization (based on your reply, this message should be related to quantization conversion).
Still, this question is subsidiary, and the main question remains: If the message is valid and some operations are actually not getting converted, why is TFLite not able to convert them in such a simple model?
Looking forward to its answer.",yes make sense still leaf wondering getting message even situation quantization based reply message related quantization conversion still question subsidiary main question remains message valid actually getting converted able convert simple model looking forward answer,issue
1965152059,"LakshmiKalaKadali, 
I have discovered a great deal about my issue, but I have not solved it. Please provide guidance:
**For clarity - I am running the example code for the Sparkfun Edge provided in the tensorflow repository
""tensorflow/tensorflow/lite/experimental/micro/examples/micro_speech"".** 

1) Upon compiling the test example code with the newly trained model, the first error encountered was ""Segmentation fault (core dumped)""
This suggests that the first problem is incorrectly allocated memory. You can see the error messages in paragraph A below.
Although I still get the opcode RESHAPE error and the dims errors. I believe these to be related to the memory allocation error.

2) I then used the tflite quantized model (not the micro model, but the tflite model) and ran it through the TFLite model analyzer.
I believe it shows that the model has the correct shape for all tensors - this is shown in paragraph B below

3) I was not able to use the TFLite Micro benchmarking tool you recommended. I found several references on Google, but was not able to apply them.
Could you direct me to a location and an example?

4) I also played with the arena size. When I changed it to 50, I got a segmentation error and similar ""dims"" results. 
However, when I changed the arena size to 20, I still got the segmentation error, but the ""dims"" errors changed to more ""normal"" numbers
as shown in paragraph C below.

Again, I would appreciate guidance,

Lonnie

A) When compiling with newly trained model from the Colab page, the compile fails with the following info at the end of the compiler messages: 
 *************************  
tensorflow/lite/experimental/micro/testing/test_linux_binary.sh tensorflow/lite/experimental/micro/tools/make/gen/linux_x86_64/bin/micro_speech_test '~~~ALL TESTS PASSED~~~'
Segmentation fault (core dumped)
tensorflow/lite/experimental/micro/tools/make/gen/linux_x86_64/bin/micro_speech_test: FAIL - '~~~ALL TESTS PASSED~~~' not found in logs.
Testing TestInvoke
Didn't find op for builtin opcode 'RESHAPE' version '1'

Failed to get registration from op code  d
 
4 == input->dims->size failed at tensorflow/lite/experimental/micro/examples/micro_speech/micro_speech_test.cc:77 (4 vs 1584022240)
1 == input->dims->data[0] failed at tensorflow/lite/experimental/micro/examples/micro_speech/micro_speech_test.cc:78 (1 vs 32677)
49 == input->dims->data[1] failed at tensorflow/lite/experimental/micro/examples/micro_speech/micro_speech_test.cc:79 (49 vs 1583772544)
40 == input->dims->data[2] failed at tensorflow/lite/experimental/micro/examples/micro_speech/micro_speech_test.cc:80 (40 vs 32677)
1 == input->dims->data[3] failed at tensorflow/lite/experimental/micro/examples/micro_speech/micro_speech_test.cc:81 (1 vs 1583773840)
kTfLiteUInt8 == input->type failed at tensorflow/lite/experimental/micro/examples/micro_speech/micro_speech_test.cc:82 (3 vs 1540943972)
Segmentation fault
make: *** [tensorflow/lite/experimental/micro/examples/micro_speech/Makefile.inc:246: test_micro_speech_test] Error 1
*********************

B) I also ran the TF Lite model analyzer on the tflite model (not the micro model) and it showed the following:
=== TFLite ModelAnalyzer ===

Your TFLite model has '1' subgraph(s). In the subgraph description below,
T# represents the Tensor numbers. For example, in Subgraph#0, the RESHAPE op takes
tensor #0 and tensor #1 as input and produces tensor #7 as output.

Subgraph#0 main(T#0) -> [T#11]
  Op#0 RESHAPE(T#0, T#1[-1, 49, 40, 1]) -> [T#7]
  Op#1 CONV_2D(T#7, T#6, T#5[-19, 14, -425, 39, 0, ...]) -> [T#8]
  Op#2 RESHAPE(T#8, T#2[-1, 4000]) -> [T#9]
  Op#3 FULLY_CONNECTED(T#9, T#4, T#3[469, -685, 534, -317]) -> [T#10]
  Op#4 SOFTMAX(T#10) -> [T#11]

Tensors of Subgraph#0
  T#0(Reshape_1) shape:[1, 1960], type:INT8
  T#1(Reshape_2/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 2, data:[-1, 49, 40, 1]
  T#2(Reshape_3/shape) shape:[2], type:INT32 RO 8 bytes, buffer: 3, data:[-1, 4000]
  T#3(final_fc_bias) shape:[4], type:INT32 RO 16 bytes, buffer: 4, data:[469, -685, 534, -317]
  T#4(MatMul) shape:[4, 4000], type:INT8 RO 16000 bytes, buffer: 5, data:[., ., ., ., ., ...]
  T#5(first_bias) shape:[8], type:INT32 RO 32 bytes, buffer: 6, data:[-19, 14, -425, 39, 0, ...]
  T#6(Conv2D) shape:[8, 10, 8, 1], type:INT8 RO 640 bytes, buffer: 7, data:[., ., ., ., ., ...]
  T#7(Reshape_2) shape:[1, 49, 40, 1], type:INT8
  T#8(Relu;add;Conv2D;first_bias) shape:[1, 25, 20, 8], type:INT8
  T#9(Reshape_3) shape:[1, 4000], type:INT8
  T#10(MatMul;add_1) shape:[1, 4], type:INT8
  T#11(labels_softmax) shape:[1, 4], type:INT8

---------------------------------------------------------------
              Model size:      18960 bytes
    Non-data buffer size:       2160 bytes (11.39 %)
  Total data buffer size:      16800 bytes (88.61 %)
    (Zero value buffers):          0 bytes (00.00 %)

* Buffers of TFLite model are mostly used for constant tensors.
  And zero value buffers are buffers filled with zeros.
  Non-data buffers area are used to store operators, subgraphs and etc.
  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs
===================================================

C)
tensorflow/lite/experimental/micro/testing/test_linux_binary.sh tensorflow/lite/experimental/micro/tools/make/gen/linux_x86_64/bin/micro_speech_test '~~~ALL TESTS PASSED~~~'
Segmentation fault (core dumped)
tensorflow/lite/experimental/micro/tools/make/gen/linux_x86_64/bin/micro_speech_test: FAIL - '~~~ALL TESTS PASSED~~~' not found in logs.
Testing TestInvoke
Didn't find op for builtin opcode 'RESHAPE' version '1'

Failed to get registration from op code  d
 
4 == input->dims->size failed at tensorflow/lite/experimental/micro/examples/micro_speech/micro_speech_test.cc:77 (4 vs 0)
1 == input->dims->data[0] failed at tensorflow/lite/experimental/micro/examples/micro_speech/micro_speech_test.cc:78 (1 vs 0)
49 == input->dims->data[1] failed at tensorflow/lite/experimental/micro/examples/micro_speech/micro_speech_test.cc:79 (49 vs 0)
40 == input->dims->data[2] failed at tensorflow/lite/experimental/micro/examples/micro_speech/micro_speech_test.cc:80 (40 vs 4)
1 == input->dims->data[3] failed at tensorflow/lite/experimental/micro/examples/micro_speech/micro_speech_test.cc:81 (1 vs 0)
kTfLiteUInt8 == input->type failed at tensorflow/lite/experimental/micro/examples/micro_speech/micro_speech_test.cc:82 (3 vs -905926320)
Segmentation fault
make: *** [tensorflow/lite/experimental/micro/examples/micro_speech/Makefile.inc:246: test_micro_speech_test] Error 1
",discovered great deal issue please provide guidance clarity running example code edge provided repository upon test example code newly trained model first error segmentation fault core first problem incorrectly memory see error paragraph although still get reshape error believe related memory allocation used model micro model model ran model analyzer believe model correct shape shown paragraph able use micro tool found several able apply could direct location example also arena size got segmentation error similar however arena size still got segmentation error normal shown paragraph would appreciate guidance newly trained model page compile following end compiler segmentation fault core fail found testing find version get registration code size data data data data type segmentation fault make error also ran lite model analyzer model micro model following model description tensor example reshape tensor tensor input tensor output main reshape reshape shape type shape type buffer data shape type buffer data shape type buffer data shape type buffer data shape type buffer data shape type buffer data shape type add shape type shape type shape type shape type model size buffer size total data buffer size zero value model mostly used constant zero value filled area used store find segmentation fault core fail found testing find version get registration code size data data data data type segmentation fault make error,issue
1965035991,"There is no legitimate use-case for passing in a bool.  As with numpy, bools are a form of integer type, so it is being interpreted as such.  There shouldn't be any confusion for legitimate use-cases.",legitimate passing bool form integer type confusion legitimate,issue
1965031743,"I don't think we should clutter our documentation with statements like this.  Using a bool won't cause a crash, and shouldn't  cause confusion to a legitimate use-case.  This is just a case of someone bombarding the API with different inputs to see what happens.",think clutter documentation like bool wo cause crash cause confusion legitimate case someone different see,issue
1964971472,"Hi @Moddingear, can you let me know what setup/installation you have performed as well as the commands which are running into issues? i.e. Have you tried compiling? Are you only using C++? Which task library are you trying to use? If you can share with us your steps & files, we can probably help you better. Thanks for your help.

Does this minimal example work for you? https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal

Please also review this https://coral.ai/docs/edgetpu/tflite-cpp/#run-an-inference-with-the-libcoral-api and see if that works for you or not.",hi let know well running tried task library trying use share u probably help better thanks help minimal example work please also review see work,issue
1964922550,"Hi @yishuangP, can you please take a look? Thanks.",hi please take look thanks,issue
1964903653,"Hi @Black3rror, if that is the configuration of your conversion then, you are correct, you should not expect any loss of precision, however with any quantization techniques included -- then of course there is a potential loss. Non-converted ops are by definition not going to go through any quantization, so they will maintain precision and still ""work"" in the sense that you can still run inference through the model. Hope that clarifies my answer?",hi configuration conversion correct expect loss precision however quantization included course potential loss definition going go quantization maintain precision still work sense still run inference model hope answer,issue
1964880087,@cantonios This is version 2 of the PR; should have fixed the ARM64 build issue that arose in overnight testing from the last version.,version fixed arm build issue arose overnight testing last version,issue
1964851619,"Python 3.9 same issue:
```
Python 3.9.18 (main, Jan  4 2024, 00:00:00) 
[GCC 11.4.1 20230605 (Red Hat 11.4.1-2)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2024-02-26 13:24:06.607339: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-26 13:24:06.609051: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-26 13:24:06.645912: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-26 13:24:06.646284: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
>>> print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))
2024-02-26 13:24:09.084670: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
Num GPUs Available:  0
```

```
pip show tensorflow
Name: tensorflow
Version: 2.13.0
```
```
pip show tf-nightly
Name: tf-nightly
Version: 2.16.0
```


",python issue python main red hat type help copyright license information import custom may see slightly different numerical due different computation turn set environment variable could find machine used could find machine used binary use available enable following rebuild appropriate compiler print available please make sure missing properly would like use follow guide setup platform skipping available pip show name version pip show name version,issue
1964439591,"Sorry for the delay, I'm trying to run this internally to ensure tests pass however a lot of tests fail. I'm not exactly sure how external PRs work versus internal CI/CD runs, but can you get external tests to pass?

I'm also getting errors such as :

```
10135 xla_op_registry.cc:105] Registrations of ResizeNearestNeighborGrad have incompatible compile time constant inputs.
xla_op_registry.cc:606] XLA op registration ResizeNearestNeighborGrad is incompatible with existing registration of the same name.
*** Check failure stack trace: ***

absl::log_internal::LogMessageFatal::~LogMessageFatal()
tensorflow::XlaOpRegistrar::XlaOpRegistrar()
 _GLOBAL__sub_I_image_resize_ops.cc
 __libc_csu_init
__libc_start_main
 _start
``` 

Which sounds like you need to mark the op as having compile time constant inputs.",sorry delay trying run internally ensure pas however lot fail exactly sure external work versus internal get external pas also getting incompatible compile time constant registration incompatible registration name check failure stack trace like need mark compile time constant,issue
1964294609,"The documentation of tf.argsort not mentioning any dtype of the argument axis explicitly.IMO, we can leave this as it is.


axis | The axis along which to sort. The default is -1, which sorts the last axis. 
-- | --


",documentation argument axis leave axis axis along sort default last axis,issue
1963982084,"@Sehun0819,

I tried to execute the mentioned code with tensorflow official builds(default) and the code execution was successful with Tf2.15 and tf-nightly. Also the output is also displayed.
Kindly find the [gist](https://colab.research.google.com/gist/tilakrayal/2259609ffe3e68eee0a666ad1faea3f3/untitled1740.ipynb) for the reference.

Also this might be an issue with debug builds. We will check and provide more information. Thank you!",tried execute code official default code execution successful also output also displayed kindly find gist reference also might issue check provide information thank,issue
1963981973,"@Sehun0819,

I tried to execute the mentioned code with tensorflow official builds(default) and the code execution was successful with Tf2.15 and tf-nightly 
Kindly find the [gist](https://colab.research.google.com/gist/tilakrayal/02745a7869988c873a41d08ea7bf2984/untitled1739.ipynb) for the reference.

Also this might be an issue with debug builds. We will check and provide more information. Thank you!",tried execute code official default code execution successful kindly find gist reference also might issue check provide information thank,issue
1963805863,"> Hi @lgeiger Can you please check @changm's comments and keep us posted ? Thank you!

I think all comments have been addressed from my side, just awaiting approval from @changm

",hi please check keep u posted thank think side approval,issue
1963749439,"Hi @rdzhabarov, can you review this PR please? Thanks!",hi review please thanks,issue
1963732268,Same issue. Colab crash without any error..,issue crash without error,issue
1963681608,"Alright, had the time to update my system to CUDA 12.3, and recompile tensorflow 2.14 (with CUDA 12.2), and the result is correct:
```
>>> tf.sysconfig.get_build_info()
OrderedDict([('cpu_compiler', '/usr/bin/x86_64-linux-gnu-gcc-11'), ('cuda_compute_capabilities', ['sm_61', 'sm_75', 'compute_86']), ('cuda_version', '12.2'), ('cudnn_version', '8'), ('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', False)])
>>> tf.reduce_sum(tf.math.bincount(tf.range(50000)))
<tf.Tensor: shape=(), dtype=int32, numpy=50000>
```

It seems the problem is cuda related, and possibly cuda-caused. Don't know if there's any reason to keep this issue open.",alright time update system recompile result correct true false false problem related possibly know reason keep issue open,issue
1963642680,"Hi @ek-ex ,

Instead of applying flat_map directly on a WindowDataset could you please try batch seprately.

```
dataset = tf.data.experimental.make_csv_dataset(
    file_pattern=""/path/stock/*1min*.csv"",
    batch_size=1,
    num_epochs=1,
    shuffle=False,
    header=False,
    column_names=['timestamp','open','high', 'low', 'close', 'volume'],
    column_defaults=[tf.string, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32]
).window(
    size=5,  # Number of rows per window
    shift=1,  # Stride for overlapping windows
    stride=1
)
dataset=dataset.batch(5)
```",hi instead directly could please try batch min number per window stride,issue
1963627616,"Hi,

The problem has been resolved

Get Outlook for iOS<https://aka.ms/o0ukef>
________________________________
From: Venkat6871 ***@***.***>
Sent: Monday, February 26, 2024 5:04:19 PM
To: tensorflow/tensorflow ***@***.***>
Cc: JASLYN ONG CHING WEN ***@***.***>; Mention ***@***.***>
Subject: Re: [tensorflow/tensorflow] Alteryx Predict (11) : Unexpected Error Occur In Plugin (Issue #62806)


CAUTION: This email originated from outside of SIT. Do not click links or open attachments unless you recognise the sender and know the content is safe.


Hi @2302187<https://github.com/2302187> ,

Could you please confirm if this issue is resolved for you ? Please feel free to close the issue if it is resolved.

Thank you!

—
Reply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/62806#issuecomment-1963623605>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/BFM5QXGK2ZAFLHJLIADJ4XTYVRFZHAVCNFSM6AAAAABB57PRW6VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSNRTGYZDGNRQGU>.
You are receiving this because you were mentioned.Message ID: ***@***.***>

________________________________

Notice: This e-mail may contain confidential and/or privileged information. If you are not the intended recipient or have received this e-mail in error, please notify the sender immediately and destroy this e-mail. Any unauthorized copying, disclosure or distribution of the material in this e-mail is strictly forbidden.
Singapore Institute of Technology, Co. Reg. No. 200917667D.
",hi problem resolved get outlook sent ching wen mention subject predict unexpected error occur issue caution outside sit click link open unless sender know content safe hi could please confirm issue resolved please feel free close issue resolved thank reply directly view id notice may contain confidential privileged information intended recipient received error please notify sender immediately destroy unauthorized disclosure distribution material strictly forbidden institute technology reg,issue
1963623605,"Hi **@2302187** ,

Could you please confirm if this issue is resolved for you ? Please feel free to close the issue if it is resolved.

Thank you!",hi could please confirm issue resolved please feel free close issue resolved thank,issue
1963477615,"AMD ROCm -- Community CI Build — rocm CI build failure was caused by `unrecognized gcc command line option`, which shouldn't be caused by my patch.
<img width=""1147"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/19923746/b9223db6-7a6b-4053-b9ba-3ad488b21260"">
",community build build failure unrecognized command line option patch image,issue
1963435798,"@drewshark, Please note that the API tf.argmin accepts axis as Tensor unlike argmax which accepts integer. If you pass a boolean Tensor for `tf.argmin` then it will raise error below.
`InvalidArgumentError: Value for attr 'Tidx' of bool is not in the list of allowed values: int32, int64`

 However if we pass python boolean then its accepting same.This behaviour also different and needs a fix IMO.",please note axis tensor unlike integer pas tensor raise error value bool list however pas python behaviour also different need fix,issue
1963428550,"Hi @vnanaware111,

As mentioned in the earlier post, in this [link](https://github.com/tensorflow/examples/tree/master/lite/examples/pose_estimation/ios), pose estimation tflite example is given. There is no pretrained model for hand pose detection available. For your usecase you can customize the pose detection example. For other sources of hand pose detection please refer [link1](https://developers.google.com/mediapipe/solutions/vision/hand_landmarker/ios
), [link2](https://mediapipe-studio.webapps.google.com/studio/demo/hand_landmarker).

Thank You",hi post link pose estimation example given model hand pose detection available pose detection example hand pose detection please refer link link thank,issue
1963426617,"@sushreebarsa 
Hi! Even there exist alternatives this should be patched to avoid crash right?
Please check python version of [reproduction](https://colab.research.google.com/drive/1R11FohcZZ-Ky1rpO-bJPOcztHdRIQE5R?usp=sharing).
```Python
import tensorflow as tf

tf.raw_ops.MatrixDiagV3(
    diagonal=tf.constant([0.0],dtype=tf.float32),
    k=tf.constant([1,2],shape=[2],dtype=tf.int32),
    num_rows=1,
    num_cols=1,
    padding_value=0.0,
    align='RIGHT_LEFT',
    name=None
)
```",hi even exist avoid crash right please check python version reproduction python import,issue
1963419718,"@sachinprasadhs Hi, I was wondering if there are any updates regarding this issue. Thank you!",hi wondering regarding issue thank,issue
1963401781,"Hi @drewshark ,

The mentioned APIs also works for `bool` data types for `axis` argument even though it's not explicitly mentioned in the documentation.This may be true for some of other APIs also.May be we can add a note in the documentation regarding bool as accepted datatype.Attached below is the line of code where `argmax_op` is registered for bool data type.

https://github.com/tensorflow/tensorflow/blob/216a1a0d4633a068fa671252541305438b353954/tensorflow/core/kernels/argmax_op.cc#L173

When it comes to string ,the API tf.argsort will try to convert this into a numeric in the line of code. This may be due to the reason that mentioned in the comment in the same line.If the string contains chars other than numbers then it will raise intended error.Only numerics in the form of strings are accepted here and these values also validated after conversion into integers.Attached [gist](https://colab.research.google.com/gist/SuryanarayanaY/1390af7af4ac75698a542269d84c0d77/63031.ipynb) for reference.

`
axis_static = int(axis_static)  # Avoids NumPy casting error
 `",hi also work bool data axis argument even though explicitly may true add note documentation regarding bool accepted line code registered bool data type come string try convert line code may due reason comment string raise intended form accepted also conversion gist reference casting error,issue
1963377035,"I solve this problem by editing the **datasetLoader.py** script directly.
To load grayscale bmp image correctly, I add a condition in `image_dataset_from_directory` function
![63008_1](https://github.com/tensorflow/tensorflow/assets/98000114/51dc3b75-1894-4440-96f4-d115d5078a2f)

To keep the output shape, I add a condition in `load_image` function
![63008_2](https://github.com/tensorflow/tensorflow/assets/98000114/5a1b8107-440c-4d22-8c20-b13016d18942)

Here is the entire script, hope it helps.
```
def image_dataset_from_directory(
    directory,
    labels=""inferred"",
    label_mode=""int"",
    class_names=None,
    color_mode=""rgb"",
    batch_size=32,
    image_size=(256, 256),
    shuffle=True,
    seed=None,
    validation_split=None,
    subset=None,
    interpolation=""bilinear"",
    follow_links=False,
    crop_to_aspect_ratio=False,
    **kwargs,
):
    
    if ""smart_resize"" in kwargs:
        crop_to_aspect_ratio = kwargs.pop(""smart_resize"")
    if kwargs:
        raise TypeError(f""Unknown keywords argument(s): {tuple(kwargs.keys())}"")
    if labels not in (""inferred"", None):
        if not isinstance(labels, (list, tuple)):
            raise ValueError(
                ""`labels` argument should be a list/tuple of integer labels, ""
                ""of the same size as the number of image files in the target ""
                ""directory. If you wish to infer the labels from the ""
                ""subdirectory ""
                'names in the target directory, pass `labels=""inferred""`. '
                ""If you wish to get a dataset that only contains images ""
                f""(no labels), pass `labels=None`. Received: labels={labels}""
            )
        if class_names:
            raise ValueError(
                ""You can only pass `class_names` if ""
                f'`labels=""inferred""`. Received: labels={labels}, and '
                f""class_names={class_names}""
            )
    if label_mode not in {""int"", ""categorical"", ""binary"", None}:
        raise ValueError(
            '`label_mode` argument must be one of ""int"", '
            '""categorical"", ""binary"", '
            f""or None. Received: label_mode={label_mode}""
        )
    if labels is None or label_mode is None:
        labels = None
        label_mode = None

    if color_mode == ""rgb"":
        num_channels = 3
    elif color_mode == ""rgba"":
        num_channels = 4
    elif color_mode == ""grayscale"":
        num_channels = 1
    elif color_mode == ""grayscale_bmp"":
        num_channels = 0
    else:
        raise ValueError(
            '`color_mode` must be one of {""rgb"", ""rgba"", ""grayscale""}. '
            f""Received: color_mode={color_mode}""
        )
    interpolation = image_utils.get_interpolation(interpolation)
    dataset_utils.check_validation_split_arg(
        validation_split, subset, shuffle, seed
    )

    if seed is None:
        seed = np.random.randint(1e6)
    image_paths, labels, class_names = dataset_utils.index_directory(
        directory,
        labels,
        formats=ALLOWLIST_FORMATS,
        class_names=class_names,
        shuffle=shuffle,
        seed=seed,
        follow_links=follow_links,
    )

    if label_mode == ""binary"" and len(class_names) != 2:
        raise ValueError(
            'When passing `label_mode=""binary""`, there must be exactly 2 '
            f""class_names. Received: class_names={class_names}""
        )

    if subset == ""both"":
        (
            image_paths_train,
            labels_train,
        ) = dataset_utils.get_training_or_validation_split(
            image_paths, labels, validation_split, ""training""
        )
        (
            image_paths_val,
            labels_val,
        ) = dataset_utils.get_training_or_validation_split(
            image_paths, labels, validation_split, ""validation""
        )
        if not image_paths_train:
            raise ValueError(
                f""No training images found in directory {directory}. ""
                f""Allowed formats: {ALLOWLIST_FORMATS}""
            )
        if not image_paths_val:
            raise ValueError(
                f""No validation images found in directory {directory}. ""
                f""Allowed formats: {ALLOWLIST_FORMATS}""
            )
        train_dataset = paths_and_labels_to_dataset(
            image_paths=image_paths_train,
            image_size=image_size,
            num_channels=num_channels,
            labels=labels_train,
            label_mode=label_mode,
            num_classes=len(class_names),
            interpolation=interpolation,
            crop_to_aspect_ratio=crop_to_aspect_ratio,
        )
        val_dataset = paths_and_labels_to_dataset(
            image_paths=image_paths_val,
            image_size=image_size,
            num_channels=num_channels,
            labels=labels_val,
            label_mode=label_mode,
            num_classes=len(class_names),
            interpolation=interpolation,
            crop_to_aspect_ratio=crop_to_aspect_ratio,
        )
        train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)
        val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)

        if batch_size is not None:
            if shuffle:
                # Shuffle locally at each iteration
                train_dataset = train_dataset.shuffle(
                    buffer_size=batch_size * 8, seed=seed
                )
            train_dataset = train_dataset.batch(batch_size)
            val_dataset = val_dataset.batch(batch_size)
        else:
            if shuffle:
                train_dataset = train_dataset.shuffle(
                    buffer_size=1024, seed=seed
                )

        # Users may need to reference `class_names`.
        train_dataset.class_names = class_names
        val_dataset.class_names = class_names

        # Include file paths for images as attribute.
        train_dataset.file_paths = image_paths_train
        val_dataset.file_paths = image_paths_val
        dataset = [train_dataset, val_dataset]
    else:
        image_paths, labels = dataset_utils.get_training_or_validation_split(
            image_paths, labels, validation_split, subset
        )
        if not image_paths:
            raise ValueError(
                f""No images found in directory {directory}. ""
                f""Allowed formats: {ALLOWLIST_FORMATS}""
            )

        dataset = paths_and_labels_to_dataset(
            image_paths=image_paths,
            image_size=image_size,
            num_channels=num_channels,
            labels=labels,
            label_mode=label_mode,
            num_classes=len(class_names),
            interpolation=interpolation,
            crop_to_aspect_ratio=crop_to_aspect_ratio,
        )
        dataset = dataset.prefetch(tf.data.AUTOTUNE)
        if batch_size is not None:
            if shuffle:
                # Shuffle locally at each iteration
                dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)
            dataset = dataset.batch(batch_size)
        else:
            if shuffle:
                dataset = dataset.shuffle(buffer_size=1024, seed=seed)

        # Users may need to reference `class_names`.
        dataset.class_names = class_names

        # Include file paths for images as attribute.
        dataset.file_paths = image_paths
    return dataset

def load_image(
    path, image_size, num_channels, interpolation, crop_to_aspect_ratio=False
):
    """"""Load an image from a path and resize it.""""""
    img = tf.io.read_file(path)
    img = tf.image.decode_image(img, channels=num_channels, expand_animations=False)
    if crop_to_aspect_ratio:
        img = image_utils.smart_resize(
            img, image_size, interpolation=interpolation
        )
    else:
        img = tf.image.resize(img, image_size, method=interpolation)
    if num_channels == 0:
        img.set_shape((image_size[0], image_size[1], 1))
    else:
        img.set_shape((image_size[0], image_size[1], num_channels))
    return img
```",solve problem script directly load image correctly add condition function keep output shape add condition function entire script hope directory bilinear raise unknown argument none list raise argument integer size number image target directory wish infer target directory pas wish get pas received raise pas received categorical binary none raise argument must one categorical binary none received none none none none else raise must one received interpolation interpolation subset shuffle seed seed none seed directory binary raise passing binary must exactly received subset training validation raise training found directory directory raise validation found directory directory none shuffle shuffle locally iteration else shuffle may need reference include file attribute else subset raise found directory directory none shuffle shuffle locally iteration else shuffle may need reference include file attribute return path interpolation load image path resize path else else return,issue
1963345032,"Hi **@Sehun0819** ,

Sorry for the delay, I tried to run your code on Colab using TF v2.15 and nightly. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/9f60934e6a7e51d8012e1f9214670383/63034_2-15-nightly.ipynb) here for reference.
Thank you!",hi sorry delay tried run code nightly please find gist reference thank,issue
1963331030,"Hello, I'm also struggling with this issue. I'm wondering if you've found a solution?",hello also struggling issue wondering found solution,issue
1963298900,"Hi **@eneskelestemur** ,
Sorry for late reply, I tried to run your code on Colab using TF v2.14 and 2.15. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/71d4dbeea1ad6d3e712ce4554d783763/63021_2-14-v-2-15.ipynb) here for reference. 
Thanks you!",hi sorry late reply tried run code please find gist reference thanks,issue
1963276133,"Hi @Sehun0819 ,

I have tested the code with Official builds(default/non-debug) and code execution success with Tf2.15 and tf-nightly as well.
Attached [gist](https://colab.research.google.com/gist/SuryanarayanaY/5d3218668209d1ff7439cca6739acdd8/63036.ipynb) for reference.

This might be issue with debug builds.Needs to check. Thanks!",hi tested code official code execution success well attached gist reference might issue check thanks,issue
1963260323,"@TimYao18 FYR. 

based on my previous experiences of using Core ML and Metal delegates on iOS devices (https://github.com/mlcommons/mobile_app_open):
1. Core ML uses relatively old version of Core ML and ops supported by it are quite limited
2. Metal delegate doesn't support all the ops either

With that, you are likely to run into ops not supported by them. That is, mostly not the ops are accelerated by either ANE or GPU.",based previous core metal core relatively old version core quite limited metal delegate support either likely run mostly accelerated either,issue
1963245504,"a minimal script to reproduce the issue
```python
import keras
import keras_nlp
import tensorflow as tf

os.environ[""KAGGLE_USERNAME""] = '....'
os.environ[""KAGGLE_KEY""] = '...'
os.environ[""KERAS_BACKEND""] = ""tensorflow"" 

gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(""gemma_2b_en"")
tf.saved_model.save(gemma_lm.backbone, '/tmp/gemma_saved_model/')

f = tf.lite.TFLiteConverter.from_saved_model('/tmp/gemma_saved_model/').convert()

```

I tested with tf-2.15, 2.16, and 2.17 nightly and their corresponding packages. None of them works.",minimal script reproduce issue python import import import tested nightly corresponding none work,issue
1963072968,"Since support here is awful and not able to respond during several months, I will post a solution here, maybe it will be useful for somebody. The problem was that `Ops.create()` creates an internal eager session under the hood and all createdconstants are stored there. To fix it we need to move ops creation inside the loop and create a different session every time for it. like
```
EagerSession eagerSession = EagerSession.create();
Ops ops = Ops.create(eagerSession);
```
with this approach memory is stable and not growing",since support awful able respond several post solution maybe useful somebody problem internal eager session hood fix need move creation inside loop create different session every time like approach memory stable growing,issue
1963066913,"Please CC me on the PR and I'll try to speed it up through the internal systems.

Thank you",please try speed internal thank,issue
1963031890,"I'd be happy to push a PR with the added definition. On Feb 25, 2024 10:11 AM, Mihai Maruseac ***@***.***> wrote:
Yes, that's why I recommended in #63039 to define a default smallish (~2 / 4?) value in case users are not passing one, instead of pinning to nproc.

—Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you authored the thread.Message ID: ***@***.***>",happy push added definition wrote yes define default smallish value case passing one instead pinning directly view id,issue
1962969959,"Yes, that's why I recommended in #63039 to define a default smallish (~2 / 4?) value in case users are not passing one, instead of pinning to `nproc`.",yes define default smallish value case passing one instead pinning,issue
1962912192,"Please any comment to figure out this error?
ImportError: cannot import name 'cast' from partially initialized module 'keras.src.backend' (most likely due to a circular import)",please comment figure error import name partially module likely due circular import,issue
1962900834,"> It sounds like the path to your saved model file is not correct. My guess is you want the full path to `ssd_mobilenet_v1_coco_2018_01_28` rather than a relative path. Can you try the same script with the absolute path specified?

Your suggestion works like a charm.
 for some reason when using relative path it goes to ""C:\wtf\Lib\site-packages\tensorflow\python\saved_model  which is site packages instead of ""C:\wtf\models\research\object_detection\ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8\saved_model\saved_model.pb""

even after being in object_detection folder!
![Uploading image.png…]()


so if used abs path mine looked something like:

python C:\wtf\models\research\object_detection\detect_from_image.py -m C:\wtf\models\research\object_detection\ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8\saved_model -l C:\wtf\models\research\object_detection\data\mscoco_label_map.pbtxt -i C:\wtf\models\research\object_detection\test_images

and it works! Thanks",like path saved model file correct guess want full path rather relative path try script absolute path suggestion work like charm reason relative path go site instead even folder used path mine something like python work thanks,issue
1962890267,"@pkgoogle
Thanks for the response. I'm a bit confused: What was supposed to happen in the course of the conversion? The conversion is basic, i.e., no quantization and just having tflite from tf model. So, since you said ""they have higher precision"", should I expect any loss of precision in this process?!

Also, it might worth mentioning that I've tested different networks with different quantization techniques (including full_int, full_int_only (input and outputs are in int as well), 16x8, ...), and they all worked. I mean, I'm still getting similar non-converted messages to those I asked in the first place, but looking at `interpreter.get_tensor_details()[...][dtype]`, it seems everything is getting converted successfully (unless I'm wrong :)) and putting these models on a microcontroller using TFLM goes well.

Can you please clarify your answer?

Yes, I also believe something is wrong since I'm not able to convert a minimum network without getting such a message, and as you said, at least `arith.constant` shouldn't be the problem.",thanks response bit confused supposed happen course conversion conversion basic quantization model since said higher precision expect loss precision process also might worth tested different different quantization input well worked mean still getting similar first place looking everything getting converted successfully unless wrong go well please clarify answer yes also believe something wrong since able convert minimum network without getting message said least problem,issue
1962882951,"Try quoting the package name 

pip install 'tf-nightly[and-cuda]'",try package name pip install,issue
1962882566,"@LakshmiKalaKadali 

`import keras_nlp.backend import ops 
`
is not needed. Sorry 

But when using all nightly versions, I got some ""GraphDef"" issue ",import import sorry nightly got issue,issue
1962881292,"> @farmaker47
> 
> I able to ran inference already. my problem is, i need to create a TFlite model for Gemma 2B. I think there is some problem still in conversion
> 
> i am very new to AI and even python.

Then we have to wait a little bit so the TF team solves this and provide us the tf-nightly version we can use to convert it.",able ran inference already problem need create model gemma think problem still conversion new ai even python wait little bit team provide u version use convert,issue
1962875569,"@farmaker47 

I able to ran inference already. my problem is, i need to create a TFlite model for Gemma 2B. I think there is some problem still in conversion 

i am very new to AI and even python. ",able ran inference already problem need create model gemma think problem still conversion new ai even python,issue
1962869119,"@farmaker47 

I hope some package conflicts ,like some packages reinstall 'stable' version of tensorflow. Let me check
",hope package like reinstall version let check,issue
1962868553,"The colab is from this example

https://ai.google.dev/gemma/docs/lora_tuning

I have changed nothing. So the idea is if you install tf-nightly the error for conversion disappears? I don't understand from your previous answer if the error is during tf-nightly installation or during conversion.",example nothing idea install error conversion understand previous answer error installation conversion,issue
1962854221,"@farmaker47 

Now, again I am getting that first mentioned error 

could you please share your notebook link ?
",getting first error could please share notebook link,issue
1962848683,"I work with Colab.
So it is

!pip install tf-nightly
!pip install -q --upgrade keras-nlp
!pip install -q -U keras>=3",work pip install pip install upgrade pip install,issue
1962848217,"@farmaker47 

How to install  TensorFlow nightly version? I tried pip install tf-nightly, but I am getting error 

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py"", line 5, in <module>
    from tensorflow.compiler.tf2xla.python.xla import dynamic_update_slice
ModuleNotFoundError: No module named 'tensorflow.compiler.tf2xla'

Name: tf-nightly
Version: 2.17.0.dev20240223",install nightly version tried pip install getting error file line module import module name version dev,issue
1962842090,I saw that training is working OK having installed first TensorFlow nightly version (2.17.0-dev20240223). @RageshAntonyHM can you try with nightly version and check again the conversion?,saw training working first nightly version try nightly version check conversion,issue
1962813716,"This is my make file 
# Copyright (c) 2019, NVIDIA Corporation. All rights reserved.
#
# This work is made available under the Nvidia Source Code License-NC.
# To view a copy of this license, visit
# https://nvlabs.github.io/stylegan2/license.html

""""""TensorFlow custom ops builder.
""""""

import os
import re
import uuid
import hashlib
import tempfile
import shutil
import tensorflow as tf
from tensorflow.python.client import device_lib # pylint: disable=no-name-in-module

#----------------------------------------------------------------------------
# Global options.

cuda_cache_path = os.path.join(os.path.dirname(__file__), '_cudacache')
cuda_cache_version_tag = 'v1'
do_not_hash_included_headers = False # Speed up compilation by assuming that headers included by the CUDA code never change. Unsafe!
verbose = True # Print status messages to stdout.

compiler_bindir_search_path = [
    'C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.14.26428/bin/Hostx64/x64',
    'C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.23.28105/bin/Hostx64/x64',
    'C:/Program Files (x86)/Microsoft Visual Studio 14.0/vc/bin',
]

#----------------------------------------------------------------------------
# Internal helper funcs.
def _find_compiler_bindir():
    for compiler_path in compiler_bindir_search_path:
        if os.path.isdir(compiler_path):
            return compiler_path
    return None

def _get_compute_cap(device):
    caps_str = device.physical_device_desc
    m = re.search('compute capability: (\\d+).(\\d+)', caps_str)
    major = m.group(1)
    minor = m.group(2)
    return (major, minor)

def _get_cuda_gpu_arch_string():
   gpus = [x for x in device_lib.list_local_devices() if x.device_type == 'GPU']
    if len(gpus) == 0:
        raise RuntimeError('No GPU devices found')
    (major, minor) = _get_compute_cap(gpus[0])
    return 'sm_%s%s' % (major, minor)
def _run_cmd(cmd):
    with os.popen(cmd) as pipe:
        output = pipe.read()
        status = pipe.close()
    if status is not None:
        raise RuntimeError('NVCC returned an error. See below for full command line and output log:\n\n%s\n\n%s' % (cmd, output))

def _prepare_nvcc_cli(opts):
    cmd = 'nvcc ' + opts.strip()
    cmd += ' --disable-warnings'
    cmd += ' --include-path ""%s""' % tf.sysconfig.get_include()
    cmd += ' --include-path ""%s""' % os.path.join(tf.sysconfig.get_include(), 'external', 'protobuf_archive', 'src')
    cmd += ' --include-path ""%s""' % os.path.join(tf.sysconfig.get_include(), 'external', 'com_google_absl')
    cmd += ' --include-path ""%s""' % os.path.join(tf.sysconfig.get_include(), 'external', 'eigen_archive')
 compiler_bindir = _find_compiler_bindir()
    if compiler_bindir is None:
        # Require that _find_compiler_bindir succeeds on Windows.  Allow
        # nvcc to use whatever is the default on Linux.
        if os.name == 'nt':
            raise RuntimeError('Could not find MSVC/GCC/CLANG installation on this computer. Check compiler_bindir_search_path list in ""%s"".' % __file__)
    else:
        cmd += ' --compiler-bindir ""%s""' % compiler_bindir
    cmd += ' 2>&1'
    return cmd
  #----------------------------------------------------------------------------
# Main entry point.

_plugin_cache = dict()

def get_plugin(cuda_file):
    cuda_file_base = os.path.basename(cuda_file)
    cuda_file_name, cuda_file_ext = os.path.splitext(cuda_file_base)

    # Already in cache?
    if cuda_file in _plugin_cache:
        return _plugin_cache[cuda_file]

    # Setup plugin.
    if verbose:
        print('Setting up TensorFlow plugin ""%s"": ' % cuda_file_base, end='', flush=True)
    try:
        # Hash CUDA source.
        md5 = hashlib.md5()
        with open(cuda_file, 'rb') as f:
        md5.update(f.read())
        md5.update(b'\n')

        # Hash headers included by the CUDA code by running it through the preprocessor.
        if not do_not_hash_included_headers:
            if verbose:
                print('Preprocessing... ', end='', flush=True)
            with tempfile.TemporaryDirectory() as tmp_dir:
                tmp_file = os.path.join(tmp_dir, cuda_file_name + '_tmp' + cuda_file_ext)
                _run_cmd(_prepare_nvcc_cli('""%s"" --preprocess -o ""%s"" --keep --keep-dir ""%s""' % (cuda_file, tmp_file, tmp_dir)))
                with open(tmp_file, 'rb') as f:
                    bad_file_str = ('""' + cuda_file.replace('\\', '/') + '""').encode('utf-8') # __FILE__ in error check macros
        good_file_str = ('""' + cuda_file_base + '""').encode('utf-8')
                    for ln in f:
                        if not ln.startswith(b'# ') and not ln.startswith(b'#line '): # ignore line number pragmas
                            ln = ln.replace(bad_file_str, good_file_str)
                            md5.update(ln)
                    md5.update(b'\n')

        # Select compiler options.
        compile_opts = ''
        if os.name == 'nt':
            compile_opts += '""%s""' % os.path.join(tf.sysconfig.get_lib(), 'python', '_pywrap_tensorflow_internal.lib')
        elif os.name == 'posix':
            compile_opts += '""%s""' % os.path.join(tf.sysconfig.get_lib(), 'python', '_pywrap_tensorflow_internal.so')
            compile_opts += ' --compiler-options \'-fPIC -D_GLIBCXX_USE_CXX11_ABI=0\''
        else:
            assert False # not Windows or Linux, w00t?
        compile_opts += ' --gpu-architecture=%s' % _get_cuda_gpu_arch_string()
        compile_opts += ' --use_fast_math'
        nvcc_cmd = _prepare_nvcc_cli(compile_opts)

        # Hash build configuration.
        md5.update(('nvcc_cmd: ' + nvcc_cmd).encode('utf-8') + b'\n')
        md5.update(('tf.VERSION: ' + tf.VERSION).encode('utf-8') + b'\n')
        md5.update(('cuda_cache_version_tag: ' + cuda_cache_version_tag).encode('utf-8') + b'\n')
        # Compile if not already compiled.
        bin_file_ext = '.dll' if os.name == 'nt' else '.so'
        bin_file = os.path.join(cuda_cache_path, cuda_file_name + '_' + md5.hexdigest() + bin_file_ext)
        if not os.path.isfile(bin_file):
            if verbose:
                print('Compiling... ', end='', flush=True)
            with tempfile.TemporaryDirectory() as tmp_dir:
                tmp_file = os.path.join(tmp_dir, cuda_file_name + '_tmp' + bin_file_ext)
                _run_cmd(nvcc_cmd + ' ""%s"" --shared -o ""%s"" --keep --keep-dir ""%s""' % (cuda_file, tmp_file, tmp_dir))
                os.makedirs(cuda_cache_path, exist_ok=True)
                intermediate_file = os.path.join(cuda_cache_path, cuda_file_name + '_' + uuid.uuid4().hex + '_tmp' + bin_file_ext)
                shutil.copyfile(tmp_file, intermediate_file)
                os.rename(intermediate_file, bin_file) # atomic
          # Load.
        if verbose:
            print('Loading... ', end='', flush=True)
        plugin = tf.load_op_library(bin_file)

        # Add to cache.
        _plugin_cache[cuda_file] = plugin
        if verbose:
            print('Done.', flush=True)
        return plugin

    except:
        if verbose:
            print('Failed!', flush=True)
        raise

#----------------------------------------------------------------------------",make file copyright corporation reserved work made available source code view copy license visit custom import o import import import import import import import global false speed compilation assuming included code never change unsafe verbose true print status visual visual visual studio internal helper return return none device capability major minor return major minor raise found major minor return major minor pipe output status status none raise returned error see full command line output log output none require allow use whatever default raise find installation computer check list else return main entry point already cache return setup verbose print try hash source open hash included code running verbose print keep open error check line ignore line number select compiler else assert false hash build configuration compile already else verbose print keep atomic load verbose print add cache verbose print return except verbose print raise,issue
1962778105,"@SomeUserName1 
Unfortunately, I got the error below when I tried to run the pip install tf-nightly[and-cuda]
RROR: Could not find a version that satisfies the requirement tf-nightly[and-cuda] (from versions: none)
ERROR: No matching distribution found for tf-nightly[and-cuda]
",unfortunately got error tried run pip install could find version requirement none error matching distribution found,issue
1962755977,"Hi Guys,

   Seems like I'm late to the party. I am running TF on WSL2. Guess what, I have completely messed up my set up which was running great on TF 2.10 configuration.
I Upgraded every thing to a stable configuration as mentioned by NVIDIA, PF below. Only to find out Microsoft killed NUMA.

TensorFlow  : 2.15
Cuda kit    : 12.3
CuDNN       : 8.9
VS          : Comm 2022
Python      : 3.10
numactl     : 2.0.14-3Ubuntu2 (force installed but no use)
UBUNTU      : 24
CPU         : Ryzen 7 5600
Mem         : 32 Gigs
GPU         : GeF RTX 4070 16 Gigs

Any help would be appreciated.
TY. I too will Update her If I find a fix.

",hi like late party running guess completely set running great configuration every thing stable configuration find kit python force use mem help would update find fix,issue
1962631872,Is there any update on this issue ? Upsampling with trilinear interpolation is very common in deep learning architectures and the feature is available in Pytorch...,update issue trilinear interpolation common deep learning feature available,issue
1962424469,"@urim85 

Yeah. Actually, till it is crashing for me in 48 GB RTX 6000. 

(What I told to @farmaker47  was,  it will crash prematurally if VRAM is low. But also crashes in final step even if you have enough VRAM) ",yeah actually till told crash low also final step even enough,issue
1962422150,"This is all fine and well. Yet, it's completely undocumented. People will find  the following in the README.md page for tensorflow/lite:
```
PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native
```
and in more cases than not, their system will crash as the compiler sees a `-j` option with no actual value. At the very least, the documentation should indicate a value is needed. Same for the following pages:
https://www.tensorflow.org/lite/guide/build_cmake_pip
https://www.tensorflow.org/lite/guide/build_cmake",fine well yet completely undocumented people find following page native system crash compiler option actual value least documentation indicate value following,issue
1962417690,"You can also define the envvar before building:

```bash
BUILD_NUM_JOBS=4 PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native
```

In fact, this is the way the scripts have been designed to be run a long time ago. One setup script defines the variables and then calls the exported scripts.",also define building bash native fact way designed run long time ago one setup script,issue
1962406653,"@farmaker47 

This conversion pipeline needs lot of Vram. At Least 24 GB. 

@LakshmiKalaKadali any updates on this please?",conversion pipeline need lot least please,issue
1962359922,"I closed it by mistake, sorry. Could you please check again?

Thanks!",closed mistake sorry could please check thanks,issue
1962359292,"> @MessDeveloper, Could you please confirm whether you are trying to execute the mentioned code on the base environment or the CUDA, tensorflow installed environment. Also make sure to execute the code in the environment where you installed the tensorflow and CUDA.
> 
> I tried to execute in our local environment and it was executed without any issue/error. Thank you!

Hello, despite appearing as a ""base"" environment in the screenshot, I'm running it in a specially created TensorFlow environment under Anaconda Navigator and running JupyterLab, I believe that dependences were downloaded, such as CUDA, how can I check it?

Thank you for your help.",could please confirm whether trying execute code base environment environment also make sure execute code environment tried execute local environment executed without thank hello despite base environment running specially environment anaconda navigator running believe check thank help,issue
1962290083,"Have the same issue  :

2024-02-24 17:46:11.024256: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-24 17:46:11.044842: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2024-02-24 17:46:11.045061: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-24 17:46:11.491969: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
>>> gpus = tf.config.experimental.list_physical_devices('GPU')
2024-02-24 17:46:11.976038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-02-24 17:46:11.992328: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices..

whereas NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3
installed and 
nvidia-tensorrt           99.0.0                   pypi_0    pypi
tensorboard               2.13.0                   pypi_0    pypi
tensorboard-data-server   0.7.2                    pypi_0    pypi
tensorboard-plugin-wit    1.8.1                    pypi_0    pypi
tensorboardx              2.6.2.2                  pypi_0    pypi
tensorflow-gpu            2.8.0                    pypi_0    pypi
tensorrt                  8.6.1.post1              pypi_0    pypi
tensorrt-bindings         8.6.1                    pypi_0    pypi
tensorrt-libs             8.6.1                    pypi_0    pypi
",issue could find machine used could find machine used binary use available enable following rebuild appropriate compiler warning could find successful node read negative value must least one node node zero see please make sure missing properly would like use follow guide setup platform skipping whereas driver version version post,issue
1962213294,"> This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.

resolved conflicts",stale open day activity closed activity thank resolved,issue
1962208743,This PR is stale because it has been open for 14 days with no activity. It will be closed if no further activity occurs. Thank you.,stale open day activity closed activity thank,issue
1962142415,"Ok, now I understand what you meant by the doc test. I’m away for the weekend, but I’ll get this resolved Sunday night",understand meant doc test away weekend get resolved night,issue
1961986534,"I followed these directions on MacOS: https://www.tensorflow.org/lite/guide/build_cmake#opencl_gpu_delegate on r2.15 and nightly. I'm running into a different issue actually:

This part works fine:
```
cmake ../tensorflow_src/tensorflow/lite -DTFLITE_ENABLE_GPU=ON
```
but
```
cmake --build . -j
```
fails here:
```
gmake[1]: *** [CMakeFiles/Makefile2:6653: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/all] Error 2
gmake[1]: *** Waiting for unfinished jobs....
[ 56%] Linking CXX static library libabsl_stacktrace.a
[ 56%] Built target ruy_tune
[ 56%] Building CXX object _deps/abseil-cpp-build/absl/base/CMakeFiles/absl_malloc_internal.dir/internal/low_level_alloc.cc.o
[ 56%] Built target ruy_thread_pool
[ 56%] Building CXX object _deps/abseil-cpp-build/absl/strings/CMakeFiles/absl_strings_internal.dir/internal/ostringstream.cc.o
[ 56%] Building CXX object _deps/abseil-cpp-build/absl/strings/CMakeFiles/absl_strings_internal.dir/internal/escaping.cc.o
[ 56%] Building CXX object _deps/abseil-cpp-build/absl/strings/CMakeFiles/absl_strings_internal.dir/internal/utf8.cc.o
[ 56%] Building CXX object _deps/abseil-cpp-build/absl/hash/CMakeFiles/absl_city.dir/internal/city.cc.o
[ 56%] Building CXX object _deps/abseil-cpp-build/absl/crc/CMakeFiles/absl_crc_cpu_detect.dir/internal/cpu_detect.cc.o
[ 56%] Building CXX object _deps/abseil-cpp-build/absl/log/CMakeFiles/absl_log_internal_conditions.dir/internal/conditions.cc.o
[ 56%] Building CXX object _deps/abseil-cpp-build/absl/debugging/CMakeFiles/absl_demangle_internal.dir/internal/demangle.cc.o
[ 56%] Building CXX object _deps/abseil-cpp-build/absl/hash/CMakeFiles/absl_low_level_hash.dir/internal/low_level_hash.cc.o
[ 56%] Built target absl_random_internal_randen_hwaes
[ 56%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/abs.c.o
[ 56%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph.c.o
[ 56%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/clamp.c.o
[ 56%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/average-pooling-2d.c.o
[ 56%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/memory-planner.c.o
[ 56%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/tensor.c.o
[ 56%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/static-transpose.c.o
[ 56%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/ceiling.c.o
[ 56%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/depthwise-convolution-2d.c.o
[ 56%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/copy.c.o
[ 56%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/global-sum-pooling.c.o
[ 56%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/static-reshape.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/add2.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/global-average-pooling.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/softmax.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/elu.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/hardswish.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/convert.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/unpooling-2d.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/concatenate.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/leaky-relu.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/fully-connected.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/maximum2.c.o
[ 57%] Linking CXX static library libabsl_log_internal_conditions.a
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/depth-to-space.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/sigmoid.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/bankers-rounding.c.o
[ 57%] Linking CXX static library libabsl_low_level_hash.a
[ 57%] Linking CXX static library libabsl_malloc_internal.a
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/deconvolution-2d.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/squared-difference.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/max-pooling-2d.c.o
[ 57%] Linking CXX static library libabsl_strings_internal.a
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/square-root.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/subtract.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/prelu.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/tanh.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/static-constant-pad.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/static-resize-bilinear-2d.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/negate.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/validation.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/runtime.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/static-slice.c.o
[ 57%] Linking CXX static library libabsl_demangle_internal.a
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/minimum2.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/divide.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/square.c.o
[ 57%] Linking CXX static library libabsl_city.a
[ 57%] Built target absl_malloc_internal
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/space-to-depth-2d.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/argmax-pooling-2d.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/convolution-2d.c.o
[ 57%] Built target absl_low_level_hash
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/even-split.c.o
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/multiply2.c.o
[ 57%] Linking CXX static library libabsl_crc_cpu_detect.a
[ 57%] Built target absl_strings_internal
[ 57%] Built target absl_city
[ 57%] Built target absl_stacktrace
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/subgraph.dir/src/subgraph/floor.c.o
[ 57%] Built target absl_demangle_internal
[ 57%] Built target absl_log_internal_conditions
[ 57%] Built target absl_crc_cpu_detect
[ 57%] Built target subgraph
gmake: *** [Makefile:136: all] Error 2
```

@terryheo, can you please take a look? Thanks.",nightly running different issue actually part work fine build error waiting unfinished linking static library built target building object built target building object building object building object building object building object building object building object building object built target building object building object building object building object building object building object building object building object building object building object building object building object building object building object building object building object building object building object building object building object building object building object building object linking static library building object building object building object linking static library linking static library building object building object building object linking static library building object building object building object building object building object building object building object building object building object building object linking static library building object building object building object linking static library built target building object building object building object built target building object building object linking static library built target built target built target building object built target built target built target built target error please take look thanks,issue
1961982943,"Looks like it made it into the 2.16 branch: https://github.com/tensorflow/tensorflow/commits/r2.16/?author=cantonios

timeline for that release is early march.  Releases come out about once a quarter.",like made branch release early march come quarter,issue
1961971936,"> No, it's not a security fix, so will not be backported.

Ah, didn't realize it was only security. Thought it would be for bugfix too.

-----

Do you know the rough timeline for 2.16?",security fix ah realize security thought would know rough,issue
1961946786,"> Will this be going into a 2.15.1 release at some point soon?

No, it's not a security fix, so will not be backported.  It will go into 2.16.

The header is somewhere in the folder structure, so there is currently a work-around.",going release point soon security fix go header somewhere folder structure currently,issue
1961921503,"Possible fix: In line 126 of file: `tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh`, replace:
```
cmake --build . --verbose -j ${BUILD_NUM_JOBS} -t _pywrap_tensorflow_interpreter_wrapper
```
with:
```
cmake --build . --verbose -j $(nproc) -t _pywrap_tensorflow_interpreter_wrapper
```",possible fix line file replace build verbose build verbose,issue
1961862550,"Hi @Dibyajyoti227, I am unsure how to use/ingest your config file, do you have a program/tool/documentation on how to use your config file? If so please share so that we may help you. Thanks for your help.",hi unsure file use file please share may help thanks help,issue
1961833999,Will this be going into a 2.15.1 release at some point soon?,going release point soon,issue
1961619039,"@sushreebarsa Thanks for your reply. I got some idea. Here is my description:

Take this [constructor](https://github.com/tensorflow/tensorflow/blob/23bc3342fcc7d4bf1da6f2f11da61da55d290d34/tensorflow/core/framework/tensor.cc#L967) as an example,

https://github.com/tensorflow/tensorflow/blob/23bc3342fcc7d4bf1da6f2f11da61da55d290d34/tensorflow/core/framework/tensor.cc#L967-L978

`buf_ = new Buffer<T>(a, shape.num_elements())` sets the `buf_` member of `Tensor`. Here `Buffer` is a subclass of `TensorBuffer`, its destructor:

https://github.com/tensorflow/tensorflow/blob/23bc3342fcc7d4bf1da6f2f11da61da55d290d34/tensorflow/core/framework/tensor.cc#L577-L585

When `Tensor` object is destroyed, it will call `buf_ `'s destructor.

The `TypedAllocator::Deallocate` then deallocate the memory which ultimately calls `DeallocateRaw` method of the allocator that is used in constructor.

Is it looks this what I describe above?
",thanks reply got idea description take constructor example new buffer member tensor buffer subclass destructor tensor object call destructor memory ultimately method allocator used constructor describe,issue
1961517057,"using `tf.nn.dropout(x, 0.5)` throws `E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inencoder_decoder_1/decoder_1/decoder_block_1/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer`

However, replacing it with `tf.keras.layers.Dropout(0.5)` resolved the issue for me. 

TF version: 2.17.0",layout size match size permutation shape however resolved issue version,issue
1961362570,"@SuryanarayanaY 
I have no idea because I just checked TF2.16 and TF2.17, and I haven't build previous versions from source.",idea checked build previous source,issue
1961083662,"Hello, sorry to insist, but is there any news about this ?",hello sorry insist news,issue
1961077762,"<img width=""587"" alt=""Screenshot 2024-02-23 at 3 56 16 PM"" src=""https://github.com/tensorflow/tensorflow/assets/156997549/c0f3bfc6-0777-41bf-8601-0681555d4121"">

I want like this in iOS swift using TensorFlow.  is it possible?",want like swift possible,issue
1960941459,"Yes, the issue is the lack of executables for Python 3.11 on PiPy, as you can see here: 
https://pypi.org/project/tflite-runtime/2.14.0/#files
(2.14.0 is the only release that supports Python 3.11).
It's not related to the above linked issue - I will have a look at the alternative repo.
",yes issue lack python pipy see release python related linked issue look alternative,issue
1960931383,"Hi @Ferev  Can you please review this PR ? Thank you very much!
cc @gbaned ",hi please review thank much,issue
1960866559,"@sachinprasadhs I was able to replicate this issue [here](https://colab.research.google.com/gist/sushreebarsa/b6e329989605b868659650bab7a09bb1/untitled9.ipynb), please have a look.
Thank you! ",able replicate issue please look thank,issue
1960858534,@sushreebarsa Thank you for your response. But I just wanted to inform you it crashes. Please check [it](https://colab.research.google.com/drive/1XtuhjL-l3x2r4KVWj2xeO6xlMD7b0Evh?usp=sharing).,thank response inform please check,issue
1960855989,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/63032/checks?check_run_id=21897307220) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.",thanks pull request like may first contribution open source project look pull request need sign contributor license agreement view invocation check information date status view section bottom pull request,issue
1960843787,"It's not resolved since I'm using a formal release, but I guess it will be resolved in the coming releases, so I'm closing the issue.
Thanks for your response.",resolved since formal release guess resolved coming issue thanks response,issue
1960828246,"@LakshmiKalaKadali 

first install ` pip install git+https://github.com/keras-team/keras-nlp` and then update Keras (`pip install -U keras`)",first install pip install update pip install,issue
1960824469,"Hi @RageshAntonyHM,

I am trying to reproduce the issue while I had another error `ModuleNotFoundError: No module named 'keras_nlp.backend`, could you please confirm the version of it.

Thank You",hi trying reproduce issue another error module could please confirm version thank,issue
1960761763,The issue resolved already.Tested with tf-nightly and attaching [gist](https://colab.research.google.com/gist/SuryanarayanaY/094c5bba489a0c7f924bb4b829fdf4a4/61530_nightly.ipynb) for reference. Thanks!,issue resolved gist reference thanks,issue
1960755046,"The `data_ `member is indeed a `void`* pointer, but it doesn't directly own the memory it points to. The actual memory management depends on the T`ensorBuffer'`s underlying data type and allocation strategy. The RefCounted::Unref function you mentioned does indeed call delete this when the reference count reaches zero. However, this delete only deallocates the TensorBuffer object itself, not the memory pointed to by data_.
The responsibility of de-allocating the actual data belongs to the specific allocation mechanism used for that data type. TensorFlow internally handles this through its memory management system.

Please let us know if it clarifies your query?
Thank you!",member indeed void pointer directly memory actual memory management underlying data type allocation strategy function indeed call delete reference count zero however delete object memory pointed responsibility actual data specific allocation mechanism used data type internally memory management system please let u know query thank,issue
1960751024,"Hi @feranick,

 As you mentioned, the issue is resolved, could please feel free to close the issue.

Thank You",hi issue resolved could please feel free close issue thank,issue
1960748279,"@Sehun0819 If removing negative values from the perm argument isn't possible, could you define a mapping function that converts negative values to their positive counterparts based on the dimensions of your tensor. If the negative values are intentional and represent specific transposition directions, please consider using alternative operations that support them. For example, tf.reverse might be suitable depending on your use case?
Thank you!",removing negative perm argument possible could define function negative positive based tensor negative intentional represent specific transposition please consider alternative support example might suitable depending use case thank,issue
1960745413,"@Dibyajyoti227 Could you try to replace `keep_aspect_ratio_resizer` with `fixed_shape_resizer` and use latest TF version. Kindly let us know if it helps?
Thank you!",could try replace use latest version kindly let u know thank,issue
1960745150,Hi. I'm doing the same thing for movenet. How did you resolve it? What flag?,hi thing resolve flag,issue
1960740726,"Hi @vnanaware111 ,

There are three CocoaPods for TensorFlow Lite:

`TensorFlowLiteSwift`: Provides the Swift APIs for TensorFlow Lite.
`TensorFlowLiteObjC`: Provides the Objective-C APIs for TensorFlow Lite.
`TensorFlowLiteC`: Common base pod, which embeds the TensorFlow Lite core runtime and exposes the base C APIs used by the above two pods. 
You should choose either `TensorFlowLiteSwift` or `TensorFlowLiteC` pod based on the language in which your app is written, but not both. The exact steps for using local builds of TensorFlow Lite differ, depending on which exact part you would like to build.
Please go through the reference [document](https://www.tensorflow.org/lite/guide/build_ios) and reference example [code](https://github.com/tensorflow/examples/tree/master/lite/examples/pose_estimation/ios) .

Thank You",hi three lite swift lite lite common base pod lite core base used two choose either pod based language written exact local lite differ depending exact part would like build please go reference document reference example code thank,issue
1960738892,"@sachinprasadhs I was able to replicate the issue reported [here](https://colab.research.google.com/gist/sushreebarsa/950efa69aa2ec419266732519795e09f/untitled5.ipynb). Please have a look at this issue.
Thank you!",able replicate issue please look issue thank,issue
1960650161,"@cantonios I think we should be good. Recopied over the outputs and just modified to make pylint happy about line lengths. Sorry about that, and thanks for your help! Definitely still learning the project.",think good make happy line sorry thanks help definitely still learning project,issue
1960506981,"@YangChenyuan  This is happening because we have a graph optimization that replaces `x * 0` with zero, regardless of what `x` is [here](https://github.com/tensorflow/tensorflow/blob/eb44a46d62a5312af79d69eaf56e26b1d4439236/tensorflow/core/grappler/optimizers/constant_folding.cc#L3073).

I'm not sure I'm convinced it's important to keep the -0.  What is your use-case?",happening graph optimization zero regardless sure convinced important keep,issue
1960358328,"> @vanskarner, I tried to execute the mentioned code with **train_data.take(1) or train_data.take(1).cache()** on both tensorflow [v2.15](https://colab.research.google.com/gist/tilakrayal/53f5ab525c088b7fd7f959f8a32c5cf6/untitled1734.ipynb) and v2.13, and observed that the output is intended and also the image is also visible. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a10ec69e60ef2fe3c67c02f3844e7669/untitled1735.ipynb) and let me know. Thank you!

Hi, when the code is executed in the google colab there are no resulting messages, however based on the template I filled in(OS platform and distribution) for this issue I still get the same message even if I upgrade to tensorflow v2.15, either using train_data.take(1) or train_data.take(1).cache() generate the same message described in Relevant log output.",tried execute code output intended also image also visible kindly find gist let know thank hi code executed resulting however based template filled o platform distribution issue still get message even upgrade either generate message relevant log output,issue
1960284056,"@milpuz01 @jondea, JFYI, this PR also enables an opt-in CSR matmul for ARM. Currently, a reference implementation will be used when running on ARM.",also arm currently reference implementation used running arm,issue
1960259377,"Even replacing `batch_shape` with `batch_input_shape`, there are still errors when converting the keras model into a tflite:
```
[/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py](https://localhost:8080/#) in _process_kwargs(self, kwargs)
    140                 )
    141             else:
--> 142                 raise TypeError(
    143                     f""{k} is not a valid argument, kwargs should be empty ""
    144                     "" for `optimizer_experimental.Optimizer`.""

TypeError: loss_scale_factor is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`.
```
`loss_scale_factor` seems to be defined only in TF2.16.0, but there not in the TF2.15 model.
",even still converting model self else raise valid argument empty valid argument empty defined model,issue
1960228557,"If you casually compare the config.jason in the keras model files you can see that the variable `batch_input_shape` in TF2.15.0 was changed to `batch_shape` in TF2.16. This variable remain undefined in TF2.15. The config.json are attached.
[TF2.16rc0_config.json](https://github.com/tensorflow/tensorflow/files/14378950/TF2.16rc0_config.json)
[TF2.15_config.json](https://github.com/tensorflow/tensorflow/files/14378952/TF2.15_config.json)
",casually compare model see variable variable remain undefined attached,issue
1960171898,"Here's also the `tf.lite.experimental.Analyzer.analyze(model_content=tflite_model)` output for both the TF2.15.0 and TF2.16.0rc0 models (the latter failing).
[analyzer_2.15_model.txt](https://github.com/tensorflow/tensorflow/files/14378749/analyzer_2.15_model.txt)
[analyzer_2.16_model.txt](https://github.com/tensorflow/tensorflow/files/14378750/analyzer_2.16_model.txt)

",also output latter failing,issue
1960152352,"> Hi @feranick, I was able to replicate on 2.16 in this [gist](https://colab.sandbox.google.com/gist/pkgoogle/ed2e7fdf2acb7b0d9c8bbe5921cb64a3/62989.ipynb) (You have to upload your TF2.16.0... file to colab). Is there any particular ops you are using in this model or do you have any additional information on the creation of this model?
> 
Hi @pkgoogle, thank you for looking into this. This below is a snapshot of the code relevant to the creation of the model. I hope it's helpful. I have been using it for a long time with TF2.15.0 and below with no issues...
[model.txt](https://github.com/tensorflow/tensorflow/files/14378663/model.txt)



",hi able replicate gist file particular model additional information creation model hi thank looking snapshot code relevant creation model hope helpful long time,issue
1960126859,"Hi @feranick, I was able to replicate on 2.16 in this [gist](https://colab.sandbox.google.com/gist/pkgoogle/ed2e7fdf2acb7b0d9c8bbe5921cb64a3/62989.ipynb) (You have to upload your TF2.16.0... file to colab). Is there any particular ops you are using in this model or do you have any additional information on the creation of this model?

Hi @majiddadashi, can you please take a look? Thanks.",hi able replicate gist file particular model additional information creation model hi please take look thanks,issue
1960111950,"I mean to just fix the pydoc that you added - the examples in your documentation don't run.  In one of those examples, you implicitly create an integer tensor.",mean fix added documentation run one implicitly create integer tensor,issue
1960086470,"HI @MinaBabahaji-ML, I was able to continue with the new instructions but am running into a different issue probably due to using an emulator.

Hi @sirakiin, can you please take a look? Thanks.
",hi able continue new running different issue probably due emulator hi please take look thanks,issue
1960082555,"> The doc test is failing with:
> 
> ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32: <tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[1, 2, 3], [4, 5, 6]], dtype=int32)>
> 
> Please double-check your examples all run.

Hmm, my unit tests are all passing when I run on my system, but I can replicate the error if I run the following:
```
>>> indices = tf.constant([[0, 0],[1,1]])
>>> indices = tf.constant([[0, 0],[1, 1]])
>>> values = tf.constant([0.5, 0.3], dtype=tf.float32)
>>> dense_shape = tf.constant([2, 2])
>>> mat1 = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3], dtype=tf.int32)
>>> mat2 = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2], dtype=tf.float32)
>>> ind, res, ds = tf.sparse.sampled_addmm(indices, values, dense_shape, mat1, mat2, output_type=tf.float32)
```
@cantonios I suppose we could add a validation at the start where if `values`, `mat1`, and `mat2` are tensors, there types match `output_type`? The supported data types I had listed in the python function doc were `bfloat16`, `float16`, `float32`, and `float64`. I only set up the unit tests to run on those types.",doc test failing tensor conversion float tensor array please run unit passing run system replicate error run following index index mat mat index mat mat suppose could add validation start mat mat match data listed python function doc float float float set unit run,issue
1959994456,"LakshmiKalaKadali, Thank you for your response. I am working through some additions to check on input and output shapes. As I am new to working with Tensorflow, I do have a couple of follow-on questions as I work this issue. Additionally, these questions may provide clarity to you on what is happening: 1)  I am using the Sparkfun Edge micro-speech code in both instances - a) the demo cdde that uses the ""yes/no"" model and b) for my ""On/Off"" model. The only data I changed (using the model I trained in Google Colab) was in the ""tiny_conv_micro_features_model_data.cc"" file (substituting the hex data and the size of the hex data) and the corresponding info in the ""micro_model_settings.cc"" (substituting ON for Yes and Off for No) and the same in the ""command_responder.cc"" files. My understanding is that these changes will not change the shape of the input or output. Could you please help me understand?
2) I was getting very similar errors using the Arduino Nano BLE Sense and Arduino IDE. That error was solved by adding the opcode for CONV-2D to the micro_op_resolver as recommended in this forum. Again the only data I changed was using the same ON/OFF model trained on the Colab page. Since the model works on the Arduino, I am puzzled how the ON/OFF model shape would be the cause of the error on the Sparkfun Edge.
Thanks in advance,
Lonnie",thank response working check input output new working couple work issue additionally may provide clarity happening edge code model model data model trained file substituting hex data size hex data corresponding substituting yes understanding change shape input output could please help understand getting similar sense ide error forum data model trained page since model work puzzled model shape would cause error edge thanks advance,issue
1959881082,"To be clear, there are a number of issues reported and pointed to the same state of abandonment of the repo for modern platforms. ",clear number pointed state abandonment modern,issue
1959878852,"@tilakrayal the issue you mentioned is not the same. It refers to adding support for Apple M2 (or at least verify it is supported). This issue is about providing python 3.11 runtimes even for platforms that are currently supported, such as x86_64 and Apple M1. This infact goes beyond MacOS. Runtimes binaries for Linux using 3.11 are missing as well. In other words, Google needs to simply compile and provide the binaries, see comment above.",issue support apple least verify issue providing python even currently apple go beyond missing well need simply compile provide see comment,issue
1959774431,"@wilsingosti , Could you please put your comment on how chaining of map functions actually accumulating memory with each chaining of map function as it seems each chain call actually creating memory for Input array and it gets accumulating each time. is this intended behaviour?",could please put comment map actually memory map function chain call actually memory input array time intended behaviour,issue
1959701840,"Hi @Sehun0819 , I mean in any older TF version debug build `tf.raw_ops.LoopCond(input=True)` works without crash? This would be helpful whether this a regression problem or it exists since a while ?",hi mean older version build work without crash would helpful whether regression problem since,issue
1959666796,"@pkgoogle @yishuangP I'm attempting to downgrade xcode, however being on macOS Sonoma means that xcode 14 is not officially supported for use. Is there any other way to link the select ops other than using the `Other Flags`?",downgrade however officially use way link select,issue
1959536151,"@yyingci,
Thank you for the issue. We are working on this. Could you please allow some time to deep dive into this issue and come back with the resolution. Thank you!",thank issue working could please allow time deep dive issue come back resolution thank,issue
1959417766,Missing gradients is a blocker for us to use the new TF Java API......,missing blocker u use new,issue
1959348141,"@vanskarner,
I tried to execute the mentioned code with **train_data.take(1) or train_data.take(1).cache()** on both tensorflow [v2.15](https://colab.research.google.com/gist/tilakrayal/53f5ab525c088b7fd7f959f8a32c5cf6/untitled1734.ipynb) and v2.13, and observed that the output is intended and also the image is also visible. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a10ec69e60ef2fe3c67c02f3844e7669/untitled1735.ipynb) and let me know. Thank you!",tried execute code output intended also image also visible kindly find gist let know thank,issue
1959173234,"@devahh,
Could you please let us know if there is any specific use-case to use C++ for CNN's. Most of the community members prefer python over C++.

AFAIK there are few external sources which provide the knowledge on how to write the code on C++ and also the compilation process.
https://discuss.tensorflow.org/t/neural-network-training-back-propagation-in-c/2283/2

Thank you!
",could please let u know specific use community prefer python external provide knowledge write code also compilation process thank,issue
1959117620,"Hi @jamwar01 ,

Please share your `faulty_input.npy` through the google drive link.

Thank You",hi please share drive link thank,issue
1959085205,"Hi @hammnii-study,

Conv1D, Conv2D and Maxpooling1D, Maxpooling2D are the in-built ops. If you want to build any custom op for your usecase, please refer to the [document](https://www.tensorflow.org/lite/guide/ops_custom). To better understand the issue please share your reproducible code and any other related information.

Thank You

",hi want build custom please refer document better understand issue please share reproducible code related information thank,issue
1958937422,"Interesting! I can reproduce the problem only when running on my GPU (RTX 3090Ti, CUDA 12.0, Driver 525.60.13, it works fine on CPU. Since it works on the GPU used by colab, it seems like it's something hardware / compute capability specific, or maybe cuda version specific. I'll try to isolate it further.

Here the output of `tf.sysconfig.get_build_info()` from my local system, where it fails:
```
OrderedDict([('cpu_compiler', '/usr/bin/x86_64-linux-gnu-gcc-11'),
             ('cuda_compute_capabilities', ['sm_61', 'sm_75', 'compute_86']),
             ('cuda_version', '12.0'),
             ('cudnn_version', '8'),
             ('is_cuda_build', True),
             ('is_rocm_build', False),
             ('is_tensorrt_build', False)])
```

And from colab, where it seems to work:
```
OrderedDict([('cpu_compiler', '/usr/lib/llvm-17/bin/clang'),
             ('cuda_compute_capabilities',
              ['sm_50', 'sm_60', 'sm_70', 'sm_75', 'compute_80']),
             ('cuda_version', '12.2'),
             ('cudnn_version', '8'),
             ('is_cuda_build', True),
             ('is_rocm_build', False),
             ('is_tensorrt_build', True)])
```",interesting reproduce problem running ti driver work fine since work used like something hardware compute capability specific maybe version specific try isolate output local system true false false work true false true,issue
1958931816,"@chatnord,
I request you to take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/61860) where a similar feature has been proposed and it is still open at the developer end. Also I request to follow the similar feature which has been proposed to have the updates on the similar issue. Thank you!

",request take look issue similar feature still open developer end also request follow similar feature similar issue thank,issue
1958915888,"For this example
```
func.func @splitv_dynamic(%input: tensor<?x6xf32>) -> (tensor<?x1xf32>, tensor<?x2xf32>, tensor<?x3xf32>) {
  %split_sizes = ""tf.Const""() {value = dense<[1, 2, 3]> : tensor<3xi32>} : () -> tensor<3xi32>
  %split_dim = ""tf.Const""() {value = dense<1> : tensor<i32>} : () -> tensor<i32>
  %0:3 = ""tf.SplitV""(%input, %split_sizes, %split_dim) : (tensor<?x6xf32>, tensor<3xi32>, tensor<i32>) -> (tensor<?x1xf32>, tensor<?x2xf32>, tensor<?x3xf32>)
  func.return %0#0, %0#1, %0#2 : tensor<?x1xf32>, tensor<?x2xf32>, tensor<?x3xf32>
}
```


it will be converted to
```
module {
  func.func @splitv_dynamic(%arg0: tensor<?x6xf32>) -> (tensor<?x1xf32>, tensor<?x2xf32>, tensor<?x3xf32>) {
    %0 = mhlo.constant dense<[1, 2, 3]> : tensor<3xi32>
    %1 = mhlo.constant dense<1> : tensor<i32>
    %cst = arith.constant dense<1> : tensor<2xindex>
    %cst_0 = arith.constant dense<0> : tensor<2xindex>
    %cst_1 = arith.constant dense<[0, 1]> : tensor<2xindex>
    %cst_2 = arith.constant dense<[0, 3]> : tensor<2xindex>
    %c0 = arith.constant 0 : index
    %dim = tensor.dim %arg0, %c0 : tensor<?x6xf32>
    %c1 = arith.constant 1 : index
    %from_elements = tensor.from_elements %dim, %c1 : tensor<2xindex>
    %2 = mhlo.real_dynamic_slice %arg0, %cst_0, %from_elements, %cst : (tensor<?x6xf32>, tensor<2xindex>, tensor<2xindex>, tensor<2xindex>) -> tensor<?x1xf32>
    %c3 = arith.constant 3 : index
    %from_elements_3 = tensor.from_elements %dim, %c3 : tensor<2xindex>
    %3 = mhlo.real_dynamic_slice %arg0, %cst_1, %from_elements_3, %cst : (tensor<?x6xf32>, tensor<2xindex>, tensor<2xindex>, tensor<2xindex>) -> tensor<?x2xf32>
    %c6 = arith.constant 6 : index
    %from_elements_4 = tensor.from_elements %dim, %c6 : tensor<2xindex>
    %4 = mhlo.real_dynamic_slice %arg0, %cst_2, %from_elements_4, %cst : (tensor<?x6xf32>, tensor<2xindex>, tensor<2xindex>, tensor<2xindex>) -> tensor<?x3xf32>
    return %2, %3, %4 : tensor<?x1xf32>, tensor<?x2xf32>, tensor<?x3xf32>
  }
}
```",example input tensor tensor tensor tensor value dense tensor xi tensor xi value dense tensor tensor input tensor tensor xi tensor tensor tensor tensor tensor tensor tensor converted module tensor tensor tensor tensor dense tensor xi dense tensor dense tensor dense tensor dense tensor dense tensor index dim tensor index dim tensor tensor tensor tensor tensor tensor index dim tensor tensor tensor tensor tensor tensor index dim tensor tensor tensor tensor tensor tensor return tensor tensor tensor,issue
1958900332,Hi @kanglant This PR is duplicate of PR[#63003](https://github.com/tensorflow/tensorflow/pull/63003). Hence closing this PR. Thank you for your contribution!,hi duplicate hence thank contribution,issue
1958842507,Hi @Tessil Can you please resolve conflicts? Thank you!,hi please resolve thank,issue
1958833523,"Hi @gbaned ,
The lint errors not introduced by this commit.I haven't done any changes to those lines causing lint errors.They seems exists since before.Please check.",hi lint done causing lint since check,issue
1958821633,"Hi @Sehun0819 ,

The suspected code pointed is right.The `InferenceContext::UnknownShapeOfRank` calls assertion below.

https://github.com/tensorflow/tensorflow/blob/e193d8ea7776ef5c6f5d769b6fb9c070213e737a/tensorflow/core/framework/shape_inference.cc#L705

The behaviour of `CHECK_GE` is such that it asserts whether `rank` is >=0 and if condition fails it aborts and terminate the program. It will be nice to have functionality like `OP_REQUIRES` which will raise exception instead of abortion.Will look into it.",hi suspected code pointed assertion behaviour whether rank condition terminate program nice functionality like raise exception instead look,issue
1958812163,"Hi @ldbrouwer,

There is a mismatch between the expected and actual feature data sizes while inferencing on Sparkfun Edge. This might be the reason for `Didn't find op for builtin opcode 'RESHAPE' version '1'` so please make sure input and output shapes while inferencing should match that of your model. No. of features(1960) should match the dimensions as well. 

Use TFLite Micro benchmarking tool to analyze your model and identify potential compatibility issues. Also, there is a dedicated [repo](https://github.com/tensorflow/tflite-micro/issues) for TFLite micro issues. Please try to post there for quick response.

Thank You",hi mismatch actual feature data size edge might reason find version please make sure input output match model match well use micro tool analyze model identify potential compatibility also micro please try post quick response thank,issue
1958799555,"@LakshmiKalaKadali @pkgoogle 

Regarding the issue encountered with the 'tf nightly' version, it's important to note that this version utilizes Keras 3, which has known compatibility issues when using the `tf.lite.TFLiteConverter.from_saved_model` or `tf.lite.TFLiteConverter.from_keras_model` method, as reported in the Keras GitHub issue [#19108](https://github.com/keras-team/keras/issues/19108). As a workaround for this compatibility issue, the `model.export` or `keras.export.ExportArchive` methods can be used for exporting models for TFLite conversion.

Here is a gist using Keras 3 to reproduce the issue with that method: [gist](https://colab.research.google.com/drive/1vrjBGoFUUXJQZacnAUFxMKmLpRiAr-km?usp=sharing).
",regarding issue nightly version important note version known compatibility method issue compatibility issue used conversion gist reproduce issue method gist,issue
1958745289,"@cbreak-black,
I tried to execute the mentioned code on tensorflow v2.15 on both colab and the Ubuntu22.04 & observed that the output is as intended. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/12e749b2b659a99b69ad6e3ed15a3aa4/untitled.ipynb) and screen shot for the reference.

![Screenshot 2024-02-22 11 02 42 AM](https://github.com/tensorflow/tensorflow/assets/81610181/37278b15-be15-459c-ada9-3e4de80ae3b3)

Thank you!
",tried execute code output intended kindly find gist screen shot reference thank,issue
1958687128,"> ### `pip uninstall tensorflow && pip install tf-nightly[and-cuda]`

This is not working either. ",pip pip install working either,issue
1958669745,"Hi @elad-c ,
I tried to run your code on Colab using TF v2.15 and nightly versions, In nightly version it is working fine. Could you please check with nightly version. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/503fcbc95d8a64d692d252ac4ca83493/63000_2-15-nightly.ipynb) here for reference.

Thank you!",hi tried run code nightly nightly version working fine could please check nightly version please find gist reference thank,issue
1958579082,"Unfortunately I had to rollback this PR since it broke a oneDNN build on arm64. The error was:

```
In file included from tensorflow/core/kernels/mkl/mkl_einsum_op.cc:19:
In file included from tensorflow/core/kernels/mkl/mkl_batch_matmul_helper.h:25:
./tensorflow/core/kernels/mkl/mkl_matmul_ops_common.h:927:38: error: no member named 'csr' in 'dnnl::memory::desc'
  927 |       const auto tmp = memory::desc::csr(
      |                        ~~~~~~~~~~~~~~^
```

@matthew-olson-intel, can you create a new PR with this issue fixed?

",unfortunately rollback since broke build arm error file included file included error member auto memory create new issue fixed,issue
1958502672,"@jakeBass TF 2.15 patch release is planned to be released before the end of March.

@nitins17 Thank you for cherry picking the fix to the r2.15 branch.",patch release end march thank cherry fix branch,issue
1958336896,"This works as intended.  The rank need not by >=2.  For a constant tensor it returns the constant, with k=0.",work intended rank need constant tensor constant,issue
1958262215,"Thanks for letting us know! #63020 updates the ml-dtypes version to 0.3.1. It should be included in the upcoming patch release for TF 2.15. 

> When will 2.15.1 be released?

I do not know the exact date but believe it should be out by end of March. Adding @learning-to-play to confirm. ",thanks u know version included upcoming patch release know exact date believe end march confirm,issue
1958195191,"> > Any update on this? Is the next step in the process for @qqfish to review? Looks like the Copybara checks are passing now.
> 
> It's going through the internal battery of tests now.

Oh, nice, sounds good.",update next step process review like passing going internal battery oh nice good,issue
1958166693,"> Any update on this? Is the next step in the process for @qqfish to review? Looks like the Copybara checks are passing now.

It's going through the internal battery of tests now.",update next step process review like passing going internal battery,issue
1958112503,Any update on this? Is the next step in the process for @qqfish to review? Looks like the Copybara checks are passing now.,update next step process review like passing,issue
1958049347,"This is resolved when setting c++17 as default.:
```
     @@ -37,7 +37,7 @@   @org_tensorflow//tensorflow:workspace0.bzl
 
     load(""@coral_crosstool//:configure.bzl"", ""cc_crosstool"")
 -   cc_crosstool(name = ""crosstool"", cpp_version = ""c++14"")  
 +   cc_crosstool(name = ""crosstool"", cpp_version = ""c++17"")

```",resolved setting default load name name,issue
1957975101,"@suyash-narain 

I did resolve this error by linking tflite for gpu in cmake file.

You can download this from https://github.com/ValYouW/tflite-dist/releases
//tensorflow/lite:libtensorflowlite.so //this is already available in your tree
tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_gl.so // this needs to be pushed at cpp/tf-lite-api/generated-libs/armeabi-v7a


Upodate the whisper_android/app/src/main/cpp/CMakeLists.txt as below

if (ANDROID)
    add_library(audioEngine SHARED TFLiteEngine.cpp TFLiteEngineJNI.cpp)
    target_include_directories(audioEngine PRIVATE ${INCLUDE_DIRS})
    # Add 'tflite' library (imported)
    message(""new"")
    add_library(tflite SHARED IMPORTED)
    set_target_properties(tflite PROPERTIES IMPORTED_LOCATION
            ${CMAKE_CURRENT_LIST_DIR}/tf-lite-api/generated-libs/${ANDROID_ABI}/libtensorflowlite.so)
    **add_library(tflite_gpu_delegate  SHARED IMPORTED)
    set_target_properties(tflite_gpu_delegate  PROPERTIES IMPORTED_LOCATION
            ${CMAKE_CURRENT_LIST_DIR}/tf-lite-api/generated-libs/${ANDROID_ABI}/libtensorflowlite_gpu_delegate.so)
    target_link_libraries(audioEngine tflite tflite_gpu_delegate)**
endif ()

I can successfully build for gpu delegate. However, it still breaks when doing inference.",resolve error linking file already available tree need android private add library message new successfully build delegate however still inference,issue
1957808406,"Thank you, I'll probably try using Docker to avoid unforseen issues in the future",thank probably try docker avoid future,issue
1957786477,"This repo below is certainly not a replacement of a proper official support for `tflite_runtime` from Google. Yet, it provides help on making up-to-date `whl` packages yourself (and some unofficial builds, including for python 3.11).

https://github.com/feranick/TFlite-builds
https://github.com/feranick/TFlite-builds/releases/tag/v2.15.0",certainly replacement proper official support yet help making unofficial python,issue
1957766579,"If I'm understanding correctly, this appears to already be fixed on `master`:
- https://github.com/tensorflow/tensorflow/blob/v2.15.0/tensorflow/tools/pip_package/setup.py#L92 
  - `'ml_dtypes ~= 0.2.0',`
- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L93
  - `'ml_dtypes ~= 0.3.1',`

So it seems like the operative questions are: 
1. Am I understanding correctly? 
2. When will 2.15.1 be released?",understanding correctly already fixed master like operative understanding correctly,issue
1957727614,"The models are quite large and beyind the github 25MB limit. Here they are from an open server:

https://gridedgedm.mit.edu/tf_tmp/TF2.15.0_model_classifier_CNN.keras
https://gridedgedm.mit.edu/tf_tmp/TF2.16.0_model_classifier_CNN.keras
",quite large limit open server,issue
1957696308,"I was able to replicate with the first gist, @abattery can you please take a look? Thanks.",able replicate first gist please take look thanks,issue
1957683711,"Hi @feranick, do you mind uploading the model on github via a zip file? There are licensing issues with dropbox so we cannot use it. Thanks for your help.",hi mind model via zip file use thanks help,issue
1957511963,"Hello @pkgoogle . I added `libc++_shared.so` to the tensorflow_lite_c_2_15_0 package, and please push it to the device with  `adb push ./tensorflow_lite_c_2_15_0/lib/aarch64/libc++_shared.so /data/local/tmp` . I also added this to the build instructions. However, I am afraid you may not be able to regenerate the problem with an emulator. I tested it on a Galaxy S20 device. ",hello added package please push device push also added build however afraid may able regenerate problem emulator tested galaxy device,issue
1957356315,I still have nan output when using my specific numpy input. Could you provide a method whereby I can transfer the faulty_input.npy to you? Its compressed size is 28MB which exceeds the 25MB limit set in this page. Thank you.,still nan output specific input could provide method whereby transfer compressed size limit set page thank,issue
1956939634,"It's still unsupported as per master code.

https://github.com/tensorflow/tensorflow/blob/9dcf314237097784e02ff1979b10b6ddec9f26a8/tensorflow/core/kernels/conv_grad_filter_ops.cc#L160-L163",still unsupported per master code,issue
1956893951,"Hi @Dakkerad ,

This is not an issue now as currently tensorflow supported on Apple metal.You need to install metal plugin additionally. Instructions for same can be referred here at [metal/tensorflow-plugin/](https://developer.apple.com/metal/tensorflow-plugin/).

Thanks!",hi issue currently apple need install metal additionally thanks,issue
1956798534,"@terryheo @pkgoogle 
Can you please help with possible debug options as this error is a major blocker in my project",please help possible error major blocker project,issue
1956789193,"> I am encountering a similar error with 2.13
> 
> Anyone knows if 2.15 solves the error? Do you need
> 
> `echo -n ""build --config=monolithic"" >> .bazelrc`
> 
> command for 2.15?

I tried with 2.15, same error is coming.",similar error anyone error need echo build command tried error coming,issue
1956786511,"> @haifang12, Could you please try to add the option `echo -n ""build --config=monolithic""` >> `/tensorflow_src/.bazelrc` to avoid the **_ZNK6google8protobuf7Message11GetTypeNameEv** error.
> 
> Also please take a look at the comment from the developer [issue1](https://github.com/tensorflow/tensorflow/issues/46989) and [issue2](https://github.com/tensorflow/tensorflow/issues/45153) for the similar error. Thank you!

Tried this, however build issue is coming for tensorflow v2.15

```
ERROR: /tensorflow_src/tensorflow/BUILD:1652:19: Action tensorflow/_api/v2/v2.py [for tool] failed: (Aborted): bash failed: error executing command (from target //tensorflow:tf_python_api_gen_v2) 
  (cd /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow && \
  exec env - \
    DOCKER_HOST_CACHEBUSTER=1682977560680045781 \
    LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-6.1.0-linux-x86_64/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/android/sdk/cmdline-tools/latest/bin:/android/sdk/platform-tools:/android/ndk \
  /bin/bash -c 'bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/_api/v2/ --apiname=tensorflow --apiversion=2  --compat_apiversion=1 --compat_apiversion=2  --compat_init_template=tensorflow/compat_template_v1.__init__.py --compat_init_template=tensorflow/compat_template.__init__.py --packages=tensorflow.python,tensorflow.dtensor.python.accelerator_util,tensorflow.dtensor.python.api,tensorflow.dtensor.python.config,tensorflow.dtensor.python.d_checkpoint,tensorflow.dtensor.python.d_variable,tensorflow.dtensor.python.input_util,tensorflow.dtensor.python.layout,tensorflow.dtensor.python.mesh_util,tensorflow.dtensor.python.tpu_util,tensorflow.dtensor.python.save_restore,tensorflow.lite.python.analyzer,tensorflow.lite.python.lite,tensorflow.lite.python.authoring.authoring,tensorflow.python.modules_with_exports --output_package=tensorflow._api.v2 --use_relative_imports=True --loading=default bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/tf_python_api_gen_v2.params')
# Configuration: 9f320c2ab265ab521267a555257ce138659ef24e739cde7fde73c7665f4cf5e4
# Execution platform: @local_execution_config_platform//:platform
2023-06-09 21:58:46.512000: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-09 21:58:46.546669: F ./tensorflow/core/framework/variant_op_registry.h:114] Check failed: existing == nullptr (0x2df3898 vs. nullptr)UnaryVariantDeviceCopy for direction: 1 and type_index: tensorflow::Tensor already registered
Target //tmp:tensorflow-lite-select-tf-ops failed to build
ERROR: /tensorflow_src/tensorflow/python/tools/BUILD:303:17 Middleman _middlemen/tensorflow_Spython_Stools_Sprint_Uselective_Uregistration_Uheader-runfiles failed: (Aborted): bash failed: error executing command (from target //tensorflow:tf_python_api_gen_v2) 
  (cd /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow && \
  exec env - \
    DOCKER_HOST_CACHEBUSTER=1682977560680045781 \
    LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-6.1.0-linux-x86_64/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/android/sdk/cmdline-tools/latest/bin:/android/sdk/platform-tools:/android/ndk \
  /bin/bash -c 'bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/_api/v2/ --apiname=tensorflow --apiversion=2  --compat_apiversion=1 --compat_apiversion=2  --compat_init_template=tensorflow/compat_template_v1.__init__.py --compat_init_template=tensorflow/compat_template.__init__.py --packages=tensorflow.python,tensorflow.dtensor.python.accelerator_util,tensorflow.dtensor.python.api,tensorflow.dtensor.python.config,tensorflow.dtensor.python.d_checkpoint,tensorflow.dtensor.python.d_variable,tensorflow.dtensor.python.input_util,tensorflow.dtensor.python.layout,tensorflow.dtensor.python.mesh_util,tensorflow.dtensor.python.tpu_util,tensorflow.dtensor.python.save_restore,tensorflow.lite.python.analyzer,tensorflow.lite.python.lite,tensorflow.lite.python.authoring.authoring,tensorflow.python.modules_with_exports --output_package=tensorflow._api.v2 --use_relative_imports=True --loading=default bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/tf_python_api_gen_v2.params')
# Configuration: 9f320c2ab265ab521267a555257ce138659ef24e739cde7fde73c7665f4cf5e4
# Execution platform: @local_execution_config_platform//:platform
```
",could please try add option echo build avoid error also please take look comment developer issue issue similar error thank tried however build issue coming error action tool aborted bash error command target configuration execution platform platform custom may see slightly different numerical due different computation turn set environment variable check direction already registered target build error middleman aborted bash error command target configuration execution platform platform,issue
1956475537,"> Issue has been resolved thanks for the help!

Hey, How do you solved this error?? I am also getting the same error
![Uploading n.png…]()
",issue resolved thanks help hey error also getting error,issue
1956437619,@fergushenderson I have the same issue with @nijat-ahmadli. Please support us,issue please support u,issue
1956307125,"> Thank

Hi @sachinprasadhs and @francescosgarlata , we have seen this performance issue with moving to new Optimizer operations in Keras which are implemented 
https://github.com/tensorflow/tensorflow/issues/59438#issuecomment-1411414605
",thank hi seen performance issue moving new,issue
1956132068,"The problem still occurs with tensorflow=2.15 and tensorflow-probability=0.23. None of the approaches in `https://github.com/tensorflow/probability/issues/742` works for me. Exporting for inference works, but the training signature throws with AttributeError: 'SymbolicTensor' object has no attribute 'log_prob'",problem still none work inference work training signature object attribute,issue
1956121772,"Hi @pkgoogle,

I have reproduced the issue without signatures and with signatures. Without signatures the model is behaving as intended but the full integer model is not quantized properly with signatures. Here is the [gist](https://colab.research.google.com/gist/LakshmiKalaKadali/5fa6c17f8635776539f72f3b2853b125/tflite_quant_keras2.ipynb#scrollTo=nM9CkvDGrpsd) for reference. Tried with the 'tf nightly' version but the session is getting crashed. Here is the [gist](https://colab.research.google.com/gist/LakshmiKalaKadali/35856a3c934e9ccb647dcf87e5c7d033/copy-of-tflite_62996_full-integer_multiple-signatures_keras2.ipynb) with nightly version.

Thank You",hi issue without without model intended full integer model properly gist reference tried nightly version session getting gist nightly version thank,issue
1956092586,"In my opinion, this problem is cause by the **num_channels** has different definition in `image_dataset_from_directory` and `tf.image.decode_image`.

If **color_mode** set as grayscale, **num_channels** will be 1, which cause a problem when `tf.image.decode_image` try to load bmp images in `tf.keras.utils.image_dataset_from_directory`.

My way to solve this problem is to add a condition in `image_dataset_from_directory` to set **num_channels** as 0 when image format is bmp and **color_mode** is grayscale. 
But it raise another problem since the output shape should be (None, image_size, image_size, 1), so I also add a condition in `load_image` to set the shape to (None, image_size, image_size, 1) if output shape is (None, image_size, image_size, 0).",opinion problem cause different definition set cause problem try load way solve problem add condition set image format raise another problem since output shape none also add condition set shape none output shape none,issue
1956069232,"@qiangpei I tried using tcmalloc but that didn't change a thing... 

I am switching to torch for my next projects ",tried change thing switching torch next,issue
1955962433,"Hi @SuryanarayanaY Please fix the below PyLint errors. Thank you!

![image](https://github.com/tensorflow/tensorflow/assets/48215717/1f4b3da3-99b9-4b16-b13f-74db7cc57f10)
",hi please fix thank image,issue
1955953162,This issue got fixed by upgrading to the later versions of tflite. Closing this issue.,issue got fixed later issue,issue
1955938991,"@aaronsuydam You are right! Microsoft paused development of the TensorFlow-DirectML-Plugin in October 2023 in favor of focusing on ONNX Runtime for inference scenarios with DirectML. Currently, there's no concrete information about potential feature integration between these two directions. Please stay informed about future developments. 
Thank you!",right development favor inference currently concrete information potential feature integration two please stay informed future thank,issue
1955932694,"@jyochichili Thank you for the response!
Could you please move this issue to closed status if it has been resolved?
Thank you!",thank response could please move issue closed status resolved thank,issue
1955927292,"I have found that within tflite(v2.6.2), the URL for downloading the KISSFFT library doesn’t involve secure download(https), which seems to be causing an incomplete download of KISSFFT library. The issue doesn’t persist in the later versions, as the URL has been updated to use a secure https connection.

Thanks.",found within library involve secure causing incomplete library issue persist later use secure connection thanks,issue
1955926101,"@sushreebarsa thanks for the quick reply. I looked into the DirectML plugin, and it looks like they paused development in favor of stuff with the ONNX Runtime.

From their repo: ""⚠️ Development of TensorFlow-DirectML-Plugin has been paused until further notice. To take advantage of the latest DirectML features and performance improvements for inference scenarios, we recommend taking a look at [ONNX Runtime](https://github.com/microsoft/onnxruntime). ⚠️""

Are there plans to support feature integration with this new direction they are taking?",thanks quick reply like development favor stuff warning development notice take advantage latest performance inference recommend taking look warning support feature integration new direction taking,issue
1955919540,"Hi @ZachHandley, thank you for letting us know about the issue with Alpine Linux. Request you to raise a separate Github issue to TensorFlow. We will work to fix it.",hi thank u know issue alpine request raise separate issue work fix,issue
1955885909,"I recommend to use [Bazelisk](https://github.com/bazelbuild/bazelisk) which can automatically select the compatible bazel version for the Tensorflow version that is being built.

Thanks!",recommend use automatically select compatible version version built thanks,issue
1955885714,"@igormintz when you assign a name to a dataset using tf.data.Dataset.from_tensor_slices, the actual name gets stored in an internal attribute ._name instead of the public name attribute. This can be confusing, as printing the dataset won't show the assigned name.

Could you use the `._name `attribute:
```
ds = tf.data.Dataset.from_tensor_slices(..., name=""my_dataset"")
actual_name = ds._name
print(f""Actual name: {actual_name}"")

```

Thank you!",assign name actual name internal attribute instead public name attribute printing wo show assigned name could use attribute print actual name thank,issue
1955883588,"Hi @zahed327 ,

For TF2.15V builds are done and tested with Bazel 6.1.0 . Please try build with tested version only as mentioned [here](https://www.tensorflow.org/install/source#cpu_2) as forward versions may raise compatibility issues.


Version | Python version | Compiler | Build tools
-- | -- | -- | --
tensorflow-2.15.0 | 3.9-3.11 | Clang from xcode 10.15 | Bazel 6.1.0

",hi done tested please try build tested version forward may raise compatibility version python version compiler build clang,issue
1955871687,"@schm0 it is not possible to directly create a constant value from a state_op variable like self.iterations in TensorFlow-DirectML 1.15.8. This is because state_ops are designed to modify existing state (tf.Variable) rather than create new constants. You could potentially manipulate the self-iterations tensor using TensorFlow operations, such as slicing or masking, to extract a constant portion as an offset.
Thank you!",possible directly create constant value variable like designed modify state rather create new could potentially manipulate tensor slicing extract constant portion offset thank,issue
1955869699,"@sushreebarsa I reported this issue in order to make it fixed before 2.16 release. Moreover, tf 2.15 with https://github.com/tensorflow/tensorflow/commit/04fb826f98b92dd172ad665d8a5522a2f8201867 applied is also internally affected by this issue. 
```
>>> import tensorflow as tf
2024-02-21 10:47:33.109381: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-02-21 10:47:33.109409: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-02-21 10:47:33.110044: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-02-21 10:47:33.113595: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
>>> range_ds = tf.data.Dataset.range(10)
2024-02-21 10:47:54.344759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22462 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6
>>> 
>>> for d in range_ds:
...    print(d)
... 
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
tf.Tensor(3, shape=(), dtype=int64)
tf.Tensor(4, shape=(), dtype=int64)
tf.Tensor(5, shape=(), dtype=int64)
tf.Tensor(6, shape=(), dtype=int64)
tf.Tensor(7, shape=(), dtype=int64)
tf.Tensor(8, shape=(), dtype=int64)
tf.Tensor(9, shape=(), dtype=int64)
2024-02-21 10:47:56.048435: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
>>> tf.__version__
'2.15.0'
```
I am not sure what causes the problem, but as a symptomatic solution it is possible to disable some warnings like this:
```
  if (!absl::StrContains(status.message(), ""End of sequence"")) {
    LOG(WARNING) << ""Local rendezvous is aborting with status: "" << status;
  }
```

",issue order make fixed release moreover applied also internally affected issue import unable register factory register factory one already registered unable register factory register factory one already registered unable register factory register factory one already registered binary use available enable following rebuild appropriate compiler device memory device name bus id compute capability print local rendezvous status end sequence sure problem symptomatic solution possible disable like end sequence log warning local rendezvous status status,issue
1955834083,"> @RoboticAutonomy This is what I did. Change first the files to tensorflow V2.
> 
> Example In the command window
> 
> pip install tf-nightly !tf_upgrade_v2 --infile AC.py --outfile AC.py
> 
> It will give an error that one command was not changed which is tf.contrib.opt.ScipyOptimizerInterface
> 
> To fix this then do the following:
> 
> Have this libraries imported just in case
> 
> import tensorflow as tf import tensorflow_probability as tfp import deepxde as dde import numpy as np import matplotlib.pyplot as plt import scipy.io from scipy.interpolate import griddata from pyDOE import lhs from plotting import newfig, savefig from mpl_toolkits.mplot3d import Axes3D import time import matplotlib.gridspec as gridspec from mpl_toolkits.axes_grid1 import make_axes_locatable
> 
> Then replace:
> 
> ```
> self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, 
>                                                         method = 'L-BFGS-B', 
>                                                         options = {'maxiter': 50000,
>                                                                    'maxfun': 50000,
>                                                                    'maxcor': 50,
>                                                                    'maxls': 50,
>                                                                    'ftol' : 1.0 * np.finfo(float).eps})
> ```
> 
> With:
> 
> self.optimizer = dde.optimizers.tensorflow_compat_v1.scipy_optimizer.ScipyOptimizerInterface(self.loss, method = 'L-BFGS-B', options = {'maxiter': 50000, 'maxfun': 50000, 'maxcor': 50, 'maxls': 50, 'ftol' : 1.0 * np.finfo(float).eps})
> 
> Hope this helps.


Is the problem solved?

",change first example command window pip install give error one command fix following case import import import import import import import import plotting import import import time import import replace method float method float hope problem,issue
1955548232,"> Unlikely that Google would switch to C++20.

This is now wrong.",unlikely would switch wrong,issue
1955428104,"Hi @sushreebarsa, could you elaborate on why the warning does not appear on Windows or MacOS?

I'm also wondering why the warning appears at all; is it specifically to tell me what you just said?
If so, could it instead say what you just said to be more informative?

Thank you for your time.",hi could elaborate warning appear also wondering warning specifically tell said could instead say said informative thank time,issue
1955192562,We are having the same problem on Linux. This issue is especially problematic as it affects Jax and related packages which demand a newer ml-dypes version.,problem issue especially problematic related demand version,issue
1955157121,"I'll also note that commit is from October (it's far too late to revert it) and essential for JAX, so someone from the TF team will need to figure out what's going on. We can't revert it without breaking JAX releases.",also note commit far late revert essential someone team need figure going ca revert without breaking,issue
1954971860,Same error on Ubuntu 22.04 LTS Install in WSL2 / Windows 11. Has anyone found solution to this?,error install anyone found solution,issue
1954955474,"Hi @michaelpoluektov, https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/mlir/lite this readme is a good high level overview of the passes TF --> TFL goes through, I would say at this point it is outdated but it's a good start to understand the original intention.

Also as noted in the README: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/tf_tfl_passes.cc has the actual passes, there are multiple Canonicalization passes, but it's best to review the code to determine where your logic may fit. If you are able to create a PR we will greatly appreciate the help. Thanks!",hi good high level overview go would say point outdated good start understand original intention also noted actual multiple best review code determine logic may fit able create greatly appreciate help thanks,issue
1954936907,"Hi @andrew-lyons, I believe you are correct that 15.0.1 does not allow 'force_load', though we are not xcode experts so you might want to check with them just in case. Another alternative is to see if you can reduce your version of xcode so that you may continue your work.

@yishuangP can you please take a look? Thanks.

",hi believe correct allow though might want check case another alternative see reduce version may continue work please take look thanks,issue
1954889974,"Hi @Black3rror, you can still run non-converted ops successfully and because they have higher precision the performance in terms of accuracy is actually usually better, however you lose the benefits of efficiency and latency performance. Not all ops are convertable but I can't imagine that arith.constant is one of them.

@zichuan-wei, can you please take a look? Thanks.",hi still run successfully higher precision performance accuracy actually usually better however lose efficiency latency performance ca imagine one please take look thanks,issue
1954856513,"@SuryanarayanaY  I am facing issues regarding bazel build in tensorflow while building from source. Is there a way to resolve the issue so that I can test the code.

Edit : I opened an issue regarding this https://github.com/tensorflow/tensorflow/issues/63002",facing regarding build building source way resolve issue test code edit issue regarding,issue
1954580684,"@fergushenderson I have just tried tensorflow-lite version 2.15.0. Now I am getting the following compilation error:

```
> Failed to transform guice-5.1.0.jar (com.google.inject:guice:5.1.0) to match attributes {artifactType=android-dex, asm-transformed-variant=NONE, dexing-enable-desugaring=true, dexing-enable-jacoco-instrumentation=false, dexing-is-debuggable=true, dexing-min-sdk=21, org.gradle.category=library, org.gradle.libraryelements=jar, org.gradle.status=release, org.gradle.usage=java-runtime}.
      > Execution failed for DexingWithClasspathTransform: /Users/nijatahmadli/.gradle/caches/modules-2/files-2.1/com.google.inject/guice/5.1.0/da25056c694c54ba16e78e4fc35f17fc60f0d1b4/guice-5.1.0.jar.
         > Error while dexing.
           Increase the minSdkVersion to 26 or above.
```
Looks like `guice` was introduced in 2.15.0 and requires minSdkVersion 26 which I guess stems from the use of Java 8 language features. Unfortunately, upgrading to 26 is not an option for our project. ",tried version getting following compilation error transform jar match execution jar error increase like guess use language unfortunately option project,issue
1954383928,@gbaned @angerson This is a quite small PR. And it would be nice to merge this to allow people to use up to date NDK.,quite small would nice merge allow people use date,issue
1954131392,"Hi, I am Anmol, I am currently learning more about Deep Learning and Computer Vision. I am new to the Open Source Contribution. I want to learn more about the technology and want to be part of the Open Source Community by actively contributing to it. Can anyone guide me, on how I can contribute to a good first issue, Thank you.",hi currently learning deep learning computer vision new open source contribution want learn technology want part open source community actively anyone guide contribute good first issue thank,issue
1953999758,"This seems to have been broken in commit a089c4b2777dcd2f88f55229b920102f10264427, reverting it seems to fix at least this problem.",broken commit fix least problem,issue
1953982695,"@MessDeveloper,
Could you please confirm whether you are trying to execute the mentioned code on the base environment or the CUDA, tensorflow installed environment. Also make sure to execute the code in the environment where you installed the tensorflow and CUDA.

I tried to execute in our local environment and it was executed without any issue/error. Thank you! 

",could please confirm whether trying execute code base environment environment also make sure execute code environment tried execute local environment executed without thank,issue
1953906307,@desenSunUBW  Upgrade to GCC 7.4 or newer version will fix the error.,upgrade version fix error,issue
1953814270,"@Mathanraj-Sharma You may use the latest TF version 2.15 and let us know?

![68Mw8bzkKTZ4owJ](https://github.com/tensorflow/tensorflow/assets/84765720/1f424915-d380-4fe4-a810-a081798a1467)


Thank you!",may use latest version let u know thank,issue
1953809610,"Well...
Isn't this the lastest version that's supports directml?
So it is not possible to create a constant value from this state_op variable?",well version possible create constant value variable,issue
1953804224,@schm0 TF v1.15 is an older version which is not actively supported. We request you to kindly upgrade to the latest TF version. Thank you!,older version actively request kindly upgrade latest version thank,issue
1953800891,"> @schm0 Could you please let us know the exact TF version you are using? In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thank you!

I use TensorFlow-DirectML 1.15.8.
It's optimizer class.
The variable gets declared as follows:
```
with tf.device('/CPU:0') :
    with tf.variable_scope(self.name):
        self.iterations = tf.Variable(0, dtype=tf.int64, name='iterations')
```
and in the actual ""main"" optimizer function:
```
....
updates = []
updates += [ state_ops.assign_add( self.iterations, 1) ]
....
return control_flow_ops.group(*updates, name=self.name+'_updates')
```
So this is a global iteration counter.
I need to calculate an offset from this value which I then can subtract from it.
But because the iterations are counted upwards by 1 the offset changes too.
So I need to create a static/constant value.",could please let u know exact version order expedite process please provide code snippet reproduce issue thank use class variable declared actual main function return global iteration counter need calculate offset value subtract upwards offset need create value,issue
1953800514,"@p-s-p-s TF v2.15 is the latest stable version so error is not appearing there.
We recommend you to use the stable TF version. 
Thank you!",latest stable version error recommend use stable version thank,issue
1953763227,"@schm0 Could you please let us know the exact TF version you are using? 
In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thank you!
",could please let u know exact version order expedite process please provide code snippet reproduce issue thank,issue
1953755849,"@aaronsuydam If you don't want to use WSL2 then there are a few alternatives. If your app's functionality can be achieved with TensorFlow 2.10 or older, you can utilize the native GPU support offered by those versions. However, this might limit access to newer features and optimizations. 
Microsoft released the DirectML Plugin for TensorFlow, enabling limited GPU acceleration via Microsoft's DirectML hardware acceleration library. 
Thank you!",want use functionality older utilize native support however might limit access limited acceleration via hardware acceleration library thank,issue
1953628359,Hi @SuryanarayanaY Any update on this PR? Please. Thank you!,hi update please thank,issue
1953549875,"Hi. I'm having the exact same 3 errors after updating tf 2.11 to 2.14. In my case, `tensorflow-gpu` is installed from `conda-forge` channel. Cuda library is installed through `cudatoolkit-dev` from `conda-forge` as well. tf 2.11 didn't show such errors. 

It does not prevent the GPU usage. But I observed around 10%-15% slowdown on model prediction speed in tf 2.14 comparing to the exact same code in tf 2.11. 

Could this be related to those errors? I'm kind of getting mixed messages from the overall discussion.",hi exact case channel library well show prevent usage around slowdown model prediction speed exact code could related kind getting mixed overall discussion,issue
1953506453,"Hi @mahdiaslanimk As mentioned above comments, please submit the PR to the github.com/keras-team/keras repository instead. Thank you!
Cc @fchollet, @qlzh727

Thank you @mihaimaruseac. ",hi please submit repository instead thank thank,issue
1953506201,"Well, I checked with the latest nightly build. The issue is still there. ",well checked latest nightly build issue still,issue
1953496426,"@sushreebarsa Hey! Sorry for the late response here. I am trying to develop a Windows app that leverages some functionality that has been implemented using tensor flow. Will the lack of GPU support impact that? As in, can a windows app that has some backend functionality that uses TF gpu stuff work on a system without wsl installed? I don't want users to have to install wsl to get that benefit, maybe i can find a workaround if that's the case? This is also just me being curious, I'm a college student, so this is really just a hobby for me. No stress either way.",hey sorry late response trying develop functionality tensor flow lack support impact functionality stuff work system without want install get benefit maybe find case also curious college student really hobby stress either way,issue
1953305873,@sushreebarsa The reason you couldn't reproduce the error in colab is because the warnings are suppressed by default. Could you please check this colab https://colab.research.google.com/drive/1JuQriKXe-aJBAbValQK-8BFGtktzw4IW?usp=sharing ?,reason could reproduce error suppressed default could please check,issue
1953234331,"> Thank you @RRiva, yes Intel releases tensorflow-intel for the Windows platform. In pip installation, TensorFlow-intel is automatically installed while installing TensorFlow on the Windows platform. We are working to fix it for Poetry Installation as well which should be available in the TF 2.16 release

Will this also fix all the other metadata issues? Can't install tensorflow-io-gcs-filesystem on alpine linux (I know not ideal but it's a specific environment requirement) Sorry if this is unrelated, wondering if the metadata will fix all",thank yes platform pip installation automatically platform working fix poetry installation well available release also fix ca install alpine know ideal specific environment requirement sorry unrelated wondering fix,issue
1952668745,"@sushreebarsa I need that specific version of tf needed to be built, I would appreciate any input/suggestion to overcome this error ",need specific version built would appreciate overcome error,issue
1952609531,"@SuryanarayanaY
Does 'success' mean successful bug reproducing?
It was TF2.17 built from source with `--config=dbg` flag.",mean successful bug built source flag,issue
1952495469,"Unfortunately, this doesn't seem to be very easy if there has been no response for a month",unfortunately seem easy response month,issue
1952433717,"> In release build, it does not crash.

Hi @Sehun0819 , As per your note above, could you please confirm which build it was success ?",release build crash hi per note could please confirm build success,issue
1952372112,"@SuryanarayanaY
I just found TF2.16 behaves in the same manner. Below is execution log in my environment.
Note that it was also built with `--config=dbg`.
```
Python 3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2024-02-19 21:38:38.963814: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
>>> tf.__version__
'2.16.0-rc0'
>>> tf.raw_ops.LoopCond(input=True)
2024-02-19 21:38:54.052798: F tensorflow/core/graph/graph_partition.cc:644] Check failed: !frame_name.empty()
Aborted (core dumped)
```",found manner execution log environment note also built python main type help copyright license information import binary use available enable following rebuild appropriate compiler check aborted core,issue
1952318382,"Occurs with tensorflow[and-cuda]==2.15.0.post0 
Tried building from source and installing via pip & conda.
```
$ python
Python 3.11.7 (main, Jan 29 2024, 16:03:57) [GCC 13.2.1 20230801] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2024-02-19 12:55:16.996299: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 12:55:17.017067: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-02-19 12:55:17.017083: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-02-19 12:55:17.017652: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-02-19 12:55:17.020872: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
>>> print(tf.config.list_physical_devices('GPU'))
2024-02-19 12:56:09.845769: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-02-19 12:56:09.870239: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-02-19 12:56:09.870422: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
>>> 

```

On tf-nightly[and-cuda] this doesn't occur anymore.
```
$ python
Python 3.11.7 (main, Jan 29 2024, 16:03:57) [GCC 13.2.1 20230801] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from tensorflow.python.platform import build_info as tf_build_info
2024-02-19 13:04:56.451101: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-19 13:04:56.474818: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
>>> print(""cudnn_version"",tf_build_info.build_info['cudnn_version'])
cudnn_version 8
>>> 
>>> print(""cuda_version"",tf_build_info.build_info['cuda_version'])
cuda_version 12.3
>>> print(tf.config.list_physical_devices('GPU'))
2024-02-19 13:13:48.002965: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-02-19 13:13:48.006970: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-02-19 13:13:48.007068: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]

```
The NUMA thing persists. Also on Linux with NUMA configured.

System information:
(Arch Linux + Zen kernel patches + KDE Plasma)
```
$ uname -a
Linux somelegion 6.7.4-zen1-1-zen #1 ZEN SMP PREEMPT_DYNAMIC Mon, 05 Feb 2024 22:07:37 +0000 x86_64 GNU/Linux
```

```
$ nvidia-smi
Mon Feb 19 13:09:28 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 4090 ...    On  | 00000000:01:00.0  On |                  N/A |
| N/A   40C    P5              18W /  95W |   1414MiB / 16376MiB |     18%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
```

```
$ zgrep CONFIG_NUMA /proc/config.gz                                                                                                                                                                                             
CONFIG_NUMA_BALANCING=y
CONFIG_NUMA_BALANCING_DEFAULT_ENABLED=y
CONFIG_NUMA=y
# CONFIG_NUMA_EMU is not set
CONFIG_NUMA_KEEP_MEMINFO=y
```

```
sudo lspci -vvv -s 01:00.0 
01:00.0 VGA compatible controller: NVIDIA Corporation GN21-X11 (rev a1) (prog-if 00 [VGA controller])
	Subsystem: Lenovo GN21-X11
	Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin A routed to IRQ 132
	IOMMU group: 16
	Region 0: Memory at 85000000 (32-bit, non-prefetchable) [size=16M]
	Region 1: Memory at 4000000000 (64-bit, prefetchable) [size=16G]
	Region 3: Memory at 4400000000 (64-bit, prefetchable) [size=32M]
	Region 5: I/O ports at 6000 [size=128]
	Expansion ROM at 86080000 [virtual] [disabled] [size=512K]
	Capabilities: [60] Power Management version 3
		Flags: PMEClk- DSI- D1- D2- AuxCurrent=0mA PME(D0+,D1-,D2-,D3hot+,D3cold-)
		Status: D0 NoSoftRst+ PME-Enable- DSel=0 DScale=0 PME-
	Capabilities: [68] MSI: Enable+ Count=1/1 Maskable- 64bit+
		Address: 00000000fee003d8  Data: 0000
	Capabilities: [78] Express (v2) Legacy Endpoint, MSI 00
		DevCap:	MaxPayload 256 bytes, PhantFunc 0, Latency L0s unlimited, L1 <64us
			ExtTag+ AttnBtn- AttnInd- PwrInd- RBE+ FLReset+
		DevCtl:	CorrErr+ NonFatalErr+ FatalErr+ UnsupReq+
			RlxdOrd+ ExtTag+ PhantFunc- AuxPwr- NoSnoop+ FLReset-
			MaxPayload 256 bytes, MaxReadReq 512 bytes
		DevSta:	CorrErr- NonFatalErr- FatalErr- UnsupReq- AuxPwr- TransPend-
		LnkCap:	Port #0, Speed 16GT/s, Width x16, ASPM L1, Exit Latency L1 <16us
			ClockPM+ Surprise- LLActRep- BwNot- ASPMOptComp+
		LnkCtl:	ASPM L1 Enabled; RCB 64 bytes, Disabled- CommClk+
			ExtSynch- ClockPM+ AutWidDis- BWInt- AutBWInt-
		LnkSta:	Speed 2.5GT/s (downgraded), Width x16
			TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt-
		DevCap2: Completion Timeout: Range AB, TimeoutDis+ NROPrPrP- LTR+
			 10BitTagComp+ 10BitTagReq+ OBFF Via message, ExtFmt- EETLPPrefix-
			 EmergencyPowerReduction Not Supported, EmergencyPowerReductionInit-
			 FRS-
			 AtomicOpsCap: 32bit- 64bit- 128bitCAS-
		DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis- LTR+ 10BitTagReq- OBFF Disabled,
			 AtomicOpsCtl: ReqEn-
		LnkCap2: Supported Link Speeds: 2.5-16GT/s, Crosslink- Retimer+ 2Retimers+ DRS-
		LnkCtl2: Target Link Speed: 16GT/s, EnterCompliance- SpeedDis-
			 Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS-
			 Compliance Preset/De-emphasis: -6dB de-emphasis, 0dB preshoot
		LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete+ EqualizationPhase1+
			 EqualizationPhase2+ EqualizationPhase3+ LinkEqualizationRequest-
			 Retimer- 2Retimers- CrosslinkRes: unsupported
	Capabilities: [b4] Vendor Specific Information: Len=14 <?>
	Capabilities: [100 v1] Virtual Channel
		Caps:	LPEVC=0 RefClk=100ns PATEntryBits=1
		Arb:	Fixed- WRR32- WRR64- WRR128-
		Ctrl:	ArbSelect=Fixed
		Status:	InProgress-
		VC0:	Caps:	PATOffset=00 MaxTimeSlots=1 RejSnoopTrans-
			Arb:	Fixed- WRR32- WRR64- WRR128- TWRR128- WRR256-
			Ctrl:	Enable+ ID=0 ArbSelect=Fixed TC/VC=ff
			Status:	NegoPending- InProgress-
	Capabilities: [250 v1] Latency Tolerance Reporting
		Max snoop latency: 34326183936ns
		Max no snoop latency: 34326183936ns
	Capabilities: [258 v1] L1 PM Substates
		L1SubCap: PCI-PM_L1.2+ PCI-PM_L1.1+ ASPM_L1.2+ ASPM_L1.1+ L1_PM_Substates+
			  PortCommonModeRestoreTime=255us PortTPowerOnTime=10us
		L1SubCtl1: PCI-PM_L1.2- PCI-PM_L1.1- ASPM_L1.2- ASPM_L1.1-
			   T_CommonMode=0us LTR1.2_Threshold=281600ns
		L1SubCtl2: T_PwrOn=10us
	Capabilities: [128 v1] Power Budgeting <?>
	Capabilities: [420 v2] Advanced Error Reporting
		UESta:	DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
		UEMsk:	DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
		UESvrt:	DLP+ SDES+ TLP- FCP+ CmpltTO- CmpltAbrt- UnxCmplt- RxOF+ MalfTLP+ ECRC- UnsupReq- ACSViol-
		CESta:	RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr-
		CEMsk:	RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr+
		AERCap:	First Error Pointer: 00, ECRCGenCap- ECRCGenEn- ECRCChkCap- ECRCChkEn-
			MultHdrRecCap- MultHdrRecEn- TLPPfxPres- HdrLogCap-
		HeaderLog: 00000000 00000000 00000000 00000000
	Capabilities: [600 v1] Vendor Specific Information: ID=0001 Rev=1 Len=024 <?>
	Capabilities: [900 v1] Secondary PCI Express
		LnkCtl3: LnkEquIntrruptEn- PerformEqu-
		LaneErrStat: 0
	Capabilities: [bb0 v1] Physical Resizable BAR
		BAR 0: current size: 16MB, supported: 16MB
		BAR 1: current size: 16GB, supported: 64MB 128MB 256MB 512MB 1GB 2GB 4GB 8GB 16GB
		BAR 3: current size: 32MB, supported: 32MB
	Capabilities: [c1c v1] Physical Layer 16.0 GT/s <?>
	Capabilities: [d00 v1] Lane Margining at the Receiver <?>
	Capabilities: [e00 v1] Data Link Feature <?>
	Kernel driver in use: nvidia
	Kernel modules: nouveau, nvidia_drm, **nvidia**
```

Might be from the NVidia side if one belives the linked doc:

> What:		/sys/bus/pci/devices/.../numa_node
Date:		Oct 2014
Contact:	Prarit Bhargava <prarit@redhat.com>
Description:
		This file contains the NUMA node to which the PCI device is
		attached, or -1 if the node is unknown.  The initial value
		comes from an ACPI _PXM method or a similar firmware
		source.  If that is missing or incorrect, this file can be
		written to override the node.  In that case, please report
		a firmware bug to the system vendor.  Writing to this file
		taints the kernel with TAINT_FIRMWARE_WORKAROUND, which
		reduces the supportability of your system.",post tried building source via pip python python main type help copyright license information import custom may see slightly different numerical due different computation turn set environment variable unable register factory register factory one already registered unable register factory register factory one already registered unable register factory register factory one already registered binary use available enable following rebuild appropriate compiler print successful node read negative value must least one node node zero see successful node read negative value must least one node node zero see successful node read negative value must least one node node zero see occur python python main type help copyright license information import custom may see slightly different numerical due different computation turn set environment variable binary use available enable following rebuild appropriate compiler print print print successful node read negative value must least one node node zero see successful node read negative value must least one node node zero see successful node read negative value must least one node node zero see thing also system information arch kernel plasma mon mon driver version version name volatile fan temp compute mig mib mib default set compatible controller corporation rev controller subsystem control status latency interrupt pin group region memory region memory region memory region expansion virtual disabled power management version status address feed data express legacy latency unlimited u port speed width exit latency u speed width completion range via message completion u disabled link target link speed transmit margin normal operating range compliance current level unsupported vendor specific information virtual channel status status latency tolerance snoop latency snoop latency power advanced error first error pointer vendor specific information secondary express physical bar bar current size bar current size bar current size physical layer lane margining receiver data link feature kernel driver use kernel might side one linked doc date contact description file node device attached node unknown initial value come method similar source missing incorrect file written override node case please report bug system vendor writing file kernel supportability system,issue
1952298868,"@sushreebarsa 
Hi!
Please note that it crashed in the middle of shape inference step, not by being checked status.
Doesn't it need appropriate input checker for preventing crash?
",hi please note middle shape inference step checked status need appropriate input checker crash,issue
1952274261,"@tilakrayal 
Hi!
As I mentioned, it leads to crash only when TensorFlow is built in Debug mode(i.e., built from source with `--config=dbg`).
I guess TF in the gists installed by `pip` is not a Debug build, and it won't lead to crash.
Would it be able to check it in TF debug build?",hi crash built mode built source guess pip build wo lead crash would able check build,issue
1952257308,"@sushreebarsa
Sorry for confusing. `colors` should be `0.0`, not `0.0f`.
Please check [it](https://colab.research.google.com/drive/1wzwA3ALjPxIBaP-aajrYnKHiOWv2FgDg?usp=sharing).",sorry color please check,issue
1952236575,"Thanks for your answer but this results in:
`TypeError: List of Tensors when single Tensor expected.`
Does it matter that the variable gets assigned with state_ops.assign_add?
I can't find much info about state_ops.",thanks answer list single tensor matter variable assigned ca find much,issue
1952122728,"@Sehun0819 If MatrixDiagV3 doesn't work due to type limitations, could you consider alternative approaches to achieve the desired functionality like tf.linalg.diag?
Thank you!",work due type could consider alternative achieve desired functionality like thank,issue
1952117865,@Sehun0819 The values in indices should be within the valid range of params along the gather_dim dimension. Accessing invalid indices results in errors. Thank you!,index within valid range along dimension invalid index thank,issue
1952113801,"@Sehun0819 Could you please provide the access to the codebase or give the gist to replicate the issue reported?
Please ensure both tensors have the same data type, usually tf.float32. For input requirements please refer to this [doc](https://www.tensorflow.org/api_docs/python/tf/image/draw_bounding_boxes). I faced a different [error](https://colab.research.google.com/gist/sushreebarsa/1eb1d147b40ffc034f11c33a47405bb1/62981.ipynb) while replicating the issue.
Thank you!",could please provide access give gist replicate issue please ensure data type usually input please refer doc faced different error issue thank,issue
1952100298,"@tymorrow TensorRT installation might require additional steps or dependencies, particularly on specific platforms like Ubuntu. TensorRT is an NVIDIA-specific library optimized for NVIDIA GPUs. It might not be available on non-NVIDIA hardware or specific Linux distributions like Ubuntu due to package management differences.
Thank you!",installation might require additional particularly specific like library might available hardware specific like due package management thank,issue
1952094140,"@khizii,
Could you please take a  look at the output_details, specifically the shape of each output tensor, you will see that the order of outputs is actually [classes, boxes, num_detections, scores] or [scores, boxes, num_detections, classes] (because the shape of output at index 3 is same as shape of output 0, so one of them is scores and other is classes). 

Its likely that your Java code assumes [boxes, classes, scores, num_detections] - which is why the error says that you are trying to copy a wrong-shaped tensor into another one.
https://github.com/tensorflow/tensorflow/issues/46692
https://github.com/tensorflow/tensorflow/issues/55341

Thank you!",could please take look specifically shape output tensor see order actually class class shape output index shape output one class likely code class error trying copy tensor another one thank,issue
1952091325,"@akuma308 The most straightforward solution is to create individual metadata entries for each of the two output tensors. Please refer to the ImageSegmenterWriter [documentation](https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/task/vision/segmenter/ImageSegmenter) for examples on how to construct metadata objects for multiple outputs. Kindly use the latest TF version and let us know?
Thank you!",straightforward solution create individual two output please refer documentation construct multiple kindly use latest version let u know thank,issue
1952070199,"@Sehun0819,
I tried to execute the mentioned code on tf-nightly(2.17.0-dev20240218) on both [GPU](https://colab.sandbox.google.com/gist/tilakrayal/34910c3146d13fe56c2ac1860a449205/untitled1733.ipynb) and [CPU](https://colab.sandbox.google.com/gist/tilakrayal/dfc7675fe1cf044fd034bb83b7404068/untitled1732.ipynb) and it was executed without any issue/error. Kindly find the gist attached. Thank you!",tried execute code executed without kindly find gist attached thank,issue
1952065721,"@Mathanraj-Sharma You are using an older version of TF which is not actively supported. Could you please check this [doc](https://www.tensorflow.org/install/source#ubuntu) for reference?
Thank you!",older version actively could please check doc reference thank,issue
1952055294,"@p-s-p-s I tried to replicate the issue on colab and didn't face the error reported. Could you check this [gist](https://colab.research.google.com/gist/sushreebarsa/243351ba215c7da47efe5f712de96d81/62963.ipynb) and let us know?
Thank you!",tried replicate issue face error could check gist let u know thank,issue
1952026763,"@andvsilva These errors suggest conflicts between different implementations of cuDNN, cuFFT, and cuBLAS libraries. 
Please verify that CUDA environment variables like CUDA_HOME and CUDA_VISIBLE_DEVICES are set correctly.
Also please do ensure you have the latest NVIDIA drivers installed for your GPU. 
I tried to replicate this on colab, please find the [gist](https://colab.research.google.com/gist/sushreebarsa/34a237fbfd90c3102aa6987a58f6e4b6/untitled933.ipynb#scrollTo=qzOkxZu-lWyX) here.
Thank you!",suggest different please verify environment like set correctly also please ensure latest tried replicate please find gist thank,issue
1951991963,This PR has been approved and taged `ready to pull` for over a month now. Is there anything I should do? Thanks! @gbaned @aaudiber ,ready pull month anything thanks,issue
1951970126,"@schm0 Directly converting a tf.Variable to a tf.constant in TensorFlow is not possible as tf.Variable is designed to be mutable and track its value through iterations. There are a few ways to achieve this such as below;
1. Access the variable's current value
```
current_value = someTFvariable.read_value()
```
2. Create a constant tensor from the value
```
new_var = tf.constant(current_value)
```

Thank you!",directly converting possible designed mutable track value way achieve access variable current value create constant tensor value thank,issue
1951915250,"Hi @Sehun0819 ;

I have replicated the issue with tf-nightly(2.17.0-dev20240218) version.Attached snapshot below for reference.


<img width=""1488"" alt=""Screenshot 2024-02-19 at 13 41 47"" src=""https://github.com/tensorflow/tensorflow/assets/116063290/95e4b31a-d22a-4ed0-a5fe-733e8aa7e04f"">
",hi replicated issue snapshot reference,issue
1951813007,"Hi @Sehun0819 ,

Thanks for reporting. Could you please confirm whether this works fine with TF2.16 or lower ? I don't see any changes in `.bazelrc` either.",hi thanks could please confirm whether work fine lower see either,issue
1951810645,"Hi, I'm Paridhi and I'm new to this community. I would like to contribute, I've looked at the issues with labels - ""good fist time"" and ""contribution welcome"" but those issues are in stale state. 
Please let me know if I can begin with this issue or any other relatively easier issue present in the queue. Thank you.",hi new community would like contribute good fist time contribution welcome stale state please let know begin issue relatively easier issue present queue thank,issue
1951786127,"@fancyerii , Just to verify please cross check whether cudnn_version: 8 & cuda_version 12.2 are there or not using the code below.

```
from tensorflow.python.platform import build_info

print(""cudnn_version"",build_info.build_info['cudnn_version'])
print(""cuda_version"",build_info.build_info['cuda_version'])
```",verify please cross check whether code import print print,issue
1951781718,"Hi @fancyerii ,

Thanks for reporting.Its known issue.It seems this might be due to duplicate registry AFAIK. Though its logging an error it seems the GPU works fine.In your logs also the GPUs also detected and it will not affecting the execution. Could you confirm whether there is a problem with execution of your code?

Same issue was discussed in #62002, #62075.May please refer for more responses there. Thanks!",hi thanks known might due duplicate registry though logging error work also also affecting execution could confirm whether problem execution code issue please refer thanks,issue
1951597438,"@cantonios When you get the chance, you mind taking a look at my solution for the dense shape validation? Appreciate it!",get chance mind taking look solution dense shape validation appreciate,issue
1951475567,"When a keras model is created with TF2.16.0 ([keras model available here)](https://www.dropbox.com/scl/fi/z0womeh2hj51i6tqj7wc0/TF2.16.0_model_classifier_CNN.keras?rlkey=c3dqmdqh3c516twhpdxjercg7&dl=0), the conversion still fails but with a different error:

[log2.txt](https://github.com/tensorflow/tensorflow/files/14324786/log2.txt)
",model model available conversion still different error,issue
1951038649,"hi @gbaned  @sagunb 

Sorry for the late reply. 
It took me sometime to figure out how to write a proper test for this function.

Hope the unit test explains the changes.

Cheers
Kevin",hi sorry late reply took sometime figure write proper test function hope unit test,issue
1950749128,This PR was closed because it has been inactive for 14 days since being marked as stale. Please reopen if you'd like to work on this further.,closed inactive day since marked stale please reopen like work,issue
1949914235,"Hello, thanks for the response.
In OpenCV (4.9.0), I didn't see any backends that would correspond with the Coral TPU. Calling dnn::getAvailableBackends() only lists DNN_BACKEND_OPENCV with DNN_TARGET_CPU (I have libusb-dev installed)
As for The TFLite version, I don't know what I should be downloading...",hello thanks response see would correspond coral calling version know,issue
1949896870,"> Hi, I have resolved this issue by changing the package I'm using when inferencing. Thanks.

Can you tell me what package?
",hi resolved issue package thanks tell package,issue
1949860153,@drewshark TensorFlow Nightly can potentially resolve this issue as the bug fix might be included there.,nightly potentially resolve issue bug fix might included,issue
1949853776,"@khizii I think there is a mismatch between the output shape of your TFLite model. The model produces 10 values, while you're expecting 40 (10 x 4).",think mismatch output shape model model,issue
1949599228,Unfortunately still doesn't work for me either. ,unfortunately still work either,issue
1949440252,"> Check NMS in our Tensorflow 3d repository https://github.com/google-research/google-research/blob/master/tf3d/object_detection/box_utils/box_ops.py

Hi,

Thanks, it seems that it does the job, however it has a bit different interface that of the 2D version in tf.image as it does not return indices. Is there a proper implementation for the task?

Thanks.",check repository hi thanks job however bit different interface version return index proper implementation task thanks,issue
1949364989,"> Thank you! Sorry for so many change requests! (It's from our internal checks.)

Hey, I get it -- totally fine! Thanks for bearing with me while I made them.",thank sorry many change internal hey get totally fine thanks bearing made,issue
1949269251,"Hi @MinaBabahaji-ML, in attempting to reproduce your issue I ran into this issue:

```
adb shell ""cd /data/local/tmp && LD_LIBRARY_PATH=. ./model_test --model=model_files/sample.tflite --input_shape=1,105 --output_shape=1,512""
CANNOT LINK EXECUTABLE ""./model_test"": library ""libc++_shared.so"" not found: needed by main executable
```

I'm using an emulator and NDK=25.2.9519653. Are we missing some steps?

Thanks for your help.",hi reproduce issue ran issue shell link executable library found main executable emulator missing thanks help,issue
1948806875,"@LakshmiKalaKadali I changed them both to nightly builds, and still receive the same issue:

```
INFO: Initialized TensorFlow Lite runtime.
TensorFlow Lite Error: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
TensorFlow Lite Error: Node number 1 (FlexMutableHashTableV2) failed to prepare.
```

I am unable to apply the linker flag in the tflite documentation [here](https://www.tensorflow.org/lite/guide/ops_select) because of the linker error I pasted in my original post body. Perhaps xcode 15 does not allow the usage of `-force-load`?",nightly still receive issue lite lite error select included given model interpreter make sure flex delegate inference android resolved dependency see lite error node number prepare unable apply linker flag documentation linker error pasted original post body perhaps allow usage,issue
1948801933,"If it doesn't even build, not really point in the pull request being active. It was more of an idea on how to speed up the ""meat"" of the implementation.",even build really point pull request active idea speed meat implementation,issue
1948766330,This code doesn't actually compile with the latest version of tensorflow - there's some kind of type mismatch that causes the convolution kernels to fail to build.,code actually compile latest version kind type mismatch convolution fail build,issue
1948657538,"Hey @tbikash62 i need your help as our problem is same but i can't figure out this problem on my own. i am getting the same error as yours Please help me
My code is below:
![Screenshot 2024-02-16 203916](https://github.com/tensorflow/tensorflow/assets/137616658/f0d94b00-0841-476f-93f3-211117a5664f)

",hey need help problem ca figure problem getting error please help code,issue
1948381252,"I just don't want to install many versions of cuda. I tried to v2.14.0, and it worked. Maybe I should use docker to avoid install cuda.",want install many tried worked maybe use docker avoid install,issue
1948282715,"Thanks @ldb1026, I had the same problem and it indeed was the order of the inputs in the representative dataset. For anyone that needs extra clarification: the input order of the actual graph (after conversion) might be different than the order as defined in your python code, and, apparently, the representative dataset needs input ordering of the post-conversion graph...",thanks problem indeed order representative anyone need extra clarification input order actual graph conversion might different order defined python code apparently representative need input graph,issue
1948194952,"> I am having the same issue when I use the latest docker colab local runner. Using CPU works. Anyone has a solution for that?

I replied below. Hope it works.",issue use latest docker local runner work anyone solution hope work,issue
1948191646,"I solved this error.
As the other people said, this error is occurred by the value of epsilon, which is one of the arguments of **LayerNormalization** layer in keras API.
https://keras.io/api/layers/normalization_layers/layer_normalization/
You can see the specification of LayerNormalization layer in the above Web page.

```
keras.layers.LayerNormalization(
    axis=-1,
    epsilon=0.001,
    center=True,
    scale=True,
    rms_scaling=False,
    beta_initializer=""zeros"",
    gamma_initializer=""ones"",
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    \*\*kwargs
)
```

The default value of epsilon is 0.001(1e-3). I fixed it into 1e-7 and now I'm free from this horrible error.

My graphic card is RTX3060 and your graphic card is RTX4070 Ti, these two graphic cards do not support float16 data type operation in keras(I don't know why). And you can see **[DT_BFLOAT16]** in your error log. In the LayerNormalization layer, there's such an operation which adds **epsilon** to some value. Its default value is 1e-3(float16 type) so the error is occurred. If you fix the value of epsilon into 1e-7, its data type will be changed into float32 and the error will not be occurred.

Like this
```
...
        self.layernorm_1 = layers.LayerNormalization(epsilon=1e-7)
        self.layernorm_2 = layers.LayerNormalization(epsilon=1e-7)
...
```",error people said error value epsilon one layer see specification layer web page default value epsilon fixed free horrible error graphic card graphic card ti two graphic support float data type operation know see error log layer operation epsilon value default value float type error fix value epsilon data type float error like,issue
1948000494,@Moddingear Please ensure you are using a TensorFlow Lite version compatible with your Coral TPU model. Your OpenCV version should support Coral TPU acceleration as well. Thank you!,please ensure lite version compatible coral model version support coral acceleration well thank,issue
1947960043,"Hi @andrew-lyons,

Try to specify both `TensorFlowLiteSwift` and  `TensorFlowLiteSelectTfOps` are the same nightly versions in `pod` file target.
```
  # Specify in your pod file target
  pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly'
  pod 'TensorFlowLiteSelectTfOps', '~> 0.0.1-nightly'

```
Thank You
",hi try specify nightly pod file target specify pod file target pod pod thank,issue
1947889969,"@fancyerii,
Could you please let us know that there is any specific reason to use cuda-**11.8** with tensorflow **2.15**.  The latest tensorflow v2.15 is compatible with the CUDA 12.2 which might be the reason. Could you please try to follow the tested build configurations from the official document for the smooth installation.
https://www.tensorflow.org/install/source#gpu

`cp -r clang+llvm-16.0.0-x86_64-linux-gnu-ubuntu-18.04/* /usr`

Thank you!

",could please let u know specific reason use latest compatible might reason could please try follow tested build official document smooth installation thank,issue
1947791331,"@sachinprasadhs,
I was able to reproduce the issue on tensorflow v2.15 and tf-nightly. Kindly find the gist of it [here](https://colab.sandbox.google.com/gist/tilakrayal/5ee8137476e04b096fbfbb5f6b3c21c6/untitled1730.ipynb).",able reproduce issue kindly find gist,issue
1947495746,@cantonios Can you get the internal checks running to test the latest version out?,get internal running test latest version,issue
1947317212,"> > > > @cantonios I can see that some internal checks failed, but can't see what they are.
> > > 
> > > 
> > > ```
> > > ERROR: tensorflow/core/kernels/mkl/BUILD:104:22: in cc_library rule //tensorflow/core/kernels/mkl:mkl_sparse_matrix_matmul_op: Visibility error:
> > > target '//tensorflow/core/kernels/sparse:mat_mul_op.h' is not visible from
> > > target '//tensorflow/core/kernels/mkl:mkl_sparse_matrix_matmul_op'
> > > ```
> > 
> > 
> > Is it possible to get a commandline reproducer for this error? Can't reproduce locally, so fixing it might be trial-and-error that spams this PR with commits and spams you with emails!
> 
> I'm not sure... I'm surprised bazel doesn't find the dependency issue. Maybe Google's internal tooling is more strict?

All right, trying some things. Thank you for your reviews, by the way!",see internal ca see error rule visibility error target visible target possible get reproducer error ca reproduce locally fixing might sure find dependency issue maybe internal tooling strict right trying thank way,issue
1947237590,I am having the same issue when I use the latest docker colab local runner. Using CPU works. Anyone has a solution for that?,issue use latest docker local runner work anyone solution,issue
1946886857,"> Thank you for your comment @qnlzgl . I have attempted to fix the issue in various ways, but none have proven successful for me.

I feel it's okay to leave the errors as it is, I have this error while importing tensorflow, but still could use GPUs like normal.",thank comment fix issue various way none proven successful feel leave error still could use like normal,issue
1946694244,"Thanks for getting in touch! 
What I want is the HTML files of Tensorflow's online document (v2.15, latest release version), especially for API references. Just like https://numpy.org/doc/, where I can download the HTML zip file. Or if you have any scripts, like in https://github.com/pytorch/pytorch/tree/main/docs, where I could use the command 'make html' to generate the API document, I would appreciate it if you could tell me where to find it.
",thanks getting touch want document latest release version especially like zip file like could use command generate document would appreciate could tell find,issue
1946578802,"Hi @Doomski99, I have not. Without diving deep into the code, I pretty much just accepted TF's behavior at that point. ",hi without diving deep code pretty much accepted behavior point,issue
1946189568,"Hello @christian-steinmeyer, I'm encountering the same problem, some Conv2d layers are DQing into int8 and some are not. Have you found a workaround or the source of the problem? ",hello problem found source problem,issue
1946066420,"@ShuyinOuyang,
Could you please confirm whether you are mentioning these official documents.
https://www.tensorflow.org/community/contribute/docs
https://www.tensorflow.org/community/contribute/docs_ref

If you are referring to the same documents, those are not changeable/modify by the community. Only the developers can modify according to the updates.

If you want any changes that need to happen, please feel free to mention here.  Internally, can discuss with the developer and try to make changes if those are valid. Community can raise the PR from here for the other changes. https://github.com/tensorflow/tensorflow/pulls

Thank you",could please confirm whether official community modify according want need happen please feel free mention internally discus developer try make valid community raise thank,issue
1945871403,"Thank you for your comment @qnlzgl . I have attempted to fix the issue in various ways, but none have proven successful for me.",thank comment fix issue various way none proven successful,issue
1945735149,"@Mistobaan This is not yet implemented nor any workaround available it seems, Please refer the existing projects like tensorflow/serving that build TensorFlow as an external dependency and could you try adapting their scripts to your specific needs. If you already have TensorFlow installed, setting up custom targets to link against it remains a viable option. You may also consider smaller libraries like TensorFlow Lite or TensorFlow.js if your project doesn't require the full TensorFlow stack.

Please create a new ticket if you still have a concern!
Thank you!",yet available please refer like build external dependency could try specific need already setting custom link remains viable option may also consider smaller like lite project require full stack please create new ticket still concern thank,issue
1945722059,"@leandro-gracia-gil Other TensorFlow functions like tf.stack and tf.concat already support heterogeneous tensor inputs and automatically perform type casting if necessary.  It is a TF 1.x related issue which needs to be upgraded to the latest as older versions are not actively supported. Kindly refer to the migration doc [here](https://www.tensorflow.org/guide/migrate). Please create a new ticket if you still have a concern?
Thank you!",like already support heterogeneous tensor automatically perform type casting necessary related issue need latest older actively kindly refer migration doc please create new ticket still concern thank,issue
1945718096,"@babak-badnava Could you please upgrade to the latest TF version as we are unable to reproduce the same on the new code base. Kindly have a look at this migration [doc](https://www.tensorflow.org/guide/migrate).
If you still have a concern then please create a new ticket!
Thank you!",could please upgrade latest version unable reproduce new code base kindly look migration doc still concern please create new ticket thank,issue
1945706002,@rasitsimsek One workaround could be to wrap the existing EditDistance class with a custom constructor that takes the desired parameters and sets the options internally. Please upgrade to the latest TF version and if you still have a concern then please create a new ticket. Thank you!,one could wrap class custom constructor desired internally please upgrade latest version still concern please create new ticket thank,issue
1945702393,"@nrhinehart Could you check this [doc](https://www.tensorflow.org/api_docs/python/tf/norm) of tf.norm.  This function can compute several different vector norms. 
If you still have any concerns, please raise a new request. Thank you!",could check doc function compute several different vector still please raise new request thank,issue
1945544179,"not sure whether it helps to solve the problem, but there are some discussions about Qualcomm disabling NNAPI support for Snapdragon 8 Gen 2:
https://ai-benchmark.net/index.php?threads/what-are-qualcomm-qnn-htp-dsp-delegates.44/",sure whether solve problem support snapdragon gen,issue
1945489968,"> I believe the issue is with your `python version`.
> 
> You've specified your python version to be 3.12. Checkout the official [TensorFlow website](https://www.tensorflow.org/install). Currently it is supported for `python 3.8-3.11`.
> 
> <img alt=""Screenshot 2024-02-15 at 11 18 00 AM"" width=""927"" src=""https://private-user-images.githubusercontent.com/76887609/304963676-c31fd2d0-517a-4dd0-bc15-ab6758c36f9d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDc5ODE2MDAsIm5iZiI6MTcwNzk4MTMwMCwicGF0aCI6Ii83Njg4NzYwOS8zMDQ5NjM2NzYtYzMxZmQyZDAtNTE3YS00ZGQwLWJjMTUtYWI2NzU4YzM2ZjlkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAyMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMjE1VDA3MTUwMFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTNkMzAyYTg3ZWI4YWI0ZTQ3MzdiY2I5ODM0NTBjYTNkZWI2YTkyYzg5MjA2MTVhYzJiNTdlYTgzMjFlM2MyMjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.iEqK1V6aNfLFZjsEbaRLIXeZWN5cKXMLqXZxGKgY8Wo"">

thank you so much, i just reinstalled my python and it work  ",believe issue python version python version official currently python thank much python work,issue
1945443762,"@rohitkumar9989,
There is an issue with the `tf-lite-modelmaker`. This issue is unlikely to be resolved soon. Below are the couple of options to use **tflite-model-maker**:

1. Just use [mediapipe model maker](https://developers.google.com/mediapipe/solutions/model_maker) in colab (You can use a GPU/TPU here as well though there are potential limitations) https://research.google.com/colaboratory/faq.html

2. Attempt to fix your current setup, can you try pip install scann?

3. Build Tensorflow from source w/o AVX support and with Cuda(Nvidia GPUs)/RocM(AMD GPUs) support https://www.tensorflow.org/install/source (For WSL you would follow linux instructions), then try to simultaneously install all your required packages with the built package so that they have the best chance to play nicely together

4. Use a lower level API and skip tflite-model-maker/mediapipe-model-maker and use [keras](https://keras.io/)/[TF](https://www.tensorflow.org/tutorials) directly

https://github.com/tensorflow/tensorflow/issues/62942#issuecomment-1938193409
https://github.com/tensorflow/tensorflow/issues/60431

Thank you!
",issue issue unlikely resolved soon couple use use model maker use well though potential attempt fix current setup try pip install build source support support would follow try simultaneously install built package best chance play nicely together use lower level skip use directly thank,issue
1945413710,"I believe the issue is with your `python version`.

You've specified your python version to be 3.12. Checkout the official [TensorFlow website](https://www.tensorflow.org/install). Currently it is supported for `python 3.8-3.11`.

<img width=""927"" alt=""Screenshot 2024-02-15 at 11 18 00 AM"" src=""https://github.com/tensorflow/tensorflow/assets/76887609/c31fd2d0-517a-4dd0-bc15-ab6758c36f9d"">
",believe issue python version python version official currently python,issue
1945388951,"I am encountering a similar error with 2.13

Anyone knows if 2.15 solves the error? Do you need 

`echo -n ""build --config=monolithic"" >> .bazelrc`

 command for 2.15?",similar error anyone error need echo build command,issue
1945078703,"> > > @cantonios I can see that some internal checks failed, but can't see what they are.
> > 
> > 
> > ```
> > ERROR: tensorflow/core/kernels/mkl/BUILD:104:22: in cc_library rule //tensorflow/core/kernels/mkl:mkl_sparse_matrix_matmul_op: Visibility error:
> > target '//tensorflow/core/kernels/sparse:mat_mul_op.h' is not visible from
> > target '//tensorflow/core/kernels/mkl:mkl_sparse_matrix_matmul_op'
> > ```
> 
> Is it possible to get a commandline reproducer for this error? Can't reproduce locally, so fixing it might be trial-and-error that spams this PR with commits and spams you with emails!

I'm not sure... I'm surprised bazel doesn't find the dependency issue.  Maybe Google's internal tooling is more strict?",see internal ca see error rule visibility error target visible target possible get reproducer error ca reproduce locally fixing might sure find dependency issue maybe internal tooling strict,issue
1944983646,"> > @cantonios I can see that some internal checks failed, but can't see what they are.
> 
> ```
> ERROR: tensorflow/core/kernels/mkl/BUILD:104:22: in cc_library rule //tensorflow/core/kernels/mkl:mkl_sparse_matrix_matmul_op: Visibility error:
> target '//tensorflow/core/kernels/sparse:mat_mul_op.h' is not visible from
> target '//tensorflow/core/kernels/mkl:mkl_sparse_matrix_matmul_op'
> ```

Is it possible to get a commandline reproducer for this error? Can't reproduce locally, so fixing it might be trial-and-error that spams this PR with commits and spams you with emails!",see internal ca see error rule visibility error target visible target possible get reproducer error ca reproduce locally fixing might,issue
1944865465,"> @cantonios I can see that some internal checks failed, but can't see what they are.

```
ERROR: tensorflow/core/kernels/mkl/BUILD:104:22: in cc_library rule //tensorflow/core/kernels/mkl:mkl_sparse_matrix_matmul_op: Visibility error:
target '//tensorflow/core/kernels/sparse:mat_mul_op.h' is not visible from
target '//tensorflow/core/kernels/mkl:mkl_sparse_matrix_matmul_op'
```",see internal ca see error rule visibility error target visible target,issue
1944858230,"@cantonios I can see that some internal checks failed, but can't see what they are.",see internal ca see,issue
1944678679,"Confirmed on Debian 12.

> Tensorflow 2.15
Debian 12
Python 3.11
Bazel 4.2.3
GCC 12.2
Cuda 11.8
CuDNN 8.5

It looks like a similar [issue](https://github.com/tensorflow/tensorflow/pull/21499) was fixed six years ago, so maybe this is a regression.",confirmed python like similar issue fixed six ago maybe regression,issue
1944542258,"@cantonios Made that change, and I think the current ""Presubmit"" test failure is unrelated to this PR.",made change think current presubmit test failure unrelated,issue
1944482939,"> Shouldn't you be using tensorboard 2.16.1?

My bad, I was reviewing the old commit. It is actually using 2.16.1",bad old commit actually,issue
1944449943,"I tried updating the tflite versions to see if that helped, it did not, I was able to reproduce with the uploaded zip file:

```
    implementation 'org.tensorflow:tensorflow-lite:+'
    implementation 'org.tensorflow:tensorflow-lite-support:+'
```

Hi, @impjdi, can you please take a look? Thanks.",tried see able reproduce zip file implementation implementation hi please take look thanks,issue
1944155671,"If it results from simply importing TF, the good fix is to identify the code that causes the warning to occur and fix that. The code in `module_wrapper.py` is there to prevent such cases, supposedly.",simply good fix identify code warning occur fix code prevent supposedly,issue
1944030892,"**Hi all,

I am working on HPC, and when I run my deep learning code using Keras and TensorFlow, I encounter this error. Could you please advise on how to fix it?**

2024-02-14 14:33:22.383781: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-02-14 14:33:22.383867: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-02-14 14:33:22.487897: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-02-14 14:33:22.680120: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-14 14:33:24.543953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT

thanks,
Noor",hi working run deep learning code encounter error could please advise fix unable register factory register factory one already registered unable register factory register factory one already registered unable register factory register factory one already registered binary use available enable following rebuild appropriate compiler warning could find thanks,issue
1943951829,"Hi all.
@Venkat6871 and @mihaimaruseac , yes, it's just a warning, but still annoying. It's not rooted in the user's code, and therefore, in addition to being annoying, it might be misleading and result in a waste of time. It needs to be fixed!
I'm getting this warning by just simply importing tensorflow and nothing else!

@MaoMakara Thanks for the quick fix. It worked at first, but as I developed my project, at some point, somehow, it resulted in an error in my code. Changing it back solved the problem, so be aware that the change you are suggesting might raise an error.
A safer solution looks like this:

# Quick fix
The warning is printed by [tensorflow/python/util/module_wrapper.py, Line 149-151](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/util/module_wrapper.py#L149-L151). Simply comment these lines and you are free of its warning's pain.",hi yes warning still annoying rooted user code therefore addition annoying might misleading result waste time need fixed getting warning simply nothing else thanks quick fix worked first project point somehow error code back problem aware change suggesting might raise error solution like quick fix warning printed line simply comment free warning pain,issue
1943824273,"Hello @Venkat6871 , @SuryanarayanaY . Is it possible to get some tensorflower assigned to help with this issue?

I've managed to resolve this issue by using `tf.image.extract_patches` as slicing mechanism for 1D or 2D arrays, but honestly this is not an universal solution and I can imagine getting into trouble in future where there is no such native high-level abstract op.

I know that it's probably my skill issue in writing tf.function code for usage in tf.data pipelines, but I couldn't really find any general solution",hello possible get assigned help issue resolve issue slicing mechanism honestly universal solution imagine getting trouble future native abstract know probably skill issue writing code usage could really find general solution,issue
1943736402,"thanks @msf-caesar I forgot about having this issue open. I have somehow resolved the issue after I remade my environment. I also run into some issue with profiler logs being in wrong directory and unavailable in Tensorboard UI, but I've also found solution in github issues",thanks forgot issue open somehow resolved issue remade environment also run issue profiler wrong directory unavailable also found solution,issue
1943445882,"@impjdi I tried again compiling the benchmark tool with GPU delegate.
In CMake this fails like this #62941 

The bazel build finishes, but at runtime I get this error on Mac (on ubuntu it says basically the same just without GPUDelegate instead of MetalDelegate)

```
INFO: Created TensorFlow Lite delegate for Metal.
INFO: GPU delegate created.
ERROR: Following operations are not supported by GPU delegate:
DEQUANTIZE:
GATHER: Only support 1D indices

507 operations will run on the GPU, and the remaining 81 operations will run on the CPU.
ERROR: TfLiteMetalDelegate Prepare: newLibraryWithSource: program_source:23:119: error: expected expression
  {half4 second_value = src_tensor_1_buffer[((((0) * U.cmp_int4_1.x + 0) * U.cmp_int4_1.y + (0)) * U.cmp_int4_0.w + (()))];
                                                                                                                      ^

ERROR: Node number 588 (TfLiteMetalDelegate) failed to prepare.
ERROR: Restored original execution plan after delegate application failure.
ERROR: Failed to apply GPU delegate.
ERROR: Benchmarking failed.
```

And I wanted to ask if there is any chance to support gather with more than one dimension.",tried tool delegate like build get error mac basically without instead lite delegate metal delegate error following delegate gather support index run run error prepare error expression half error node number prepare error original execution plan delegate application failure error apply delegate error ask chance support gather one dimension,issue
1943381332,"Hello, are there some plans to fix this issue? This is very useful to train models much faster.",hello fix issue useful train much faster,issue
1943294566,"@edgimar It is raising this [error](https://colab.research.google.com/gist/sushreebarsa/3f787d1c833de9a1732b281564601535/12019.ipynb). Please have a look at the [migration](https://www.tensorflow.org/guide/migrate) document and try to upgrade to the latest stable version. 
If you still face the issue then Kindly create a new ticket as you are using an older version of TF which is not actively supported. Thank you!",raising error please look migration document try upgrade latest stable version still face issue kindly create new ticket older version actively thank,issue
1943233367,"@sushreebarsa
Hi!
First, I'm using tf 2.17 as I mentioned.

I think this issue is related to [intel code](https://github.com/tensorflow/tensorflow/blob/8340d650e1c9d396b49900c8e374da87a3d24cea/tensorflow/core/kernels/mkl/mkl_conv_ops.h#L643-L645) which lacks check of strides size. In my opinion the invocation in gist seems to execute [here](https://github.com/tensorflow/tensorflow/blob/8340d650e1c9d396b49900c8e374da87a3d24cea/tensorflow/core/kernels/conv_grad_input_ops.h#L294-L296) instead of MKL kernel, because of the environment of gist is not intel.
I was able to get the same output when I ran the script with `TF_ENABLE_ONEDNN_OPTS=0`.
So, would you check the reproducibility in intel environment?",hi first think issue related code check size opinion invocation gist execute instead kernel environment gist able get output ran script would check reproducibility environment,issue
1943182080,"@Sehun0819 Could you please let us know which TF version you are using ? I tried to replicate the issue reported [here](https://colab.research.google.com/gist/sushreebarsa/53bc365aabf476ce3a6bdea5ec62e32d/62950.ipynb) but I faced `InvalidArgumentError` . Kindly check the gist and let us know?
Thank you!",could please let u know version tried replicate issue faced kindly check gist let u know thank,issue
1943032075,"I confronted the same problem. the cuda and cudnn in devel-gpu image is 11.2 and 8.1. 
Accordding to https://www.tensorflow.org/install/source#tested_build_configurations:
```
tensorflow-2.15.0	3.9-3.11	Clang 16.0.0	Bazel 6.1.0	8.9	12.2
tensorflow-2.14.0	3.9-3.11	Clang 16.0.0	Bazel 6.1.0	8.7	11.8
tensorflow-2.13.0	3.8-3.11	Clang 16.0.0	Bazel 5.3.0	8.6	11.8
tensorflow-2.12.0	3.8-3.11	GCC 9.3.1	Bazel 5.3.0	8.6	11.8
tensorflow-2.11.0	3.7-3.10	GCC 9.3.1	Bazel 5.3.0	8.1	11.2
```

it seems this image is for building tensorflow-2.11.0. But when I search https://hub.docker.com/r/tensorflow/tensorflow/tags/?page=1&name=devel-gpu&ordering=last_updated. the latest update is one year ago. ",problem image clang clang clang image building search latest update one year ago,issue
1942555203,"Hi @CaptainDario,

I was able to replicate with your exact steps and cmake. I also tried with bazel and that seemed to work fine:
```
bazel build tensorflow/lite/tools/benchmark:benchmark_model
```

Hi @terryheo, can you please take a look for the cmake flow? Thanks.",hi able replicate exact also tried work fine build hi please take look flow thanks,issue
1942545906,I think I've made all of the appropriate changes; @cantonios @penpornk does this look OK to you?,think made appropriate look,issue
1942479983,"> Native Windows CUDA was cool but it's still not much of a hassle to just use WSL2 anyway

The problem with that is that it requires the user's Windows PC to have WSL2 installed. 

It is a very poor developer indeed who creates a Windows application that cannot run under Windows.",native cool still much hassle use anyway problem user poor developer indeed application run,issue
1942336460,"Hi,

Providing additional outputs does not result in matching outputs. This behavior is expected.",hi providing additional result matching behavior,issue
1942252456,"Hi @JoshPPrieto, I wouldn't call it expected, we always appreciate it when the users share any research they have already done as it usually does help us. Feel free to make a PR that helps resolve the issue... if it affects other builds we'll have to analyze whether the effect is a regression or not but it'll be easier for us to examine when the PR is created.

@terryheo, can you please take a look? Thanks.",hi would call always appreciate share research already done usually help u feel free make resolve issue analyze whether effect regression easier u examine please take look thanks,issue
1942148174,"@suyash-narain https://www.tensorflow.org/lite/performance/gpu#troubleshooting_gpu_support seems to state that it should just fallback to running parts of the model on CPU (despite it being slower). So this should be a bug... do you mind uploading your project so that we may reproduce? (Often times people change more than they think, that affects our current understanding of the issue.)",state fallback running model despite bug mind project may reproduce often time people change think current understanding issue,issue
1942026437,"@angerson @gbaned No tests are found inside `tensorflow/tools/ci_build` but BUILD was successful.

<details>
tensorflow/tools/ci_build:*
WORKSPACE: /workspaces/tensorflow
CI_DOCKER_BUILD_EXTRA_PARAMS: 
CI_DOCKER_EXTRA_PARAMS: 
COMMAND: bazel test //tensorflow/tools/ci_build:*
CI_COMMAND_PREFIX: ./tensorflow/tools/ci_build/builds/with_the_same_user ./tensorflow/tools/ci_build/builds/configured cpu
CONTAINER_TYPE: cpu
BUILD_TAG: tf_ci
 (docker container name will be tf_ci.cpu)

Building container (tf_ci.cpu)...
[+] Building 1.0s (18/18) FINISHED                                       docker:default
 => [internal] load .dockerignore                                             0.1s
 => => transferring context: 2B                                              0.0s
 => [internal] load build definition from Dockerfile.cpu                                 0.1s
 => => transferring dockerfile: 702B                                           0.0s
 => [internal] load metadata for docker.io/library/ubuntu:16.04                              0.5s
 => [ 1/13] FROM docker.io/library/ubuntu:16.04@sha256:1f1a2d56de1d604801a9671f301190704c25d604a416f59e03c04f5c6ffee0d6  0.0s
 => [internal] load build context                                             0.1s
 => => transferring context: 1.73kB                                            0.0s
 => CACHED [ 2/13] COPY install/*.sh /install/                                      0.0s
 => CACHED [ 3/13] RUN /install/install_bootstrap_deb_packages.sh                             0.0s
 => CACHED [ 4/13] RUN add-apt-repository -y ppa:openjdk-r/ppa &&   add-apt-repository -y ppa:george-edison55/cmake-3. 0.0s
 => CACHED [ 5/13] RUN /install/install_deb_packages.sh                                  0.0s
 => CACHED [ 6/13] RUN /install/build_and_install_python.sh 3.9.18                            0.0s
 => CACHED [ 7/13] RUN /install/install_pip_packages.sh                                  0.0s
 => CACHED [ 8/13] RUN /install/install_bazel.sh                                     0.0s
 => CACHED [ 9/13] RUN /install/install_proto3.sh                                     0.0s
 => CACHED [10/13] RUN /install/install_buildifier.sh                                   0.0s
 => CACHED [11/13] RUN /install/install_auditwheel.sh                                   0.0s
 => CACHED [12/13] RUN /install/install_golang.sh                                     0.0s
 => CACHED [13/13] COPY install/.bazelrc /etc/bazel.bazelrc                                0.0s
 => exporting to image                                                  0.0s
 => => exporting layers                                                  0.0s
 => => writing image sha256:7c5a8e70f3dbc138e6dc4b80287af162145e156a8606179861da4692a1063196               0.0s
 => => naming to docker.io/library/tf_ci.cpu                                       0.0s
Running 'bazel test //tensorflow/tools/ci_build:*' inside tf_ci.cpu...
Reading package lists...
Building dependency tree...
Reading state information...
sudo is already the newest version (1.8.16-0ubuntu1.10).
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
Adding group `codespace' (GID 1000) ...
Done.
/workspace /workspace
You have bazel 6.5.0 installed.
Found possible Python library paths:
 /usr/lib/python2.7/dist-packages
 /usr/local/lib/python2.7/dist-packages
Please input the desired Python library path to use. Default is [/usr/lib/python2.7/dist-packages]
Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow.

Do you want to use Clang to build TensorFlow? [Y/n]: Clang will be used to compile TensorFlow.

Please specify the path to clang executable. [Default is /usr/bin/clang]: 

You have Clang 3.8.0-2ubuntu4 installed.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
    --config=mkl      # Build with MKL support.
    --config=mkl_aarch64  # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
    --config=monolithic   # Config for mostly static monolithic build.
    --config=numa      # Build with NUMA support.
    --config=dynamic_kernels    # (Experimental) Build kernels into separate shared objects.
    --config=v1       # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
    --config=nogcp     # Disable GCP support.
    --config=nonccl     # Disable NVIDIA NCCL support.
/workspace
TF_BUILD_INFO = {container_type: ""cpu"", command: ""bazel test //tensorflow/tools/ci_build:*"", source_HEAD: ""258780cb34bfa9fc03cffe2371eeebdb974d8df4"", source_remote_origin: ""https://github.com/giuliocn/tensorflow"", OS: ""Linux"", kernel: ""6.2.0-1019-azure"", architecture: ""x86_64"", processor: ""AMD EPYC 7763 64-Core Processor"", processor_count: ""2"", memory_total: ""8120292 kB"", swap_total: ""0 kB"", Bazel_version: ""Build label: 6.5.0"", Java_version: ""1.8.0_292"", Python_version: ""2.7.12"", gpp_version: ""g++ (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609"", swig_version: """", NVIDIA_driver_version: """", CUDA_device_count: ""0"", CUDA_device_names: """", CUDA_toolkit_version: """"}
INFO: Options provided by the client:
 Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'test' from /etc/bazel.bazelrc:
 Inherited 'common' options: --color=yes
INFO: Reading rc options for 'test' from /workspace/.bazelrc:
 Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'test' from /etc/bazel.bazelrc:
 Inherited 'build' options: --verbose_failures --spawn_strategy=standalone --strategy=Genrule=standalone
INFO: Reading rc options for 'test' from /workspace/.bazelrc:
 Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'test' from /workspace/.tf_configure.bazelrc:
 Inherited 'build' options: --host_force_python=PY2 --action_env PYTHON_BIN_PATH=/usr/bin/python --action_env PYTHON_LIB_PATH=/usr/lib/python2.7/dist-packages --python_path=/usr/bin/python --action_env CLANG_COMPILER_PATH=/usr/lib/llvm-3.8/bin/clang --repo_env=CC=/usr/lib/llvm-3.8/bin/clang --repo_env=BAZEL_COMPILER=/usr/lib/llvm-3.8/bin/clang
INFO: Reading rc options for 'test' from /etc/bazel.bazelrc:
 'test' options: --spawn_strategy=standalone --verbose_failures --test_output=errors --test_verbose_timeout_warnings
INFO: Reading rc options for 'test' from /workspace/.tf_configure.bazelrc:
 'test' options: --test_size_filters=small,medium
INFO: Found applicable config definition build:short_logs in file /workspace/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /workspace/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition test:v2 in file /workspace/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-gpu,-v1only
INFO: Found applicable config definition build:linux in file /workspace/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /workspace/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
Loading: 
Loading: 
Loading: 
Loading: 0 packages loaded
Analyzing: 35 targets (1 packages loaded, 0 targets configured)
INFO: Analyzed 35 targets (1 packages loaded, 35 targets configured).
INFO: Found 35 targets and 0 test targets...
[0 / 1] [Prepa] BazelWorkspaceStatusAction stable-status.txt
INFO: Elapsed time: 4.206s, Critical Path: 0.09s
INFO: 1 process: 1 internal.
INFO: Build completed successfully, 1 total action
ERROR: No test targets were found, yet testing was requested
</details>",found inside build successful command test docker container name building container building finished docker default internal load transferring context internal load build definition transferring internal load sha internal load build context transferring context copy run run run run run run run run run run copy image writing image sha naming running test inside reading package building dependency tree reading state information already version newly remove group gid done found possible python library please input desired python library path use default wish build support support wish build support support want use clang build clang used compile please specify path clang executable default clang please specify optimization use compilation option default would like configure android android build use build command see build support build compute library arm architecture mostly static monolithic build build support experimental build separate build instead build disable default disable support disable support command test o kernel azure architecture processor processor build label provided client reading reading reading reading define define opt reading reading reading medium found applicable definition build file found applicable definition build file found applicable definition test file found applicable definition build file prefix prefix prefix found applicable definition build file loading loading loading loading loaded loaded loaded found test time critical path process internal build successfully total action error test found yet testing,issue
1941616699,"you can try this:
```python
config = tf.compat.v1.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 1
session = tf.compat.v1.InteractiveSession(config=config)
```

It forces tf to use all not used memory",try python session use used memory,issue
1941610704,"@sushreebarsa Thanks for your reply! I'm working on a TFLite backend, and unfortunately I don't get to pick which operators our clients use.

I hacked together a pass that does this, but I don't see a reason some version of this shouldn't be included upstream. I don't think this is best practice though: I believe the MLIR way of doing this is to define a canonicalization rule for strided slice?

I am willing to submit a PR but I'm not particularly experienced with the TF codebase: where do you have canonicalizations defined? Are you running an MLIR `CanonicalizerPass` anywhere?

```cpp
#include ""mlir/Pass/Pass.h""
#include ""mlir/Transforms/GreedyPatternRewriteDriver.h""
#include ""tensorflow/compiler/mlir/lite/ir/tfl_ops.h""

namespace mlir {

namespace {
// Replace TFL StridedSlice with TFL Slice wherever possible.
struct ReplaceStridedSlice
    : public PassWrapper<ReplaceStridedSlice, OperationPass<func::FuncOp>> {
  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(ReplaceStridedSlice)

  void getDependentDialects(DialectRegistry &registry) const final {
    registry.insert<TFL::TensorFlowLiteDialect>();
  }
  StringRef getArgument() const final { return ""xcore-replace-stridedslice""; }
  StringRef getDescription() const final {
    return ""Replace TFL StridedSlice with TFL Slice"";
  }
  void runOnOperation() override;
};

// Utility to check if a StridedSliceOp can be replaced with a SliceOp
bool canReplaceWithSlice(TFL::StridedSliceOp stridedSliceOp) {
  // Check input shape is static
  auto inputType = stridedSliceOp.getInput().getType().dyn_cast<ShapedType>();
  if (!inputType || !inputType.hasStaticShape()) {
    return false;
  }

  // Check all strides are 1
  DenseIntElementsAttr stridesAttr;
  matchPattern(stridedSliceOp.getStrides(), m_Constant(&stridesAttr));
  if (!stridesAttr)
    return false;
  for (auto stride : stridesAttr)
    if (!stride.isOne())
      return false;

  if (stridedSliceOp.getEllipsisMask() != 0 ||
      stridedSliceOp.getNewAxisMask() != 0) {
    return false;
  }
  return true;
}

struct ReplaceStridedSlicePattern
    : public OpRewritePattern<TFL::StridedSliceOp> {
  using OpRewritePattern<TFL::StridedSliceOp>::OpRewritePattern;

  LogicalResult matchAndRewrite(TFL::StridedSliceOp stridedSliceOp,
                                PatternRewriter &rewriter) const override {

    if (!canReplaceWithSlice(stridedSliceOp))
      return failure();

    auto inputType = stridedSliceOp.getInput().getType().dyn_cast<ShapedType>();
    auto rank =
        stridedSliceOp.getInput().getType().cast<ShapedType>().getRank();

    // Get begin/end attributes
    DenseIntElementsAttr beginAttr;
    matchPattern(stridedSliceOp.getBegin(), m_Constant(&beginAttr));
    if (!beginAttr)
      return failure();
    auto begin = beginAttr.getValues<int32_t>();

    DenseIntElementsAttr endAttr;
    matchPattern(stridedSliceOp.getEnd(), m_Constant(&endAttr));
    if (!beginAttr)
      return failure();
    auto end = endAttr.getValues<int32_t>();

    std::vector<int32_t> newBegin(rank), newSize(rank);

    // If mask is set, set begin and end to 0 and input shape
    // respectively
    // If mask is not set, set begin and end to the actual values
    // If the value is negative, it means size - value
    // StridedSliceOp has an end attribute, SliceOp has size
    // Size is end - begin.
    for (int i = 0; i < rank; i++) {
      if (stridedSliceOp.getBeginMask() & (1 << i))
        newBegin[i] = 0;
      else
        newBegin[i] =
            begin[i] < 0 ? inputType.getShape()[i] + begin[i] : begin[i];
      if (stridedSliceOp.getEndMask() & (1 << i))
        newSize[i] = inputType.getShape()[i] - newBegin[i];
      else {
        auto currentEnd =
            end[i] < 0 ? inputType.getShape()[i] + end[i] : end[i];
        newSize[i] = currentEnd - newBegin[i];
      }
    }
    int64_t shrinkMask = stridedSliceOp.getShrinkAxisMask();
    std::vector<int32_t> newOutputShape;
    for (int i = 0; i < rank; ++i) {
      if (!(shrinkMask & (1 << i))) {         // Check if we should NOT shrink
        newOutputShape.push_back(newSize[i]); // Retain size
      }
    }

    auto shapeAttrType =
        RankedTensorType::get({rank}, rewriter.getIntegerType(32));

    // create constant ops for begin and size
    auto beginConstantOp = rewriter.create<arith::ConstantOp>(
        stridedSliceOp.getLoc(), shapeAttrType,
        DenseIntElementsAttr::get(shapeAttrType, newBegin));
    auto sizeConstantOp = rewriter.create<arith::ConstantOp>(
        stridedSliceOp.getLoc(), shapeAttrType,
        DenseIntElementsAttr::get(shapeAttrType, newSize));

    // RankedTensorType needs int64_t
    std::vector<int64_t> newSize64(newSize.begin(), newSize.end());

    // create sliceOp
    auto sliceOp = rewriter.create<TFL::SliceOp>(
        stridedSliceOp.getLoc(),
        RankedTensorType::get(ArrayRef<int64_t>(newSize64),
                              stridedSliceOp.getType().getElementType()),
        stridedSliceOp.getInput(), beginConstantOp, sizeConstantOp);

    // add reshape if shrinkMask is not 0
    if (shrinkMask != 0) {
      auto newShapeAttrType =
          RankedTensorType::get({static_cast<int64_t>(newOutputShape.size())},
                                rewriter.getIntegerType(32));
      auto shapeConstantOp = rewriter.create<arith::ConstantOp>(
          stridedSliceOp.getLoc(), newShapeAttrType,
          DenseIntElementsAttr::get(newShapeAttrType, newOutputShape));
      std::vector<int64_t> newOutputShape64(newOutputShape.begin(),
                                            newOutputShape.end());
      auto newOutputType = RankedTensorType::get(
          newOutputShape64, sliceOp.getType().getElementType());
      auto reshape = rewriter.create<TFL::ReshapeOp>(
          stridedSliceOp.getLoc(), newOutputType, sliceOp, shapeConstantOp);
      rewriter.replaceOp(stridedSliceOp, reshape.getOutput());
    } else {
      rewriter.replaceOp(stridedSliceOp, sliceOp.getOutput());
    }
    return success();
  }
};

void ReplaceStridedSlice::runOnOperation() {
  auto *ctx = &getContext();
  func::FuncOp func = getOperation();
  RewritePatternSet patterns(ctx);
  patterns.insert<ReplaceStridedSlicePattern>(ctx);
  (void)applyPatternsAndFoldGreedily(func, std::move(patterns));
}
} // namespace

// Creates an instance of the ReplaceStridedSlice pass.
std::unique_ptr<OperationPass<func::FuncOp>> createReplaceStridedSlicePass() {
  return std::make_unique<ReplaceStridedSlice>();
}

static PassRegistration<ReplaceStridedSlice> pass;

} // namespace mlir

```",thanks reply working unfortunately get pick use hacked together pas see reason version included upstream think best practice though believe way define rule slice willing submit particularly experienced defined running anywhere include include include replace slice wherever possible public void registry final final return final return replace slice void override utility check bool check input shape static auto return false check return false auto stride return false return false return true public rewriter override return failure auto auto rank get return failure auto begin return failure auto end rank rank mask set set begin end input shape respectively mask set set begin end actual value negative size value end attribute size size end begin rank else begin begin begin else auto end end end rank check shrink retain size auto rank create constant begin size auto auto need create auto add reshape auto auto auto auto reshape else return success void auto void instance pas return static pas,issue
1941530770,@impjdi That are actually really interesting news! Do you have some pointers for me to get Metal on Mac working? Until now I always failed to get OpenCL working on Ubuntu 22.04 maybe you have some pointers there too?,actually really interesting news get metal mac working always get working maybe,issue
1941448712,"Hi, was someone able to resolve @jmtc7 's question? I too am trying to run some python code(string operations) inside the graph mode tf operation.",hi someone able resolve question trying run python code string inside graph mode operation,issue
1941363942,"Unfortunately this didn't help, the outcome is the same:

* [build log](https://github.com/tensorflow/tensorflow/files/14261254/spack-build-out.txt)
* [build env](https://github.com/tensorflow/tensorflow/files/14261256/spack-build-env-mods.txt)

Based on the build log, it looks like NCCL is correctly detected (both before and after adding these new env vars).",unfortunately help outcome build log build based build log like correctly new,issue
1941282328,"@swapnilyadavmpsedc,
There are multiple options for the similar taks. One of them is extracting features from images using pre-trained models like **VGG16** or **Inceptionv3** and then comparing those features with metrics like cosine similarity or Euclidean distance.

https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/VGG16
https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/InceptionV3

Also you can try training a **Siamese network** with image pairs and labels (similar/dissimilar). This approach learns complex similarity relationships. A Siamese Network is a type of network architecture that contains two or more identical subnetworks used to generate feature vectors for each input and compare them.

https://keras.io/examples/vision/siamese_network/

Reference: https://www.tensorflow.org/hub/tutorials/tf_hub_delf_module

Thank you!",multiple similar one like metric like cosine similarity distance also try training network image approach complex similarity network type network architecture two identical used generate feature input compare reference thank,issue
1940871181,"@amlinux A reference could be [golang-tensorflow](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/README.md)  that provides Go wrappers for TensorFlow C++.
If you still have a concern then please create a new ticket as you are using an older version of TF which is not actively supported.
Thank you!",reference could go still concern please create new ticket older version actively thank,issue
1940846649," @haraldurt Instead of relying on ""SAME"" padding, we could try specifying the padding values manually to control the output shape more precisely. If you still have a concern then please create a new ticket as you are using an older version of TF which is not actively supported.
Thank you!
",instead padding could try padding manually control output shape precisely still concern please create new ticket older version actively thank,issue
1940820253,"@shamazharikh Could you up-sample its 3D data to a higher resolution before applying regular max/avg pooling. This offers some control over the output size but adds computational overhead. Please refer to [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/lite.py)https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/lite.py) doc as well. 
If you still have a concern then please create a new ticket as you are using an older version of TF which is not actively supported.
Thank you!",could data higher resolution regular control output size computational overhead please refer doc well still concern please create new ticket older version actively thank,issue
1940603142,"We need to put the data on wsl space if we need to prototype models.

On Mon, Feb 12, 2024, 9:55 p.m. Frost Lord ***@***.***> wrote:

> Native Windows CUDA was cool but it's still not much of a hassle to just
> use WSL2 anyway
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorflow/tensorflow/issues/59918#issuecomment-1940423961>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AZRPJAE2HNINKYU5DLTRCR3YTLW45AVCNFSM6AAAAAAVSGMMRKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSNBQGQZDGOJWGE>
> .
> You are receiving this because you commented.Message ID:
> ***@***.***>
>
",need put data space need prototype mon frost lord wrote native cool still much hassle use anyway reply directly view id,issue
1940527592,@adamjstewart You might need to set environment variables like NCCL_HOME or NCCL_LIBRARY_DIR to point to the NCCL installation directory. Thank you!,might need set environment like point installation directory thank,issue
1940524701,"@basnetr I was able to replicate the issue reported [here](https://colab.research.google.com/gist/sushreebarsa/e8fc1806183c88fe31aa0e162f068f01/62927.ipynb).  This issue might be occurring due to Keras and TensorFlow incompatibility in versions between 2.12 and 2.15. While downgrading Keras can be a temporary fix, it's generally not recommended due to potential dependency conflicts and lack of bug fixes/security updates. Thank you!",able replicate issue issue might due incompatibility temporary fix generally due potential dependency lack bug thank,issue
1940520690,"@chmendoza Yes, there are a few other apis are also there which are having the same issue as you mentioned. We are expecting it to be fixed in the next release. 
Thank you! ",yes also issue fixed next release thank,issue
1940516406,"I solved this problem using Docker

Here is the Dockerfile

```
FROM tensorflow/tensorflow:2.13.0-gpu

RUN apt-get update && \
    apt-get install -y libusb-1.0-0 libusb-1.0-0-dev

RUN pip install tflite-model-maker
RUN pip install pycocotools
```",problem docker run update install dev run pip install run pip install,issue
1940515926,"@jyochichili Sometimes, downloads can fail due to temporary network hiccups. Please retry downloading the kissfft library a few times. If possible, kindly change the download directory to a location with sufficient space. 
Thank you!",sometimes fail due temporary network please retry library time possible kindly change directory location sufficient space thank,issue
1940463767,"Hi @sushreebarsa, 
Thank you for the response.
you are right that the kissfft lib is not properly downloaded or corrupted, but I doubt if network issue is the culprit here, as the other libs (tf_lite_micro_person_data_grayscale_2020_05_27, ruy, gemmlowp) has no issue in downloading. I have also noticed that the in the case of failure, the incorrect checksum is always same, which I assume won't be the case when the file is corrupted. Also switching to the latest version won't help in my case as my models are only compatible to v2.6.2.
Can you please help through this issue?

Thanks.

Please note the disk size of downloaded Kissftt lib(tensorflow-2.6.2/tensorflow/lite/micro/tools/make/downloads/kissfft) in the failure scenario is only 4K (original size 248K), so I can confirm that the file is not properly downloaded.",hi thank response right properly corrupted doubt network issue culprit issue also case failure incorrect always assume wo case file corrupted also switching latest version wo help case compatible please help issue thanks please note disk size failure scenario original size confirm file properly,issue
1940423961,Native Windows CUDA was cool but it's still not much of a hassle to just use WSL2 anyway,native cool still much hassle use anyway,issue
1939605675,"Thank you @RRiva, yes Intel releases tensorflow-intel for the Windows platform. In pip installation, TensorFlow-intel is automatically installed while installing TensorFlow on the Windows platform. We are working to fix it for Poetry Installation as well which should be available in the TF 2.16 release",thank yes platform pip installation automatically platform working fix poetry installation well available release,issue
1939498277,"i think i figured out the reason behind the error. the model is not compatible with the delegate and hence it fails to even create the same on android studio.
the thing i cannot understand is, if a model is CPU compatible but not compatible with the delegate, why can't the model be simply fallback on CPU instead of giving 'error invoking delegate' error?",think figured reason behind error model compatible delegate hence even create android studio thing understand model compatible compatible delegate ca model simply fallback instead giving delegate error,issue
1939329518,"@SuryanarayanaY Yes, but it'd be nice to have always success. Currently it's mostly error.",yes nice always success currently mostly error,issue
1939307054,"@sushreebarsa Incorporating inputs and outputs doesn't work (these are tensors that give errors: `TypeError: Cannot serialize object KerasTensor`), and I don't think it's the main issue, attributes such as `layers`, `input_layers` and `output_layers` are still missing in the inherited class that are present in the parent class. Also no difference using `.keras` vs. `.h5` in this case.

I think the issue might be related to keras and tensorflow incompatibility - at least for tensorflow 2.12, 2.13 and 2.15. 
A quick fix for the issue was to force install keras 2.11 (downgrading from 2.13.1) for tensorflow 2.13.1. This creates dependency conflicts but works in my case. 

Another fix for just the custom model was to inherit CustomModel from `tensorflow.python.keras.Model` instead of `tensorflow.keras.Model` but this will further cause other issues moving forward, example: `tensorflow.keras.optimizers` are not supported for training `tensorflow.python.keras.Model`.",work give serialize object think main issue still missing class present parent class also difference case think issue might related incompatibility least quick fix issue force install dependency work case another fix custom model inherit instead cause moving forward example training,issue
1939281997,"@eulercat That's a surprise; I was told that the libshaderc is not available and that's why we never launched out the Vk backend.  We will have to try it out.

@CaptainDario Yeah, the op set is going to be the same.  Believe it or not, even inside Google, there's been no new ops added to the op set (maybe only a handful of highly customized ops not related to general neural network); the op set you see is what we use.  We don't aim to support server-side models or implement all 3000 TF ops, but only performance critical production models.

Even TFLite GPU (OpenGL / Metal / OpenCL) work on desktop (Linux & Mac) when configured correctly.  It's that we don't support the use case officially as we don't have the capacity to do so.  The Vulkan backend (to be released) will be primarily for Android, but nothing should prevent you from doing additional tweaks and making it runnable on desktop (we've seen people do that).",surprise told available never try yeah set going believe even inside new added set maybe handful highly related general neural network set see use aim support implement performance critical production even metal work mac correctly support use case officially capacity primarily android nothing prevent additional making runnable seen people,issue
1939260060,"Right, I miss that part, sorry about it. Will remove approval",right miss part sorry remove approval,issue
1939132587,@damiles Sorry for the delay. Feel free to open a new PR using the code of this one as long as the commit history of this PR is preserved. It might be good to check the interest of the TensorFlow team in this feature beforehand though.,sorry delay feel free open new code one long commit history might good check interest team feature beforehand though,issue
1938908891,"Hi @gbaned, can you help to identify what is wrong with the CLA? Based on the report Irina's CLA is ok, and my is missing. 
![image](https://github.com/tensorflow/tensorflow/assets/56120470/29d99dde-f7a7-41ed-8e96-1bb49817faff)
What is strange, as I have the same corporate CLA, and it got verified fine e.g. on PR here   [https://github.com/tensorflow/tensorflow/pull/62933](https://github.com/tensorflow/tensorflow/pull/62933)  ",hi help identify wrong based report missing image strange corporate got fine,issue
1938770484,"Just to highlight here that it would be great to have this fix ready for the 2.16 cut, so that TF Java can continue to support Windows properly in its forthcoming  next release.",highlight would great fix ready cut continue support properly forthcoming next release,issue
1938739105,"I can work on a something yes, though I don't have myself a Windows machine and it may take me some time. Meanwhile, you can look at our CI/CD where our own build (TF Java) is failing: https://github.com/tensorflow/java/actions/runs/7777626622/job/21206146416#step:6:1655 (compiler command is [here](https://github.com/tensorflow/java/actions/runs/7777626622/job/21206146416#step:6:1593)).

All other platforms (linux, macos) succeed to link successfully to the C lib, including to TSL symbols.",work something yes though machine may take time meanwhile look build failing compiler command succeed link successfully,issue
1938560066,"@elfringham, thanks for your fix.

> @milpuz01 Do these features still need to be disabled for the best performance with ACL?


Yes, they are still needed. 

Unfortunately, we should have updated the tests initially when the changes were made in this PR: https://github.com/tensorflow/tensorflow/pull/60723, but I think back then the `remapper_test` was disabled and we didn't see it failing.

Probably the right fix for AArch64 would be to still execute the tests, but check the name of the operators and arguments matches the operators and arguments without the optimisation. In this way we will have testing of `remapper` phase too for occasions when we do not lower to oneDNN on AArch64 and if there are any changes of future we will be able to catch them. We will look into making those changes.",thanks fix still need disabled best performance yes still unfortunately initially made think back disabled see failing probably right fix would still execute check name without way testing phase lower future able catch look making,issue
1938430224,"It should be. We're setting:
```bash
export NCCL_HDR_PATH=<prefix>/include
export NCCL_INSTALL_PATH=<prefix>
export TF_NCCL_VERSION=2
```
Not sure how to debug whether or not NCCL is configured properly, but the PyTorch build succeeds with the same NCCL installation. Any other env vars we should set to help Bazel find NCCL?",setting bash export prefix export prefix export sure whether properly build installation set help find,issue
1938416212,"@jyochichili Please double-check that the downloaded KissFFT-v130-RELEASE-X86_64.zip file is complete and hasn't been corrupted during download. Some file-hosting services might introduce errors. Kindly ensure your internet connection is stable and that no interruptions occurred during the download. We recommend you to kindly upgrade to the latest TF version as you are using an older version which is not actively supported.
Thank you!",please file complete corrupted might introduce kindly ensure connection stable recommend kindly upgrade latest version older version actively thank,issue
1938410519,"@sushreebarsa Not sure how would that be possible, is there a documentation how to perform such a task? Currently my workaround is to split the Model `call()` code. I think a solution could be something similar like the `ignored_scope` at openvino. [reference](https://docs.openvino.ai/2022.3/basic_qauntization_flow.html#tune-quantization-parameters)",sure would possible documentation perform task currently split model call code think solution could something similar like reference,issue
1938409537,"@aaronsuydam The introduction of Windows Subsystem for Linux 2 (WSL 2) provided a more efficient and consistent platform for GPU acceleration on Windows, leveraging established Linux builds. This delivers full access to GPU-enabled builds and superior performance.
Thank you!",introduction subsystem provided efficient consistent platform acceleration established full access superior performance thank,issue
1938404241,"@adamjstewart Please ensure NCCL is correctly installed and accessible on your system. This error suggests the @local_config_nccl// package isn't loaded, which often happens if NCCL libraries are missing or not configured properly.
Thank you!",please ensure correctly accessible system error package loaded often missing properly thank,issue
1938387361,"@basnetr Could you try to modify your CustomModel class to incorporate the inputs and output attributes into the configuration returned by get_config(). It's recommended to use model.save('my_model.keras') for the native Keras format.
Thank you!",could try modify class incorporate output configuration returned use native format thank,issue
1938372057,"@AshebirGetuBelete  Could you please provide more context on the issue reported here? 
Thank you!",could please provide context issue thank,issue
1938366215,"@adamp87 One workaround could be to quantize the entire model, then fine-tune specific layers with FP32 weights and activation. This approach can be less efficient and might not perfectly address accuracy concerns. Thank you!",one could quantize entire model specific activation approach le efficient might perfectly address accuracy thank,issue
1938338270,"@zavataafnan,
We can't mention right now that for which tensorflow releases the upper(higher) Bazel version would be compatible. It will be mentioned in the same official document which was provided. Also for the smooth tensorflow usage, we strongly recommend the community to use the compatible versions rather than higher or lower versions.

Also please take a look at this official [document](https://github.com/bazelbuild/bazelisk?tab=readme-ov-file#how-does-bazelisk-know-which-bazel-version-to-run) for the reference which provides the information about the Bazelisk for which Bazel version to run. Thank you!",ca mention right upper higher version would compatible official document provided also smooth usage strongly recommend community use compatible rather higher lower also please take look official document reference information version run thank,issue
1938313527,"Hi @mraunak, thanks a lot! It is now possible to import TensorFlow on both Windows and Linux 🙂 The only difference is that I had to specify that `tensorflow-intel` is only available for Windows, since Poetry cannot infer it from the metadata. The pyproject.toml now looks like
```
python = "">=3.9,<3.12""
tensorflow-io-gcs-filesystem = ""0.31.0""
tensorflow = ""2.15.0""
tensorflow-intel = {version=""^2.15.0"", platform = ""win32""}
```
It should be noted though that `tensorflow-io-gcs-filesystem` 0.31.0  is not compatible with `tensorflow` 2.15.
",hi thanks lot possible import difference specify available since poetry infer like python platform win noted though compatible,issue
1938193409,"Hi @dsbyprateekg ,

The Google team is working on the tflite-model-maker issue and it will take  time to resolve, in the mean time please try using mediapipe-model-maker instead: here is an example [gist](https://colab.sandbox.google.com/gist/pkgoogle/93fb7581fab1ea14728c61adf584ca13/media_pipe_example.ipynb). Let us know if for some reason you can't use mediapipe model maker to accomplish your goals.

Thank You",hi team working issue take time resolve mean time please try instead example gist let u know reason ca use model maker accomplish thank,issue
1938132194,"@Santabot123,
I tried to execute the mentioned official document with the **tensorflow v2.15, keras v3.0 and the tensorflow-model-optimization - 0.8.0** and it was executed without any issue/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/0eaea793a5186842e8aa4d3b2bd6bfa3/pruning_with_keras.ipynb). Thank you!",tried execute official document executed without kindly find gist thank,issue
1938085953,@Venkat6871 Can you please tell me if Googl's team has resolved this issue since it has been from a long time in colab environment. ,please tell team resolved issue since long time environment,issue
1938024854,"Hi @mahdiaslanimk  It looks like your PR relates to the Keras component. Please submit it to the github.com/keras-team/keras repository instead. Thank you!
@mihaimaruseac, @fchollet, @qlzh727 ",hi like component please submit repository instead thank,issue
1937880428,"``` bash
cmake ../tensorflow/lite/c
cmake --build . -j
```

also works",bash build also work,issue
1937876243,"Building just the tf lite binary works
``` bash
cmake --build . -j
```",building lite binary work bash build,issue
1937831367,"@zhuochenKIDD Are you suggesting that I fix the TensorRT warning and the problem with *Non-Converted Ops* will get resolved automatically?

@LakshmiKalaKadali Hi, and thanks for the response.
I've converted the models to TFLM and executed them on a microcontroller. It was successful in giving output. So, if it gets converted successfully and runs on microcontrollers without any problem, should we just ignore the non-converted ops message? (Hope to see this message removed in the next fix, if it's supposed to be ignored)",suggesting fix warning problem get resolved automatically hi thanks response converted executed successful giving output converted successfully without problem ignore message hope see message removed next fix supposed,issue
1937426572,"> It works with version 2.14.1 on Python too. However, I'm encountering the same issue when trying to use it on Android. If anyone has successfully converted Whisper to tflite for Android, could you please share the versions of the libraries you used, or any modifications you made?

I needed to remove the app to change the assets of the app.",work version python however issue trying use android anyone successfully converted whisper android could please share used made remove change asset,issue
1937344582,"@impjdi Does the Vulkan backend have the same limitations as the OpenCL backend? By that I mean

* (Very) limited operator support
* Works only on Android / Linux

Or would the Vulkan backend work on all platforms (Windows/Mac/Linux/iOS/Android)?",mean limited operator support work android would work,issue
1937071826,"@mohantym somehow I did not notice your ping. I am still interested in this.
I do not fully understand how the mentioned solution is supposed to work.
Could you give me step-by-step instructions? One cannot just build using CMake?
Something like this is not possible, right?

```bash
cmake --build . -j -t select_ops
```",somehow notice ping still interested fully understand solution supposed work could give one build something like possible right bash build,issue
1936988409,"I solved this problem with: <br>
```! pip install tf-keras ```<br>
and replace ```from tensorflow import keras ``` with  ```import tf_keras as keras```",problem pip install replace import import,issue
1936983981,"Hello, 
I had a similar issue, and this is how I resolved it:
I defined the model's architecture using the same code,
Then I used the 
model.load_weights('model.h5')
You can find the full code here on my github: https://github.com/ImenMasmoudiEm/LeukemiaDetection/tree/main
I hope it helps!",hello similar issue resolved defined model architecture code used find full code hope,issue
1936898500,"> @ben-arnao Sorry for the late response! There are two ways to run predict() for the same batch of data on two different sets of weights (same model architecture):
> 
> Firstly, you could create multiple instances of the model's architecture and load different weights for each instance. Then, you can use the predict() method for each of these different instances of the original model for the same batch of data.
> 
> Secondly, you can use the set_weights() method to set the weights of the model to the desired set of weights. Then, you can use the predict() method as usual.
> 
> For any further queries on this please post this issue on TF [Forum](https://discuss.tensorflow.org/) where there is a larger community to get you the right help. Thank you!

Hi, thanks for the response. I am more asking if there is a way to parallelize the evaluation of different sets of weights. I have been trying to find something to fit my use case but it seems to be unique.",sorry late response two way run predict batch data two different model architecture firstly could create multiple model architecture load different instance use predict method different original model batch data secondly use method set model desired set use predict method usual please post issue forum community get right help thank hi thanks response way parallelize evaluation different trying find something fit use case unique,issue
1936837149,"Apologies if creating noise, but here is another one with a broken link: https://www.tensorflow.org/api_docs/python/tf/nn/softmax",noise another one broken link,issue
1936808930,"Hi, everyone! I have the same problem: 

```[cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version```

date Feb  9, 2024

OS: ```Linux Ubuntu 23.10```

Tensorflow: ```2.15.0``` 

I solved the problem using : ```sudo apt-get install libcudnn8=8.6.0.163-1+cuda11.8```

Thanks! I lost a lot of time to solve this issue.



",hi everyone problem status driver version insufficient version date o problem install thanks lost lot time solve issue,issue
1936793806,"Hi @RRiva, we are working to fix the issues of separate poetry installation of dependent packages",hi working fix separate poetry installation dependent,issue
1936771189,"Hi @RRiva, please run the command below to fix your issue. 
poetry add tensorflow-intel 
![image](https://github.com/tensorflow/tensorflow/assets/83710963/66f33259-e951-4ea3-89b3-7305f19d3c7b)
",hi please run command fix issue poetry add image,issue
1936728503,"Hi @mdfaijul, 

as @elfringham identified the failure comes in `Setup()` method in `MklMatMulPrimtive` class when you are trying to set `b_mem` in `context_` by reading weight description from `prim_desc` that still wasn't identified. We do it this way on AArch64 so that we can set weights as `any` as we need to reorder them in order to select the most optimal kernel from Arm Compute Library (more information on this approach are available in this PR: https://github.com/tensorflow/tensorflow/pull/61171). To fix the failure you would need to set `b_mem` later when `prime_desc` is actually created (between lines 1052 and 1059 in your PR). 

One way how to do it is with [this](https://github.com/tensorflow/tensorflow/files/14227019/58204_aarch64_fix.patch) patch that @elfringham tested and those failing tests pass now on AArch64. 

Hope this helps.

Best,
Milos.
",hi failure come setup method class trying set reading weight description still way set need reorder order select optimal kernel arm compute library information approach available fix failure would need set later actually one way patch tested failing pas hope best,issue
1936694346,"Hi, is there any update on this issue? Thanks in advance.",hi update issue thanks advance,issue
1936620482,Can you provide a short reproducible code snippet for this?,provide short reproducible code snippet,issue
1936231426,"> Could you please help make more changes? There are some more errors.

Sure. Sorry, didn't see those errors in my local tests, and I didn't see which target in the CI threw those errors.",could please help make sure sorry see local see target threw,issue
1936228497,"Thanks for fast response. I understand that the link leads to the correct Class and to correct page, but in the text it's highlighted like class instance with incorrect name (bold blue) which is confusing. The tf.data.Dataset is literally the class name without s at the end.",thanks fast response understand link correct class correct page text like class instance incorrect name bold blue literally class name without end,issue
1936003202,"const tf = ***@***.***/tfjs-node');

// Load MNIST dataset
const mnist = tf.data.mnist;
const { trainImages, trainLabels, testImages, testLabels } = mnist.getData();

// Normalize the input image so that each pixel value is between 0 and 1
const normalizedTrainImages = trainImages.div(255);
const normalizedTestImages = testImages.div(255);

// Define the model architecture
const model = tf.sequential();
model.add(tf.layers.inputLayer({ inputShape: [28, 28] }));
model.add(tf.layers.reshape({ targetShape: [28, 28, 1] }));
model.add(tf.layers.conv2d({ filters: 12, kernelSize: [3, 3], activation: 'relu' }));
model.add(tf.layers.maxPooling2d({ poolSize: [2, 2] }));
model.add(tf.layers.flatten());
model.add(tf.layers.dense({ units: 10 }));

// Compile the model
model.compile({
  optimizer: 'adam',
  loss: 'sparseCategoricalCrossentropy',
  metrics: ['accuracy'],
});

// Train the model
await model.fit(normalizedTrainImages, trainLabels, { epochs: 4, validationSplit: 0.1 });

// Evaluate baseline test accuracy
const baselineModelAccuracy = await model.evaluate(normalizedTestImages, testLabels);
console.log('Baseline test accuracy:', baselineModelAccuracy[1]);

// Define the pruning parameters
const pruningParams = {
  pruningSchedule: tfmot.sparsity.polynomialDecay({
    initialSparsity: 0.50,
    finalSparsity: 0.80,
    beginStep: 0,
    endStep: 100,
  }),
};

// Apply pruning to the model
const prunedModel = tfmot.sparsity.pruneLowMagnitude(model, pruningParams);

// Compile the pruned model
prunedModel.compile({
  optimizer: 'adam',
  loss: 'sparseCategoricalCrossentropy',
  metrics: ['accuracy'],
});

// Fine-tune the pruned model
await prunedModel.fit(normalizedTrainImages, trainLabels, { epochs: 2, validationSplit: 0.1 });

// Evaluate pruned model accuracy
const prunedModelAccuracy = await prunedModel.evaluate(normalizedTestImages, testLabels);
console.log('Pruned model test accuracy:', prunedModelAccuracy[1]);

// Save the pruned model
await prunedModel.save('file://path/to/your/model');
console.log('Pruned model saved.');

REGARDS,
KRISHNA MISHRA
JANAKPUR-2,NEPAL
E-MAIL:- ***@***.***
________________________________
From: Santabot123 ***@***.***>
Sent: Friday, February 9, 2024 6:59 PM
To: tensorflow/tensorflow ***@***.***>
Cc: Subscribed ***@***.***>
Subject: [tensorflow/tensorflow] An error in the official pruning guide (Issue #62929)


Issue type

Bug

Have you reproduced the bug with TensorFlow Nightly?

No

Source

source

TensorFlow version

2.15

Custom code

No

OS platform and distribution

Google Colab

Mobile device

No response

Python version

No response

Bazel version

No response

GCC/compiler version

No response

CUDA/cuDNN version

No response

GPU model and memory

No response

Current behavior?

I followed this official pruning guide: https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras


I opened code in google colab (https://colab.research.google.com/github/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/pruning/pruning_with_keras.ipynb) and ran all cells..


And there is an ValueError: `prune_low_magnitude` can only prune an object of the following types: keras.models.Sequential, keras functional model, keras.layers.Layer, list of keras.layers.Layer. You passed an object of type: Sequential. in line model_for_pruning = prune_low_magnitude(model, **pruning_params)


I tried this code on my local machine and it gives the same error.
This looks like an bug.

Standalone code to reproduce the issue

Code from offical guide: https://colab.research.google.com/github/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/pruning/pruning_with_keras.ipynb

Relevant log output

No response

—
Reply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/62929>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AZCZ53ZDUVNIQNPWNCCHJYLYSYOMRAVCNFSM6AAAAABDBODQVKVHI2DSMVQWIX3LMV43ASLTON2WKOZSGEZDOMJRHA3TANI>.
You are receiving this because you are subscribed to this thread.Message ID: ***@***.***>
",load normalize input image value define model architecture model activation compile model loss metric train model await evaluate test accuracy await test accuracy define pruning apply pruning model model compile model loss metric model await evaluate model accuracy await model test accuracy save model await model saved sent subject error official pruning guide issue issue type bug bug nightly source source version custom code o platform distribution mobile device response python version response version response version response version response model memory response current behavior official pruning guide code ran prune object following functional model list object type sequential line model tried code local machine error like bug code reproduce issue code guide relevant log output response reply directly view id,issue
1935994433,"`import tensorflow as tf
import tempfile
from tensorflow import keras

# Load MNIST dataset
mnist = keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Normalize the input image so that each pixel value is between 0 and 1
train_images = train_images / 255.0
test_images = test_images / 255.0

# Define the model architecture
model = keras.Sequential([
    keras.layers.InputLayer(input_shape=(28, 28)),
    keras.layers.Reshape(target_shape=(28, 28, 1)),
    keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),
    keras.layers.MaxPooling2D(pool_size=(2, 2)),
    keras.layers.Flatten(),
    keras.layers.Dense(10)
])

# Compile the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Train the model
model.fit(train_images, train_labels, epochs=4, validation_split=0.1)

# Evaluate baseline test accuracy and save the model for later usage
_, baseline_model_accuracy = model.evaluate(test_images, test_labels, verbose=0)
print('Baseline test accuracy:', baseline_model_accuracy)

# Define the pruning parameters
pruning_params = {
    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,
                                                              final_sparsity=0.80,
                                                              begin_step=0,
                                                              end_step=100)
}

# Apply pruning to the model
model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)

# Compile the pruned model
model_for_pruning.compile(optimizer='adam',
                          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                          metrics=['accuracy'])

# Fine-tune the pruned model
model_for_pruning.fit(train_images, train_labels, epochs=2, validation_split=0.1)

# Evaluate pruned model accuracy
_, pruned_model_accuracy = model_for_pruning.evaluate(test_images, test_labels, verbose=0)
print('Pruned model test accuracy:', pruned_model_accuracy)

# Save the pruned model
_, keras_file = tempfile.mkstemp('.h5')
tf.keras.models.save_model(model_for_pruning, keras_file, include_optimizer=False)
print(f""Model saved at {keras_file}"")
`",import import import load normalize input image value define model architecture model compile model train model evaluate test accuracy save model later usage print test accuracy define pruning apply pruning model model compile model model evaluate model accuracy print model test accuracy save model print model saved,issue
1935807503,"Hi there, on my Windows 10 machine I get the same error when trying to use the CPU version.
```
poetry add tensorflow==2.15.0.post1
```
yields

```
Because tensorflow (2.15.0.post1) depends on tensorflow-intel (2.15.0.post1) which doesn't match any versions, tensorflow is forbidden.
So, because tf depends on tensorflow (2.15.0.post1), version solving failed.
```",hi machine get error trying use version poetry add post post post match forbidden post version,issue
1935770830,"When I run

```
poetry add ""tensorflow==2.15.0.post1[and-cuda]""
```

I still get

```
Updating dependencies
Resolving dependencies... (0.6s)

Because tensorflow (2.15.0.post1) depends on tensorflow-intel (2.15.0.post1) which doesn't match any versions, tensorflow is forbidden.
So, because tf depends on tensorflow (2.15.0.post1), version solving failed.
```

I tried on `tf-nightly` as well, but there I already run into a different (more fundamental?) issue: https://github.com/tensorflow/tensorflow/issues/62928",run poetry add post still get post post match forbidden post version tried well already run different fundamental issue,issue
1935682864,"@tilakrayal,
thank you for your response. My Bazel version is 7.0.2. Unfortunately, downgrading Bazel to 6.1.0 is not straightforward in Mac. it has many dependencies like OpenJDK. 
when do you think it will be released with the upper Bazel version?
Can I also use a snapshot of tensorflow that maybe is okay with higher Bazel?

Best Regards,
Mostafa",thank response version unfortunately straightforward mac many like think upper version also use snapshot maybe higher best,issue
1935651702,"I'm just really stupid, I was looking at totally different TF version. Sorry to waste your time. Still old v1.13.1 backward convolution code with my modification is 20% faster than the current version in 2.15.
",really stupid looking totally different version sorry waste time still old backward convolution code modification faster current version,issue
1935625145,"Hi @mraunak, thanks for trying, but it doesn't work yet. To demonstrate it, I made a new clean repo, with the pyproject.toml that you have written. It's available at

https://gitlab.windenergy.dtu.dk/surrogate-models/test-poetry-and-tensorflow

As you can see, the last pipeline still fails with `ModuleNotFoundError: No module named 'tensorflow'`.

https://gitlab.windenergy.dtu.dk/surrogate-models/test-poetry-and-tensorflow/-/jobs/238494",hi thanks trying work yet demonstrate made new clean written available see last pipeline still module,issue
1935547270,Closing this as stale. Please reopen if this is still a valid request. Thank you!,stale please reopen still valid request thank,issue
1935544820,"@zavataafnan,
The image which you provided is related to the **I(information)** which might not be affecting the execution of the code. Also I suspect this issue might be a mismatch of the Bazel version you are trying with the tensorflow v2.15. 
https://www.tensorflow.org/install/source#cpu_2

Could you please try with the compatible Bazel 6.1.0 with the tensorflow v2.15. Thank you!",image provided related information might affecting execution code also suspect issue might mismatch version trying could please try compatible thank,issue
1935532482,Hi @akhilnev Can you please check @mihaimaruseac's [comments](https://github.com/tensorflow/tensorflow/pull/62570#issuecomment-1872377159) and keep us posted ? Thank you!,hi please check keep u posted thank,issue
1935527153,Hi @Zantares Can you please rebase your branch and resolve conflicts? Thank you!,hi please rebase branch resolve thank,issue
1935521645,"Hi @Tai78641 Can you please resolve conflicts? Thank you!
",hi tai please resolve thank,issue
1935448358,"@chmendoza You're right, there have been reports already of some TensorFlow API GitHub source code links not working correctly.  It's a known issue. Tensorflow has been released at 2.15.0.post1 ([see pypi](https://pypi.org/project/tensorflow/)), so the docs reflect like that. Seems like the repo is missing the appropriate tag for which an internal fix was raised. Thank you!",right already source code link working correctly known issue post see reflect like like missing appropriate tag internal fix raised thank,issue
1935408094,"@gpokat,
Thank you for the issue. The document states that **All datasets are exposed as tf.data.Datasets**  in this case, it seems to refer to multiple datasets simultaneously. Also when trying to access information about `tf.data.Datasets` leads to the documentation for a correct `tf.data.dataset` API, which might not be the issue.

Meanwhile I will try to check with developer team and provide more information. Thank you!",thank issue document exposed case refer multiple simultaneously also trying access information documentation correct might issue meanwhile try check developer team provide information thank,issue
1935237490,"Hi @pkgoogle @impjdi,

I get the same error when i use any whisper based tflite models. On digging a bit deeper I found out that the delegate is giving runtime errors because the model contains an op which has dynamic sized tensors whereas the delegate can support only static sized tensors. 
My question now is, why are these ops not falling back onto CPU instead and giving a runtime error on GPU?
Is there a way i can convert dynamic tensors to static while converting the model?


I use the below script to generate my whisper tflite model

```
import tensorflow as tf
import transformers


from datasets import load_dataset
from transformers import WhisperProcessor, WhisperFeatureExtractor, TFWhisperForConditionalGeneration, WhisperTokenizer

target = ""openai/whisper-tiny.en""

feature_extractor = WhisperFeatureExtractor.from_pretrained(target)
tokenizer = WhisperTokenizer.from_pretrained(target, predict_timestamps=True)
processor = WhisperProcessor(feature_extractor, tokenizer)
model = TFWhisperForConditionalGeneration.from_pretrained(target)
# Loading dataset
ds = load_dataset(""hf-internal-testing/librispeech_asr_dummy"", ""clean"", split=""validation"")

inputs = feature_extractor(
    ds[0][""audio""][""array""], sampling_rate=ds[0][""audio""][""sampling_rate""], return_tensors=""tf""
)
input_features = inputs.input_features

# Generating Transcription
generated_ids = model.generate(input_features=input_features)
print(generated_ids)
transcription = processor.tokenizer.decode(generated_ids[0])
print(transcription)

# Save the model
model.save('./content/tf_whisper_saved')

class GenerateModel(tf.Module):
  def __init__(self, model):
    super(GenerateModel, self).__init__()
    self.model = model

  @tf.function(
    input_signature=[
      tf.TensorSpec((1, 80, 3000), tf.float32, name=""input_features""),
    ],
  )
  def serving(self, input_features):
    outputs = self.model.generate(
      input_features,
      max_new_tokens=100,
      return_dict_in_generate=True,
    )
    return {""sequences"": outputs[""sequences""]}

saved_model_dir = './content/tf_whisper_saved'
tflite_model_path = 'whisper_tiny.tflite'

generate_model = GenerateModel(model=model)
tf.saved_model.save(generate_model, saved_model_dir, signatures={""serving_default"": generate_model.serving})

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,
tf.lite.OpsSet.SELECT_TF_OPS]  # enable TensorFlow Lite ops.
 # enable TensorFlow ops.
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# Float16 quantization reduces the size to 50%
converter.target_spec.supported_types = [tf.float16]
tflite_model = converter.convert()

# Save the model
with open(tflite_model_path, 'wb') as f:.
    f.write(tflite_model)
```

the generated model has an OP named 'WHILE' which is INT32, and is the second last op, having multiple inputs. How can i give it static inputs instead or ensure this op fallsback onto CPU instead of the delegate?

thanks",hi get error use whisper based digging bit found delegate giving model dynamic sized whereas delegate support static sized question falling back onto instead giving error way convert dynamic static converting model use script generate whisper model import import import import target target target processor model target loading clean validation audio array audio generating transcription print transcription print transcription save model class self model super self model serving self return convert model converter enable lite enable float quantization size save model open model second last multiple give static instead ensure onto instead delegate thanks,issue
1935138338,"We are building tensorflow and running auditwheel repair within the docker container tensorflow/build:2.16-python3.11. I have attached the full auditwheel repair verbose output. Thank you!
[auditwheel_repair_verbose.txt](https://github.com/tensorflow/tensorflow/files/14215860/auditwheel_repair_verbose.txt)
",building running repair within docker container attached full repair verbose output thank,issue
1935052936,"Hi @RRiva, the issue has been addressed in https://github.com/tensorflow/tensorflow/issues/58674#issuecomment-1593891706
I just successfully imported TF 2.15 on the Windows using the Poetry tool.
Before running the command poetry add tensorflow==2.15.0
please run the command poetry add tensorflow-io-gcs-filesystem==0.31.0
TF 2.15 supports Python 3.9-3.11, please add it in the pyproject.toml
![image](https://github.com/tensorflow/tensorflow/assets/83710963/a2f9e167-1817-401e-98b8-0dff6827a898)

![image](https://github.com/tensorflow/tensorflow/assets/83710963/ffaee371-bdb3-4bfc-8340-be81daa2dea9)

",hi issue successfully poetry tool running command poetry add please run command poetry add python please add image image,issue
1935048129,"> Might have found even better solution... Switching the order of contraction from output.contract(input) to input.contract(output) without any force eval and with post contraction shuffling, resulted in even faster speeds without possibly higher memory consumption: 2.2 seconds instead of the original 9 seconds or the 3 second with force eval code. Example of the return statement for col major layout (of course template arguments have to be modified accordingly):
> 
> ```
> input
> .extract_image_patches(
> kernelRows, kernelCols, row_stride, col_stride,
> row_in_stride, col_in_stride, 1, 1, padding_top,
> padding_bottom, padding_left, padding_right, OutScalar(0))
> .reshape(pre_contract_dims)
> .contract(
> output_backward.reshape(output_dims),
> contract_dims).shuffle(DSizes<TensorIndex, 2>{1,0})
> .reshape(kernel_dims)
> ```
> 
> This does work with 3D input/output tensors correctly, ~didn't test it with higher dimensional outputs, but I have feeling there has to be some additional shuffling to be done.~ Confirmed that the method behaves correctly with 4D Tensors, it is even up to 5x faster than the original with batch size of 16.

Feel free to submit a pull request with your changes.",might found even better solution switching order contraction input output without force post contraction shuffling even faster without possibly higher memory consumption instead original second force code example return statement col major layout course template accordingly input work correctly test higher dimensional feeling additional shuffling confirmed method correctly even faster original batch size feel free submit pull request,issue
1934862037,"I don't have a VM which matches this architecture with GPU closely enough,

Hi @impjdi, can you please take a look? Thanks.",architecture closely enough hi please take look thanks,issue
1934783719,"Unfortunately I'm not very familiar with the behavior here. The documentation for ConcatOffset seems to mention that it is used for gradient computations: https://www.tensorflow.org/mlir/tf_ops#tfconcatoffset_tfconcatoffsetop 

Tagging @ishark and @wangpengmit to see if they know more about the gradient computation / concat op. ",unfortunately familiar behavior documentation mention used gradient see know gradient computation,issue
1934516809,"@ChrisnaMishra 

Yes, you explain it correctly. The only one issues is a warning: `W tensorflow/core/lib/png/png_io.cc:88] PNG warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG`",yes explain correctly one warning warning profile profile gray color space permitted,issue
1934392368,@milpuz01 Could you please help fixing the failure?,could please help fixing failure,issue
1934371664,"import tensorflow as tf

# Assume you have a dataset of image files in ""/path/to/images""
image_dir = ""/path/to/images""

# Create a dataset from image files
train_ds = tf.keras.utils.image_dataset_from_directory(
    image_dir,
    validation_split=0.1,  # 10% for validation
    subset=""training"",
    image_size=(128, 128),
    batch_size=32,
    color_mode=""grayscale"",
    shuffle=True,
    seed=123,
)

# Define a preprocessing function
def preprocessing(image, label):
    # Normalize pixel values to [0, 1]
    image = tf.clip_by_value(tf.cast(image, dtype=tf.float32) / 255.0, 0.0, 1.0)
    return image, label

# Apply the preprocessing function to each image in the dataset
train_ds = train_ds.map(preprocessing, num_parallel_calls=tf.data.AUTOTUNE)

# Shuffle the dataset
train_ds = train_ds.shuffle(1024)

# Create batches
batch_size = 64
train_ds = train_ds.batch(batch_size, drop_remainder=True)

# Prefetch for efficient loading
train_ds = train_ds.prefetch(tf.data.AUTOTUNE)

# Now you can use this preprocessed dataset for training your model!
In this example:

We load image files from a directory using image_dataset_from_directory.
The preprocessing function normalizes pixel values.
We shuffle the dataset, create batches, and prefetch data for training.
Feel free to adapt this code to your specific use case! 😊





",import assume image create image validation training define function image label normalize image image return image label apply function image shuffle create efficient loading use training model example load image directory function shuffle create data training feel free adapt code specific use case,issue
1934366576,"Certainly! The code you've provided appears to be related to setting up a **TensorFlow image dataset** for training and validation. Let's break it down:

1. **`tf.keras.utils.image_dataset_from_directory`**:
   - This function creates a dataset from image files in a directory.
   - It takes the following parameters:
     - `""/code/dataset/train_dataset""`: The path to the directory containing your training images.
     - `validation_split=0.1`: Specifies that 10% of the data will be used for validation.
     - `subset=""both""`: Includes both training and validation subsets.
     - `image_size=(128, 128)`: Resizes the images to 128x128 pixels.
     - `interpolation=""bicubic""`: Interpolation method for resizing.
     - `batch_size=None`: The batch size (you can set this to a specific value).
     - `color_mode=""grayscale""`: Converts images to grayscale.
     - `shuffle=True`: Shuffles the dataset.
     - `seed=123`: Sets a random seed for reproducibility.

2. **`preprocessing` function**:
   - This function normalizes the pixel values of the images.
   - It clips the values between 0 and 1.
   - The function is applied to each image in the dataset using `map`.

3. **Data Processing Steps**:
   - The dataset is shuffled (`train_ds.shuffle(1024)`).
   - Batches are created (`train_ds.batch(config.batch_size, drop_remainder=True)`).
   - Data is prefetched for efficient loading (`train_ds.prefetch(tf.data.AUTOTUNE)`).

4. **Model Training**:
   - The `model.fit` function trains the model using the training dataset (`train_ds`).
   - It specifies the number of epochs (`config.epochs`) and uses the validation dataset (`val_ds`) for validation during training.

Remember to replace `config.batch_size` and `config.epochs` with actual values from your configuration. If you have any further questions or need additional assistance, feel free to ask! 😊",certainly code provided related setting image training validation let break function image directory following path directory training data used validation training validation interpolation method batch size set specific value random seed reproducibility function function clip function applied image map data data efficient loading model training function model training number validation validation training remember replace actual configuration need additional assistance feel free ask,issue
1934226610,"The provided code is a part of huge project. I cannot send more, but the error what you mention is:

> /usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py in list_directory_v2(path)
>     766   """"""
>     767   if not is_directory(path):
> --> 768     raise errors.NotFoundError(
>     769         node_def=None,
>     770         op=None,
> 
> NotFoundError: Could not find directory /code/dataset/train_dataset

This error is about path to folder with images contained in dataset. The images are combinations of RGBA and Grayscale (mode = L and mode = RGBA by Pillow). I cannot provide the whole dataset of images here. Please try to model this situation.

Thanks.",provided code part huge project send error mention path path raise could find directory error path folder mode mode pillow provide whole please try model situation thanks,issue
1934025186,"@markub3327,
I was facing a different issue while executing the above provided code. Could you please provide the complete code and the dependencies which helps e=debug the issue in an effective way. Kindly find the [gist](https://colab.research.google.com/gist/tilakrayal/d9e4362fd2620f3c99535d0aeb1f6c0d/untitled1726.ipynb). Thank you",facing different issue provided code could please provide complete code issue effective way kindly find gist thank,issue
1933820584,"Might have found even better solution...
Switching the order of contraction from output.contract(input) to input.contract(output) without any force eval and with post contraction shuffling, resulted in even faster speeds without possibly higher memory consumption: 2.2 seconds instead of the original 9 seconds or the 3 second with force eval code.
Example of the return statement for col major layout (of course template arguments have to be modified accordingly): 
``` 
input
.extract_image_patches(
kernelRows, kernelCols, row_stride, col_stride,
row_in_stride, col_in_stride, 1, 1, padding_top,
padding_bottom, padding_left, padding_right, OutScalar(0))
.reshape(pre_contract_dims)
.contract(
output_backward.reshape(output_dims),
contract_dims).shuffle(DSizes<TensorIndex, 2>{1,0})
.reshape(kernel_dims)
```
This does work with 3D input/output tensors correctly, ~~didn't test it with higher dimensional outputs, but I have feeling there has to be some additional shuffling to be done.~~
Confirmed that the method behaves correctly with 4D Tensors, it is even up to 5x faster than the original with batch size of 16.",might found even better solution switching order contraction input output without force post contraction shuffling even faster without possibly higher memory consumption instead original second force code example return statement col major layout course template accordingly input work correctly test higher dimensional feeling additional shuffling confirmed method correctly even faster original batch size,issue
1933747605,"@jpmartin2 If compatibility with tfcompile isn't crucial, we can explore using TensorFlow's SaveModel format for saving and loading models. It supports graphs without explicit inputs. TF 1.x is not actively supported so we recommend you to upgrade.
If you still have a concern then please create a new ticket.
Thank you!",compatibility crucial explore format saving loading without explicit actively recommend upgrade still concern please create new ticket thank,issue
1933733413,"@danijar Maybe we can Try-Except Block Around session.run() by Enclose session.run() calls within a try-except block to catch the KeyboardInterrupt exception.
TensorFlow 2.x uses a different approach for training, with eager execution and tf.function for defining computation graphs.  TF 1.x is not actively supported so we recommend you to upgrade.
If you still have a concern then please create a new ticket.

Thank you!",maybe block around enclose within block catch exception different approach training eager execution computation actively recommend upgrade still concern please create new ticket thank,issue
1933720838,"@MarvinTeichmann An lternative approaches for memory reduction would be as follows;

1. Gradient checkpointing: Saves intermediate activations during backpropagation, allowing reuse and reducing GPU memory footprint.
2. Mixed precision training: Uses lower precision formats like fp16 for calculations without significantly impacting accuracy. If you still have a concern then please create a new ticket.
Thank you!",memory reduction would gradient intermediate reuse reducing memory footprint mixed precision training lower precision like without significantly accuracy still concern please create new ticket thank,issue
1933716750,"@smrtslckr TensorFlow's tf.feature_column.categorical_column_with_hash_bucket provides a hashing mechanism for categorical features, but not specifically designed for embedding layers. But tf.feature_column is not recommended for new code. Instead, feature preprocessing can be done directly using either [Keras preprocessing layers](https://www.tensorflow.org/guide/migrate/migrating_feature_columns) or through the one-stop utility [tf.keras.utils.FeatureSpace](https://www.tensorflow.org/api_docs/python/tf/keras/utils/FeatureSpace) built on top of them. See the [migration guide](https://tensorflow.org/guide/migrate) for details.

If you still have a concern then please create a new ticket.

Thank you!",mechanism categorical specifically designed new code instead feature done directly either utility built top see migration guide still concern please create new ticket thank,issue
1933672192,"So, I've tried experimenting. Adding force eval past the patch extraction / reshape on the input tensor:
```
input
.extract_image_patches(
kernelRows, kernelCols, row_stride, col_stride,
row_in_stride, col_in_stride, 1, 1, padding_top,
padding_bottom, padding_left, padding_right, OutScalar(0))
.reshape(pre_contract_dims).eval()
```
makes this method 3x faster, from 9,155 seconds to 3.026 seconds on the previously aformentioned (64,800,800) input and backward output with (64,64,5,5) kernel, which is more in-line with expected performance. 
Please, look into this further.",tried force past patch extraction reshape input tensor input method faster previously input backward output kernel performance please look,issue
1933667089,"I have found a working solution. The issue can be closed, but here are my findings for future reference/others:

Profiling on WSL2 was added with CUDA 12, so everything before tf 2.15 does not work. 
For tf 2.15 the packaged CUDNN Version (from [and-cuda] does not work with profiling, but a custom install of the Toolkit (12.2) and CUDNN (8.9) finally works. Here it is important to correctly set the LD_LIBRARY_PATH and XLA_FLAGS environmental variable so that the tensorflow installation finds the correct CUDA libraries.

So if someone wants to investigate this further, I would start by comparing the libraries installed by [and-cuda] with the custom install and check if something is missing/different here.


",found working solution issue closed future added everything work version work custom install finally work important correctly set environmental variable installation correct someone investigate would start custom install check something,issue
